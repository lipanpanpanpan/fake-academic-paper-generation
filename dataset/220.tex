
\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
\usepackage{FG2019}

\FGfinalcopy % *** Uncomment this line for the final submission



\IEEEoverridecommandlockouts                              % This command is only
\overrideIEEEmargins

\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{epsfig} % for postscript graphics files
\usepackage{mathptmx} % assumes new font selection scheme installed
\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{hhline}
\usepackage{tabularx}
\usepackage{verbatim}
\usepackage{amsmath}


\usepackage{pgffor}
\usepackage{caption}


\DeclareMathOperator{\EX}{\mathbb{E}}% expected value


\def\FGPaperID{198} % *** Enter the FG 2019 Paper ID here

\title{\LARGE \bf
A Multi-Task Learning \& Generation Framework: Valence-Arousal, Action Units \&  Primary Expressions
}




\begin{document}


\ifFGfinal



\author{\parbox{16cm}{\centering
    {\large Dimitrios Kollias$^1$ and Stefanos Zafeiriou$^{1,2}$}\\ 
    {\normalsize
    $^1$  Department of Computing, Imperial College London, UK \\
    $^2$ Centre for Machine Vision and Signal Analysis, University of Oulu, Finland}
    }
}
\thispagestyle{empty}
\pagestyle{empty}
\fi
\maketitle




\begin{abstract} 
Over the past few years many research efforts have been devoted to the field of affect analysis. Various approaches have been proposed for: i) discrete emotion recognition in terms of the primary facial expressions; ii) emotion analysis in terms of facial Action Units (AUs), assuming a fixed expression intensity; iii) dimensional emotion analysis, in terms of valence and arousal (VA). These approaches can only be effective, if they are developed using large, appropriately annotated databases, showing behaviors of people in-the-wild, i.e., in uncontrolled environments. Aff-Wild  has been the first, large-scale, in-the-wild database (including around 1,200,000 frames of 300 videos), annotated in terms of VA. In the vast majority of existing emotion databases, their annotation is limited to either primary expressions, or valence-arousal, or action units. In this paper, we first annotate a part (around $234,000$ frames) of the Aff-Wild database in terms of $8$ AUs and another part (around $288,000$ frames) in terms of the $7$ basic emotion categories, so that parts of this database are annotated in terms of VA, as well as AUs, or primary expressions. Then, we set up and tackle multi-task learning for emotion recognition, as well as for facial image generation. Multi-task learning is performed using: i) a deep neural network with shared hidden
layers, which learns emotional attributes by exploiting their inter-dependencies; ii) a discriminator of a generative adversarial network (GAN). On the other hand, image generation is implemented through the generator of the GAN. For these two tasks, we carefully design loss functions that fit the examined set-up. Experiments are presented which illustrate the good performance of the proposed approach when applied to the new  annotated parts of the Aff-Wild database.

\end{abstract}

\begin{comment}
To do so, we carefully design i) deep neural networks (DNN) and generative adversarial networks (GAN) and ii) objective functions that help the DNNs reach state-of-the-art performance and the GANs   


Over the past years a lot of research has been done for affect analysis. In total, approaches has been made towards the: i) recognition of the 7 basic expressions, ii) binary occurrence of expressions as Action Units, assuming that expression intensity is fixed and iii) dimensional emotion analysis, mostly in terms of valence and arousal. Research in emotion theory requires very large annotated databases of in-the-wild images of facial expressions of emotion. That is why some databases have been developed for the above emotion recognition tasks. The Aff-Wild database was the first large-scale in-the-wild database (around 1,200,000 frames) that was created and annotated in terms of valence and arousal. However, the developed databases' annotation was limited to only one of the above described cases. To this end, in this paper, we first annotate a part (around $234,000$ frames) of the Aff-Wild database in terms of $8$ action units and another part (around $288,000$ frames) in terms of the $7$ basic expressions, so that a part of this database will be annotated for valence-arousal, action units and basic expressions.


Over the past years a lot of research has been done for affect analysis. At first, approaches has been made towards the recognition of the 7 basic expressions. Also, some Facial Expression Recognition and Analysis systems proposed in the literature focused on the binary occurrence of expressions as Action Units, assuming that expression intensity is fixed. More recently, affective computing researches are shifting towards dimensional emotion analysis, mostly in terms of valence and arousal. Research in emotion theory requires very large annotated databases of in-the-wild images of facial expressions of emotion. That is why some databases have been developed for the above emotion recognition tasks. The Aff-Wild database was the first large-scale in-the-wild database (around 1,200,000 frames) that was created and annotated in terms of valence and arousal. However, research lacks databases annotated in terms of 2 or all of the above


Nevertheless, due to the complexity of human behaviour, it soon became evident that universal expressions are not so commonly displayed in real-world settings. That is why action unit detection became the next step. In parallel, action unit intensity estimation was a step towards representing subtle, i.e., not only extreme, emotions appearing in everyday human computer interactions. It is, however, the dimensional emotion representation, in terms of valence and arousal, which is more appropriate for this task. 
Research in emotion theory requires very large annotated databases of in-the-wild images of facial expressions of emotion. That is why some databases have been developed for the above emotion recognition tasks. The Aff-Wild database was the first large-scale in-the-wild database (around 1,200,000 frames) that was created. However it was annotated in terms of valence and arousal. 
In this paper, we first annotate a part (around $234,000$ frames) of this database in terms of $8$ action units and another part (around $288,000$ frames) in terms of the $7$ basic expressions. To this end, a part of the Aff-Wild database will be annotated for valence arousal, action unit and the seven basic expressions, helping the scientific society into building models for joint emotion estimation or performing domain adaptation techniques. Next, we build state-of-the-art deep neural networks, including convolutional, recurrent and generative adversarial networks, for joint estimation (and generation) of dimensional, categorical and action unit emotion recognition.

\end{comment}


\section{INTRODUCTION}


Representing human emotions has been a basic topic of research in psychology. The most frequently used emotion representation 
is the categorical one, including the seven basic categories, i.e., Anger, Disgust, Fear, Happiness, Sadness, Surprise and Neutral \cite{dalgleish2000handbook}\cite{cowie2003describing}. Discrete emotion representation can also be described in terms of the 
Facial Action Coding System (FACS) model, in which all possible facial actions are described in terms of Action
Units (AUs) \cite{ekman1977facial}. Finally, the dimensional model of affect \cite{whissel1989dictionary}\cite{russell1978evidence} has been proposed as a means to distinguish between subtly different displays of affect and encode small changes in the intensity of each emotion on a continuous scale. The 2-D Valence and Arousal Space (VA-Space) is the most usual dimensional emotion representation. Figure \ref{2d-wheel-au} shows: i) on the left hand side, the 2-D Emotion Wheel \cite{plutchik1980emotion}, with valence ranging from very positive to very negative and arousal from very active to very passive; ii) on the right hand side, some of the most common AUs along with definitions of the represented actions. 

\begin{comment}
\begin{table}[h]
\caption{Most common AUs and their description}
\label{AU}
\centering
\begin{tabular}{|c|c|c|c|}
\hline
AU\# & Action & AU\# & Action  \\ 
\hline
1 & inner brow raiser & 2 & outer brow raiser \\
\hline
4 & brow lowerer & 6 & cheek raiser \\
\hline
12 & lip corner puller & 15 & Lip Corner Depressor \\
\hline 
 20 & lip stretcher & 25 & lips part \\
\hline
\end{tabular}
\end{table}

\begin{table}[h]
\caption{Most common AUs and their description}
\label{AU}
\centering
\begin{tabular}{|c|c|}
\hline
AU\# & Action   \\ 
\hline
1 & inner brow raiser \\
\hline
2 & outer brow raiser \\
\hline
4 & brow lowerer \\
\hline
6 & cheek raiser \\
\hline
12 & lip corner puller \\
\hline
15 & \begin{tabular}{@{}c@{}}Lip Corner \\ Depressor \end{tabular} \\
\hline 
 20 & lip stretcher \\
 \hline
25 & lips part \\
\hline
\end{tabular}
\end{table}
\end{comment}

\begin{figure}[h]
\centering
\adjincludegraphics[height=4.5cm,width=8.5cm]{au_va.png}
\caption{The 2D Emotion Wheel (left hand side); the most common AUs with their definitions (right hand side) }
\label{2d-wheel-au}
\vskip -0.5cm
\end{figure}

Automatic understanding of human affect using visual signals is a problem that has attracted significant interest over the past 20 years.
Current research in automatic analysis of facial affect aims at developing systems, such as robots and virtual humans, that will interact with humans in a naturalistic way under real-world settings. To this end, such systems should automatically sense and interpret facial signals relevant to emotions, appraisals and intentions. 

Basic research in face perception and emotion theory cannot be completed without large annotated databases of
images and video sequences of facial expressions and underlying emotions.
Some datasets that have been developed in the labs and are still used in many recent works include the Cohn-Kanade database \cite{tian2001recognizing}\cite{lucey2010extended}, MMI database  \cite{pantic2005web}\cite{valstar2010induced}, Multi-PIE database \cite{gross2010multi} and BU-3D/BU-4D ones \cite{yin20063d}\cite{yin2008high}. 

Previous studies have reported good results in the automatic analysis of facial expressions and related
emotions \cite{corneanu2016survey}. However, these results were obtained with
analysis of images and videos captured in laboratory environments. That is, even when the expressions were spontaneous,
the filming was done in controlled conditions, with full awareness of the participants.

Hence, efforts have been made in order to collect videos of subjects displaying behaviors in-the-wild. To this end, Aff-Wild was created \cite{zafeiriou2017aff}, constituting the first large-scale "in-the-wild" database, with over 60 hours of video data, annotated in terms of valence-arousal dimensions. However, even in this case,  annotation is limited only to a single emotion representation, i.e., the VA one. Generating databases which are annotated in terms of more than a single  emotion representation could assist in developing domain adaptation and image generation techniques, as well as multi-task learning.
Deep generative models have become widely popular for generative modeling of data. Generative adversarial networks (GANs) \cite{goodfellow2014generative}, in particular, have shown remarkable success in generating very realistic images in several cases \cite{radford2015unsupervised}\cite{berthelot2017began}. A typical GAN consists of the discriminator - which tries to tell apart real from fake examples by minimizing an appropriate loss function - and the generator - which tries to generate samples that maximize that loss \cite{tu2007learning}.

One of the primary motivations for studying deep generative models has been semi-supervised learning.
Indeed, several recent works have shown promising empirical results on semi-supervised learning with GANs. Most state-of-the-art semi-supervised learning methods based on GANs  \cite{salimans2016improved} use the GAN discriminator as a classifier which outputs $k+1$ probabilities ($k$ probabilities for the $k$ real classes and one for the fake class). The generator is mainly used as a source of additional data (fake samples) which the discriminator tries to classify under the $(k+1)$th label.



Multi-task learning (MTL) \cite{caruana1997multitask} is a machine learning paradigm for learning a number of supervised learning tasks simultaneously, exploiting commonalities between them. MTL proved to successfully boost the performance of an individual task with the inclusion of other correlated tasks in the training process \cite{ganin2014unsupervised}\cite{hinton2015distilling}.
MTL was first studied in \cite{caruana1997multitask}, where the authors proposed to jointly learn parallel tasks sharing a common representation, and transferring part of the knowledge learned to solve one task to improve the learning of the other related tasks. 
One of the main difficulties with multitask approaches using different databases is the fact that not all the samples are labeled for all the tasks.

In this work we make the following contributions: \\ %% se subsubsection
1) We annotate a part of the Aff-Wild database in terms of eight AUs and another part in terms of the seven basic expressions, exploiting its in-the-wild nature with:  i) great variability of behaviors, ii) wide range of emotions, iii) rapid emotional changes and iv) different head poses, illumination conditions and occlusions. All videos and emotion labels will be made publicly available upon publication of the current paper.\\
2) By adapting current state-of-the-art GAN architectures to semi-supervised settings and by using the above annotations, we generate: i) realistic and vivid images of the persons appearing in the newly annotated parts of Aff-Wild and ii) new images of either unseen people, or new expressions and features of people already appearing in them.\\
3) By designing and testing new appropriate loss functions for our data, we perform MTL experiments: i) using CNN-RNN networks with shared hidden
layers, that jointly learn emotional attributes by exploiting their inter-dependencies and ii) using the GAN described in (2) above, so that it can generate realistic images, whilst serving as a good classifier and regressor.





\begin{comment}
\begin{figure}[h]
\centering
\begin{tabular}{ c  }
\includegraphics[height=0.95cm]{au_only/0.jpg}  \includegraphics[height=0.95cm]{au_only/1.jpg} \includegraphics[height=0.95cm]{au_only/2.jpg} \includegraphics[height=0.95cm]{au_only/3.jpg}  \includegraphics[height=0.95cm]{au_only/4.jpg} \includegraphics[height=0.95cm]{au_only/5.jpg} \includegraphics[height=0.95cm]{au_only/6.jpg}  \includegraphics[height=0.95cm]{au_only/7.jpg} \\

\includegraphics[height=0.95cm]{au_only/8.jpg} \includegraphics[height=0.95cm]{au_only/9.jpg}  \includegraphics[height=0.95cm]{au_only/10.jpg} \includegraphics[height=0.95cm]{au_only/11.jpg} \includegraphics[height=0.95cm]{au_only/12.jpg} \includegraphics[height=0.95cm]{au_only/13.jpg} 
 \includegraphics[height=0.95cm]{au_only/15.jpg} \\

\includegraphics[height=0.95cm]{au_only/16.jpg} \includegraphics[height=0.95cm]{au_only/17.jpg} \includegraphics[height=0.95cm]{au_only/18.jpg} \includegraphics[height=0.95cm]{au_only/19.jpg} \includegraphics[height=0.95cm]{au_only/20.jpg}  \includegraphics[height=0.95cm]{au_only/22.jpg} \includegraphics[height=0.95cm]{au_only/23.jpg} \includegraphics[height=0.95cm]{au_only/24.jpg} \\

\includegraphics[height=0.95cm]{au_only/25.jpg} \includegraphics[height=0.95cm]{au_only/26.jpg} \includegraphics[height=0.95cm]{au_only/27.jpg}
\includegraphics[height=0.95cm]{au_only/28.jpg} \includegraphics[height=0.95cm]{au_only/29.jpg} \includegraphics[height=0.95cm]{au_only/30.jpg} \includegraphics[height=0.95cm]{au_only/31.jpg} \includegraphics[height=0.95cm]{au_only/32.jpg} \\
\includegraphics[height=0.95cm]{au_only/33.jpg} \includegraphics[height=0.95cm]{au_only/34.jpg} \includegraphics[height=0.95cm]{au_only/35.jpg} 
\includegraphics[height=0.95cm]{au_only/36.jpg} \includegraphics[height=0.95cm]{au_only/21.jpg} 
\end{tabular}
\caption{Genereated images from config. 2 GAN: au only}
\end{figure}

\begin{figure}[h]
\centering
\begin{tabular}{ c  }
\includegraphics[height=0.95cm]{va_only/0.jpg}  \includegraphics[height=0.95cm]{va_only/1.jpg} \includegraphics[height=0.95cm]{va_only/2.jpg} \includegraphics[height=0.95cm]{va_only/3.jpg}  \includegraphics[height=0.95cm]{va_only/4.jpg} \includegraphics[height=0.95cm]{va_only/5.jpg} \includegraphics[height=0.95cm]{va_only/6.jpg}  \includegraphics[height=0.95cm]{va_only/7.jpg} \\

\includegraphics[height=0.95cm]{va_only/8.jpg} \includegraphics[height=0.95cm]{va_only/9.jpg}  \includegraphics[height=0.95cm]{va_only/10.jpg} \includegraphics[height=0.95cm]{va_only/11.jpg} \includegraphics[height=0.95cm]{va_only/12.jpg} \includegraphics[height=0.95cm]{va_only/13.jpg} 
\includegraphics[height=0.95cm]{va_only/14.jpg} \includegraphics[height=0.95cm]{va_only/15.jpg} \\

\includegraphics[height=0.95cm]{va_only/16.jpg} \includegraphics[height=0.95cm]{va_only/17.jpg} \includegraphics[height=0.95cm]{va_only/18.jpg} \includegraphics[height=0.95cm]{va_only/19.jpg} \includegraphics[height=0.95cm]{va_only/20.jpg}  \includegraphics[height=0.95cm]{va_only/22.jpg} \includegraphics[height=0.95cm]{va_only/23.jpg} \includegraphics[height=0.95cm]{va_only/24.jpg} \\

 \includegraphics[height=0.95cm]{va_only/26.jpg} \includegraphics[height=0.95cm]{va_only/27.jpg}
 \includegraphics[height=0.95cm]{va_only/30.jpg} \includegraphics[height=0.95cm]{va_only/31.jpg} \includegraphics[height=0.95cm]{va_only/32.jpg} \\
\includegraphics[height=0.95cm]{va_only/33.jpg} \includegraphics[height=0.95cm]{va_only/34.jpg} \includegraphics[height=0.95cm]{va_only/35.jpg} 
\includegraphics[height=0.95cm]{va_only/36.jpg} \includegraphics[height=0.95cm]{va_only/21.jpg} 
\end{tabular}
\caption{Genereated images from config. 2 GAN: va only}
\end{figure}



\begin{figure}[h]
\centering
\begin{tabular}{ c  }
\includegraphics[height=0.95cm]{7x7/0.jpg}  \includegraphics[height=0.95cm]{7x7/1.jpg} \includegraphics[height=0.95cm]{7x7/2.jpg} \includegraphics[height=0.95cm]{7x7/3.jpg}  \includegraphics[height=0.95cm]{7x7/4.jpg} \includegraphics[height=0.95cm]{7x7/5.jpg} \includegraphics[height=0.95cm]{7x7/6.jpg}  \includegraphics[height=0.95cm]{7x7/7.jpg} \\

\includegraphics[height=0.95cm]{7x7/8.jpg} \includegraphics[height=0.95cm]{7x7/9.jpg}  \includegraphics[height=0.95cm]{7x7/10.jpg} \includegraphics[height=0.95cm]{7x7/11.jpg} \includegraphics[height=0.95cm]{7x7/12.jpg} \includegraphics[height=0.95cm]{7x7/13.jpg} 
\includegraphics[height=0.95cm]{7x7/14.jpg} \includegraphics[height=0.95cm]{7x7/15.jpg} \\

\includegraphics[height=0.95cm]{7x7/16.jpg}  \includegraphics[height=0.95cm]{7x7/18.jpg} \includegraphics[height=0.95cm]{7x7/19.jpg} \includegraphics[height=0.95cm]{7x7/20.jpg}  \includegraphics[height=0.95cm]{7x7/22.jpg} \includegraphics[height=0.95cm]{7x7/23.jpg} \includegraphics[height=0.95cm]{7x7/24.jpg} \\

\includegraphics[height=0.95cm]{7x7/25.jpg} \includegraphics[height=0.95cm]{7x7/26.jpg} \includegraphics[height=0.95cm]{7x7/27.jpg}
\includegraphics[height=0.95cm]{7x7/28.jpg} \includegraphics[height=0.95cm]{7x7/29.jpg} \includegraphics[height=0.95cm]{7x7/30.jpg} \includegraphics[height=0.95cm]{7x7/31.jpg} \includegraphics[height=0.95cm]{7x7/32.jpg} \\
\includegraphics[height=0.95cm]{7x7/33.jpg} \includegraphics[height=0.95cm]{7x7/34.jpg} \includegraphics[height=0.95cm]{7x7/35.jpg} 
\includegraphics[height=0.95cm]{7x7/36.jpg} \includegraphics[height=0.95cm]{7x7/37.jpg} \includegraphics[height=0.95cm]{7x7/21.jpg}
\includegraphics[height=0.95cm]{7x7/38.jpg}
\end{tabular}
\caption{Genereated images from config. 2 GAN: only va 7x7}
\end{figure}
\end{comment}








\begin{comment}


\begin{table}
    \tiny
    \centering
    \scalebox{0.7}{
    \renewcommand{\arraystretch}{0.1}
    \resizebox{0.5\textwidth}{!}{\begin{tabular}{|c|c|c|}
        \hline
        {Action Unit} & {Description} & {Example}  \\
        \hline
           1 & Inner brow raiser & \begin{minipage}{0.15\textwidth}
            \includegraphics[width=\linewidth]{au_01.png}
            \end{minipage} \\
            \hline
        2 & Outer brow raiser & 
        \begin{minipage}{0.15\textwidth}
            \includegraphics[width=\linewidth]{au_02.png}
            \end{minipage} \\
        \hline
        4 & Brow lowerer & 
        \begin{minipage}{0.15\textwidth}
            \includegraphics[width=\linewidth]{au_04.png}
        \end{minipage} \\
        \hline
        6 & Cheek raiser & 
        \begin{minipage}{0.15\textwidth}
            \includegraphics[width=\linewidth]{au_06.png}
        \end{minipage} \\
        \hline
        12 & Lip corner puller & 
        \begin{minipage}{0.15\textwidth}
            \includegraphics[width=\linewidth]{au_12.png}
        \end{minipage} \\
        \hline
        15 & Lip corner depressor & 
        \begin{minipage}{0.15\textwidth}
            \includegraphics[width=\linewidth]{au_15.png}
        \end{minipage} \\
        \hline
        20 & Lip stretcher & 
        \begin{minipage}{0.15\textwidth}
            \includegraphics[width=\linewidth]{au_20.png}
        \end{minipage} \\
        \hline
        25 & Lips part & 
        \begin{minipage}{0.15\textwidth}
            \includegraphics[width=\linewidth]{au_25.png}
        \end{minipage} \\
        \hline
    \end{tabular}}
    }
    \caption{Action Units labelled on Aff-Wild}
    \label{tab:AU}
\end{table}




\begin{figure}[h]
   \centering
      \includegraphics[height=5cm]{AU_total_dist.png}
      \caption{Pie chart of the distribution of Action Units in whole dataset}
      \label{fig:au_total_dist}
\end{figure} 


\begin{figure}[h]
\begin{tabular}{cc}
  \includegraphics[scale=0.25]{au1_va.png} &   \includegraphics[scale=0.25]{au2_va.png} \\
 \includegraphics[scale=0.25]{au4_va.png} &   \includegraphics[scale=0.25]{au6_va.png} \\
   \includegraphics[scale=0.25]{au12_va.png} &   \includegraphics[scale=0.25]{au15_va.png} \\
 \includegraphics[scale=0.25]{au20_va.png} &   \includegraphics[scale=0.25]{au25_va.png} 
\end{tabular}
\caption{Graphs of the distribution of each Action Unit in the VA-Space}
\label{fig:au_dist_va}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[scale=0.17 ]{annotation_software.png}
    \caption{The GUI for the Action Unit annotation software. The GUI for the basic expression software was exactly the same; their difference  being the titles in the annotation tabs}
    \label{fig:annotation_interface}
\end{figure}
\end{comment}






\section{The Multi-labelled  Aff-Wild parts}


\subsection{The existing Aff-Wild annotated for  Valence \& Arousal}


The Aff-Wild was the first, large scale, in-the-wild, database, consisting of 298 videos and displaying reactions
of 200 subjects, with a total video duration of more than 30 hours. The total number of frames of this database was 1,180,000. The total number of subjects was 200, with 130 of them being male and 70 of them female.
This database has been annotated by 8 lay experts with regards to valence and arousal (VA). The Aff-Wild database served as benchmark for the Aff-Wild Challenge, organized in conjunction with CVPR 2017. The aim for this database was to collect spontaneous facial behaviors in arbitrary recording conditions. To
this end, the videos were collected by searching the Youtube video sharing web-site, mainly with the "reaction" keyword. 


\subsection{The Aff-Wild part annotated for eight Action Units}\label{action_unit_section}

\begin{comment}
\begin{figure}
\begin{tabular}{c}
  \includegraphics[width=1.1cm]{01_00001.jpg}\includegraphics[width=1.1cm]{02_02932.jpg}\includegraphics[width=1.1cm]{03_00013.jpg}\includegraphics[width=1.1cm]{05_00063.jpg}\includegraphics[width=1.1cm]{06_00468.jpg}\includegraphics[width=1.1cm]{07_00046.jpg}\includegraphics[width=1.1cm]{08_00148.jpg}\\
\end{tabular}
\caption{Some representatives of the new database - Part 1}
\label{fig:face_database_1}
\end{figure}
\end{comment}

We carefully selected $64$ videos from the Aff-Wild database with a total length of $2$ hours and $10$ mins. All the videos were in MP4 format. The videos showed people being active and doing facial movements, thus leading to AU activation. 

 The selected 64 videos had $234,000$ number of frames. They contained 64  subjects, with 40 of them being male and 24 female. Table \ref{attrs_au} shows the attributes of the annotated, in terms of AUs, part of Aff-Wild.  

\begin{table}[h]
\caption{Attributes of the AU annotated part of Aff-Wild}
\label{attrs_au}
\centering
\begin{tabular}{|c|c|}
\hline
AU \# & Description \\ 
\hhline{|=|=|}
No of frames  & $234,000$ \\
\hline
No of videos & $64$ \\
\hline
No of subjects  & $64$ ($40$ male; $24$ female)  \\
\hline
No of annotators  & $3$ \\
\hline
Length of videos & $3$ secs $-$ $16$ mins $21$ secs  \\
\hline
Video format & MP$4$\\
\hline
Mean Image Resolution & $827 \times 516 $\\
\hline
\end{tabular}
\end{table}

The annotation was performed with respect to AU 1, 2, 4, 6, 12, 15, 20, 25, that are shown in Figure \ref{2d-wheel-au}. Three experts annotated those videos. Table \ref{table:ann_1} shows some images with their corresponding VA and AU annotations. 


\begin{table}[h]
\caption{ Images with their corresponding VA and AU annotations}
\label{table:ann_1}
\centering
\scalebox{1}{
\begin{tabular}[scale=1]{|c|c|c|c|c|}
 \hline
 \multicolumn{1}{|c|}{Annotation} & \multicolumn{4}{c|}{Images}  \\

			\hline
			 & a & b & c & d   \\
            \hline
              &
 \includegraphics[width=1.2cm]{20_01399.jpg} &   \includegraphics[width=1.2cm]{20_02451.jpg} &
  \includegraphics[width=1.2cm]{55_03028.jpg} &  \includegraphics[width=1.2cm]{55_01397.jpg}
 \\
  
            \hline
            Valence  & -0.69 & -0.54 & 0.38 & -0.30  \\
            Arousal   & 0.92 & 0.52 & 0.35 & 0.51 \\
            AU 1  &  x & &  &  \\
            AU 2  &  & & & \\
            AU 4  &  & x &  & x \\
            AU 6 &  & x &  & \\
            AU 12    & & & x & \\
            AU 15   & & & & x \\
            AU 20   & & &  & \\
            AU 25   & x & & x & \\
            \hline
		\end{tabular}}
\end{table}


From those 64 videos, the total number of frames that contained at least one of the above AUs was $139,298$. Table \ref{tab:frame_number_action_unit} shows the AU distribution in the annotated Aff-Wild part. %total number of frames that have been annotated with each specific AU.

\begin{table}[h]
    \centering
        \caption{Total number of frames with a specific AU}
    \label{tab:frame_number_action_unit}
\begin{tabular}{|c|c|c|c|}\hline
  Action Unit \# & \begin{tabular}{@{}c@{}}Total Number \\ of Frames \end{tabular} & Action Unit \# & \begin{tabular}{@{}c@{}}Total Number \\ of Frames \end{tabular} \\
  \hline
   AU 1 & 43,948  
   & AU 2  & 25,312 \\
   \hline
   AU 4 & 38,879  
&
   AU 6  & 48,185  \\
   \hline
   AU 12  & 45,291 
&
   AU 15 & 7,023  \\
      \hline
   AU 20 & 9,270  
&   AU 25 & 17,741 \\
   \hline
\end{tabular}
\end{table}

Figure \ref{hist_va_au} provides a histogram of the valence and arousal
values in the annotated part of Aff-Wild. 

\begin{figure}[h]
\centering
\adjincludegraphics[height=4.4cm,width=6.9cm]{val_hist.png}\\
\adjincludegraphics[height=4.4cm,width=6.9cm]{ar_hist.png}
\caption{Histogram of valence and arousal annotations of the part of Aff-Wild database that was annotated for AU.}
\vskip -0.5cm
\label{hist_va_au}
\end{figure}









\subsection{The part of Aff-Wild annotated for the basic expressions}


We carefully selected $55$ videos from the Aff-Wild database with a total length of $2$ hours and $40$ mins. All the videos were in MP4 format.
Aff-Wild contains both subtle and extreme human behaviours in real-world settings. Due to the complexity of such behaviours, the seven basic expressions and underlying emotions are not so commonly displayed in them. 
 


Nevertheless, $55$ videos were found and selected, consisting of $288,000$ number of frames. They contained 56 subjects, with $25$ of them being male and $31$ female. Table \ref{attrs2} shows the attributes of the annotated part of Aff-Wild. 


\begin{table}[h]
\caption{Attributes of the annotated part of Aff-Wild for the basic expressions}
\label{attrs2}
\centering
\begin{tabular}{|c|c|}
\hline
Basic Expressions \# & Description \\ 
\hhline{|=|=|}
No of frames  & $288,000$ \\
\hline
No of videos & $55$ \\
\hline
No of subjects  & $56$ ($25$ male; $31$ female)  \\
\hline
No of annotators  & $3$ \\
\hline
Length of videos & $4$ secs $-$ $26$ mins $22$ secs  \\
\hline
Video format & MP$4$\\
\hline
Mean Image Resolution & $1297 \times 775 $\\
\hline
\end{tabular}
\end{table}

The same three experts, who annotated a part of the Aff-Wild in terms of AUs in Section \ref{action_unit_section}, annotated the above videos in terms of the seven basic expressions, as well.
From those 55 videos, the total number of frames that contained at least one of the seven basic expressions was $115,640$. Figure \ref{tab:frame_number_basic_expr} shows the histogram of the seven basic expressions in the annotated part.


\begin{figure}[h]
\centering
\includegraphics[height=4.2cm,width=6cm]{all_distr_cate.png}
\caption{Histogram of the seven basic expressions in the annotated part of Aff-Wild}
\label{tab:frame_number_basic_expr}
\vskip -0.2cm
\end{figure}

Figure \ref{hist_va_bas_expr} provides a histogram of the valence and arousal
values in the annotated part of Aff-Wild.

\begin{figure}[h]
\centering
\adjincludegraphics[height=4.5cm,width=6.5cm]{val_hist_7expr.png}\\
\adjincludegraphics[height=4.5cm,width=6.5cm]{ar_hist_7expr.png}
\caption{Histogram of valence and arousal annotations of the part of Aff-Wild annotated for the seven basic expressions.}
\label{hist_va_bas_expr}
\vskip -0.3cm
\end{figure}


\begin{comment}
\begin{figure}[h]
\scalebox{1}{
\begin{tabular}{cc}
  \includegraphics[scale=0.15]{happy.png} &   \includegraphics[scale=0.21]{surprise.png} \\
 \includegraphics[scale=0.15]{disgust.png} &   \includegraphics[scale=0.15]{fear.png} \\
   \includegraphics[scale=0.15]{sadness.png} &   \includegraphics[scale=0.15]{angry.png} \\
\end{tabular}
}
\caption{Distributions of six basic expressions in the VA-Space}
\label{fig:basi_expr_dist_va}
\vskip -0.5cm
\end{figure}
\end{comment}


\subsection{Database partition sets}\label{sets}


\subsubsection{AU Sets}\label{sets}

The total set that we kept was the 64 videos consisting of $139,298$ number of frames, that had at least one AU activated, plus other $40,702$ that did not have any AU activated. So our total set had $180,000$ frames. We partitioned this set into training, validation and test sets. The partitioning was done in a subject independent manner, meaning that if a video in one set contained one person that was also present in other videos, then all videos containing this person should be added to the same set. 
The resulting training set consisted of $38$ videos and $107,661$ frames, the validation set consisted of $8$ videos and $23,134$ frames and the test set consisted of $18$ videos and $49,205$ frames.

\subsubsection{Basic Expression Sets}\label{sets2}

Here, we kept $55$ videos consisting of $115,640$ number of frames, that belonged to one of the basic expressions. We partitioned this set into training, validation and test sets. The partitioning was done again in a subject independent manner. 
The resulting training set consisted of $30$ videos and $67,525$ frames, the validation set consisted of $10$ videos and $20,675$ frames and the test set consisted of $15$ videos and $27,440$ frames.






\section{Pre-processing and Annotation}

\subsection{Database pre-processing}

Each of those $64$ and $55$ videos had a different frames per second (fps) rate, close or equal to 30. We converted all the videos to be in MP4 format and have 30 fps.

\subsection{Annotation tool}

We developed our own annotation software that enabled us to annotate each AU independently and frame-by-frame for each video.
For the annotation of the seven basic expressions, the same tool has been used; the only difference was the annotation tags, where the seven emotion categories were used instead of the eight AUs.

The expert-annotator could either select a time range, or some specific frames, then watch the corresponding time instances and select the AU(s)/basic expressions he/she wanted to annotate and finally perform the annotation. It should also be added that the annotation tool  also provided the ability to show the inserted annotation, while displaying a respective video. This was used for annotation verification in a post-processing step.


\subsection{Annotation procedure}

In total, three expert human coders performed the labelling, independently from each other. 
For annotating AUs, the whole video was watched first, so that the annotator could spot the most important parts, i.e. the parts in which the person reacted the most. Then, the annotation for each Action Unit was performed, by having the annotator watch the video again and select the frames where the AU was present. This process was repeated for each Action Unit.

For annotating the primary expressions, before starting the annotation of each video, the experts watched the whole video so as to know what to expect regarding the emotions being displayed in the video. Then the annotation was performed for each expression at a time. The experts took into account the fact that the seven emotion categories are not mutually exclusive (as in the case of AUs), meaning that once a frame was labelled as, e.g., happy, this frame could not be further annotated. Each frame belonged to only one specific emotion category. 


\subsection{Post-processing}

Every expert-annotator watched all videos for a second
time, in order to verify that the recorded annotations
were in accordance with the shown emotions in the videos,
or to change the annotations accordingly. In this way, a further
validation of annotations was achieved.

After the annotations have been validated by the annotators,
a final annotation selection step followed.
The 64 videos contained in total $234,000$ number of frames. However, not all frames had at least one AU activated. For the AUs that were activated, the agreement between the coders was not always 100\%. That is why we decided to keep only the AU annotations in frames where all three experts agreed on the activation of an AU. In the primary expression case, experts should have a 100\% agreement in their annotations.


\subsection{Face detection}

For detecting the faces in all videos, the FFLD2 Face Detector \cite{mathias2014face} was used.
The resulting facial images were resized to resolution of $96 \times 96 \times 3$ or $32 \times 32 \times 3$ , with their
intensity values being normalized to the range $[-1, 1]$.


\section{The Deep Neural Architectures}
 
In this Section, at first we describe the GANs that we adapted and used, and then present the CNN-RNN architecture, followed by the adopted error minimization criteria.

\subsection{GANs for image generation, but also joint classification and regression}\label{gans}


Here we aim at using GANs for semi-supervised learning, where the discriminator also serves as a classifier (AU detection) and regressor (VA estimation). For a semi-supervised learning problem with $k$ classes (in our case $k=2+8=10$), the discriminator has $k+1$ outputs; the last output corresponds to the fake examples, originating from the generator of the GAN (let us call it fake class). We tested two different configurations for the generator and the discriminator.

The first configuration was based on the SSGAN. The generator of this configuration is shown in Table \ref{gen_ssgan}. A 100 dimensional-vector Z is sampled from a uniform distribution in the range $[-1,1]$ and is reshaped into a 4-dimensional tensor $[1, 1, 1, 100]$, which is then passed through a series of four fractionally-strided convolutions (denoted as conv2d transpose);an image with resolution $32 \times 32 \times 3$ is generated in the end. Table \ref{disc_ssgan} shows the architecture of the corresponding discriminator. 

\begin{table}[h]
\centering
\caption{Configuration 1: Generator network of our semi-supervised GAN; output is a $32 \times 32 \times 3$ image}
\label{gen_ssgan}
\scalebox{1}{
\begin{tabular}{|c|c|c|c|}
\hline
Layer & filter  & stride & padding \\
\hline
conv2d transpose 1 & [2, 2, 100, 384]  & [1, 1, 1, 1] & 'SAME'  \\
batch normalization &&&\\
relu &&&\\
\hline
conv2d transpose 2  & [4, 4, 384, 128]  & [1, 2, 2, 1] & 'SAME'  \\
batch normalization &&&\\
relu &&&\\
\hline
conv2d transpose 3  & [4, 4, 128, 64]  & [1, 2, 2, 1] & 'SAME'  \\
batch normalization&&& \\
relu &&&\\
\hline
conv2d transpose 4  & [6, 6, 64, 3]  & [1, 2, 2, 1] & 'SAME'  \\
tanh &&&\\
\hline
\end{tabular}}
\end{table}


\begin{table}[h]
\centering
\caption{Configuration 1: Discriminator network of our semi-supervised GAN; input is a $32 \times 32 \times 3$ image}
\label{disc_ssgan}
\scalebox{0.85}{
\begin{tabular}{|c|c|c|c|c|}
\hline
Layer & filter  & stride & padding & no of units \\
\hline
conv2d 1 & [5, 5, 3, 64]  & [1, 2, 2, 1] & 'SAME' & \\
batch normalization &&&&\\
leaky relu &&&&\\
\hline
conv2d 2  & [5, 5, 64, 128]  & [1, 2, 2, 1] & 'SAME' & \\
batch normalization &&&&\\
leaky relu &&&&\\
\hline
conv2d 3  & [5, 5, 128, 256]  & [1, 2, 2, 1] & 'SAME' & \\
batch normalization&&&& \\
leaky relu &&&&\\
\hline
fully connected  &&&&2+8+1\\
sigmoid on AUs &&&& \\
sigmoid on Fake &&&& \\
\hline
\end{tabular}}
\end{table}

The second configuration was a modification of the DCGAN. The generator of this configuration is shown in Table \ref{gen_dcgan}. A $100$ dimensional-vector Z is sampled from a uniform distribution in range $[-1,1]$, projected and reshaped into a 4-dimensional tensor $[1, 6, 6, 1024]$, which is then passed through a series of four fractionally-strided convolutions and at the end an image with resolution $96 \times 96 \times 3$ is generated. Table \ref{discr_dcgan} shows the architecture of the corresponding discriminator. We also tested the same configuration for the generator and the discriminator networks, but instead of using $5 \times 5$ filters, we used $7 \times 7$.


\begin{table}[h]
\centering
\caption{Configuration 2: Generator network of our semi-supervised GAN; output is a $96 \times 96 \times 3$ image}
\label{gen_dcgan}
\scalebox{0.85}{
\begin{tabular}{|c|c|c|c|c|}
\hline
Layer & filter  & stride & padding & no of units \\
\hline
fully connected  &&&&1*6*6*1024\\
batch normalization&&&& \\
relu &&&&\\
\hline
conv2d transpose 1 & [5, 5, 1024, 512]  & [1, 2, 2, 1] & 'SAME' & \\
batch normalization &&&&\\
relu &&&&\\
\hline
conv2d transpose 2  & [5, 5, 512, 256]  & [1, 2, 2, 1] & 'SAME' & \\
batch normalization &&&&\\
relu &&&&\\
\hline
conv2d transpose 3  & [5, 5, 256, 128]  & [1, 2, 2, 1] & 'SAME' & \\
batch normalization&&&& \\
relu &&&&\\
\hline
conv2d transpose 4  & [5, 5, 128, 3]  & [1, 2, 2, 1] & 'SAME' & \\
tanh &&&&\\
\hline
\end{tabular}}
\end{table}

\begin{table}[h]
\centering
\caption{Configuration 2: Discriminator network of our semi-supervised GAN; input is a $96 \times 96 \times 3$ image}
\label{discr_dcgan}
\scalebox{0.85}{
\begin{tabular}{|c|c|c|c|c|}
\hline
Layer & filter  & stride & padding & no of units \\
\hline
conv2d 1 & [5, 5, 3, 64]  & [1, 2, 2, 1] & 'SAME' & \\
batch normalization &&&&\\
leaky relu &&&&\\
\hline
conv2d 2  & [5, 5, 64, 128]  & [1, 2, 2, 1] & 'SAME' & \\
batch normalization &&&&\\
leaky relu &&&&\\
\hline
conv2d 3  & [5, 5, 128, 256]  & [1, 2, 2, 1] & 'SAME' & \\
batch normalization&&&& \\
leaky relu &&&&\\
\hline
conv2d 4  & [5, 5, 256, 512]  & [1, 2, 2, 1] & 'SAME' & \\
batch normalization&&&& \\
leaky relu &&&&\\
\hline
fully connected  &&&&2+8+1\\
sigmoid on AUs &&&& \\
sigmoid on Fake &&&& \\
\hline
\end{tabular}}
\end{table}


Note that in both configurations there are $11$ outputs in the discriminator, while the output activation function is linear for VA, sigmoid for the eight AUs and sigmoid for the fake class. 
For comparison purposes, we tested the above two configurations with the discriminator being only classifier (of AUs), or only regressor (VA estimation), apart from predicting the fake class.


\subsection{CNN-RNN for multi-task learning: joint  classification and regression}\label{cnn-rnn_va_expr}

Here, we aim at multi-task learning of VA and seven basic expressions. The architecture that we used was based on the AffWildNet \cite{kollias2017recognition}, which is a VGGFACE-GRU \cite{parkhi2015deep}\cite{chung2014empirical} network that provided the best results in the Aff-Wild database for VA estimation. We have modified the architecture so as to include an attention layer on top of the RNN layer and 9 outputs, with the output activation function being linear for VA and softmax for the 7 basic expressions. Figure \ref{va-expr-model} shows in more detail the architecture of the model being used.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.27 ]{va_rnn_model.png}
    \caption{The multi-task learning model: a VGGFACE-GRU-attention model predicting VA and Seven Basic Expressions}
    \label{va-expr-model}
\end{figure}




\subsection{Objective Functions \& Training implementation details}

\subsubsection{Case with Valence-Arousal \& Action Units}

The loss function used for training the GANs was: 
\vskip -0.5cm

\begin{equation} \label{eq_1}
\mathcal{L}_{total} = \mathcal{L}_{gen} + \mathcal{L}_{discr} = \mathcal{L}_{gen} + \mathcal{R}_{images} + \mathcal{F}_{images},
\end{equation}

\noindent
where $\mathcal{L}_{gen}$ is the loss of the generator, $\mathcal{R}_{images}$ and  $\mathcal{F}_{images}$ are the losses of the discriminator when presented at input with real and fake images, respectively.

For the generator loss, a reconstruction loss with an annealed weight is applied as an auxiliary loss to help the generator get rid of the initial local minimum:

\vskip -0.3cm

\begin{equation} \label{eq_2}
\mathcal{L}_{gen} = \log x + w_{r} * \mathcal{L}_{\delta}(real-fake),
\end{equation}

where $logx$ is the logarithm of the output, $x$, of the fake class of the discriminator when  presented with a fake image, $w_r$ is the annealed weight with initial value $1$ and $\mathcal{L}_{\delta}(real-fake)$ is the huber loss \cite{huber1964robust} with $\delta = 1$ between the real image and the generated fake image.

During training, for a real image the corresponding label for the fake class was 0. For a fake image, label smoothing is applied and the resulting label has a value of 0.01 for valence, arousal and the eight AUs and 0.9 for the fake class. The loss for real images was defined as:

\vskip -0.3cm

\begin{equation} \label{eq_3}
\mathcal{R}_{images} = \mathcal{R}_{VA} + \mathcal{R}_{AU} + \mathcal{R}_{fake} , 
\end{equation}

where $\mathcal{R}_{i}$ denotes the loss of class $i \in \{VA,AU,fake\} $. Note that the loss for fake images was similar.

For $\mathcal{L}_{VA}$ loss, two different losses were tested.
The first loss function was based on the Concordance Correlation Coefficient (CCC) and was defined as:

\begin{equation} \label{eq_4}
\mathcal{L}_{VA} = 1 - \frac{\rho_a + \rho_v}{2},
\end{equation}

where $\rho_a$ and $\rho_v$ is the CCC for arousal and valence, respectively and was defined as follows  :

\begin{equation} \label{eq_5}
\rho_c = \frac{2 s_{xy}}{s_x^2 + s_y^2 + (\bar{x} - \bar{y})^2},
\end{equation}

\noindent
where $s_x$ and $s_y$ are the variances of the valence/arousal labels and predicted values respectively, $\bar{x}$ and $\bar{y}$ are the corresponding mean values,  $s_{xy}$ is the respective covariance value and $c$ can be either $a$ (stands for arousal) or $v$ (stands for valence).

The second loss function was the usual Mean Squared Error (MSE). We calculated the MSE for valence and for arousal and $\mathcal{L}_{VA}$ was their average.

For $\mathcal{L}_{AU}$ loss, we used the cross entropy loss, averaged across all eight AUs. 
For the fake class, we used the cross entropy loss.

We used separate learning rates for the generator and the discriminator. The generator's learning rate was constantly set to  $10^{-4}$ and the discriminator's to $10^{-5}$. To avoid the fast convergence of the discriminator network, we updated the generator network more frequently. For configuration 1, the discriminator was updated five times before the generator, whereas for configuration 2 the discriminator was updated twice. We further applied gradient clipping with value 20, so as to stabilize training. All models were trained using the Adam optimizer with $\beta_1 = 0.5$ and $\beta_2 = 0.999$, with a mini-batch size of 64. In the LeakyReLU, the slope of the leak was set to 0.2 in all models. 









\subsubsection{Case with Valence-Arousal \& Basic Expressions}\label{loss_va_au}

The loss function used for training the networks was: 

\vskip -0.3cm

\begin{equation} \label{eq:1}
\mathcal{L}_{total} = \alpha * \mathcal{L'}_{VA} + \beta * \mathcal{L}_{basic\_expressions},
\end{equation}

\noindent
where $\mathcal{L'}_{VA}$ is the loss function for valence and arousal, $\mathcal{L}_{basic\_expressions}$ is the loss function for the seven basic expressions, $\alpha$ and $\beta$ are constants with values in $[0,1]$. Note that if $\alpha=0$ and $\beta=1$, or $\alpha=1$ and $\beta=0$, then the network acts as only a classifier or only a regressor, respectively.

Different loss functions were used. $\mathcal{L'}_{VA}$ was either $\mathcal{L}_{VA}$ of Eq. \ref{eq_4} or MSE. $\mathcal{L}_{basic\_expressions}$ was either cross entropy or MSE. In the basic expressions case, when the loss function was the MSE, two approaches were tried: either passing the output of the softmax (i.e., the probabilities of the emotion categories), or the output of the fully connected layer before the softmax.



The network was trained either end-to-end, or we kept the CNN weights fixed and trained the RNN, attention and output layers.
For network training, we utilized the Adam optimizer; the batch size was set to 800 (consisting of 10 different sequences, each having 80 consecutive frames), the attention length was chosen to be 32 and the learning rate was set to 0.001 or 0.0001.
The platform used was Tensorflow.






\subsection{The evaluation criteria}\label{criteria}


The criteria that were considered for evaluating the performance of the networks were:
\begin{itemize}
    \item[i)] for AUs and basic expressions: total accuracy, weighted and macro f1 score 
    \item[ii)] for VA: CCC
\end{itemize}
Taking into account the imbalanced distribution of the AUs and basic expressions, we did not want to evaluate the models only on the accuracy metric, as it is sensitive to bias and not really effective for imbalanced data. Also the macro f1 score (unweighted average of f1 scores of all AUs) does not account for imbalanced classes. That is why we also considered the weighted f1 score (weighted average of f1 scores of all AU).

\section{Experimental Results}\label{experiments}


\subsection{Case with Valence-Arousal \& Action Units}



\subsubsection{Results on Performance of Classifier and/or Regressor}

Here, we examine the performance of the GANs described in Section \ref{gans}, using different loss functions for the discriminator network, when: predicting only the VA; classifying only into the basic expressions; serving as both regressor and classifier. Table \ref{gan_au_va} shows that a better performance is obtained when the discriminator classifies only the AUs, or  performs only VA estimation, than when it jointly predicts VA and AUs. The Table also shows that the discriminator has a better performance in predicting only the VA, when the MSE is used as loss function, than  when the loss function is based on CCC.

\begin{table}[h]
\caption{Performance of GAN's Discriminator when trained with different loss functions and when serving as a regressor-only, a classifier-only or a regressor-classifier}
\label{gan_au_va}
\centering
\scalebox{0.9}{
\begin{tabular}{ |c|c|c|c|c|c|c|  }
 \hline
 \begin{tabular}{@{}c@{}} Discriminator \\ that also \\ classifies \end{tabular} & Loss function & \begin{tabular}{@{}c@{}} CCC-V \\ CCC-A \end{tabular}  & \begin{tabular}{@{}c@{}} F1 Score: \\ weighted \\ macro \end{tabular} & \begin{tabular}{@{}c@{}} Total  Accuracy \end{tabular}  \\
\hhline{|=|=|=|=|=|}
only VA & CCC based & \begin{tabular}{@{}c@{}} 0.440 \\ 0.370 \end{tabular}  &  - & -     \\
\hline
\textbf{only VA} & \textbf{MSE}  & \begin{tabular}{@{}c@{}} \textbf{0.484} \\ \textbf{0.384} \end{tabular}  &  - & -     \\
\hline
\textbf{only AU} &  \textbf{cross entropy} & -  &  \begin{tabular}{@{}c@{}} \textbf{0.404} \\ \textbf{0.40} \end{tabular} &  \begin{tabular}{@{}c@{}} \textbf{0.824}  \end{tabular}  \\
\hline
 \begin{tabular}{@{}c@{}}VA \\ + \\ AU  \end{tabular} & \begin{tabular}{@{}c@{}} CCC based \\ + \\  cross entropy \end{tabular}  & \begin{tabular}{@{}c@{}} 0.447 \\ 0.374 \end{tabular} & \begin{tabular}{@{}c@{}} 0.328 \\ 0.330 \end{tabular}  &  \begin{tabular}{@{}c@{}} 0.661  \end{tabular}  \\
\hline
 \begin{tabular}{@{}c@{}}VA \\ + \\ AU  \end{tabular} & \begin{tabular}{@{}c@{}} MSE \\ + \\  cross entropy \end{tabular}  & \begin{tabular}{@{}c@{}} 0.451 \\ 0.333 \end{tabular} & \begin{tabular}{@{}c@{}} 0.332 \\ 0.347 \end{tabular}  &  \begin{tabular}{@{}c@{}} 0.667  \end{tabular}  \\
\hline
\end{tabular}
}
\end{table}



\subsubsection{Image Generation}

Here we qualitatively evaluate the quality of image synthesis by the generator of the GANs. Figure \ref{all_identities} shows all subjects appearing in the training set of the Aff-Wild videos that have been annotated for AUs. Figure \ref{synthetic} shows examples of generated images.

\begin{figure}[h]
\centering
\begin{tabular}{ c  }
\includegraphics[height=0.95cm]{both/0.jpg}\includegraphics[height=0.95cm]{both/1.jpg}\includegraphics[height=0.95cm]{both/2.jpg}\includegraphics[height=0.95cm]{both/3.jpg}\includegraphics[height=0.95cm]{both/4.jpg}\includegraphics[height=0.95cm]{both/5.jpg}\includegraphics[height=0.95cm]{both/6.jpg}\includegraphics[height=0.95cm]{hulk.png}\\
\includegraphics[height=0.95cm]{both/8.jpg}\includegraphics[height=0.95cm]{both/9.jpg}\includegraphics[height=0.95cm]{both/10.jpg}\includegraphics[height=0.95cm]{au_only/14.jpg}\includegraphics[height=0.95cm]{both/13.jpg}\includegraphics[height=0.95cm]{both/14.jpg}\includegraphics[height=0.95cm]{both/21.jpg}\includegraphics[height=0.95cm]{both/18.jpg}\\
 \includegraphics[height=0.95cm]{both/19.jpg}\includegraphics[height=0.95cm]{both/22.jpg}\includegraphics[height=0.95cm]{7x7/17.jpg}\includegraphics[height=0.95cm]{both/24.jpg}\includegraphics[height=0.95cm]{both/25.jpg}\includegraphics[height=0.95cm]{both/26.jpg}\includegraphics[height=0.95cm]{both/27.jpg}\includegraphics[height=0.95cm]{both/28.jpg}\\
\includegraphics[height=0.95cm]{both/29.jpg}\includegraphics[height=0.95cm]{both/32.jpg}\includegraphics[height=0.95cm]{both/33.jpg}\includegraphics[height=0.95cm]{va_only/28.jpg}\includegraphics[height=0.95cm]{va_only/29.jpg}\includegraphics[height=0.95cm]{00420.jpg} 
\end{tabular}
\caption{Images with all subjects appearing in the training set of the part of Aff-Wild that was annotated for AUs}
\label{all_identities}
\end{figure}
 





\begin{figure}[h]
\centering
\begin{tabular}{ c  }

\includegraphics[height=0.95cm]{both/17.jpg}\includegraphics[height=0.95cm]{mix/2.jpg}\includegraphics[height=0.95cm]{mix/3.jpg}\includegraphics[height=0.95cm]{mix/4.jpg}\includegraphics[height=0.95cm]{mix/38.jpg}\includegraphics[height=0.95cm]{mix/159.jpg}\includegraphics[height=0.95cm]{mix/5.jpg}\includegraphics[height=0.95cm]{mix/12.jpg}\includegraphics[height=0.95cm]{mix/13.jpg}\\

\includegraphics[height=0.95cm]{mix/17.jpg}\includegraphics[height=0.95cm]{mix/19.jpg}\includegraphics[height=0.95cm]{mix/72.jpg}\includegraphics[height=0.95cm]{mix/26.jpg}\includegraphics[height=0.95cm]{mix/156.jpg}\includegraphics[height=0.95cm]{mix/27.jpg}\includegraphics[height=0.95cm]{mix/29.jpg}\includegraphics[height=0.95cm]{mix/31.jpg}\includegraphics[height=0.95cm]{mix/71.jpg}\\

\includegraphics[height=0.95cm]{mix/37.jpg}\includegraphics[height=0.95cm]{mix/21.jpg}\includegraphics[height=0.95cm]{mix/36.jpg}\includegraphics[height=0.95cm]{mix/70.jpg}\includegraphics[height=0.95cm]{mix/39.jpg}\includegraphics[height=0.95cm]{mix/8.jpg}\includegraphics[height=0.95cm]{mix/73.jpg}
\includegraphics[height=0.95cm]{mix/40.jpg}\includegraphics[height=0.95cm]{mix/119.jpg}\\

\includegraphics[height=0.95cm]{mix/42.jpg}\includegraphics[height=0.95cm]{mix/45.jpg}\includegraphics[height=0.95cm]{mix/46.jpg}\includegraphics[height=0.95cm]{mix/47.jpg}\includegraphics[height=0.95cm]{mix/55.jpg}\includegraphics[height=0.95cm]{mix/51.jpg}\includegraphics[height=0.95cm]{mix/52.jpg}\includegraphics[height=0.95cm]{mix/53.jpg}\includegraphics[height=0.95cm]{mix/58.jpg}\\

\includegraphics[height=0.95cm]{mix/77.jpg}\includegraphics[height=0.95cm]{mix/54.jpg}\includegraphics[height=0.95cm]{mix/59.jpg}\includegraphics[height=0.95cm]{mix/62.jpg}\includegraphics[height=0.95cm]{mix/79.jpg}\includegraphics[height=0.95cm]{mix/78.jpg}\includegraphics[height=0.95cm]{mix/76.jpg}\includegraphics[height=0.95cm]{mix/57.jpg}\includegraphics[height=0.95cm]{mix/153.jpg}\\

\includegraphics[height=0.95cm]{mix/155.jpg}\includegraphics[height=0.95cm]{mix/92.jpg}\includegraphics[height=0.95cm]{mix/94.jpg}\includegraphics[height=0.95cm]{mix/131.jpg}\includegraphics[height=0.95cm]{mix/154.jpg}\includegraphics[height=0.95cm]{mix/132.jpg}\includegraphics[height=0.95cm]{mix/103.jpg}\includegraphics[height=0.95cm]{mix/101.jpg}\includegraphics[height=0.95cm]{mix/300.jpg}\\

\includegraphics[height=0.95cm]{mix/129.jpg}\includegraphics[height=0.95cm]{mix/114.jpg}\includegraphics[height=0.95cm]{mix/115.jpg}\includegraphics[height=0.95cm]{mix/41.jpg}\includegraphics[height=0.95cm]{mix/113.jpg}\includegraphics[height=0.95cm]{mix/110.jpg}\includegraphics[height=0.95cm]{mix/111.jpg}\includegraphics[height=0.95cm]{mix/112.jpg}\includegraphics[height=0.95cm]{mix/125.jpg}\\

\includegraphics[height=0.95cm]{mix/121.jpg}\includegraphics[height=0.95cm]{mix/122.jpg}\includegraphics[height=0.95cm]{mix/120.jpg}\includegraphics[height=0.95cm]{mix/126.jpg}\includegraphics[height=0.95cm]{mix/128.jpg}\includegraphics[height=0.95cm]{mix/135.jpg}\includegraphics[height=0.95cm]{mix/138.jpg}\includegraphics[height=0.95cm]{mix/137.jpg}\includegraphics[height=0.95cm]{mix/136.jpg}\\

\includegraphics[height=0.95cm]{mix/140.jpg}\includegraphics[height=0.95cm]{mix/141.jpg}\includegraphics[height=0.95cm]{mix/142.jpg}\includegraphics[height=0.95cm]{mix/143.jpg}\includegraphics[height=0.95cm]{mix/144.jpg}\includegraphics[height=0.95cm]{mix/146.jpg}\includegraphics[height=0.95cm]{mix/147.jpg}\includegraphics[height=0.95cm]{mix/149.jpg}\includegraphics[height=0.95cm]{mix/150.jpg}\\

\includegraphics[height=0.95cm]{mix/89.jpg}\includegraphics[height=0.95cm]{mix/82.jpg}\includegraphics[height=0.95cm]{mix/83.jpg}\includegraphics[height=0.95cm]{mix/84.jpg}\includegraphics[height=0.95cm]{mix/63.jpg}\includegraphics[height=0.95cm]{mix/87.jpg}\includegraphics[height=0.95cm]{mix/88.jpg}\includegraphics[height=0.95cm]{mix/66.jpg}\includegraphics[height=0.95cm]{mix/91.jpg}\\


\end{tabular}
\caption{Generated images from GAN}
\label{synthetic}
\end{figure}


 It can be observed that the generator has: i) created new faces and ii) created new expressions or modified some attributes (i.e., gender, hair, skin or eye color) of a subject appearing in the training set. Such example cases are a girl having moustache or a guy having girl's hair, or a subject wearing glasses (but did not wear in the real images), or a guy having now beard, or a girl having different hair color. Although the training set did not contain a lot of different subjects, the generator was able to adequately learn the in-the-wild nature of Aff-Wild (with great variability in expressed emotions, with different head poses and illumination conditions and with occlusions), 'transferring' this to the generated images.

\subsection{Case with Valence-Arousal \& Basic Expressions}


Here, we evaluate the performance, on the test set, of the deep neural architecture described in Section \ref{cnn-rnn_va_expr}. 
At first, we compare the performance of this architecture when trained with different loss functions described in Section \ref{loss_va_au} and with different learning rates. We should mention that for these experiments, we kept the values of $\alpha$ and $\beta$ fixed, at value 0.5.

Table \ref{all} shows the obtained performance in VA and basic expression estimation, using different loss functions and learning rate values. It can be seen that the best performance, under all three evaluation criteria (CCC, Total Accuracy, F1 Score) was achieved when the valence arousal loss was CCC based, the basic expressions loss was cross entropy and the learning rate was $10^{-3}$. 


Next, we used the above best performing network with different values for $\alpha$ and $\beta$ in the loss function in Eq. \ref{eq:1}. It can be seen, from Table \ref{diff_a_b}, that when $\alpha = \beta = 0.5$ (as in the experiments of Table \ref{all}) the network had the best performance according to all three evaluation criteria. This performance was better than when training the network to only predict valence-arousal (which is the case when $\alpha=1$ and $\beta=0$), or to only classify in the seven basic expressions (which is the case when $\alpha=0$ and $\beta=1$). This shows that the multi-task learning provided an improved performance compared to both single learning cases, by taking advantage of the relations existing in the learned tasks.


\begin{table}[h]
\caption{Obtained performances when comparing best model from Table \ref{all} with different values for $\alpha$ and $\beta$ in the loss function in Eq. \ref{eq:1}}
\label{diff_a_b}
\centering
\scalebox{0.9}{
\begin{tabular}{ |c|c|c|c|  }
 \hline
 \begin{tabular}{@{}c@{}} Network \\ with ($\alpha$ , $\beta$) \end{tabular} & \begin{tabular}{@{}c@{}} CCC: \\ V-A \end{tabular}  & \begin{tabular}{@{}c@{}} Total \\ Accuracy \end{tabular} & \begin{tabular}{@{}c@{}} F1: \\ Weighted - Unweighted \end{tabular}  \\
\hhline{|=|=|=|=|}
\begin{tabular}{@{}c@{}} (0 , 1) $\equiv$  only expr.  \end{tabular} & - & 0.494  &  \begin{tabular}{@{}c@{}} 0.488 - 0.415 \end{tabular}  \\
\hline
\begin{tabular}{@{}c@{}} (1 , 0) $\equiv$  only VA\end{tabular} & \begin{tabular}{@{}c@{}} 0.579 - 0.409 \end{tabular} & -  & -  \\
\hline
\begin{tabular}{@{}c@{}} (0.25 , 0.75)  \end{tabular} & \begin{tabular}{@{}c@{}} 0.556 - 0.419 \end{tabular} & 0.547  &  \begin{tabular}{@{}c@{}} 0.542 - 0.452 \end{tabular}  \\
\hline
\begin{tabular}{@{}c@{}} (0.75 , 0.25)  \end{tabular} & \begin{tabular}{@{}c@{}} 0.589 - 0.424 \end{tabular} & 0.527  &  \begin{tabular}{@{}c@{}} 0.514 - 0.422 \end{tabular}  \\
\hline
\begin{tabular}{@{}c@{}} \textbf{(0.5 , 0.5)} \end{tabular} & \begin{tabular}{@{}c@{}} \textbf{0.616} - \textbf{0.510} \end{tabular} & \textbf{0.645}  &  \begin{tabular}{@{}c@{}} \textbf{0.643} - \textbf{0.514} \end{tabular}  \\
\hline
\begin{tabular}{@{}c@{}} (0.75 , 0.75)  \end{tabular} & \begin{tabular}{@{}c@{}} 0.585 - 0.453 \end{tabular} & 0.561  &  \begin{tabular}{@{}c@{}} 0.555 - 0.476 \end{tabular}  \\
\hline
\begin{tabular}{@{}c@{}} (1 , 1) \end{tabular} & \begin{tabular}{@{}c@{}} 0.521 - 0.356 \end{tabular} & 0.513  & \begin{tabular}{@{}c@{}} 0.501 - 0.411 \end{tabular}   \\
\hline
\end{tabular}
}
\end{table}


\begin{table*}[h]
\caption{Obtained values when testing different loss functions and learning rates}
\label{all}
\centering
\begin{tabular}{ |c|c|c|c|c|c|c|c| }
 \hline
 \multicolumn{3}{|c|}{Parameters} & \multicolumn{2}{c|}{CCC} & \multicolumn{1}{c|}{Accuracy} & \multicolumn{2}{c|}{F1 Score}  \\
 \hline
 VA-Loss & Basic Expressions-Loss & Learning Rate  & Valence & Arousal & & Unweighted & Weighted    \\
\hhline{|=|=|=|=|=|=|=|=|}
CCC based & Cross Entropy & $10^{-4}$ &0.510 &0.445 & 0.644  & 0.619  & 0.481   \\
 \hline
\textbf{CCC based} & \textbf{Cross Entropy} & $\textbf{10}^{\textbf{-3}}$ &\textbf{0.616} &\textbf{0.510} & \textbf{0.645}  & \textbf{0.643}  & \textbf{0.514}  \\
\hline
CCC based & MSE before softmax & $10^{-4}$ &0.491 &0.433 & 0.588  & 0.581  & 0.497  \\
 \hline
CCC based & MSE before softmax & $10^{-3}$ &0.507 &0.441 & 0.517 & 0.501  & 0.469  \\
\hline
CCC based & MSE after softmax & $10^{-4}$ &0.462 &0.406 & 0.575 & 0.562  & 0.488  \\
 \hline
CCC based & MSE after softmax & $10^{-3}$ &0.486 &0.422 & 0.569  & 0.511  & 0.447 \\
\hline
MSE & Cross Entropy & $10^{-4}$ &0.502 &0.350 & 0.641  & 0.620  & 0.492   \\
\hline
MSE & Cross Entropy & $10^{-3}$ &0.532 &0.362 & 0.601  & 0.606  & 0.480   \\
\hline
MSE & MSE before softmax & $10^{-4}$ &0.537 &0.482 & 0.600  & 0.598  & 0.476   \\
\hline
MSE & MSE before softmax & $10^{-3}$ &0.561 &0.487 & 0.556  & 0.541  & 0.448   \\
\hline
MSE & MSE after softmax & $10^{-4}$ &0.495 &0.465 & 0.602  & 0.577  & 0.471   \\
\hline
MSE & MSE after softmax & $10^{-3}$ &0.515 &0.468 & 0.581  & 0.527  & 0.423   \\
\hline
\end{tabular}
\end{table*}

\section{CONCLUSIONS AND FUTURE WORK}

A multi-task learning for emotion recognition and for facial image generation was presented in this paper. This was made possible by annotating large parts of the Aff-Wild database in terms of Facial Action Units and in terms of the seven Basic Expressions and underlying emotions. Deep neural architectures and GANs were appropriately designed and trained using these annotated datasets. High performances have been obtained in all multi-task experiments. Moreover, enrichment of the above datasets was achieved through the image generation approach implemented by GANs. Future work will include extension of the developed multi-tasking models and datasets, so as to further improve the capabilities of the proposed systems.        

\bibliographystyle{ieee}
\bibliography{egbib}


\end{document}


