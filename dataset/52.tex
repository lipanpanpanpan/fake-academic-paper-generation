\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage[utf8]{inputenc} % Swedish letters
\usepackage{float}

\def\x{{\mathbf x}}
\def\L{{\cal L}}

\title{Generating Diffusion MRI scalar maps from T1 weighted images using generative adversarial networks}
\name{Xuan Gu$^{13}$ \qquad Hans Knutsson$^{13}$ \qquad Anders Eklund$^{123}$}

\address{$^{1}$ Division of Medical Informatics, Department of Biomedical Engineering\\
    $^{2}$ Division of Statistics and Machine learning, Department of Computer and Information Science\\
    $^{3}$ Center for Medical Image Science and Visualization\\
    Linkping University, Linkping, Sweden}
\begin{document}
\maketitle
\begin{abstract}
  Diffusion magnetic resonance imaging (diffusion MRI) is a non-invasive microstructure assessment method. Scalar measures quantifying micro-structural tissue properties can be obtained using diffusion models and data processing pipelines. However, it is costly and time consuming to collect high quality  diffusion data. We demonstrate how Generative Adversarial Networks (GANs) can be used to generate diffusion scalar measures from  structural MR images in a single optimized step, without diffusion models and diffusion data. We show that the used Cycle-GAN model can synthesize visually realistic and quantitatively accurate diffusion-derived scalar measures.
\end{abstract}
\begin{keywords}
Diffusion MRI, GAN
\end{keywords}
\section{Introduction}
\label{sec:intro}

Diffusion MRI is a non-invasive technique used for studying brain tissue microstructures. Diffusion-derived scalar maps provide rich information about microstructural characterization. There are two major bottlenecks in obtaining the diffusion-derived scalar maps. First, it is expensive and time consuming to acquire high quality diffusion data. Second, accuracy of the diffusion-derived scalar maps relies on elaborate diffusion data processing pipelines, including preprocessing (head motion, eddy current distortion and susceptibility-induced distortion corrections), diffusion model fitting and diffusion scalar calculation. Small errors occurring at any of these steps can contribute to the bias of the diffusion-derived scalars. For some advanced diffusion models e.g. mean apparent propagator (MAP) MRI \cite{ozarslan2013mean}, processing of a single slice of the brain can take hours to finish.

Generative Adversarial Networks (GANs) is one of the most important ideas in machine learning in the last 20 years \cite{goodfellow2014generative}. GANs have already been widely used for medical image processing applications, such as denoising, reconstruction, segmentation, detection, classification and image synthesis. However, GANs for medical image translation are still rather unexplored, especially for cross-modality translation of MR images \cite{kazeminia2018gans}.

Implementations of Cycle-GAN and unsupervised image-to-image translation (UNIT) for 2D T1-T2 translation were reported in \cite{welander2018generative}, and results showed that visually realistic synthetic T1 and T2 images can be generated from the other modality, proven via a perceptual study.
In \cite{dar2018image} a conditional GAN  was proposed to do the translation between T1 and T2 images, in which a probabilistic GAN (PGAN) and a Cycle-GAN were trained for paired and unpaired source-target images, respectively. The proposed GAN demonstrated visually and quantitatively accurate translations for both healthy subjects and glioma patients.
A patch-based conditional GAN \cite{olut2018generative} was proposed to generate magnetic resonance angiography (MRA) images from T1 and T2 images jointly. The steerable filter responses were incorporated in the loss function to extract directional features of the vessel structure.
MR image translation based on downsampled images was investigated in \cite{dar2018synergistic}, to reduce scan time. Three different types of input were fed to the GANs: downsampled target images, downsampled source images, and downsampled target and source images jointly. It was demonstrated that the GAN with the combination of downsampled target and source images as the input outperformed its two competitors, which resulted in a reduction of the scan time up to a factor of 50.
A 3D conditional GAN \cite{yu20183d} was applied to synthesize FLAIR images from T1, and the synthetic FLAIR images  improved brain tumor segmentation, compared with using only T1.






In \cite{golkov2015q}, deep learning was used to reduce the diffusion scan time by a factor 12.
In this work, we explored the possibility to reduce the diffusion data collection and processing pipelines to a single optimized step, generating diffusion scalar maps without any diffusion data. We propose a new application of Cycle-GAN \cite{zhu2017unpaired} to translate T1 images to diffusion-derived scalar maps. To the best of our knowledge, this is the first study of GAN-based MR image translation between structural space and diffusion space. Both qualitative and quantitative evaluations of generated images were carried out in order to assess effectiveness of the method.



\section{Theory}
\label{sec:format}
\subsection{Diffusion tensor model}

In a diffusion experiment, the diffusion-weighted signal $S_i$ of the $i$th measurement for one voxel  is modeled by
\begin{align}
  S_i = S_0 \exp(-b \mathbf{g}_i^T \mathbf{Dg}_i), \quad \mathrm{for} \quad i = 1, 2, \cdots, T,
\end{align}
where $S_0$ is the signal without diffusion weighting, $b$ is the diffusion weighting factor, $\mathbf{D}=\begin{bmatrix}
  D_{xx} & D_{xy} & D_{xz} \\
  D_{xy} & D_{yy} & D_{yz} \\
  D_{xz} & D_{yz} & D_{zz}
\end{bmatrix}$ is the diffusion tensor in the form of a $3 \times 3$ positive definite matrix, $\mathbf{g}_i$ is a $3 \times 1$ unit vector of the gradient direction, and $T$ is the total number of measurements. The mean diffusivity (MD) and the fractional anisotropy (FA) are given by

\begin{align}
  MD&=(\lambda_1+\lambda_2+\lambda_3)/3, \\
  FA&=\sqrt{\frac{(\lambda_1-\lambda_2)^2+(\lambda_2-\lambda_3)^2+(\lambda_3-\lambda_1)^2}{2(\lambda_1^2+\lambda_2^2+\lambda_3^2)}},
\end{align}
where $\lambda_1$, $\lambda_2$ and $\lambda_3$ are the three eigenvalues of the diffusion tensor $\mathbf{D}$.
Diffusion tensor fitting was first carried out using weighted least squares.


\subsection{Cycle-GAN}
A Cycle-GAN \cite{zhu2017unpaired} can be trained using two unpaired groups of images, to translate images between domain A and domain B. A Cycle-GAN usually consists of four main components, two generators ($G_{A2B}$, $G_{B2A}$) and two discriminators ($D_A$ and $D_B$). The two generators are generating domain A/B images based on domain B/A. The two discriminators are making the judgement if input images belong to domain A/B. The translation between the two image domains are guaranteed by
\begin{align}
  G_{B2A}(G_{A2B}(I_A)) &\approx I_A \\
  G_{A2B}(G_{B2A}(I_B)) &\approx I_B
\end{align}
where $I_A$ and $I_B$ are two images of domain A and B. We used 15 convolutional layers in the Generator and 4 layers in the Discriminator. We used the Keras implementation of Cycle-GAN provided in \cite{welander2018generative}.

\section{Data and Methods}
\label{sec:pagestyle}

We used diffusion and T1 images from the Human Connectome Project (HCP)\footnote{Data collection and sharing for this project was provided by the Human Connectome Project (U01-MH93765) (HCP; Principal Investigators: Bruce Rosen, M.D., Ph.D., Arthur W. Toga, Ph.D., Van J.Weeden, MD). HCP fund- ing was provided by the National Institute of Dental and Craniofacial Re- search (NIDCR), the National Institute of Mental Health (NIMH), and the National Institute of Neurological Disorders and Stroke (NINDS). HCP data are disseminated by the Laboratory of Neuro Imaging at the University of Southern California.} \cite{van2013wu,glasser2013minimal}  for 1065 subjects. The data were collected using a customized Siemens 3T Connectom scanner. The diffusion data were acquired with 3 different b-values (1000, 2000, and 3000 $s/mm^2$) and have already been pre-processed for gradient nonlinearity correction, motion correction and eddy current correction. The diffusion data consist of 18 non-diffusion weighted volumes (b = 0) and 90 volumes for each b-value, which yields 288 volumes of $145 \times 174 \times 145$ voxels with an 1.25 mm isotropic voxel size. The T1 data was acquired with a $0.7 \times 0.7 \times 0.7$ mm isotropic voxel size and then downsampled to the same resolution as the diffusion data.

We trained the network with 1000 subjects and the remaining 65 subjects were used for testing. One slice of each subject was extracted from the volume and then cropped to the size of $120 \times 152$ to reduce processing time. The training took about 10 hours using an Nvidia Titan X Pascal graphics card.

We used two widely used measures from the literature \cite{wolterink2017deep,emami2018generating}, the correlation coefficient (CC) and the structural similarity (SSIM), to quantify the accuracy of the image translation. The CC measures the degree of global correlation between two images and SSIM can measure local structural similarity between two images.
The CC is defined as \cite{lee1988thirteen}
\begin{align}
  CC=\frac{\sum_{n}^{}\left [I_A(n)-\bar{I}_A\right ]\left [I_B(n)-\bar{I}_B\right ]}{\sqrt{\sum_{n}^{}\left [I_A(n)-\bar{I}_A\right ]^2 \sum_{n}^{}\left [I_B(n)-\bar{I}_B\right ]^2}},
\end{align}
where $I_A$ and $I_B$ are images A and B, $\bar{I}_A$ and $\bar{I}_B$ are the mean of A and B, $n$ is the pixel index.
The SSIM quantifies the degree of similarity of two images based on the impact of three characteristics: luminance, contrast and structure. The SSIM of pixel $(x,y)$ in images A and B can be calculated as \cite{wang2004image}
\begin{align}
  SSIM(x,y) = \frac{(2 \mu_{w_A} \mu_{w_B} +c_1)(2 \sigma_{w_Aw_B}+c_2)}{(\mu_{w_A}^2+\mu_{w_B}^2+c1)(\sigma_{w_A}^2+\sigma_{w_B}^2+c_2)},
\end{align}
where $w_A$ and $w_B$ are local neighborhoods centered at $(x,y)$ in images A and B, $\mu_{w_A}$ and $\mu_{w_B}$ are the local means, $\sigma_{w_A}$ and $\sigma_{w_B}$ are the local standard deviations, $\sigma_{w_Aw_B}$ is the covariance, $c_1$ and $c_2$ are two variables to stabilize the division. The mean SSIM ($MSSIM=SSIM/N_{voxel}$) within the brain area can be used as a global measure of the similarity between the synthetic image and the ground truth.

\begin{figure*}
\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[width=0.88\textwidth]{HCP_T1_FA}}
\end{minipage}
\caption{T1-to-FA image translation results for 8 subjects. First row: True T1 images, second row: true FA images, third row: synthetic FA images, fourth row: absolute error of true and synthetic FA. T1 and FA images were normalized to the scale of [0,1]. The absolute error were plotted in the scale of [0, 0.2].}
\label{figure1}
\end{figure*}

\begin{figure*}
\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[width=0.88\textwidth]{HCP_T1_MD}}
\end{minipage}
\caption{T1-to-MD image translation results for 8 subjects. First row: True T1 images, second row: true MD images, third row: synthetic MD images, fourth row: absolute error of true and synthetic MD. T1 and MD images were normalized to the scale of [0,1]. The absolute error were plotted in the scale of [0, 0.1].}
\label{figure2}
\end{figure*}

\section{Results and discussion}
\label{sec:typestyle}

Figure \ref{figure1} shows the qualitative results of T1-to-FA image translation for 8 test subjects. The results show a good match between the synthetic FA images and their ground truth, for both texture of white matter tracts and global content.
However, when compared with the ground truth, it is observed that the synthetic FA images have a reduced level of  details on white matter tracts. The difference image shows the absolute error between the synthetic and true FA images.
Results of T1-to-MD image translation are shown in Figure \ref{figure2}. The synthetic MD images demonstrate great visual similarity to the ground truth. The CSF region and its boundaries are accurately synthesized.

\begin{figure}[H]
\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[width=1\textwidth]{HCP_MSSIM_Corr_FA_MD}}
\end{minipage}
\caption{MSSIM and CC results for 65 subjects. The mean MSSIM of synthetic FA and MD are 0.804 and 0.863, respectively. The mean CC of synthetic FA and MD are 0.939 and 0.972, respectively. Both plots show that synthetic MD images demonstrate better accuracy than synthetic FA.}
\label{figure3}
\end{figure}

Figure \ref{figure3} shows the MSSIM of synthetic FA and MD images for the 65 test subjects. MSSIM values showed high consistency among the different test subjects. The intervals of the MSSIM are $0.804 \pm 0.020$ and $0.863 \pm 0.031$ for synthetic FA and MD images, respectively. The MSSIM results for synthetic MD are better compared to synthetic FA. This may be partly due to that FA images contain richer structure information, thus it is more difficult to synthesize. This conclusion is again confirmed by the plot of CC in which the mean CC of synthetic MD images is 0.972 for the 65 test subjects, while it is 0.939 for the mean CC of synthetic FA images. The intervals of the CC are $0.939 \pm 0.011$ and $0.972 \pm 0.004$ for synthetic FA and MD images, respectively.

\section{Conclusion}
Translation between MR structural and diffusion domains has been shown using Cycle-GAN. The synthetic FA and MD images
are remarkably similar to their ground truth.
Quantitative evaluation using MSSIM and CC of 65 test subjects shows
that
the trained Cycle-GAN works well for all test subjects.

Future research may focus on creating other diffusion-derived scalar maps from more advanced diffusion models, such as mean apparent propagator (MAP) MRI. Also, we will investigate whether training using more than one slice per subject can further improve performance. In this work, we have only used 2D Cycle-GAN, but it has been reported that 3D GANs using spatial information \cite{nie2017medical} across slices yield better mappings between two domains. A comparison study of image translation using 2D and 3D GANs is thus worth looking into.


\section*{Acknowledgements}

This study was supported by Swedish research council grants 2015-05356 and 2017-04889. Funding was also provided by the Center for Industrial Information Technology (CENIIT) at Linkping University, and the Knut and Alice Wallenberg foundation project ”Seeing organ function”. The Nvidia Corporation, who donated the Nvidia Titan X Pascal graphics card used to train the GANs, is also acknowledged.

\vfill
\pagebreak


\bibliographystyle{IEEEbib}
\bibliography{refs}

\end{document}


