\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}


\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}


\begin{document}

\title{Double Refinement Network for Efficient Indoor Monocular Depth Estimation}

\author{
\thanks{These authors contributed equally to this work}
Nikita Durasov\\
Samsung AI Center Moscow,\\
Moscow Institute of \\ 
Physics and Technology \\
(State University)\\
{\tt\small n.durasov@samsung.com}
\and
\footnotemark[1]
Mikhail Romanov\\
Samsung AI Center Moscow \\
{\tt\small m.romanov@samsung.com}
\and
\footnotemark[1]
Valeriya Bubnova\\
Samsung AI Center Moscow, \\
Higher School of Economics \\
(State University) \\
{\tt\small v.bubnova@samsung.com}
\and
Anton Konushin \\
Samsung AI Center Moscow, \\
Moscow State University \\
{\tt\small a.konushin@samsung.com}
}


\maketitle

\begin{abstract}
Monocular Depth Estimation is an important problem of Computer Vision that may be solved with Neural Networks and Deep Learning nowadays. Though recent works in this area have shown significant improvement in accuracy, state-of-the-art methods require large memory and time resources. The main purpose of this paper is to improve performance of the latest solutions with no decrease in accuracy. To achieve this, we propose a Double Refinement Network architecture. We evaluate the results using the standard benchmark RGB-D dataset NYU Depth v2. The results are equal to the current state-of-the-art, while frames per second rate of our approach is significantly higher (up to 15 times speedup per image with batch size 1), RAM per image is significantly lower.
\end{abstract}

\section{Introduction}

Monocular Depth Estimation is an important problem of Computer Vision that may be solved with Neural Networks and Deep Learning nowadays. Besides the interest in the Depth Estimation problem itself for mobile photography and other applications, the network that can produce good depth estimations may be used as a backbone for such problems as Simultaneous Localization and Mapping (SLAM), Visual Odometry (VO), Augmented Reality applications and many others. It also may be used as a base model for the better Stereo Depth Estimation.

Unlike Stereo Depth Estimation, this approach implies depth reconstruction from a single image and does not require usage of stereo cameras. That is the reason why monocular depth estimation is a perspective method for industry uses. One of the examples may be Computer Vision in Robotics, where stereo capturing is associated with such difficulties as non-portable setting and extra expenditures.

However, the main obstacle while solving 2D Depth Reconstruction problem is ambiguity of depth estimation without prior knowledge about sizes and focal length. That seems to be one of the reasons why deep learning methods are popular for solving Monocular Depth Estimation problem.

Though recent works in this area have shown significant improvement in accuracy, we believe that there is still some space for improvement. One of the possible areas for research is memory and time consumption. Typically, such problems are being solved with Deep Convolutional Neural Networks, which succeed in depth detection given only the image. However, along with convolutional layers, big amount of bilinearly interpolated deep feature maps play important role in the architecture. Such layers require large memory and time resources. That is one of the most important drawbacks of the latest state-of-the-art solutions. The main purpose of this paper is to improve performance of the latest solutions with no decrease in accuracy. We evaluate the results on standard benchmark RGB-D dataset NYU Depth v2.

The problem is stated as a regression problem of point-wise depth reconstruction. Ground truth for $H \times W \times 3$ image is represented as $H \times W \times 1$ depth map. In other words, we have to make depth estimation for every pixel.

The paper plan is the following: in the next section we will overview recent works addressing the problem of Monocular Depth Estimation, section 3 contains the description of the proposed solution and the experiments can be found in Section 4. Finally, we summarize the paper in Section 5.


\section{Related Work Overview}

\subsection{Before Deep Learning}

The Depth Estimation is not a new research field. Scientists were interested
in this area before Deep Learning became a leading technology in Computer Vision
area. And even though Deep Learning took over the traditional Computer Vision
around 2012-2013, for a long time the dominating methods in Depth Estimation were based
on other techniques.

A widely known algorithm was proposed by Saxena et al. \cite{saxena2009make3d}. They use Markov Random Field for oversegmentation of the image. "Superpixels" obtained by the algorithm are used for building a 3D structure of the image by inferring 3D  position and orientation of the 3D surface that it came from. Geometry-based algorithms for indoor Depth Estimaiton were introduced in \cite{hedau2010thinking} and \cite{gupta2010estimating}: they base their models on several assumptions about object orientation in a room with a known shape and some prior knowledge of usual objects placement in such rooms. Gupta et al. use SVM to compute a feature vector for each surface.  \cite{hoiem2005geometric} presented a system that models geometric classes that depend on the orientation of a physical scene and build the geometric structure progressively. Geometry-based approaches work with certain constraints and are relevant for only narrow type of images, such as images of a regular room in a house.



\subsection{Deep Learning Era}

Most recent works on Depth Reconstruction are based on Deep Convolutional Neural Networks \cite{eigen2015predicting, krizhevsky2012imagenet, kuznietsov2017semi, laina2016deeper, liu2016learning}. It is a common practice to use pretrained layers (ex. VGG \cite{simonyan2014very}, SENet \cite{hu2017squeeze} or ResNet \cite{he2016deep}) for feature extraction. Another noticeable trend is modifying encoder-decoder architectures. In such architectures convolutions and max-pooling layers are widely used for implementing encoder and upsampling layers often play role of a decoder.
Here we overview the latest works with the highest benchmark results.

In \cite{gurram2018monocular} Gurram et al. proposed a solution which includes training a model with use of two heterogeneous datasets, which are datasets with depth and semantic map labels. Common NN layers are trained alternately on data from either of the datasets. In particular, the network consists of two flows that can be used for getting depth or semantic map of an image respectively.

In the work by Fu et al. \cite{fu2018deep} DCNN architecture performs Monocular Depth Estimation formulated as ordinal regression problem, i.e. it works with discretized depth. The architecture includes dense feature extractor (convolutions and dense layers), scene understanding modular (concatenation of dilated convolutions, full-image encoder and convolutions layers) and ordinal regression block.

The ambiguity caused by focal length mentioned above was studied by He et al. \cite{he2018learning}. They suggested that using datasets with various focal lengths may be helpful in handling this issue. The proposed method implies generating multiple images with different focal lengths from one fixed-focal-length image. Thus, the model is trained on varying-focal-length dataset. Embedded focal lengths is given to a network as well as the images.

We should also mention another network that does Depth Estimation in real-time presented by Spek et al. \cite{spek2018cream}.

\subsection{Main Competitor}

We base on the work of Hu et al. \cite{hu2018revisiting}. This method is based
on utilizing residual network activation maps from all the network layers interpolated to the size
of the input image concatenate next to the extra final activation maps of the
custom decoder. This network shows good performance on the majority of the common metrics.

\section{Method}

We improve the Main Competitor's solution by introducing the following:
\begin{itemize}
    \item New Double Refinement Network Architecture
    \item Auxiliary Losses
\end{itemize}

\subsection{Double Refinement Network Architecture}

The architecture of the proposed network can be seen at Figure \ref{fig:w_net}.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{images/w_net.png}
    \caption{Double Refinement Network architecture. It contains one downsampling branch based on the backbone (yellow branch)
    and two upsampling branches(red and green branches). Red branch (\texttt{up I}) merges the high-level features together 
    with low-level features. The upsampling in it is done by Pixel Shuffle. 
    The green branch (\texttt{up II}) predicts only the Depth Map. In the right branch the upsampling is performed 
    by Bilinear Interpolation. The \texttt{up II} branch outputs a depth approximation 
    on each level, thus we can compute loss
    function on every level and force the high layers to learn as efficiently as low levels. The arrows that are marked with red color are referred to as diagonal connections in the text.}
    \label{fig:w_net}
\end{figure*}

\subsection{Drawbacks of State-of-the-Art Architectures}

Current State-of-the-Art architectures are very time and memory consuming.
Some of the architectures (such as Main Competitor's architecture) use Bilinear Interpolation
of the deep activation maps. This procedure is very memory hungry. Talking about standard
ResNet (\textgreater 50) architecture applied to an image of size $224 \times 224$, 
deep layers give $64 \times 112 \times 112$, $256 \times 56 \times 56$, 
$512 \times 28 \times 28$, $1024 \times 14 \times 14$, $2048 \times 7 \times 7$.

Now, in case we follow the Main Competitor's strategy, we have to upsample all of
the activation maps to half of the size of the original image and concatenate results.
The resulting tensor will contain $3904 \times 224 \times 224$ values! More than that,
it is necessary to make some operations with this tensor to get the depths. This
leads to low fps rate.

Speaking about other segmentation networks that are also popular for depth estimation
problem, bilinear interpolation is also used in those architectures. Besides, 
PSP-Net \cite{zhao2017pyramid} and DeepLab v3 \cite{chen2017rethinking}, \cite{chen2018encoder} 
use dilated convolutions that are also memory and time consuming.
The only architecture that gives good fps rate is the RefineNet architecture \cite{lin2017refinenet} that
may be reinterpreted as U-net based on ResNet.

In our architecture we get rid of dilated convolutions and bilinear interpolation of
big amounts of activation maps. Instead, we do Pixel Shuffle upsampling of the high-level features.
But in case we do only this then we loose in accuracy. To avoid this, we follow the logic
that the high-level outputs are profitable for coarse structure, while the low-level outputs
should only refine coarser guess. To compute the correction, we need less information. Thus, we do
not need to make bilinear upsampling of deep tensors. Thus, we iteratively improve our coarse guess
for the depth from layer to layer (we increase the depth map side by factor of $2$ for each layer) 
making it finer and finer without doing time and memory complex operations.

\subsubsection{Architecture Description}

\begin{table*}
    \begin{center}
    \begin{tabular}{ccccccccccc}
    \hline
    & RMSE & Log10 & $\delta_{1.25}$ & $\delta_{1.25 ^ 2}$ & $\delta_{1.25^3}$  \\
    \hline
\hline

\hline

Laina et al. \cite{laina2016deeper} & 0.573 & 0.055 & 0.811 & 0.953 & 0.988 \\
Fu et al. \cite{fu2018deep} & \textbf{0.509} & 0.051 & 0.828 & 0.965 & 0.992 \\
Spek et al. \cite{spek2018cream} & 0.687 & 0.161 & 0.704 & 0.917 & 0.977 \\
RSIDE ResNet-50 (paper) & 0.555 & 0.054 & 0.843 & 0.968 & 0.991 \\
RSIDE DenseNet-161 (paper) & 0.544 & 0.053 & 0.855 & 0.972 & 0.993 \\
RSIDE SENet-154 (paper) & 0.530 & \textbf{0.050} & 0.866 & \textbf{0.975} & 0.993 \\
RSIDE SENet-154 (reproduced) & 0.574 & 0.052 & 0.845 & 0.967 & 0.991 \\

\hline
(ours) DRNet ResNet-50  & 0.587 & 0.061 & 0.810 & 0.961 & 0.990 \\
(ours) DRNet ResNet-152 & 0.528 & 0.050 & 0.866 & 0.973 & 0.993 \\
(ours) DRNet DenseNet-161 & 0.534 & 0.052 & 0.865 & 0.974 & \textbf{0.994} \\
(ours) DRNet SeNet-154 & 0.527 & \textbf{0.050} & \textbf{0.868} & \textbf{0.975} & 0.993 \\
\hline
\end{tabular}
\end{center}
\caption{Comparison of our network with different backbones against
Revisiting Single Image Depth Estimation's (RSIDE) \cite{hu2018revisiting} and others' results. 
It is observable that our network gives around the same result 
(it is sometimes a bit higher though). We should note that with ResNet-50 backbone our network 
does not reach the same results as in the Main Competitor's work. 
Possibly, this can be fixed by doing more accurate fine-tuning.}
\label{fig:table_of_results}
\end{table*}

\begin{table*}
    \begin{center}
    \begin{tabular}{ccccccccccc}
    \hline
    & fps (b 1) & fps (max b) & net RAM & RAM / img \\
    \hline
\hline

\hline
RSIDE ResNet-50 (reproduced) & 2 & 2 (b 1) & 793 Mb & 21 Gb \\
RSIDE DenseNet-161 (reproduced) & 1.7 & 1.7 (b 1) & 853 Mb & 22 Gb  \\
RSIDE SENet-154 (reproduced) & 1.5 & 1.5 (b 1) & 1.7 Gb & 21.9 Gb \\

\hline
(ours) DRNet ResNet-50 & 36 & 69 (b 36) & 757 Mb & 2 Gb\\
(ours) DRNet ResNet-152 & 25 & 55 (b 19) & 1067 Mb & 2.9 Gb \\
(ours) DRNet DenseNet-161 & 17 & 33 (b 14) & 839 Mb & 3.2 Gb \\
(ours) DRNet SeNet-154 & 6 & 13 (b 9) & 1421 Mb & 4 Gb \\
\hline
\end{tabular}
\end{center}
\caption{Comparison of our network with different backbones against the 
Revisiting Single Image Depth Estimation's (RSIDE) \cite{hu2018revisiting} fps and memory. 
It is observable that our network uses less RAM per image and fps rate is higher. 
FPS rates are measured using Tesla P40 GPU. In this table "b" stands for the batch size. }
\label{fig:speed_comparison}
\end{table*}

\begin{table*}
    \begin{center}
    \begin{tabular}{cccccccccc}
    \hline
    & RMSE & $\delta_{1.25}$ & $\delta_{1.25 ^ 2}$ & $\delta_{1.25^3}$ \\
    \hline
\hline

\hline
 DRNet ResNet-152 & 0.528 & 0.866 & 0.972 & 0.993 \\
 DRNet ResNet-152 no diagonal connections & 0.533 & 0.866 & 0.973 & 0.993 \\
 DRNet ResNet-152 no auxiliary losses & 0.544 & 0.860 & 0.971 & 0.992 \\
 DRNet ResNet-152 no second branch & 1.900 & 0.351 & 0.596 & 0.732 \\
\hline
\end{tabular}
\end{center}
\caption{Reasoning of new architecture's elements. As we can see, diagonal connections do not
improve the result, auxiliary losses improves the result a bit.
The biggest improvement gives the presence of the second upsampling branch.}

\label{fig:elements_reasoning}
\end{table*}

\begin{table*}[h!]
    \begin{center}
        \centering
        \begin{tabular}{c c c c c}
        
             \rotatebox{90}{~ ~ ~ ~ ~ ~Image} & \includegraphics[width=0.2\textwidth]{images/image_0.jpg} & \includegraphics[width=0.2\textwidth]{images/image_2.jpg} & \includegraphics[width=0.2\textwidth]{images/image_4.jpg} & \includegraphics[width=0.2\textwidth]{images/image_6.jpg}\\
             \rotatebox{90}{~ ~ ~ ~ Predicted} & \includegraphics[width=0.2\textwidth]{images/pred_depth_0.jpg} & \includegraphics[width=0.2\textwidth]{images/pred_depth_2.jpg} & \includegraphics[width=0.2\textwidth]{images/pred_depth_4.jpg} & \includegraphics[width=0.2\textwidth]{images/pred_depth_6.jpg}\\
             \rotatebox{90}{~ ~ Ground Truth} & \includegraphics[width=0.2\textwidth]{images/gt_depth_0.jpg} & \includegraphics[width=0.2\textwidth]{images/gt_depth_2.jpg} & \includegraphics[width=0.2\textwidth]{images/gt_depth_4.jpg} & \includegraphics[width=0.2\textwidth]{images/gt_depth_6.jpg}\\
             
        \end{tabular}
    \end{center}
    \caption{Network predictions compared to the ground truth.}
    \label{fig:visual_results}
\end{table*}

While reading this part we strongly recommend to keep an eye on the Figure \ref{fig:w_net}.

We base our network on the Residual Network architecture introduced by He
et al. \cite{he2016deep}. This architecture is known for its high results
on the ImageNet dataset. More specifically, we take pretrained
ResNet-50, ResNet-152, DenseNet-161 \cite{huang2017densely} architectures
from the PyTorch torchvision library \cite{paszke2017automatic} and 
SENet-154 \cite{hu2018revisiting}.

The schema on the figure above is designed for pretrained ResNet $\geq$ 
50 backbone, but the logic for the other networks is the same. 
The network consists of one downsampling branch (BackBone) and two up branches. 
We will use the following notation: $\texttt{down}_i$ is a result of the
\texttt{down} branch on the level $i$. For the upsampling
branches we will count from the top, $i$-th block of the \texttt{upI} branch 
we will denote as \texttt{upI$_i$}.

We take the pretrained network as is, cutting off only the last Fully Connected layer
and AveragePooling layer. Thus, we get the backbone for our network that
we build on. The network is split into several residual blocks
(\texttt{layer 1,2,3} and \texttt{4}) plus several preprocessing elements:

\begin{itemize}
  \item \texttt{Conv2D(7 $\times$ 7, 3 $\rightarrow$ 64, stride 2)}
  \item \texttt{BatchNorm2D(64)}
  \item \texttt{ReLU}
  \item \texttt{MaxPool2D(3 $\times$ 3, stride 2)}
\end{itemize}

This preprocessing block we denote as \texttt{layer 0}. Note that the preprocessing block
downsamples the image's side 4 times.


Then the \texttt{down} branch of the network can be represented as:
\begin{itemize}
  \item \texttt{layer 0}
  
  \dots
  
  \item \texttt{layer 4}
\end{itemize}

We will denote the input image as $\texttt{input}$, the output of the
\texttt{layer 0} as $\texttt{down}_1$, 
the output of \texttt{layer i} as $\texttt{down}_{i+1}$.
We do the forward propagation through the BackBone branch as it is done in 
the backbone networks.

The next step is encouraged by the U-Network architecture introduced by
Ronneberger et al. \cite{ronneberger2015u} and RefineNet.

To start describing the upsampling process, we denote $\texttt{down}_5$
as $\texttt{upI}_5$ for convenience.

To get the most coarse estimation of the depths $\texttt{upII}_5$
we apply \texttt{Conv2D} 1 $\times$ 1 to the $\texttt{upI}_5$.

This coarse depth approximation takes into account
only the high-level outputs (note that in this output high-level features
are entangled with low-level features due to the residual connections).

Now to get the $\texttt{upI}_i$, we take $\texttt{upI}_{i+1}$ tensor, 
performing the following operations to it:

\begin{itemize}
  \item \texttt{Conv2d 1$\times$1} (except $\texttt{upI}_{5}$ block)
  \item \texttt{BatchNorm2D} (except $\texttt{upI}_{5}$ block)
  \item \texttt{ReLU} (except $\texttt{upI}_{5}$ block)
  \item \texttt{PixelShuffle $\times$2} (except $\texttt{upI}_{1}$ block; for $\texttt{upI}_{0}$ block -- \texttt{PixelShuffle $\times 4$}).
\end{itemize}

The output is concatenated  with depth approximation $\texttt{upII}_{i + 1}$ upsampled by
factor of 2 using bilinear interpolation.

The depth approximation $\texttt{upII}_{i}$ is the coarser depth
$\texttt{upII}_{i + 1}$ upsampled by factor of $2$ using Bilinear Interpolation 
plus the correction. The correction is computed from
$\texttt{upII}_{i}$ concatenated with downsampled image and upsampled coarser depth 
using \texttt{Conv2D 1$\times$1} with one output activation.

It is worth noting that the second upsampling branch of the network
\texttt{upII} is inspired by Fourier Transform. We first make a coarse
prediction and then, ascending from the deepest levels to the shallow ones,
we make the corrections to those predictions. Each level increases the side of an
image by a factor of $2$.
To get the finest depth correction, we take the input image, concatenate it
with 4 activation maps and depth $\texttt{upII}_1$ and apply a convolution. 


\subsubsection{Relation to the Main Competitor's Network}

The Double Refinement Network architecture can be converted easily to the network from
the work \cite{hu2018revisiting} by doing the following:

\begin{itemize}
    \item Substitute the upsampling blocks in \texttt{upI} with Bilinear Interpolation
    \item Remove the \texttt{upII} branch of the network, leave only the finest output.
    \item Add extra decoder subnetwork.
\end{itemize}

From this we can see that doing some simple manipulations, we can
come back to the network that gives the current State-of-the-Art result.

But doing bilinear interpolation of the residual blocks' activation maps
to the size of the original image, as it is done in the Main Competitor's work,
is suboptimal as this procedure is
computationally complex, the resulting tensor is large and does not change much from
pixel to pixel (especially when we interpolate the \texttt{level 4}
block's features).

\subsubsection{Auxiliary Loss Functions}

We have
added the auxiliary losses on each of the outputs. We take the $i$-th level depth
and penalize it as we do it for the finest depth approximation (the target is downsampled
$2^i$ times).

Thus, the overall loss function is computed as:
\begin{equation}
    \mathcal{\tilde{L}} = \sum_{level=0}^5 \mathcal{L}_{level}.
\end{equation}

We downsample the target depth using bilinear interpolation to compute the level-specific loss.
We have also tried upsampling the level-specific result using bilinear interpolation -- 
and that worked a little bit worse than downsampling the target.

\subsection{Loss Function}

Depth estimation requires not only pixel-wise accuracy, but also spatially 
coherent result. That is the reason why many models use losses that includes 
depth gradient and normals. The loss we use consists of four parts:

\begin{equation}
    \mathcal{L} = \mathcal{L}_{depth} + \mathcal{L}_{grad} + \mathcal{L}_{normal}.
\end{equation}

These loss functions were already presented in the 
Main Competitor's work, we take them as is. 

\section{Results}

We have done our experiments on the NYUv2 dataset. We based our code on the
Main Competitor's code {\footnote{\url{https://github.com/JunjH/Revisiting_Single_Depth_Estimation}}} 
in order to show that our network gives the reported
results in \textit{exactly} the same settings and to avoid result deviations caused
by unrelated code changes.

We use the following common metrics to compare our model to the State-of-the-Art
model:

\begin{itemize}
\item Root mean squared error: 
\begin{equation}
    \sqrt{\frac{1}{|N|} \sum_{i \in N} |d_i - d_i^*|^2}
\end{equation}
\item Log 10 error:
\begin{equation}
  \frac{1}{|N|} \sum_{i \in N} |\log_{10}(d_i) - log_{10}(d_i^*)|
\end{equation}
\item Threshold ($\delta$):
\begin{equation}
    \text{\% of $d_i$ s.t.} \max{\left(\frac{d_i}{d_i^*}, \frac{d_i^*}{d_i}\right) < t},
\end{equation}
where
$t \in {1.25, 1.25^2, 1.25^3}$
\end{itemize}

We test our network against the Main Competitor's network and some other
related works. We thank our Main Competitor for sharing the original reproducible and
clear code.

We train the network in the following way: we take pretrained backbone network. The
training is performed using Adam algorithm (amsgrad modification) with learning rate
$10^{-4}$, weight decay $10^{-4}$. The betas are selected to be default.

From the numerical results (see Table \ref{fig:table_of_results}, \ref{fig:speed_comparison}) it is possible to see that the new architecture gives close metrics to the Main Competitor's architecture values with higher fps rate. Also, the new architecture consumes significantly less memory per image due to the fact that we do not make \textit{that} many bilinearly upsampled feature maps. The visual results can be found in the Table \ref{fig:visual_results}.

We should note that during our experiments we used to get a bit higher values than the Main Competitor's architecture. We doubt that those deviations are statistically important. More than that, we should note that in the dataset there are many errors in the density maps. For example, in many cases the straight lines that are straight on the images are not straight on the depth maps. This causes the estimated depth maps to be blurry and very inaccurate. Possibly is a good idea to train the depth estimation on
a simulated data.

\subsection{Element Reasoning}

In our approach we have introduced several new elements to the network.
To be sure that every element that we introduce improves the result, 
we do element reasoning experiments on the Table \ref{fig:elements_reasoning}.

As we can see, the largest improvement gives the second upsampling branch, thus
this element is the most important in the presented architecture.
The auxiliary losses improve result slightly, but throughout our experiments there
was no cases when it without those losses metrics were higher. Thus, 
auxiliary losses are also an important
element. The diagonal connections' efficiency is under question as it sometimes
improve metrics a little bit, but sometimes it doesn't. In cases when diagonal connections
improve the result -- the improvement is very small, when it doesn't -- the losses in metrics 
are very small. Thus we have to admit that it is possible that those connections are useless.



\section{Summary}

We have introduced a new architecture for Monocular Depth Estimation. The network works
significantly (15 times) faster on single image, gives Real-Time fps rate, 
uses 10 times less memory per image.
Plus it gives the results comparable to the current State-of-the-Art. 
We have shown which parts of
our network improve the result and how big the improvement is.




{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}


