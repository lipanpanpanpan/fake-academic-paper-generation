\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{authblk}


\usepackage{graphicx} % more modern
\usepackage{caption}
\usepackage{subcaption}

\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{xcolor}
\usetikzlibrary{arrows}
\usepackage{hyperref}
\allowdisplaybreaks

\usepackage{mathrsfs}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{hyperref}
\usepackage{bm,todonotes}



\newcommand{\vct}{\boldsymbol }
\newcommand{\rnd}{\mathsf}
\newcommand{\ud}{\mathrm d}
\newcommand{\nml}{\mathcal{N}}
\newcommand{\loss}{\mathcal{L}}
\newcommand{\hinge}{\mathcal{R}}
\newcommand{\kl}{\mathrm{KL}}
\newcommand{\cov}{\mathrm{cov}}
\newcommand{\dir}{\mathrm{Dir}}
\newcommand{\mult}{\mathrm{Mult}}
\newcommand{\err}{\mathrm{err}}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\argmin}{\mathrm{argmin}}
\newcommand{\argmax}{\mathrm{argmax}}
\newcommand{\poly}{\mathrm{poly}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\conv}{\mathrm{conv}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\diag}{\mat{diag}}
\newcommand{\acc}{\mathrm{acc}}

\newcommand{\aff}{\mathrm{aff}}
\newcommand{\range}{\mathrm{Range}}
\newcommand{\Sgn}{\mathrm{sign}}

\newcommand{\hit}{\mathrm{hit}}
\newcommand{\cross}{\mathrm{cross}}
\newcommand{\Left}{\mathrm{left}}
\newcommand{\Right}{\mathrm{right}}
\newcommand{\Mid}{\mathrm{mid}}
\newcommand{\bern}{\mathrm{Bernoulli}}
\newcommand{\ols}{\mathrm{ols}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\opt}{\mathrm{opt}}
\newcommand{\ridge}{\mathrm{ridge}}
\newcommand{\unif}{\mathrm{unif}}
\newcommand{\Image}{\mathrm{im}}
\newcommand{\Kernel}{\mathrm{ker}}
\newcommand{\supp}{\mathrm{supp}}
\newcommand{\pred}{\mathrm{pred}}
\newcommand{\distequal}{\stackrel{\mathbf{P}}{=}}
\newcommand{\gege}{{A(\vect{w},\vect{w}_*)}}
\newcommand{\gele}{{A(\vect{w},-\vect{w}_*)}}
\newcommand{\lele}{{A(-\vect{w},-\vect{w}_*)}}
\newcommand{\lege}{{A(-\vect{w},\vect{w}_*)}}
\newcommand{\firstlayer}{\mathbf{W}}
\newcommand{\firstlayerWN}{v}
\newcommand{\secondlayer}{a}
\newcommand{\inputvar}{\vect{x}}
\newcommand{\anglemat}{\mathbf{\Phi}}
\newcommand{\holder}{H\"{o}lder}

\def\R{\mathbb{R}}
\def\Z{\mathbb{Z}}
\def\cA{\mathcal{A}}
\def\cB{\mathcal{B}}
\def\cD{\mathcal{D}}
\def\cE{\mathcal{E}}
\def\cF{\mathcal{F}}
\def\cG{\mathcal{G}}
\def\cH{\mathcal{H}}
\def\cI{\mathcal{I}}
\def\cL{\mathcal{L}}
\def\cM{\mathcal{M}}
\def\cN{\mathcal{N}}
\def\cP{\mathcal{P}}
\def\cS{\mathcal{S}}
\def\cT{\mathcal{T}}
\def\cW{\mathcal{W}}
\def\cZ{\mathcal{Z}}
\def\bP{\mathbf{P}}
\def\TV{\mathrm{TV}}
\def\MSE{\mathrm{MSE}}

\def\vw{\mathbf{w}}
\def\va{\mathbf{a}}
\def\vZ{\mathbf{Z}}

\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\expect}{\mathbb{E}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\prox}[2]{\textbf{Prox}_{#1}\left\{#2\right\}}
\newcommand{\event}[1]{\mathscr{#1}}
\newcommand{\set}[1]{#1}
\newcommand{\diff}{\text{d}}
\newcommand{\difference}{\triangle}
\newcommand{\inputdist}{\mathcal{Z}}
\newcommand{\indict}{\mathbb{I}}
\newcommand{\rotmat}{\mathbf{R}}
\newcommand{\normalize}[1]{\overline{#1}}
\newcommand{\vectorize}[1]{\text{vec}\left(#1\right)}

\newcommand{\relu}[1]{\sigma\left(#1\right)}
\newcommand{\reluder}[1]{\sigma'\left(#1\right)}
\newcommand{\act}[1]{\sigma\left(#1\right)}

\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}{Lemma}[section]
\newtheorem{cor}{Corollary}[section]
\newtheorem{prop}{Proposition}[section]
\newtheorem{asmp}{Assumption}[section]
\newtheorem{defn}{Definition}[section]
\newtheorem{fact}{Fact}[section]
\newtheorem{conj}{Conjecture}[section]
\newtheorem{rem}{Remark}[section]
\newtheorem{example}{Example}[section]


\newcommand{\simon}[1]{\textcolor{blue}{[Simon: #1]}}




\title{Improved Learning of One-hidden-layer Convolutional Neural Networks with Overlaps}
\author[1]{Simon S. Du}
\author[2]{Surbhi Goel}
\affil[1]{Machine Learning Department, Carnegie Mellon University}
\affil[2]{Department of Computer Science, University of Texas at Austin}

\begin{document}

\maketitle
\begin{abstract}
We propose a new algorithm to learn a one-hidden-layer convolutional neural network where both the convolutional weights and the outputs weights are parameters to be learned. Our algorithm works for a general class of (potentially overlapping) patches, including commonly used structures for computer vision tasks. Our algorithm draws ideas from (1) isotonic regression for learning neural networks and (2) landscape analysis of non-convex matrix factorization problems. We believe these findings may inspire further development in designing provable algorithms for learning neural networks and other complex models.

\end{abstract}

\section{Introduction}
\label{sec:intro}
Giving provably efficient algorithms for learning neural networks is a core challenge in machine learning theory.  Convolutional architectures have recently attracted much interest due to their many practical applications.  Recently \citet{brutzkus2017globally} showed that distribution-free learning of one simple non-overlapping convolutional filter is NP-hard.  A natural open question is whether we can design provably efficient algorithms to learn convolutional neural networks under mild assumptions. 

We consider a convolutional neural network of the form
\begin{align}
f\left(\vect{x},\vect{w},\vect{a}\right)= \sum_{j=1}^{k} a_j \relu{\vect{w}^\top\mat{P}_j\vect{x}} \label{eqn:cnn_structure}
\end{align}
where $\vect{w} \in \mathbb{R}^r$ is a shared convolutional filter, $\vect{a} \in \mathbb{R}^{k}$ is the second linear  layer and\begin{align*}
\mat{P}_j = [
\underbrace{\mat{0}}_{(j-1)s}  \underbrace{\mat{I}}_r  \underbrace{\mat{0}}_{d-(j-1)s+r}]\in \mathbb{R}^{r \times d}
\end{align*} 
selects the $((j-1)s+1)$-th to $((j-1)s+r)$-th coordinates of $\vect{x}$ with stride $s$ and $\sigma\left(\cdot\right)$ is the activation function.
Note here that both $\vect{w}$ and $\vect{a}$ are unknown vectors to be learned.
Further, in our model, there may be overlapping patches because the stride size $s$ may be smaller than the filter size $r$.

We give the {\emph first} efficient algorithm that can provably learn a convolutional neural network with \emph{two} unknown layers with commonly used overlapping patches. Our main result is the following theorem.
\begin{thm}[Main Theorem (Informal)]
	\label{thm:main_informal}
	Suppose $s \ge \lfloor \frac{r}{2} \rfloor + 1$ and the marginal distribution is symmetric and isotropic.  Then the convolutional neural network of the form~\eqref{eqn:cnn_structure} with piecewise linear activation functions is learnable in polynomial time.
\end{thm}
We refer readers to Theorem~\ref{thm:main} for the precise statement.  

\paragraph{Technical Insights}
Our algorithm is a novel combination of isotonic regression and the landscape analysis of non-convex problems.
First, inspired by recent work on isotonic regression, we extend the idea in \cite{goel2018learning} to reduce learning a convolutional neural network with piecewise linear activation to learning a convolutional neural network with \emph{linear} activation (c.f. Section~\ref{sec:analysis}).
Second, we show learning a linear convolutional filter can be reduced to a non-convex matrix factorization problem which admits a provably efficient algorithm based on non-convex geometry~\citep{ge2017no}.
Third, in analyzing our algorithm, we present a robust analysis of Convotron algorithm proposed by~\citet{goel2018learning}, in which we draw connections to the spectral properties of Toeplitz matrices.
We believe these ideas may inspire further development in designing provable learning algorithms for neural networks and other complex models.

\paragraph{Related Work}\label{sec:rel}\paragraph{Organization}
This paper is organized as follows.
In Section~\ref{sec:rel} we review previous work on provably learning neural networks 
In Section~\ref{sec:pre} we formally state our setup.
In Section~\ref{sec:main}, we present our algorithm and our main theorem.
In Section~\ref{sec:analysis}, we illustrate our main analysis ideas.
We conclude and list future directions in Section~\ref{sec:con}.
We defer most of our technical proofs to the appendix.



\section{Preliminaries}
\label{sec:pre}
We use bold-faced letters for vectors and matrices.
We use $\norm{\cdot}_2$ to denote the Euclidean norm of a finite-dimensional vector.
For a matrix $\mat{A}$, we use $\lambda_{\max}\left(\mat{A}\right)$ to denote its eigenvalue and $\lambda_{\min}\left(\mat{A}\right)$ its smallest singular value.
Let $O(\cdot)$ and $\Omega\left(\cdot\right)$ denote standard Big-O and Big-Omega  notations, only hiding absolute constants.

In our setting, we have $n$ data points $\left\{\vect{x}_i,y_i\right\}_{i=1}^n$ where $\vect{x}_i \in \mathbb{R}^d$ and $y \in \mathbb{R}$.
We assume the label is generated by a one-hidden-layer convolutional neural network with filter size $r$, stride $s$ and $k$ hidden neurons.
Compactly we can write the formula in the following form:
 \[
y_i = f(\vect{x}_i,\vect{w}^*,\vect{a}^*),\quad \vect{x}_i \sim \inputdist
\] where  the prediction function $f$  is defined in Equation~\eqref{eqn:cnn_structure}.
To obtain a proper scaling, we let $\norm{\vect{w}^*}_2\norm{\vect{a}^*}_2 = \sigma_1$.
We also define the induced patch matrix as \[
\mat{P}\left(\vect{x}\right) = \begin{bmatrix}
\mat{P}_1\vect{x} & \ldots & \mat{P}_k \vect{x}
\end{bmatrix} \in \mathbb{R}^{r \times k}
\]
which will play an important role in our algorithm design.
Our goal is to properly learn this convolutional neural network, i.e., design a polynomial time algorithm which outputs a pair $(\vect{w},\vect{a})$ that satisfies \begin{align*}
	\expect_{\vect{x} \sim \inputdist}\left[ \left(f(\vect{w},\vect{a},\vect{x})-f(\vect{w}^*,\vect{a}^*,\vect{x})\right)^2\right] \le \epsilon.
\end{align*}

\section{Main Result}
\label{sec:main}
In this section we describe our main result. We first list our main assumptions, followed by the detailed description of our algorithm. Lastly we state the main theorem  which gives the convergence guarantees of our algorithm.
\subsection{Assumptions}\label{sec:assumptions}
Our first assumption is on the input distribution $\inputdist$.
We assume the input distribution is symmetric, bounded and has identity covariance.
\begin{asmp}[Input Distribution Assumptions]
\label{asmp:input_dist}
We assume the input distribution satisfies the following conditions.
\begin{itemize}
\item Symmetry: $\prob\left(\vect{x}\right) = \prob\left(-\vect{x}\right).$
\item Identity covariance: $\expect_{\vect{x}\sim \inputdist }\left[\vect{x}\vect{x}^T\right] = \mat{I}$.
\item Boundedness: $\forall \vect{x}\sim \inputdist, \norm{\vect{x}}_2 \le B$ almost surely for some $B > 0$.
\end{itemize}
\end{asmp}
The symmetry assumption is used in \cite{goel2018learning} and many learning theory papers~\cite{baum1990polynomial}. The identity covariance assumption is true if the data is whitened. 
Further, in many architectures, the input of certain layers is assumed to have these properties because of the use of batch normalization~\citep{ioffe2015batch}.
Lastly, the boundedness is a standard regularity assumption to exclude pathological input distributions.
We remark that this assumption is considerably weaker than the standard Gaussian input distribution assumption used in \cite{tian2017analytical,zhong2017learning,du2017spurious}, which has the rotational invariant property.



Our second assumption is on the patch structure.
In this paper we assume the stride is larger than half of the filter size.
\begin{asmp}[Large Stride]
\label{asmp:large_stride}
$s \ge \lfloor \frac{r}{2}\rfloor + 1$.
\end{asmp}
This is indeed true for a wide range of convolutional neural network used in computer vision.
For example some architectures have convolutional filter of size $3$ and stride $2$ and some use non-overlapping architectures~\citep{he2016deep}.



Next we assume the activation function is piecewise linear. 
\begin{asmp}[Piece-wise Linear Activation]
  \label{asmp:activation}
  \[\sigma(x) = \begin{cases}
  x &\text{ if } x \ge 0\\
  \alpha x &\text{ if } x < 0
  \end{cases}.
  \]
\end{asmp}
Commonly used activation functions like rectified linear unit (ReLU), Leaky ReLU and linear activation all belong to this class.

Now we are ready to describe our algorithm (see Algorithm \ref{algo:main}). The algorithm has three stages, first we learn the outer layer weights upto sign, second we use these fixed outer weights to recover the filter weight and last we choose the best weight combination thus recovered.

\begin{algorithm}[t]
\floatname{algorithm}{Algorithm}
  \caption{Learning One-hidden-Layer Convolutional Network\label{algo:main}}
  \begin{algorithmic}[1]
    \Statex \textbf{Input}: Input distribution $\inputdist$. Number of iterations: $T_1, T_2$. Number of samples: $T_3$% \Statex \textbf{Input}: Three set of samples $\mathcal{S}_1 = (\textbf{x}^{i},y^{i})_{i=1}^{T_1}$, $\mathcal{S}_2 = (\textbf{x}^{i},y^{i})_{i=T_1 + 1}^{T_1+T_2}$ and
    Step sizes: $\eta_1 > 0$, $\eta_2 > 0$.
   \Statex \textbf{Output}: Parameters of the one-hidden-layer CNN: $\vect{w}$ and $\vect{a}$.
     \State \textbf{Stage 1}: Run Double Convotron (Algorithm \ref{alg:doubleconvotron}) for $T_1$ iterations with step size $\eta_1$ to obtain $\vect{a}^{(T_1)}$.
     \State \textbf{Stage 2}: Run Convotron (Algorithm~\ref{alg:convotron})  using  $\vect{a}^{(T_1)}$ and $-\vect{a}^{(T_1)}$ for $T_2$ iterations and step size $\eta_2$ to obtain $\vect{w}^{(+)}$ and $\vect{w}^{(-)}$.
   \State \textbf{Stage 3}: Choose parameters with lower empirical loss on $T_3$ samples drawn from $\inputdist$ from $\left(\vect{w}^{(+)},\vect{a}^{(T_1)}\right)$ and $\left(\vect{w}^{(-)},-\vect{a}^{(T_1)}\right)$.
\end{algorithmic}
\end{algorithm}%\surbhi{We can add the exact algorithm descriptions later}\paragraph{Stage 1: Learning the Non-overlapping Part of the Convolutional Filter and Linear Weights}
Our first observation is even if there may be overlapping patches, as long as there exists some non-overlapping part, we can learn this part and the second layer jointly.
To be specific, with filter size being $r$ and stride being $s$, if $s \ge \lfloor \frac{r}{2}\rfloor + 1$, for $j=1.\ldots,k$ we define the selection matrix for the non-overlapping part of each patch\begin{align*}
\mat{P}_j^{non} = 
[\underbrace{\mat{0}}_{js}  \underbrace{\mat{I}}_{r-2s}   \underbrace{\mat{0}}_{d-(j-2)s-r}] \in \mathbb{R}^{(r-2s) \times d}.
\end{align*} 
Note that for any $j_1 \neq j_2$, there is no overlap between the selected coordinates by $\mat{P}_{j_1}^{non}$ and 
$\mat{P}_{j_2}^{non}$. 
Therefore, for a filter $\vect{w}$, there is a segment $\left[w_{r-s+1},\ldots,w_{r}\right]$ with length $(r-2s)$ which acts on the non-overlapping part of each patch. 
We denote $\vect{w}_{non} = \left[w_{r-s+1},\ldots,w_{r}\right]$ and our goal in this stage is to learn $\vect{w}_{non}^*$ and $\vect{a}^*$ jointly.

In this stage, our algorithm proceeds as follows.
Given $\vect{w}_{non}$, $\vect{a}$ and a sample $\left(\vect{x},y\right)$, we define \begin{align}
\vect{g}\left(\vect{w}_{non},\vect{a},\vect{x},y\right) = &\frac{2}{1+\gamma}\left(
	\hat{f}(\vect{w}_{non},\vect{a},\vect{x}) - y
	\right) \sum_{j=1}^{k}a_i\mat{P}_j^{non} \vect{x}+\frac{1}{4}\left(\norm{\vect{w}_{non}}_2^2-\norm{\vect{a}}_2^2\right)\vect{w}_{non} \label{eqn:w_non_update}\\
\vect{h}\left(\vect{w}_{non},\vect{a},\vect{x},y\right) =  &\frac{2}{1+\gamma}\left(
	\hat{f}(\vect{w}_{non},\vect{a},\vect{x}) - y\right) \begin{pmatrix}
		\vect{w}_{non}^\top\mat{P}_1^{non} \vect{x}\\
		\ldots\\
		\vect{w}_{non}^\top\mat{P}_k^{non} \vect{x}
	\end{pmatrix}+ \frac{1}{4}\left(\norm{\vect{a}}_2^2-\norm{\vect{w}_{non}}_2^2\right)\vect{a} \label{eqn:a_update}
\end{align}
where $\hat{f}\left(\vect{w}_{non},\vect{a},\vect{x}\right) = \sum_{j=1}^{k}a_j\relu{\vect{w}_{non}^\top \mat{P}_j^{non}\vect{x}}$ is the prediction function only using $\vect{w}_{non}$.

As will be apparent in Section~\ref{sec:analysis}, $\vect{g}$ and $\vect{h}$ are unbiased estimates of the gradient for the loss function corresponding to learning a linear CNN.
The term $\frac{1}{4}\left(\norm{\vect{w}_{non}}_2^2-\norm{\vect{a}}_2^2\right)\vect{w}_{non}$  and $\frac{1}{4}\left(\norm{\vect{a}}_2^2-\norm{\vect{w}_{non}}_2^2\right)\vect{a}$ are is the gradient induced by the regularization  $\frac{1}{4}\left(\norm{\vect{w}_{non}}_2^2-\norm{\vect{a}}_2^2\right)^2$, which is used to balance the magnitude between $\vect{w}_{non}$ and $\vect{a}$ and make the algorithm more stable.  

With some initialization $\vect{w}_{non}^{(0)}$, $\vect{a}^{(0)}$,
we use the following iterative updates inspired by isotonic regression~\citep{goel2018learning}, for $t=0,\ldots,T_1-1$% \simon{maybe add more intuition here}\begin{align}
	\vect{w}_{non}^{(t+1)} \leftarrow &\vect{w}_{non}^{(t)} - \eta_1\vect{g}\left(\vect{w}_{non}^{(t)},\vect{a}^{(t)},\vect{x}^{(t)},y^{(t)}\right) + \eta_1 \xi_{\vect{w}_{non}}^{(t)},\\ 
	\vect{a}^{(t+1)} \leftarrow&\vect{a}^{(t)} -  \eta_1\vect{h}\left(\vect{w}_{non}^{(t)},\vect{a}^{(t)},\vect{x}^{(t)},y^{(t)}\right) + \eta_1 \xi_{\vect{a}}^{(t)}	\label{eqn:double_convotron}
\end{align} where $\eta_1 > 0$ is the step size parameter, $\xi_{\vect{w}_{non}}^{(t)}$ and $\xi_{\vect{a}}^{(t)}$ are uniformly sampled a unit sphere and  at iteration we use a fresh sample $\left(\vect{x}^{(t)},y^{(t)}\right)$.
Here we add isotropic noise $\xi_{\vect{w}_{non}}^{(t)}$ and $\xi_{\vect{a}}^{(t)}$ because the objective function for learning  a linear CNN is non-convex and there may exist saddle points.
Adding noise can help escape from these saddle points.
We refer readers to \cite{ge2015escaping} for more technical details regarding this.
As will be apparent in Section~\ref{sec:analysis}, after sufficient iterations, we obtain a pair $\left(\vect{w}^{(T_1)},\vect{a}^{(T_1)}\right)$ such that either it is close to the truth $\left(\vect{w}_{non}^*,\vect{a}^*\right)$ or close to the negative of the truth $\left(-\vect{w}_{non}^*,-\vect{a}^*\right)$.

\begin{algorithm}[t]
   \caption{Double Convotron}
   \label{alg:doubleconvotron}
\begin{algorithmic}
   \State Initialize $\vect{w}^{(0)}_{non} \in \mathbb{R}^{r-2s}$ and $\vect{a}^{(0)} \in \mathbb{R}^{k}$ randomly
   \For{$t=1$ {\bfseries to} $T$}
   \State Draw $(\vect{x}^{(t)}, y^{(t)}) \sim \inputdist$
   \State Compute $\vect{g}\left(\vect{w}_{non},\vect{a},\vect{x}^{(t)},y^{(t)}\right) $ and $\vect{h}\left(\vect{w}_{non},\vect{a},\vect{x}^{(t)},y^{(t)}\right)$ according to Equation (\ref{eqn:w_non_update}) and (\ref{eqn:a_update}).
   \State Set $\vect{w}_{non}^{(t+1)} = \vect{w}_{non}^{(t)} - \eta_1\vect{g}\left(\vect{w}_{non}^{(t)},\vect{a}^{(t)},\vect{x}^{(t)},y^{(t)}\right) + \eta_1 \xi_{\vect{w}_{non}}^{(t)}$
   \State Set $\vect{a}^{(t+1)} = \vect{a}^{(t)} -  \eta_1\vect{h}\left(\vect{w}_{non}^{(t)},\vect{a}^{(t)},\vect{x}^{(t)},y^{(t)}\right) + \eta_1 \xi_{\vect{a}}^{(t)}$
   \EndFor
   \State{Return $\vect{a}^{(T+1)}$}
\end{algorithmic}
\end{algorithm}\paragraph{Stage 2: Convotron with fixed Linear Layer}% \simon{Surbhi: please add your algorithm description here. % We will need to try both $\vect{a}$ and $-\vect{a}$ that we obtained from the first step and we will have two possible filters $\vect{w}_1$ or $\vect{w}_2$.% }
In Stage 1 we have learned a good approximation to the second layer (either $\vect{a}^{(T_1)}$ or $-\vect{a}^{(T_1)}$).
Therefore, the problem reduces to learning a convolutional filter.
We run Convotron (Algorithm~\ref{alg:convotron}) proposed in~\cite{goel2018learning} using $\vect{a}^{(T_1)}$ and $-\vect{a}^{(T_1)}$ to obtain corresponding weight vectors $\vect{w}^{(+)}$ and $\vect{w}^{(-)}$. We show that the Convotron analysis can be extended to handle approximately known outer layer weight vectors as long as their is less than half overlap.

\begin{algorithm}[t]
   \caption{Convotron \citep{goel2018learning}}
   \label{alg:convotron}
\begin{algorithmic}
   \State Initialize $\vect{w}_1 := 0 \in \mathbb{R}^r$.
   \For{$t=1$ {\bfseries to} $T$}
   \State Draw $(\vect{x}^{(t)}, y^{(t)}) \sim \inputdist$
   \State Let $\vect{g}^{(t)} = (y^{(t)} - f(\vect{w}^{(t)}, \vect{a}, \vect{x}^{(t)}))  \left(\sum_{i=1}^k a_i \mat{P}_i  \vect{x}^{(t)}\right)$
   \State Set $\vect{w}^{(t+1)} = \vect{w}^{(t)} + \eta \vect{g}^{(t)}$
   \EndFor
   \State{Return $\vect{w}_{T+1}$}
\end{algorithmic}
\end{algorithm}\paragraph{Stage 3: Validation}
In stage 2 we have obtained two possible solutions $\left(\vect{w}^{(+)},\vect{a}^{(T)}\right)$ and $\left(\vect{w}^{(-)},-\vect{a}^{(T)}\right)$.
We know at least one of them is close to the ground truth. Closeness in ground truth implies small squared loss (c.f. Lemma \ref{lem:loss}). 
In the last stage we use a validation set to choose the right one.
To do this, we simply use $T_3 = \poly\left(k,B,\frac{1}{\epsilon}\right)$ fresh samples and output the solution which gives lower squared error. 
\begin{align}
\left(\vect{w},\vect{a}\right) = \argmin_{\left(\vect{w},\vect{a}\right) \in \left\{\left(\vect{w}^{(+)},\vect{a}^{(T)}\right), \left(\vect{w}^{(-)},-\vect{a}^{(T)}\right)\right\}} \frac{1}{T_3}\sum_{i=1}^{T_3}\left(y^{(i)} - f\left(\vect{w},\vect{a},\vect{x}^{(i)}\right) \right)^2.\label{eqn:hypo_testing}
\end{align}
Since we draw many samples, the empirical estimates will be close to the true loss using standard concentration bounds and choosing the minimum will give us the correct solution.
The following theorem shows that Algorithm \ref{algo:main} is guaranteed to learn the target convolutinoal neural network in polynomial time.
\begin{thm}[Theorem \ref{thm:main_informal} (Formal)]
  \label{thm:main}{}
Under Assumptions~\ref{asmp:input_dist}-\ref{asmp:activation}, if we set $\abs{\mathcal{S}_1},\abs{\mathcal{S}_2},\abs{\mathcal{S}_3} = \Omega\left(\poly\left(k,B,\frac{1}{\epsilon}\right)\right)$ and $\eta_1, \eta_2 = O\left(\poly\left(\frac{1}{k},\frac{1}{B},\epsilon\right)\right)$ then with high probability, Algorithm~\ref{algo:main} returns a pair $\left(\vect{w},\vect{a}\right)$ which satisfies\begin{align*}
  \expect_{\vect{x} \sim \inputdist}\left[ \left(f(\vect{w},\vect{a},\vect{x})-f(\vect{w}^*,\vect{a}^*,\vect{x})\right)^2\right] \le \epsilon.
\end{align*}
\end{thm}
To our knowledge, this is the first polynomial time proper learning algorithm for convolutional neural networks with two unknown layers with overlapping patches\footnote{Note that \citet{goel2018learning} gave the first polynomial time proper algorithm for convolutional network with overlapping patches. 
However they required the outer layer to be average pooling.}.


\section{Proofs and Technical Insights}
\label{sec:analysis}
In this section we list the key ideas used for designing Algorithm~\ref{algo:main} and proving its correctness. We discuss the analysis stage-wise for ease of understnading. 
Some technical derivations are deferred to the appendix.

\subsection{Analysis of Stage 1}\label{sec:stage1_analysis}\paragraph{Learning a non-overlapping CNN with linear activation.}
We first consider the problem of learning a convolutional neural network with linear activation function and non-overlapping patches.
For this setting, we can write the prediction function in a compact form:\begin{align*}
	f_{lin}\left(\vect{w},\vect{a},\vect{x}\right) = \vect{w}^\top \mat{P}\left(\vect{x}\right)\vect{a} = \langle \mat{P}\left(\vect{x}\right),\vect{w}\vect{a}^\top \rangle.
\end{align*}
The label also admits this form:\[
y = \langle \mat{P}\left(\vect{x}\right),\vect{w}^*\left(\vect{a}^*\right)^\top \rangle.
\]
A natural way to learn $\vect{w}^*$ and $\vect{a}^*$ is to consider solving a square loss minimization problem.
\begin{align*}
 \ell\left(\vect{w},\vect{a},\vect{x}\right)= &\left(\langle \mat{P}\left(\vect{x}\right),\vect{w}\vect{a}^\top \rangle- \langle \mat{P}\left(\vect{x}\right),\vect{w}^*\left(\vect{a}^*\right)^\top \rangle\right)^2 \\
	= &\vectorize{\vect{w}\vect{a}^\top - \vect{w}^*\left(\vect{a}^*\right)^\top}^\top  \vectorize{\mat{P}\left(\vect{x}\right)}\vectorize{\mat{P}\left(\vect{x}\right)}^\top \vectorize{\vect{w}\vect{a}^\top - \vect{w}^*\left(\vect{a}^*\right)^\top}
\end{align*}
Now, taking expectation with respect to $\vect{x}$, we have \begin{align}
	L\left(\vect{w},\vect{a}\right) = &\expect_{\vect{x}} \left[\ell\left(\vect{w},\vect{a},\vect{x}\right)\right]  \nonumber\\
	= & \vectorize{\vect{w}\vect{a}^\top - \vect{w}^*\left(\vect{a}^*\right)^\top}^\top  \expect_{\vect{x}}\left[\vectorize{\mat{P}\left(\vect{x}\right)}\vectorize{\mat{P}\left(\vect{x}\right)}^\top \right]\vectorize{\vect{w}\vect{a}^\top - \vect{w}^*\left(\vect{a}^*\right)^\top} \nonumber\\
	= &\norm{\vect{w}\vect{a}^\top - \vect{w}^*\left(\vect{a}^*\right)^\top}_F^2 \label{eqn:why_mf}
\end{align}
where the last step we used our assumptions that patches are non-overlapping and the covariance of $\vect{x}$ is the identity.
From Equation~\eqref{eqn:why_mf}, it is now apparent that the population $L_2$ loss is just the standard loss for rank-$1$ matrix factorization problem!

Recent advances in non-convex optimization shows the following regularized loss function
\begin{align}
	L_{reg}\left(\vect{w},\vect{a}\right) = \frac{1}{2}\norm{\vect{w}\vect{a}^\top - \vect{w}^*\left(\vect{a}^*\right)^\top}_F^2 +
	\frac{1}{8}\left(\norm{\vect{w}}_2^2-\norm{\vect{a}}_2^2\right)^2.
	\label{eqn:mf_obj_reg}
\end{align}
satisfies all local minima are global and all saddles points and local maxima has a negative curvature~\cite{ge2017no} and thus allows simple local search algorithm to find a global minimum.
Though the objective function~\eqref{eqn:mf_obj_reg} is a population risk, we can obtain its stochastic gradient by our samples if we use fresh sample at each iteration.
We define \begin{align}
	\vect{g}_\vect{w}^{(t)} = & \left(f\left(\vect{w}^{(t)},\vect{a}^{(t)},\vect{x}^{(t)}\right)-y^{(t)}\right)\mat{P}\left(\vect{x}^{t}\right)\vect{a}^{(t)} + \frac{1}{2}\left(\norm{\vect{w}^{(t)}}_2^2-\norm{\vect{a}^{t}}_2^2\right)\vect{w}^{(t)} \label{eqn:w_sg_oracle}\\
	\vect{g}_\vect{a}^{(t)} = &\left(f\left(\vect{w}^{(t)},\vect{a}^{(t)},\vect{x}^{(t)}\right)-y^{(t)}\right)\mat{P}\left(\vect{x}^{t}\right)^\top\vect{w}^{(t)} +
	\frac{1}{2}\left(\norm{\vect{a}^{t}}_2^2-\norm{\vect{w}^{(t)}}_2^2\right)\vect{a}^{(t)} \label{eqn:a_sg_oracle}
\end{align}
where $\left(\vect{x}^{(t)},y^{(t)}\right)$ is the sample we use in the $t$-th iteration.
In expectation this is the standard gradient descent algorithm for solving~\eqref{eqn:mf_obj_reg}:
\begin{align*}
	\expect_{\vect{x}}\left[\vect{g}_\vect{w}^{(t)} \right] = \frac{\partial L_{reg}\left(\vect{w}^{(t)},\vect{a}^{(t)}\right)}{\partial \vect{w}^{(t)}}, \quad
	\expect_{\vect{x}}\left[\vect{g}_\vect{a}^{(t)} \right] = \frac{\partial L_{reg}\left(\vect{w}^{(t)},\vect{a}^{(t)}\right)}{\partial \vect{a}^{(t)}}.
\end{align*}
With this stochastic gradient oracle at hand, we can implement the noise-injected stochastic gradient descent proposed in \cite{ge2015escaping}.
\begin{align*}
	\vect{w}^{(t+1)} = \vect{w}^{(t)} - \eta \vect{g}_\vect{w}^{(t)}  +\eta\xi_{\vect{w}}^{(t)}, \\ 
	\vect{a}^{(t+1)} = \vect{w}^{(t)} - \eta \vect{g}_\vect{a}^{(t)} + \eta\xi_{\vect{a}}^{(t)}
\end{align*}
where $\vect{\xi}_{\vect{w}}^{(t)}$ and $\vect{\xi}_{\vect{a}}^{(t)}$ are sampled from a unit sphere.
Theorem 6 in \cite{ge2015escaping} implies after polynomial iterations, this iterative procedure returns an $\epsilon$-optimal solution of the objective function~\eqref{eqn:mf_obj_reg} with high probability.

\paragraph{Learning non-overlapping part of a CNN with piece-wise linear activation function}
Now we consider piece-wise linear activation function.
Our main observation is that we can still obtain a stochastic gradient oracle for the \emph{linear} convolutional neural network using Equation~\eqref{eqn:w_non_update} and~\eqref{eqn:a_update}.
Formally, we have the following theorem.
\begin{lem}[Properties of Stochastic Gradient for Linear CNN]\label{thm:unbiased_for_linear}
Define \[
L_{reg}\left(\vect{w}_{non},\vect{a}\right) = \frac{1}{2}\norm{\vect{w}_{non}\vect{a}^\top - \vect{w}^*_{non}\left(\vect{a}^*\right)^\top}_F^2 + \frac{1}{8}\left(\norm{\vect{w}_{non}}_2^2 - \norm{\vect{a}}_2^2\right)^2.
\]
Under Assumption~\ref{asmp:input_dist}, we have \begin{align*}
\expect_{\vect{x}}\left[\vect{g}\left(\vect{w}_{non},\vect{a},\vect{x},y\right)\right] =
\frac{\partial L_{reg}\left(\vect{w}_{non},\vect{a}\right)}{\partial \vect{w}_{non}}, \expect_{\vect{x}}\left[\vect{h}\left(\vect{w}_{non},\vect{a},\vect{x},y\right)\right] =
\frac{\partial L_{reg}\left(\vect{w}_{non},\vect{a}\right)}{\partial \vect{a}}
\end{align*} where $\vect{g}\left(\vect{w}_{non},\vect{a},\vect{x},y\right)$ and $\vect{h}\left(\vect{w}_{non},\vect{a},\vect{x},y\right)$ are defined in Equation~\eqref{eqn:w_non_update} and~\eqref{eqn:a_update}, respectively.
Further, if $\norm{\vect{w}_{non}}_2 = O(\poly\left(\sigma_1\right)), \norm{\vect{a}}_2 = O(\poly\left(\sigma_1\right))$, then the differences are also bounded \begin{align*}
\norm{\vect{g}\left(\vect{w}_{non},\vect{a},\vect{x},y\right) - \frac{\partial L_{reg}\left(\vect{w}_{non},\vect{a}\right)}{\partial \vect{w}_{non}}} \le D,  \norm{\vect{h}\left(\vect{w}_{non},\vect{a},\vect{x},y\right)-\frac{\partial L_{reg}\left(\vect{w}_{non},\vect{a}\right)}{\partial \vect{a}}}_2 \le D
\end{align*} for some $D = O\left(\poly\left(B,k,\sigma_1\right)\right)$.
\end{lem}% \simon{
Here the expectation of $\vect{g}$ and $\vect{h}$ are equal to the gradient of the objective function for linear CNN because we assume the input distribution is symmetric and the activation function is piece-wise linear.
This observation has been stated in \cite{goel2018learning} and based on this property, \citet{goel2018learning} proposed Convotron algorithm (Algorithm~\ref{alg:convotron}), which we use in our stage 2.
Lemma~\ref{thm:unbiased_for_linear} is a natural extension of Lemma 2 of \cite{goel2018learning} that we show even for one-hidden-layer CNN, we can still obtain an unbiased estimate of the gradient descent for linear CNN.
Now with Lemma~\ref{thm:unbiased_for_linear} at hand, we can use  the theory from non-convex matrix factorization.
\cite{ge2017no} has shown if $\eta_1 = O\left(\poly\left(\frac{1}{k},\frac{1}{B},\epsilon\right)\right)$ then for all iterates, with high probability, $\norm{\vect{w}_{non}^{(t)}}_2 = O(\poly\left(\sigma_1\right)), \norm{\vect{a}^{t}}_2 = O(\poly\left(\sigma_1\right))$.
Therefore, we can apply the algorithmic result in \cite{ge2015escaping} and obtain the following convergence theorem.
\begin{thm}[Convergence of Stage 1]
\label{thm:linear_cnn}
If $\vect{w}^{(0)} = O\left(\sqrt{\sigma_1}\right)$, $\vect{a}^{(0)} = O\left(\sqrt{\sigma_1}\right)$, and $\eta_1 = O\left(\poly\left(\frac{1}{k},\frac{1}{B},\epsilon\right)\right)$ then after $T_1 =O\left(\poly\left(r,k,B,\frac{1}{\epsilon}\right)\right)$
 we have 
	\begin{align*}
		\norm{\frac{\vect{a}^{(T_1)}}{\norm{\vect{a}^{(T_1)}}_2}-\frac{\vect{a}^*}{\norm{\vect{a}^*}_2}}_2 \le \epsilon. \text{ or }\norm{\frac{\vect{a}^{(T_1)}}{\norm{\vect{a}^{(T_1)}}_2}+\frac{\vect{a}^*}{\norm{\vect{a}^*}_2}}_2 \le \epsilon.
	\end{align*}
\end{thm}%Learning the original convolutional filter\subsection{Analysis of Stage 2}\label{sec:learning_single_filter}After Stage 1, we have approximately recovered the outer layer weights. We use these as fixed weights and run Convotron to obtain the filter weights. The analysis of Convotron inherently handles average pooling as the outer layer. Here we extend the analysis of Convotron to handle any fixed outer layer weights and also handle noise in these outer layer weights as long as overlap is less than half. 
Formally, we obtain the following theorem:
\begin{thm}(Learning the Convolutional Filter)\label{thm:convotron}
Suppose $\norm{\vect{a}-\vect{a}^*}_2 \leq \epsilon$ for $\epsilon \leq \frac{1}{k^3\norm{\vect{w}^*}_2}$ and without loss of generality\footnote{Note that we can assume that the outer layers have norm 1 by using the normalized weight vectors since the activations are scale invariant.} let $\norm{\vect{a}}_2 = \norm{\vect{a}^*}_2 = 1$. For suitably chosen $\eta = O\left(\poly\left(\frac{1}{k},\frac{1}{B}\right)\right)$, Convotron (modified) returns $\vect{w}$ such that with a constant probability, $\norm{\vect{w}-\vect{w}^*}_2 \leq O(k^3\norm{\vect{w}^*}\epsilon)$ in $\poly(k, \norm{\vect{w}^*}, B, \log(1/\epsilon))$ iterations.
\end{thm}
Note that we present the theorem and proof for covariance being identity and no noise in the label but it can be easily extended to handle non-identity convariance with good condition number and bounded (in expectation) probabilistic concept noise.

Our analysis closely follows that from~\cite{goel2018learning}. However, in contrast to the known second layer setting considered in \cite{goel2018learning}, we only know an approximation to the second layer and a robust analysis is needed.
Another difficulty arises from the fact that the convergence rate depends on the least eigenvalue of $\mat{P}^{\vect{a}}:= \sum_{1 \leq i,j \leq k} a_i a_j P_i P_j^T$. By simple algebra, we can show that the matrix has the following form:
\[
\mat{P}^{\vect{a}}(i,j) = 	\begin{cases}
								1 &\text{if } i = j\\
								\sum_{i=1}^{k-1} a_i a_{i+1} & \text{if } |i - j| = s\\
								0 &\text{otherwise}.
							\end{cases}
\]
Using property of Toeplitz matrices, we show the least eigenvalue of $\mat{P}^{\vect{a}}$ is lower bounded by $1-\cos\left(\frac{\pi}{k+1}\right)$ (c.f. Theorem~\ref{thm:eigen}) for all $\vect{a}$ with norm 1. We refer readers to Section~\ref{sec:proofs_of_section_analysis} for the full proof.
\subsection{Analysis of Stage 3}
In section we just need to show we can pick the right hypothesis.
Under our assumptions, the individual loss $(y^{(i)}-f(\vect{w},\vect{a},\vect{x}^{(i)}))^2$ is bounded.
Thus, a direct application of Hoeffding inequality gives the following guarantee.
\begin{thm}\label{thm:validation}
Suppose $T_3 = \Omega\left(\poly\left(r,k,B,\frac{1}{\epsilon}\right)\right)$ and let $\left(\vect{w},\vect{a}\right)$.
If either $\left(\vect{w}^{(+)},\vect{a}^{T_1}\right)$ or $\left(\vect{w}^{(-)},-\vect{a}^{T_1}\right)$ has population risk smaller than $2\epsilon$, then let $\left(\vect{w},\vect{a}\right)$
 be the output according to Equation~\eqref{eqn:hypo_testing}, then with high probability \begin{align*}
\expect_{\vect{x}\sim \inputdist} \left[f(\vect{w},\vect{a},\vect{x}-f(\vect{w}^*,\vect{a}^*,\vect{x}))^2\right] \le \epsilon.
\end{align*}
\end{thm}\subsection{Putting Things Together: Proof of Theorem~\ref{thm:main}}
Now we put our analyses for Stage 1-3 together and prove Theorem~\ref{thm:main}.
By Theorem~\ref{thm:linear_cnn}, we know we have $\vect{a}^{\left(T_1\right)}$ such that $\norm{\vect{a}^{T_1}-\vect{a}^*} \le O\left(\frac{\epsilon}{r^{1/2}k^{5/2}\sigma_1}\right)$ (without loss of generality, we assume $\vect{a}$ and $\vect{a}^*$ are normalized) with $\eta_1 = O\left(\poly\left(\frac{1}{k},\frac{1}{B},\frac{1}{\sigma_1},\epsilon\right)\right)$ and $\abs{S_1} = \poly\left(r,k,B,\sigma_1,\frac{1}{\epsilon}\right)$.

Now with Theorem~\ref{thm:convotron}, we know with $\eta = O\left(\poly\left(\frac{1}{k},\frac{1}{B}\right)\right)$ and $\abs{S_2} = O\left(\poly\left(k,\sigma_1,\log\frac{1}{\epsilon}\right)\right)$ we have either \begin{align*}
\norm{\vect{w}^{\left(+\right)} - \vect{w}^*}_2 \le \frac{\epsilon}{\sigma_1 r^{1/2}k^{3/2}}. \text{ or }\norm{\vect{w}^{\left(-\right)}-\vect{w}^*}_2 \le \frac{\epsilon}{\sigma_1 r^{1/2}k^{3/2}}.
\end{align*}

The following lemma bounds the squared loss of each instance in terms of the closeness of parameters.
\begin{lem}\label{lem:bound}
For any $\vect{a}$ and $\vect{w}$,
\[
\left(f\left(\vect{w}^*, \vect{a}^*, \vect{x}\right) - f\left(\vect{w}^{\left(t\right)}, \vect{a}, \vect{x}\right)\right)^2 \leq 2k \left(\norm{\vect{a}}_2^2\norm{\vect{w} - \vect{w}^*}_2^2 + \norm{\vect{a} - \vect{a}^*}_2^2\norm{\vect{w}^*}_2^2\right)\norm{\vect{x}}_2^2.
\]
\end{lem}
Therefore, we know either $\left(\vect{w}^{(+)},\vect{a}^{(T_1)}\right)$ or $\left(\vect{w}^{(-)},-\vect{a}^{(T_1)}\right)$ achieves $\epsilon$ prediction error.
Now combining Theorem~\ref{thm:validation} and Lemma~\ref{lem:loss} we obtain the desired result.




\section{Conclusion and Future Work}
\label{sec:con}
In this paper, we propose the first efficient algorithm for learning a one-hidden-layer convolutional neural network with two unknown layers and possibly overlapping patches.
Our algorithm draws ideas from isotonic regression, landscape analysis of non-convex problem and spectral analysis of Toeplitz matrices.
These findings can inspire further development in this field.

Our next step is extend our ideas to design provable algorithms that can learn complicated models consisting of multiple filters.
To solve this problem, we believe the recent progress on landscape design~\citep{ge2017learning} may be useful.



\section{Acknowledgement}
\label{sec:ack}
The authors would thank Adam Klivans and Barnab\'{a}s P\'{o}czos for useful discussions and feedbacks and Hanzhang Hu, Xiaolong Wang and Jiajun Wu for useful discussions on convolutional neural network structures.







\bibliography{simonduref}
\bibliographystyle{plainnat}

\newpage
\appendix
\section{Useful Lemmas/Theorems}\label{sec:useful_lemmas}
In this section we present a few lemmas/theorems that are useful for our analysis.

\begin{proof}[Proof of Lemma~\ref{lem:bound}]
Observe that,
\begin{align*}
&\left(f\left(\vect{w}^*, \vect{a}^*, \vect{x}\right) - f\left(\vect{w}^{\left(t\right)}, \vect{a}, \vect{x}\right)\right)^2 \\
& \leq 2\left(\left(f\left(\vect{w}^*, \vect{a}^*, \vect{x}\right) - f\left(\vect{w}^*, \vect{a}, \vect{x}\right)\right)^2 + \left(f\left(\vect{w}^*, \vect{a}, \vect{x}\right) -f\left(\vect{w}, \vect{a}, \vect{x}\right)\right)^2\right)
\end{align*}
since $\left(a + b\right)^2 \leq 2\left(a^2  + b^2\right)$ for all $a, b \in \mathbb{R}$.

The first term can be bounded as follows,
\begin{align*}
\left(f\left(\vect{w}^*, \vect{a}^*, \vect{x}\right) - f\left(\vect{w}^*, \vect{a}, \vect{x}\right)\right)^2 &= \left(\sum_{i=1}^k \left(a^*_i - a_i\right) \sigma \left(\left(\vect{w}^*\right)^T \mat{P}_i \vect{x}\right)\right)^2\\
& \leq \left(\sum_{i=1}^k |a^*_i - a_i| |\left(\vect{w}^*\right)^T \mat{P}_i \vect{x}|\right)^2\\
& \leq \left(\sum_{i=1}^k |a^*_i - a_i| \norm{\vect{w}^*}_2 \norm{\vect{x}}_2\right)^2\\
& \leq k \norm{\vect{a}^* - \vect{a}}_2^2\norm{\vect{w}^*}_2^2 \norm{\vect{x}}_2^2.
\end{align*}
Here the first inequality follows from observing that $\sigma\left(a\right) \leq |a|$ and the last follows from $\norm{\vect{v}}_1 \leq \sqrt{k} \norm{\vect{v}}_2$ for all $\vect{v} \in \mathbb{R}^k$.

Similarly, the other term can be bounded as follows,
\begin{align*}
\left(f\left(\vect{w}^*, \vect{a}, \vect{x}\right) - f\left(\vect{w}, \vect{a}, \vect{x}\right)\right)^2 &= \left(\sum_{i=1}^k a_i \left(\sigma \left(\left(\vect{w}^*\right)^T \mat{P}_i \vect{x}\right) - \sigma \left(\vect{w}^T \mat{P}_i \vect{x}\right)\right)\right)^2\\
& \leq \left(\sum_{i=1}^k |a_i| |\left(\vect{w}^* - \vect{w}\right)^T \mat{P}_i \vect{x}|\right)^2\\
& \leq \left(\sum_{i=1}^k |a_i| \norm{\vect{w}^* - \vect{w}}_2 \norm{\vect{x}}_2\right)^2\\
& \leq k \norm{\vect{a}}_2^2\norm{\vect{w}^* - \vect{w}}_2^2 \norm{\vect{x}}_2^2.
\end{align*}
Here we use the Lipschitz property of $\sigma$ to get the first inequality. The lemma follows from combining the above two.
\end{proof}

The following lemma extends this to the overall loss.
\begin{lem}\label{lem:loss}
For any $\vect{a}$ and $\vect{w}$,
\[
\E[\left(f\left(\vect{w}^*, \vect{a}^*, \vect{x}\right) - f\left(\vect{w}^{\left(t\right)}, \vect{a}, \vect{x}\right)\right)^2] \leq 2k B\left(\norm{\vect{a}}_2^2\norm{\vect{w} - \vect{w}^*}_2^2 + \norm{\vect{a} - \vect{a}^*}_2^2\norm{\vect{w}^*}_2^2\right).
\]
\end{lem}

The following lemma from \cite{goel2018learning} is key to our analysis.
\begin{lem}[Lemma 1 of \cite{goel2018learning}]\label{lem:switch}
For all $\vect{a},\vect{b} \in \mathbb{R}^n$, if $\inputdist$ is symmetric then,
\[
\E_{\vect{x} \sim \inputdist}[\sigma\left(\vect{a}^T  \vect{x}\right)\left(\vect{b}^T  \vect{x}\right)] = \frac{1 + \alpha}{2}\E_{\vect{x} \sim \inputdist}[\left(\vect{a}^T  \vect{x}\right)\left(\vect{b}^T  \vect{x}\right)].
\]
\end{lem}

The following well-known theorem is useful for bounding eigenvalues of matrices.
\begin{thm}[Gershgorin Circle Theorem~\cite{weisstein2003gershgorin}]\label{thm:circle}
For a $n \times n$ matrix $\mat{A}$, define $R_i:= \sum_{j = 1, j \neq i}^n |\mat{A}_{i,j}|$. Each eigenvalue of $\mat{A}$ must lie in at least one of the disks $\{z: |z - \mat{A}_{i,i}| \leq R_i\}$.
\end{thm}

The following lemma bounds the eigenvalue of the weighted patch matrices.
\begin{thm}\label{thm:eigen}
For all $\vect{a} \in \mathbb{S}^{k-1}$, 
\[
\lambda_{\min}\left(\mat{P}^{\vect{a}}\right) \geq 1 - \text{cos}\left(\frac{\pi}{k+1}\right) \quad \text{and} \quad \lambda_{\min}\left(\mat{P}^{\vect{a}}\right) \leq 2.
\]
\end{thm}\begin{proof}
 Since $s \geq \lfloor \frac{r}{2} \rfloor + 1$, only adjacent patches overlap, and it is easy to verify that the matrix $\mat{P}^{\vect{a}}$ has the following structure:
\[
\mat{P}^{\vect{a}}\left(i,j\right) = 	\begin{cases}
								1 &\text{if } i = j\\
								\sum_{i=1}^{k-1} a_i a_{i+1} & \text{if } |i - j| = s\\
								0 &\text{otherwise}.
							\end{cases}
\]
Using the Gershgorin Circle Theorem, stated below, we can bound the eigenvalues, $\lambda_{\min}\left(\mat{P}^{\vect{a}}\right) \geq 1 - \left|\sum_{i=1}^{k-1} a_i a_{i+1}\right|$ and $\lambda_{\max}\left(\mat{P}^{\vect{a}}\right) \leq 1 + \left|\sum_{i=1}^{k-1} a_i a_{i+1}\right|$.

To bound the maximum eigenvalue, we have,
\[
\lambda_{\max}\left(\mat{P}^{\vect{a}}\right) \leq 1 + \left|\sum_{i=1}^{k-1} a_i a_{i+1}\right| \leq 1 + \frac{1}{2}\sum_{i=1}^{k-1} \left(a_i^2 +  a_{i+1}^2\right) \leq 1 + \norm{\vect{a}}_2^2 = 2.
\]

To bound the minimum eigenvalue, we will bound $1 - \left|\sum_{i=1}^{k-1} a_i a_{i+1}\right|$ by minimizing it over all $\vect{a}$ such that $\norm{\vect{a}}_2 = 1$. We have $\min \left\{1 - \left|\sum_{i=1}^{k-1} a_i a_{i+1}\right|\right\} = \min \left\{1 - \sum_{i=1}^{k-1} a_i a_{i+1}\right\}$ since the minimum can be achieved by setting all $a_i$ to be non-negative. This can alternatively be viewed as $\min_{\norm{\vect{a}}_2 = 1} \vect{a}^T \mat{M} \vect{a} = \lambda_{\min}\left(\mat{M}\right)$ where $\mat{M}$ is a tridiagonal symmetric Toeplitz matrix as follows:
\[
\mat{M}\left(i,j\right) = 	\begin{cases}
					1 &\text{if } i = j\\
					-1/2 & \text{if } |i - j| = 1\\
					0 &\text{otherwise}.
				\end{cases}
\]
It is well known that the eigenvalues of this matrix are of the form $1 + \text{cos}\left(\frac{i\pi}{k+1}\right)$ for $i = 1, \ldots, k$ (c.f. \cite{boeottcher2005spectral}). The minimum eigenvalue is thus $1 + \text{cos}\left(\frac{k\pi}{k+1}\right) = 1 - \text{cos}\left(\frac{k\pi}{k+1}\right)$. This gives us the result.
\end{proof}\section{Omitted Proofs}\label{sec:proofs_of_section_analysis}\subsection{Proof of Lemma~\ref{thm:unbiased_for_linear}}
First by definition, we have
\begin{align*}
\expect_{\vect{x}}\left[\vect{g}\left(\vect{w}_{non},\vect{a},\vect{x},y\right)\right] = & \expect_{\vect{x}} \left[\frac{2}{1+\gamma}\left(
	\hat{f}\left(\vect{w}_{non},\vect{a},\vect{x}\right) - y
	\right) \sum_{j=1}^{k}a_j\mat{P}_j^{non} \vect{x}+\frac{1}{4}\left(\norm{\vect{w}_{non}}_2^2-\norm{\vect{a}}_2^2\right)\vect{w}_{non}\right].
\end{align*}
Because the input distribution is symmetric and the covariance is identity, by Lemma \ref{lem:switch}, we have  \begin{align*}
\expect_{\vect{x}}\left[\frac{2}{1+\gamma} \hat{f}\left(\vect{w}_{non},\vect{a},\vect{x}\right)\sum_{j=1}^{k}a_j\mat{P}_j^{non}\vect{x}\right] = & \expect_{\vect{x}}\left[\frac{2}{1+\gamma} \sum_{j=1}^{k}a_j\relu{\vect{w}_{non}^\top\mat{P}_j^{non}\vect{x}}\sum_{j=1}^{k}a_j\mat{P}_j^{non}\vect{x}\right] \\
= &\expect_{\vect{x}}\left[ \sum_{j=1}^{k}a_j\vect{w}_{non}^\top\mat{P}_j^{non}\vect{x}\sum_{j=1}^{k}a_j\mat{P}_j^{non}\vect{x}\right] \\
= & \norm{\vect{a}}_2^2 \vect{w}_{non}.
\end{align*}
Similarly, we have \begin{align*}
\expect_{\vect{x}}\left[y\sum_{j=1}^{k}a_j\mat{P}_j^{non} \vect{x}\right] = \vect{a}^\top \vect{a}^* \vect{w}^*.
\end{align*}
Also recall \begin{align*}
\frac{\partial L_{reg}\left(\vect{w},\vect{a}\right)}{\partial \vect{w}} = \norm{\vect{a}}_2^2 \vect{w}_{non} - \vect{a}^\top \vect{a}^* \vect{w}^* + \frac{1}{4}\left(\norm{\vect{w}_{non}}_2^2-\norm{\vect{a}}_2^2\right)\vect{w}_{non}.
\end{align*}
Thus \[
\expect_{\vect{x}}\left[\vect{g}\left(\vect{w}_{non},\vect{a},\vect{x},y\right)\right]  = \frac{\partial L_{reg}\left(\vect{w},\vect{a}\right)}{\partial \vect{w}}.
\]
The proof for $\vect{h}\left(\vect{w}_{non},\vect{a},\vect{x},y\right)$ is similar.

To obtain a bound of the gradient, note that \begin{align*}
\norm{\sum_{j=1}^{k}a_j\relu{\vect{w}_{non}^\top\mat{P}_j^{non}\vect{x}}\sum_{j=1}^{k}a_j\mat{P}_j^{non}\vect{x}} \le &\sum_{j=1}^{k}\abs{a_j}\abs{\vect{w}_{non}^\top \mat{P}_j^{non}\vect{x}}\norm{\sum_{j=1}^{k}a_j\mat{P}_j^{non}\vect{x}} \\
\le & \max_{j}\abs{a_j} \sum_{j=1}^{k}\norm{\vect{w}_{non}}_2 \norm{\mat{P}_j^{non}\vect{x}}_2\cdot\norm{\vect{a}}_2\norm{\vect{x}}_2 \\
\le & k \norm{\vect{a}}_2^2 \norm{\vect{x}}_2^2 \norm{\vect{w}}_2 \\
= & \poly\left(k,\sigma_1,B\right).
\end{align*} 
Similar argument applies to $y \sum_{j=1}^{k}a_j\mat{P}_j^{non} \vect{x}$.

We follow the Convotron analysis and include the changes. Define $S_t = \{\left(\vect{x}_1,y_1\right), \ldots, \left(\vect{x}_t,y_t\right)\}$. The modified gradient update is as follows,
\[
\vect{g}^{\left(t\right)} = \left(y_t - f\left(\vect{w}^{\left(t\right)}, \vect{a}, \vect{x}_t\right)\right)  \left(\sum_{i=1}^k a_i P_i  \vect{x}_t\right)
\]
The dynamics of Convotron can then be expressed as follows:
\[
\E_{\vect{x}_t, y_t}[\norm{\vect{w}^{\left(t\right)} - \vect{w}^*}_2^2 - \norm{\vect{w}^{\left(t+1\right)} - \vect{w}^*}_2^2|S_{t-1}] = 2 \eta\E_{\vect{x}_t, y_t}[\left(\vect{w}^* - \vect{w}^{\left(t\right)}\right)^T  \vect{g}^{\left(t\right)}| S_{t-1}] - \eta^2 \E_{\vect{x}_t, y_t}[||\vect{g}^{\left(t\right)}||^2|S_{t-1}].
\]

We have,
\begin{align}
&\E_{\vect{x}_t, y_t}\left[\left(\vect{w}^* - \vect{w}^{\left(t\right)}\right)^T  \vect{g}^{\left(t\right)} | S_{t-1}\right] \nonumber \\
& = \E_{\vect{x}_t, y_t}\left[\left(\vect{w}^* - \vect{w}^{\left(t\right)}\right)^T  \left(y_t - f\left(\vect{w}^{\left(t\right)}, \vect{a}, \vect{x}_t\right)\right)  \left(\sum_{i=1}^k a_i P_i  \vect{x}_t\right) \middle| S_{t-1}\right]\nonumber\\
& = \E_{\vect{x}_t}\left[\left(\vect{w}^* - \vect{w}^{\left(t\right)}\right)^T  \left(f\left(\vect{w}^*, \vect{a}^*, \vect{x}_t\right)  - f\left(\vect{w}^{\left(t\right)}, \vect{a}, \vect{x}_t\right)\right)  \left(\sum_{i=1}^k a_i P_i  \vect{x}_t\right) \middle| S_{t-1}\right]\nonumber\\
&=\sum_{1 \leq i,j \leq k} \E_{\vect{x}_t}[\left(a^*_i \sigma\left(\left(\vect{w}^*\right)^T  P_i  \vect{x}_t\right) - a_i \sigma\left(\left(\vect{w}^{\left(t\right)}\right)^T  P_i  \vect{x}_t\right)\right) \left(a_j\left(\vect{w}^*\right)^T - a_j\left(\vect{w}^{\left(t\right)}\right)^T\right)  P_j  \vect{x}_t | S_{t-1}]\nonumber\\
&= \frac{1 + \alpha}{2}\sum_{1 \leq i,j \leq k}\E_{\vect{x}_t}\left[\left(\left(a^*_i \left(\vect{w}^*\right)^T - a_i \left(\vect{w}^{\left(t\right)}\right)^T\right)  P_i  \vect{x}_t\right) \left(a_j\left(\vect{w}^*\right)^T - a_j\left(\vect{w}^{\left(t\right)}\right)^T\right)  P_j  \vect{x}_t | S_{t-1}\right] \label{eq:switch}\\
&= \frac{1 + \alpha}{2}\sum_{1 \leq i,j \leq k}\left(a^*_i \left(\vect{w}^*\right)^T - a_i \left(\vect{w}^{\left(t\right)}\right)^T\right)  P_i \E_{\vect{x}_t}[\vect{x}_t \vect{x}_t^T] P_j^T \left(a_j \vect{w}^* - a_j \vect{w}^{\left(t\right)}\right) \label{eq:cov}\\
&= \frac{1 + \alpha}{2}\sum_{1 \leq i,j \leq k}\left(a^*_i \left(\vect{w}^*\right)^T - a_i \left(\vect{w}^*\right)^T + a_i \left(\vect{w}^*\right)^T - a_i \left(\vect{w}^{\left(t\right)}\right)^T\right)  P_i P_j^T \left(a_j \vect{w}^* - a_j \vect{w}^{\left(t\right)}\right)\nonumber \\
&= \frac{1 + \alpha}{2}\left(\left(\left(\vect{w}^*\right)^T -\left(\vect{w}^*\right)^T\right) \mat{P}^{\vect{a}} \left(\vect{w}^* -\vect{w}^*\right) + \sum_{1 \leq i,j \leq k}\left(a^*_i - a_i\right)a_j \left(\vect{w}^*\right)^T  P_i P_j^T\left(\vect{w}^* - \vect{w}^{\left(t\right)}\right)\right) \label{eq:Pa}\\
&\geq \frac{1 + \alpha}{2} \left(\lambda_{\min}\left(\mat{P}^{\vect{a}}\right)\norm{\vect{w}^{\left(t\right)} - \vect{w}^*}_2^2 - \norm{\vect{w}^*}_2\norm{\sum_{1 \leq i \leq k}\left(a^*_i - a_i\right)P_i}_2\norm{\sum_{1 \leq j \leq k}a_jP_j}_2 \norm{\vect{w}^{\left(t\right)} - \vect{w}^*}_2\right) \label{eq:lmin}\\
&\geq \frac{1 + \alpha}{2} \left(\lambda_{\min}\left(\mat{P}^{\vect{a}}\right)\norm{\vect{w}^{\left(t\right)} - \vect{w}^*}_2^2 - k\norm{\vect{w}^*}\norm{\vect{a}^* - \vect{a}}_2\norm{\vect{a}}_2 \norm{\vect{w}^{\left(t\right)} - \vect{w}^*}_2\right)\nonumber \\
&\geq \frac{1 + \alpha}{2} \left(\lambda_{\min}\left(\mat{P}^{\vect{a}}\right)\norm{\vect{w}^{\left(t\right)} - \vect{w}^*}_2^2 - k\epsilon \norm{\vect{w}^*}_2\norm{\vect{w}^{\left(t\right)} - \vect{w}^*}_2\right)\nonumber
\end{align}
(\ref{eq:switch}) follows from using Lemma \ref{lem:switch}, (\ref{eq:Pa}) follows from defining $\mat{P}^{\vect{a}}:= \sum_{1 \leq i,j \leq k} a_i a_j P_i P_j^T$, (\ref{eq:cov}) follows from setting the covariance matrix to be identity and (\ref{eq:lmin}) follows from observing that $\mat{P}^{\vect{a}}$ is symmetric, thus $\forall\vect{x}, \vect{x}^T\mat{P}^{\vect{a}}\vect{x} \geq \lambda_{\min}\left(\mat{P}^{\vect{a}}\right)\norm{\vect{x}}_2^2$ as well as lower bounding the second term in terms of the norms of the corresponding parts. 

Now we bound the variance of $\vect{g}^{\left(t\right)}$.
\begin{align}
&\E_{\vect{x}_t, y_t}[||\vect{g}^{\left(t\right)}||^2|S_{t-1}] \nonumber\\
&= \E_{\vect{x}_t, y_t}\left[\left(y_t - f\left(\vect{w}^{\left(t\right)}, \vect{a}, \vect{x}_t\right)\right)^2  \left|\left|\sum_{i=1}^k a_i P_i  \vect{x}_t\right|\right|^2 \middle| S_{t-1}\right]\nonumber\\
&\leq \lambda_{\max}\left(\mat{P}^{\vect{a}}\right) \E_{\vect{x}_t}\left[\left(f\left(\vect{w}^*, \vect{a}^*, \vect{x}_t\right) - f\left(\vect{w}^{\left(t\right)}, \vect{a}, \vect{x}_t\right)\right)^2||\vect{x}_t||^2 \middle| S_{t-1}\right]\label{eq:norm}\\
&\leq 2k \lambda_{\max}\left(\mat{P}^{\vect{a}}\right) \left(\norm{\vect{a}}_2^2\norm{\vect{w}^{\left(t\right)} - \vect{w}^*}_2^2 + \norm{\vect{a} - \vect{a}^*}_2^2\norm{\vect{w}^*}_2^2\right)\E_{\vect{x}_t}\left[\norm{\vect{x}_t}_2^4\right]\label{eq:bound}\\
&\leq 2kB\lambda_{\max}\left(\mat{P}^{\vect{a}}\right) \left(\norm{\vect{w}^{\left(t\right)} - \vect{w}^*}_2^2 + \epsilon^2\norm{\vect{w}^*}_2^2\right)
\end{align}
(\ref{eq:norm}) follows from observing that $\left|\left|\sum_{i=1}^k a_i \mat{P}_i \vect{x}\right|\right|^2 \leq \lambda_{\max}\left(\mat{P}^\vect{a}\right) \norm{\vect{x}}_2^2$ for all $\vect{x}$ and (\ref{eq:bound}) follows from Lemma \ref{lem:bound}.

Combining the above equations and taking expectation over $S_{t-1}$, we get 
\begin{align*}
\E_{S_t}[\norm{\vect{w}^{\left(t+1\right)} - \vect{w}^*}_2^2] &\leq \left(1 - 2\eta \beta + \eta^2 \gamma\right)\E_{S_{t-1}}[\norm{\vect{w}^{\left(t\right)} - \vect{w}^*}_2^2] + 2\eta\alpha \epsilon \E_{S_{t-1}}[\norm{\vect{w}^{\left(t\right)} - \vect{w}^*}_2] + \eta^2 \chi \epsilon^2\\
&\leq \left(1 - 2\eta \beta + \eta^2 \gamma\right)\E_{S_{t-1}}[\norm{\vect{w}^{\left(t\right)} - \vect{w}^*}_2^2] + 2\eta\delta \epsilon \sqrt{\E_{S_{t-1}}[\norm{\vect{w}^{\left(t\right)} - \vect{w}^*}_2^2]} + \eta^2 \chi \epsilon^2
\end{align*}
for $\beta = \frac{1 + \alpha}{2}\lambda_{\min}\left(\mat{P}^{\vect{a}}\right)$, $\gamma = 2\lambda_{\max}\left(\mat{P}^{\vect{a}}\right) k B$, $\delta = \frac{1 + \alpha}{2} k\norm{\vect{w}^*}_2$ and $\chi = 2\lambda_{\max}\left(\mat{P}^{\vect{a}}\right) k B \norm{\vect{w}^*}_2^2$. 

From Theorem \ref{thm:eigen}, we have that $\lambda_{\min}\left(\mat{P}^{\vect{a}}\right) = 1 - \text{cos}\left(\frac{\pi}{k+1}\right) = \Omega\left(1/k^2\right)$ (by Taylor expansion) implying $\beta = \omega\left(1/k^2\right)$ and $\gamma = O\left(kB\right)$, $\chi = O\left(kB\norm{\vect{w}^*}_2^2\right)$.

We set $\eta = \beta\min\left(\frac{1}{\gamma}, \frac{1}{\chi}\right)$. First we show that $\E_{S_{t-1}}[\norm{\vect{w}^{\left(t\right)} - \vect{w}^*}_2^2] \leq 1$ for all iterations $t$. We prove this inductively. For $t=1$, since $w_1 = 0$, this is satisfied. Let us assume it holds for iteration $t$, then we have that,
\begin{align*}
\E_{S_t}[\norm{\vect{w}^{\left(t+1\right)} - \vect{w}^*}_2^2] &\leq \left(1 - 2\eta \beta + \eta^2 \gamma\right)\E_{S_{t-1}}[\norm{\vect{w}^{\left(t\right)} - \vect{w}^*}_2^2] + 2\eta\delta \epsilon \sqrt{\E_{S_{t-1}}[\norm{\vect{w}^{\left(t\right)} - \vect{w}^*}_2^2]} + \eta^2 \chi \epsilon^2\\
&\leq 1 - 2\eta \beta + \eta^2 \gamma + 2\eta\delta \epsilon  + \eta^2 \chi \epsilon^2 \\
&\leq 1 - 2\eta \beta + \eta \beta + 2\eta\delta \epsilon  + \eta \beta \epsilon\\
& \leq 1 - \eta\left(\beta - \left(\delta + \beta\right)\epsilon\right) \leq 1
\end{align*}
The last inequality follows from $\epsilon \leq \frac{1}{k^3\norm{\vect{w}^*}_2} \leq \frac{\beta}{\delta + \beta}$. Thus we have that for each iteration, $\E_{S_{t-1}}[\norm{\vect{w}^{\left(t\right)} - \vect{w}^*}_2^2] \leq 1$. Substituting this in the recurrence and solving the recurrence gives us,
\begin{align*}
\E_{S_t}[\norm{\vect{w}^{\left(t+1\right)} - \vect{w}^*}_2^2] &\leq \left(1 - 2\eta \beta + \eta^2 \gamma\right)\E_{S_{t-1}}[\norm{\vect{w}^{\left(t\right)} - \vect{w}^*}_2^2] + 2\eta\delta \epsilon + \eta^2 \chi \epsilon^2\\
&\leq \left(1 - \eta \beta\right)\E_{S_{t-1}}[\norm{\vect{w}^{\left(t\right)} - \vect{w}^*}_2^2] + 2\eta\delta \epsilon + \eta \beta \epsilon^2\\
&\leq \left(1 - \eta \beta\right)^t\norm{\vect{w}_1 - \vect{w}^*}_2^2 + \left(2\eta\delta \epsilon + \eta \beta \epsilon^2\right)\sum_{i= 0}^{t-1} \left(1 - \eta \beta\right)^i\\
&\leq \left(1 - \eta \beta\right)^t + \frac{2\delta \epsilon}{\beta} + \epsilon^2
\end{align*}

Thus for $T = O\left(\frac{1}{\eta\beta}\log\left(\frac{1}{\epsilon}\right)\right)$, we have,
\[
\E_{S_t}[\norm{\vect{w}^{\left(t+1\right)} - \vect{w}^*}_2^2] \leq O\left(\frac{\delta \epsilon}{\beta}\right) = O\left(k^3 \norm{\vect{w}^*}_2 \epsilon\right).
\]
Now using Markov's inequality, we know that the above holds for some constant probability.
For $i=T_2+1,\ldots,T_3$, define $z^{\left(i\right)} = \left(y^{\left(i\right)}-f\left(\vect{w},\vect{a},\vect{x}^{\left(i\right)}\right)\right)^2$.
Using our assumptions, we know $z^{\left(i\right)} \le O\left(
\poly\left(r,k,B\right)
\right)$ almost surely.
Now applying Hoeffding inequality we obtain our desired result.


\end{document}


