\documentclass[runningheads,a4paper]{llncs}

\usepackage{booktabs} % For formal tables
\usepackage{microtype}
\usepackage{xspace}

\usepackage{array}
\usepackage{dsfont}
\usepackage{pifont}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{color}
\usepackage{colortbl}
\usepackage{mathrsfs}
\usepackage{amscd}
\usepackage{stmaryrd}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{algorithm} 
\usepackage[noend]{algpseudocode}
\algrenewcommand\algorithmicindent{0.750em}%
\usepackage{subfig}
\usepackage{multirow}

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}
            
\newcommand{\xiaowei}[1]{{\color{blue}#1}}
\newcommand{\youcheng}[1]{{\color{red}#1}}

\newcommand{\labels}{\mathcal{L}}
\newcommand{\networks}{\mathcal{N}}
\newcommand{\requirements}{\mathcal{R}}
\newcommand{\testsuites}{\mathcal{T}}
\newcommand{\neuronpairs}{\mathcal{O}}
\newcommand{\distance}[2]{ ||#1||_{#2}}
\newcommand{\valuefunction}{g}
\newcommand{\distancefunction}{h}
\newcommand{\neuronpair}{\alpha}
\newcommand{\covered}[3]{cov_{#1}^{#2}#3}
\newcommand{\coverage}[3]{C_{#1}^{#2}#3}
\newcommand{\metric}{M}



\begin{document}

\title{Testing Deep Neural Networks}

\author{Youcheng Sun\inst{1} \and Xiaowei Huang\inst{2} \and Daniel Kroening\inst{1}}

\institute{Department of Computer Science, University of Oxford, UK
\and 
Department of Computer Science, University of Liverpool, UK}

\authorrunning{Y. Sun et al.}
\maketitle

\newcommand\true{\textsf{true}\xspace}
\newcommand\false{\textsf{false}\xspace}

\begin{abstract}
Deep neural networks (DNNs) have a wide range of applications, and software
employing them must be thoroughly tested, especially in safety
critical domains.  However, traditional software testing methodology,
including test coverage criteria and test case generation algorithms, cannot
be applied directly to DNNs.  This paper bridges this gap.  First, inspired
by the traditional MC/DC coverage criterion, we propose a set of four test
criteria that are tailored to the distinct features of DNNs.  Our novel
criteria are incomparable and complement each other.  Second, for each
criterion, we give an algorithm for generating test cases based on linear
programming (LP).  The algorithms produce a new test case (i.e., an input to
the DNN) by perturbing a given one.  They encode the test requirement and a
fragment of the DNN by fixing the activation pattern obtained from the given
input example, and then minimize the difference between the new and the
current inputs.  Finally, we validate our method on a set of networks
trained on the MNIST dataset.  The utility of our method is shown
experimentally with four objectives: (1)~bug finding; (2)~DNN safety
statistics; (3)~testing efficiency and (4)~DNN internal structure analysis.
\end{abstract}

\keywords{neural networks, test criteria, test case generation}

\section{Introduction}

Artificial intelligence, and specifically algorithms that implement deep
neural networks (DNNs), can deliver human-level results in some specialist
tasks such as full-information two-player games~\cite{alphaGoZero}, image
classification~\cite{imageNetChallenge} and some areas of natural language
processing~\cite{Goldberg2016}.  There is now a prospect of a wide-scale
deployment of DNNs, including in safety-critical applications such as
self-driving cars.  This naturally raises the question how software
implementing this technology would be tested, validated and ultimately
certified to meet the requirements of the relevant safety standards.

The software industry relies on testing as primary means to provide
stakeholders with information about the quality of the software product or
service under test~\cite{kaner2006}.
Research in software engineering has resulted in a broad range of approaches
to test software (\cite{ZHM1997,JH2011,SWMPHS2017} give comprehensive
reviews).  In white-box testing, the structure of a program is exploited to
(perhaps automatically) generate test cases according to test requirements. 
Code coverage criteria (or metrics) have been designed to guide the
generation of test cases, and evaluate the completeness of a test suite. 
E.g., a test suite with 100\% statement coverage exercises all instructions
at least once.  While it is arguable whether this ensures correct
functionality, high coverage is able to increase users' confidence (or
trust) in the program~\cite{ZHM1997}. Structural coverage metrics are used
as a means of assessment in a number of high-tier safety standards.

Artificial intelligence systems are typically implemented in software.  This
includes AI that uses DNNs, either as a monolithic end-to-end
system~\cite{NVIDIA} or as a system component.  However, (white-box) testing
for traditional software cannot be directly applied to DNNs, because the
software that implements DNNs does not have suitable structure.  In
particular, DNNs do not have traditional flow of control and thus it is not
obvious how to define criteria such as branch coverage for them.

In this paper, we bridge this gap by proposing a novel (white-box) testing
methodology for DNNs, including both test coverage criteria and test case
generation algorithms.  Technically, DNNs contain not only an architecture,
which bear some similarity with traditional software programs, but also a
large set of parameters, which are calculated by the training procedure. 
Any approach to testing DNNs needs to consider the distinct features of
DNNs, such as the syntactic connections between neurons in adjacent layers
(neurons in a given layer interact with each other and then pass information
to higher layers), the ReLU activation functions, and the semantic
relationship between layers (e.g., neurons in deeper layers represent more
complex features~\cite{YCNFL2015,olah2018the}).

The contributions of this paper are three-fold. 
First, we propose four test criteria, inspired by the MC/DC test
criteria~\cite{HVCR2001} from traditional software testing, that fit the
distinct features of DNNs mentioned above. MC/DC was developed by NASA and 
has been widely adopted. It is used in avionics software development guidance to 
ensure adequate testing of applications with the highest criticality.
There exist two coverage criteria for DNNs: neuron coverage~\cite{PCYJ2017} and
safety coverage~\cite{WHK2018}, both of which have been proposed recently.  Our
experiments show that neuron coverage is too coarse: 100\% coverage can be
achieved by a simple test suite comprised of few input vectors from the training dataset.  On
the other hand, safety coverage is black-box, too fine, and it is
computationally too expensive to compute a test suite in reasonable time.
Moreover, our four proposed criteria are incomparable with each other, and
complement each other in guiding the generation of test cases.

Second, we develop an automatic test case generation algorithm for each of
our criteria.  The algorithms produce a new test case (i.e., an input
vector) by perturbing a given one using linear programming (LP).  They
encode the test requirement and a fragment of the DNN by fixing the
activation pattern obtained from the given input vector, and then optimize
over an objective that is to minimize the difference between the new and the
current input vector.  LP can be solved efficiently in practice, and thus,
our test case generation algorithms can generate a test suite with
low computational cost.

Finally, we implement our testing approaches in a software tool named
\emph{DeepCover}\footnote{Available from github link \url{https://github.com/theyoucheng/deepcover}},
and validate it by conducting
experiments on a set of DNNs obtained by training on the MNIST dataset. 
We~observe that (1)~the generated test suites are effective in detecting
safety bugs (i.e., adversarial examples) of the DNNs; (2)~our approach can
provide metrics, including coverage, adversarial ratio and adversarial
quality, to quantify the safety/robustness of a DNN and are shown to be
efficient; (3)~the testing is efficient enough and (4)~internal behaviors of
DNNs have been also investigated through the experiments.  Overall, the
method is able to handle networks of non-trivial size ($>$10,000 hidden
neurons).  This compares very favourably with current approaches based on
SMT, MILP and SAT, which can only handle networks with a few hundred hidden
neurons.

\section{Preliminaries: Deep Neural Networks}\newcommand{\real}{\mathds{R}}

A (feedforward and deep) neural network, or DNN, is a tuple $\networks=(L,
T, \Phi)$, where each of its elements is defined as follows.

\begin{itemize}
  \item $L=\{L_k|k\in\{1,\dots,K\}\}$ is a set of layers such that
    $L_1$ is the \emph{input} layer, $L_{K}$ is the \emph{output} layer,
    and layers other than input and output layers are called \emph{hidden layers}.
    
    \begin{itemize}
      \item Each layer $L_k$ consists of $s_k$ nodes, which are
        also called \emph{neurons}.
      \item The $l$-th neuron of layer $k$ is
        denoted by $n_{k,l}$. 
    \end{itemize}


  \item $T\subseteq L\times L$ is a set of connections between layers such that, 
    except for the input and output layers, each layer has an incoming connection and an
    outgoing connection.
  \item $\Phi=\{\phi_k|k\in\{2,\dots,K\}\}$ is a set of \emph{activation functions}
    $\phi_k:D_{L_{k-1}}\rightarrow D_{L_k}$, one for each non-input layer. 

    \begin{itemize}
      \item We use $v_{k,l}$ to denote the value of $n_{k,l}$.
      \item Except for inputs, every node is connected to nodes in the preceding
        layer by pre-defined weights such that for all $k$ and $l$ with
        $2 \leq k\leq K$ and  $1\leq l\leq s_k$
        \begin{equation}
          \label{eq:sum}
          v_{k,l}=\delta_{k,l}+\sum_{1\leq h \leq s_{k-1}} w_{k-1, h, l}\cdot v_{k-1,h}
        \end{equation}
        where $w_{k-1,h,l}$ is the pre-trained weight for the connection between
        $n_{k-1,h}$ (i.e., the $h$-th node of layer $k-1$) and $n_{k,l}$
        (i.e., the $l$-th node of layer $k$) and $\delta_{k,l}$ the the
        so-called \emph{bias} for node $n_{k,l}$.  We note that this
        definition can express both fully-connected functions and
        convolutional functions.

      \item Values of neurons in hidden layers need to pass through a 
        Rectified Linear Unit (ReLU) \cite{relu}, where the \emph{final activation 
        value} of each neuron of hidden layers is defined as
        \begin{equation}
          \label{eq:relu}
           v_{k,l}=ReLU(v_{k,l})=
            \begin{cases}
              v_{k,l} &\mbox{  if } v_{k,l}> 0 \\
              0 & \mbox{  otherwise}
            \end{cases}
        \end{equation}

      ReLU is by far the most popular and effective activation function for neural networks.

    \item Finally, for any input, the neural network assigns a \emph{label}, that is,
      the index of the node of output layer with the largest value:
      \begin{equation}
        \label{eq:label}
        \mathit{label}=\mathrm{argmax}_{1\leq l\leq s_K}\{v_{K,l}\}
      \end{equation}
      Let $\labels$ be the set of labels. 
    \end{itemize}

\end{itemize}\begin{example}
Figure \ref{fig:nn} is a simple neural network with four layers. 
Its input space is $D_{L_1}=\real^2$ where $\real$ the set of real numbers.
\begin{figure}[htp!]
\centering

\def\layersep{1.8cm}

\scalebox{1}{
\begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=\layersep]
    \tikzstyle{every pin edge}=[<-,shorten <=1pt]
    \tikzstyle{neuron}=[circle,fill=black!25,minimum size=15pt,inner sep=0pt]
    \tikzstyle{input neuron}=[neuron, fill=green!50];
    \tikzstyle{output neuron}=[neuron, fill=red!50];
    \tikzstyle{hidden neuron}=[neuron, fill=blue!50];
    \tikzstyle{annot} = [text width=4em, text centered]

    \foreach \name / \y in {1,...,2}
        \node[input neuron, pin=left:$v_{1,\y}$] (I-\name) at (0,-\y) {};

    \foreach \name / \y in {1,...,3}
        \path[yshift=0.5cm]
            node[hidden neuron] (H1-\name) at (\layersep,-\y cm) {};

    \foreach \name / \y in {1,...,3}
        \path[yshift=0.5cm]
            node[hidden neuron] (H2-\name) at (\layersep*2,-\y cm) {};

    \node[output neuron,pin={[pin edge={->}]right:$v_{4,1}$}, right of=H2-2, yshift=0.5cm] (O1) {};
    \node[output neuron,pin={[pin edge={->}]right:$v_{4,2}$}, right of=H2-2, yshift=-0.5cm] (O2) {};

    \foreach \source in {1,...,2}
        \foreach \dest in {1,...,3}
            \path (I-\source) edge (H1-\dest);

    \foreach \source in {1,...,3}
        \foreach \dest in {1,...,3}
            \path (H1-\source) edge (H2-\dest);

    \foreach \source in {1,...,3}
         \path (H2-\source) edge (O1);

    \foreach \source in {1,...,3}
         \path (H2-\source) edge (O2);

    \node[annot,above of=H1-1, node distance=1cm] (hl1) {Hidden layer};
    \node[annot,above of=H2-1, node distance=1cm] (hl2) {Hidden layer};
    \node[annot,left of=hl1] {Input layer};
    \node[annot,right of=hl2] {Output layer};

    \node[annot, right of=H1-1, node distance=0.0cm] (hl1) {\small $n_{2,1}$};
    \node[annot, right of=H1-2, node distance=0.0cm] (hl1) {\small $n_{2,2}$};
    \node[annot, right of=H1-3, node distance=0.0cm] (hl1) {\small $n_{2,3}$};
    \node[annot, right of=H2-1, node distance=0.0cm] (hl1) {\small $n_{3,1}$};
    \node[annot, right of=H2-2, node distance=0.0cm] (hl1) {\small $n_{3,2}$};
    \node[annot, right of=H2-3, node distance=0.0cm] (hl1) {\small $n_{3,3}$};
\end{tikzpicture}
}
  \caption{A simple neural network}
  \label{fig:nn}
\end{figure}
\end{example}

Owing to the use of the ReLU as in \eqref{eq:relu}, the behavior of a neural
network is highly non-linear.  Given a neuron $n_{k,l}$, its ReLU is said to
be \emph{activated} iff its value $v_{k,l}$ is strictly positive; otherwise,
when $ReLU(v_{k,l})=0$, the ReLU is deactivated.

\paragraph{Neural network instance}

Given one particular input $x$, we say that the neural work $\networks$ is
\emph{instantiated} and we use $\networks[x]$ to denote this instance of the
network.

\begin{itemize}
  \item Given a network instance $\networks[x]$, the activation value of
    each node $n_{k,l}$ of the network and the final classification label
    are fixed and they are denoted as $v_{k,l}[x]$ and $\mathit{label}[x]$
    respectively.

    \item When the input of a neural network is given, the activation or
deactivation of each ReLU operator in the network is also determined.  We
write $u_{k,l}[x]$ for the value before applying the ReLU and $v_{k,l}[x]$
for the value after applying the ReLU.  Moreover, we write
  \begin{equation}
    \label{eq:sign}
    \mathit{sign}(v_{k,l}[x])=
    \begin{cases}
      +1 &\mbox{  if } u_{k,l}[x] = v_{k,l}[x] \\
      -1 & \mbox{  otherwise}
    \end{cases}
  \end{equation}
\end{itemize}

When there is no ReLU operator for a neuron, $\mathit{sign}$ simply returns
the sign (positive or negative) of its activation value.

\begin{example}\label{example:weights}
Let $\networks$ be a network whose architecture is given in Figure \ref{fig:nn}.  
Assume that the weights for the first three layers are as follows
\[
W_{1,2}=
\begin{bmatrix}
  4 & 0 & -1\\
  1 & -2 & 1
\end{bmatrix},\,\,
W_{2,3}=
\begin{bmatrix}
  2 & 3 & -1\\
  -7 & 6 & 4 \\
  1 & -5 & 9
\end{bmatrix}
\]
and that all biases are 0. When given an input 
$x=[0, 1]$, we get $\mathit{sign}(v_{2,1}[x])=+1$, since
$u_{2,1}=v_{2,1}=1$, and $\mathit{sign}(v_{2,2}[x])=-1$,
since $u_{2,2} = -2 \neq 0 = v_{2,2}$. 
\end{example}%\xiaowei{we need a running example of two hidden layers. It explains all the criteria. } \youcheng{yes! }%For example, Figure \ref{fig:toy} show a top network, where weights%are labeled on edges between nodes and all biases are assumed to be 0. Suppose that input $v_{1,1}=0.5$,%then activation values for 3 neurons in the hidden layers are $v_{2,1}=ReLU(1\times v_{1,1})=0.5$,%$v_{2,2}=ReLU(-1\times v_{1,1})=0$ and $v_{3,1}=ReLU(1\times v_{1,1})=0.5$. Thus, the ReLU for node%$n_{2,2}$ is not active. Finally, the output value %$v_{3,1}=1\times v_{2,1}+1\times v_{2,2}+1\times v_{2,3}=1$.%%\input{images/toy}%\begin{definition}(The sign of ReLU)%  Given one neural network instance $N[\pi]$, the sign of ReLU for each its neuron is defined as%  $\forall 1<k<K,\forall 1\leq l\leq s_k$%  \begin{equation}%    \label{eq:sign}%    sign(v_{k,l}[\pi])%    \begin{cases}%      +1 &\mbox{  if } v_{k,l}[\pi]> 0 \\%      0 & \mbox{  otherwise}%    \end{cases}%  \end{equation}%\end{definition}%For the simple example in Figure \ref{fig:toy}, we have $sign(v_{2,2}[v_{1,1}=0.5])=0$.

\section{Automated Test Case Generation}\label{sec:test-gen}\newcommand{\constraints}{\mathcal{C}}

We introduce for each test requirement $\requirements f$ for $f \in F$ an
automatic test case generation algorithm
$\textbf{Test-Gen}(\networks,\requirements f)$ to compute $\testsuites f$. 
As we will explain later in Section~\ref{sec:random}, the generation of test
suites for the criteria is non-trivial: a random testing approach will not
achieve a test suite with high adequacy.  In this paper, we consider
approaches based on constraint solving.  The algorithms are based on Linear
Programming (LP).  We~remark that, as usual in conventional software
testing, our test requirements/criteria are broader than any particular test
case generation algorithm, and there may exist alternative algorithms for a
given requirement/criterion.

Given a network $\networks$ and an input $x$, the activation pattern of
$\networks[x]$ is a function $ap[\networks,x]$, mapping from the set of
hidden neurons to the signs $\{+1,-1\}$.  We may write $ap[\networks,x]$ as
$ap[x]$ if $\networks$ is clear from the context.
\end{definition}

For an activation pattern $ap[x]$, we use $ap[x]_{k,i}$ to denote the sign
of the $i$-th neuron at layer $k$.

For input $x=(0,-1)$, by the activation values in
Table~\ref{tab:value-table}, we have $ap[x]_{2,1}=-1$, $ap[x]_{2,2}=+1$,
$ap[x]_{2,3}=-1$, $ap[x]_{2,2}=-1$, $ap[x]_{2,2}=+1$.
\end{example}

The function $\hat{f}$ represented by a DNN is highly non-linear and cannot
be encoded with linear programming (LP) in general.  Therefore, other
constraint solvers such as SMT
\cite{HKWW2017,katz2017reluplex,bunel2017piecewise}, MILP
\cite{LM2017,CNR2017,bunel2017piecewise,xiang2017output}, SAT
\cite{NKPSW2017} have been considered.  However, it is computationally hard,
if not impossible, for such direct encodings to handle large
networks, because their underlying constraint solving problems are
intractable (at least NP-hard).  In this paper, for the efficient generation
of a test case $x'$, we consider (1) an LP-based approach by fixing the
activation pattern $ap[x]$ according to a given input $x$, and (2) encoding
a prefix of the network, instead of the entire network, with respect to a
given neuron pair.


In the following, we explain the LP encoding of a DNN instance
$\networks[x]$ (Section~\ref{sec:encoding}), introduce a few operations on
the given activation pattern (Section~\ref{sec:operations}), discuss a
safety requirement (Section~\ref{sec:safety}), and then present the
algorithms based on them (Section \ref{sec:algorithms}).

\subsection{LP Model of a DNN Instance}\label{sec:encoding}

The variables used in the LP model are distinguished in \textbf{bold}.  All
variables are real-valued.  Given an input $x$, the input variable $\mathbf{x}$,
whose value is to be synthesized with LP, is required to have the identical
activation pattern as $x$, i.e., $ap[\mathbf{x}]=ap[x]$.

We use $\mathbf{u_{k,i}}$ and $\mathbf{v_{k,i}}$ to denote the valuations of a neuron $n_{k,i}$ before
and after the application of ReLU, respectively.
Then, we have the following set $\constraints_1[x]$ of constraints
\label{eq:lp-dir1}
\begin{array}{l}
 \{\mathbf{u_{k,i}}\geq 0 \wedge \mathbf{v_{k,i}}=\mathbf{u_{k,i}} ~|~ ap[x]_{k,i}\geq 0, k\in [2,K), i\in [1..s_k] \} \\
\cup 
  \{\mathbf{u_{k,i}}< 0 \wedge \mathbf{v_{k,i}}=0~|~ap[x]_{k,i}<0, k\in [2,K), i\in [1..s_k]\}
\end{array}
\end{equation}%whenever $ap[x]_{k,i}<0$ for $1<k<K$ and $1\leq i\leq s_k$ . %\paragraph{Activation valuation of each neuron}

Moreover, the activation valuation $\mathbf{u}_{k,i}$ of each neuron is
decided by the activation values $\mathbf{v}_{k-1,j}$ of those neurons in
the prior layer.  Therefore, we add the following set $\constraints_2[x]$
of constraints

\begin{equation} \label{eq:lp-v}
\{  \mathbf{u}_{k,i}=\sum_{1\leq j \leq s_{k-1}} \{{w}_{k-1, j, i}\cdot \mathbf{v}_{k-1,j}\} + \delta_{k,i} ~|~ k\in [2,K), i\in [1..s_k]\}
\end{equation}

Please note that the resulting LP model $\constraints[x] =
\constraints_1[x]\cup \constraints_2[x]$  represents \emph{a symbolic set of
inputs} that have the identical activation pattern as $x$.
Further, we can specify some optimization objective $obj$, and call an LP solver to find the optimal $\textbf{x}$ (if one exists). 

We define a few operations on the set $\constraints[x]$ of constraints. First, 
Let
$\constraints[1\dots k']$ be a partial encoding of $\constraints$ up to layer $k'$. More precisely, we write $\constraints[1\dots k']$ if we substitute $K$ in Expression (\ref{eq:lp-dir1}) and (\ref{eq:lp-v}) by $k'$. 
\end{definition}%%that keeps identical activation signs for neurons until layer $k$. %That is, %However, for %For neurons beyond%layer $k$, there are no constraints on $ap[x][:k]$. % does not require or specify their signs. %\paragraph{Actiavtion pattern with constraint set}

Note that, for each neuron $n_{k,i}$, in Expression~\ref{eq:lp-dir1}, we
have either $\mathbf{u}_{k,i}\geq 0 \wedge
\mathbf{v}_{k,i}=\mathbf{u}_{k,i}$ or $\mathbf{u_{k,i}}< 0 \wedge
\mathbf{v}_{k,i}=0$, but not both.  We write $\constraints_1[x](k,i)$ to
denote the expression that is taken, and write $\constraints_1[x](k,i)_{-1}$
to denote the other expression that is not taken.

\begin{definition}\label{def:ap-except}
Let $\constraints[(k,i)]$ be the same set of constraints as $\constraints$,
except that the constraint  $\constraints_1[x](k,i)$ is substituted by
$\constraints_1[x](k,i)_{-1}$.
\end{definition}

Assume that the value function $\valuefunction$ and the distance function
$\distancefunction$ are linearly encodable.  Without loss of generality, we
still use $\valuefunction$ and $\distancefunction$ to denote their linear
encodings, respectively.

\begin{definition}
Let $\constraints[+\valuefunction]= \constraints\cup \{\valuefunction\}$ and $\constraints[+\distancefunction]=\constraints\cup \{\distancefunction \}$.  
\end{definition}%%Let $lb_{k,i}, ub_{k,i} \in \real$ be the real numbers such that $lb_{k,i} \leq ub_{k,i}$. Now, we define $C[\alpha]$ for $\alpha \subseteq \Omega$ where %\begin{equation}%\Omega= \{(k,i), \mathbf{u_{k,i}} \in [lb_{k,i}, ub_{k,i}]~|~ k\in (2,K], i\in [1..s_k]\}\end{equation} %%
For example, when requiring a neuron $n_{k,i}$ to be activated with values
bounded by $[lb_{k,i}, ub_{k,i}]$, we can use the following operation
$\constraints[+ (\mathbf{u_{k,i}} \in [lb_{k,i}, ub_{k,i}])]$.

Furthermore, we can generalize the above notation and define
$\constraints[ops]$ for $ops$ being a set of operations.  It is obvious that
the ordering of the operations does not matter if there does not exist a
neuron whose value is affected by more than one operations in $ops$.


In this section, we discuss a \emph{safety} requirement that is independent of the test criteria. This is to check \emph{automatically} whether a given test case $x$ is a bug. 
The neural network  $\networks$ represents a function $\hat{f}(x)$, which approximates $f(x): D_{L_1} \to \labels$, a function that models the human perception capability in labeling input examples.
Therefore, a straightforward safety requirement is to require that for all test cases $x\in D_{L_1}$, we have $\hat{f}(x) = f(x)$. However, a direct use of such  requirement is not practical because the number of inputs in $D_{L_1}$  can be too large, if not infinite, to be labeled by human. 

A pragmatic way, as done in many other works including \cite{SZSBEGF2014,HKWW2017}, is to study the following safety requirement. 

\begin{definition}[Safety  Requirement] \label{def:requirement}
  Given a finite set $X$ of correctly labeled inputs, the safety  requirement is to ensure that for all  inputs $x'$ that are \emph{close} to an input $x\in X$, we have $\hat{f}(x') \neq f(x)$. 
\end{definition}%Ideally, the \emph{closeness} between two inputs $x$ and $x'$ is to be measured with respect to the human's perception capability. In practise, this has been approximated by various approaches, including norm-based distance measures.  In this paper, the closeness of two inputs $x_1$ and $x_2$ in Definition~\ref{def:requirement} is concretised as the norm $L^\infty$, i.e., $\distance{x_1-x_2}{\infty} \leq b$ for some bound $b \in \real$. Specifically, we define function %\begin{equation}%not\_safe(x_1,x_2) = \distance{x_1-x_2}{\infty} > b%\end{equation}

Ideally, the \emph{closeness} between two inputs $x$ and $x'$ is to be measured with respect to the human perception capability. In practice, this has been approximated by various approaches, including norm-based distance measures.  In this paper, the closeness of two inputs $x_1$ and $x_2$  is concretised as the norm $L^\infty$, i.e., $\distance{x_1-x_2}{\infty} \leq b$ for some bound $b \in \real$. Specifically, we define the predicate
\begin{equation}
close(x_1,x_2) = \distance{x_1-x_2}{\infty} \leq b
\end{equation}
We remark that our algorithms can work with any definition of closeness as long as it can be encoded as a set of linear constraints. The test generation algorithms in this paper enable the computation of $x_1$ and $x_2$ such that $\distance{x_1-x_2}{\infty}$ 
can be upper bounded by a small number.
A pair of inputs that satisfy the closeness definition are called \emph{adversarial examples if only one of them is correctly labeled by the DNN}.
In our experiments, to exhibit the behavior of adversarial examples, 
we instantiate $b$ with a large enough number, and study the percentage of adversarial examples with respect to the number $b$ (as illustrated in Figure~\ref{fig:ss-distance-map} for one of the criteria). 

Our testing criteria defined in Section~\ref{sec:criteria} feature that the
(sign or value) change of every decision neuron must be supported by (sign
or distance) changes of its condition neurons.  Given a neuron pair
and a testing criterion, we are going to find two activation patterns, and
the two patterns together shall exhibit the changes required by the
corresponding testing criterion.  The inputs matching these patterns
will be added to the final test suite.

At first, we define a routine to call an LP solver, which takes as input a
set $\constraints[x_1]$ of constraints and an optimization objective $obj$,
and returns a valuation $x_2$ for input variable $\textbf{x}$ if one exists. 
For instance, a solver call can be written  as follows.
  \label{eq:lp-call}
 x_2 = lp\_call(\constraints[x_1], obj)
\end{equation}%It returns an input $\textbf{x}$ (if there exists), which 
If returned successfully, $x_2$ satisfies the linear constraints of $\constraints[x_1]$, with respect to
the optimization objective $obj$.


The test suite generation framework is given as in Algorithm
\ref{algo:test-gen}: for every $(n_{k,i},n_{k+1,j})$ of
$\neuronpairs(\networks)$, it calls the $get\_input\_pair(f,n_{k,i},n_{k+1,j})$
method to find an input pair that satisfies the test requirement, according
to the covering method $f\in
\{\covered{SS}{}{},\covered{DS}{d}{},\covered{SV}{\valuefunction}{},$$\covered{DV}{d,\valuefunction}{}\}$.
We remind that $\neuronpairs(\networks)$ is the set of neuron pairs of a DNN (see Definition~\ref{def:neuron-pair}).

\begin{algorithm}[!htp]
  \caption{Test-Gen}\label{algo:test-gen}
  \begin{flushleft}
    \textbf{INPUT:} DNN $\networks$, covering method $f$\\
    \textbf{OUTPUT:} test suite $\mathcal{T}$, adversarial examples $\mathcal{T}'$
  \end{flushleft}
  \begin{algorithmic}[1]
    \For{each neuron pair $n_{k,i}, n_{k+1,j}\in\mathcal{O}(\networks)$}
      \State $x_1,x_2=get\_input\_pair(f,n_{k,i}, n_{k+1,j})$
      \If{$x_1,x_2$ are valid inputs}
        \State $\mathcal{T}\leftarrow\mathcal{T}\cup\{x_1, x_2\}$
        \If{$\networks[x_1].label\neq \networks[x_2].label$ and $close(x_1,x_2)$}
          \State $\mathcal{T}'\leftarrow\mathcal{T}'\cup\{x_1, x_2\}$
        \EndIf
      \EndIf
    \EndFor
    \State\Return{$\mathcal{T}, \mathcal{T}'$}
  \end{algorithmic}
\end{algorithm}

To get an input pair of tests for each corresponding neuron pair, we assume
the existence of a $data\_set$ of correctly labeled inputs.  An input $x_1$
in $data\_set$ serves for the reference activation pattern, by modifying
which we obtain another activation pattern such that the two together shall
be able to support testing conditions specified by the covering method for a
neuron pair $n_{k,i}$ and $n_{k+1,j}$.

Algorithm \ref{algo:aac-get-input-pair} tries to find a pair of inputs that
satisfy requirements of SS Cover for a neuron pair, which specify that
the sign change of $n_{k,i}$ must independently change also the sign of
$n_{k+1,j}$.  In particular, the LP call in
Algorithm~\ref{algo:aac-get-input-pair} comes with the constraints that a
new activation pattern must share the partial encoding of $x_1$'s activation
pattern until layer $k$ (by Definition~\ref{def:ap-partial}), but with signs
of neuron $n_{k,i}$ and $n_{k+1,j}$'s activation being negated (by
Definition \ref{def:ap-except}).  As a matter of fact, this encoding is even
stronger than the requirement in SS Cover, which does not
specify constraints for neurons in prior to layer $k$.  The encoding in the
$lp\_call$ is a compromise for efficiency, as it is computationally
un-affordable to consider combinations of even a subset of neuron
activations.  Consequently, the LP call may fail because the proposed
activation pattern is infeasible, and this explains why we need to prepare a
$data\_set$ for potentially multiple LP calls.

\begin{algorithm}[!htp]
  \caption{$get\_input\_pair$ with the first argument being $\covered{SS}{}{}$}
  \label{algo:aac-get-input-pair}
  \begin{flushleft}
    \textbf{INPUT:} $\covered{SS}{}{}$, neuron pair $n_{k,i},n_{k+1,j}$\\
    \textbf{OUTPUT:} input pair $x_1, x_2$
  \end{flushleft}
  \begin{algorithmic}[1]
    \For{each $x_1 \in data\_set$}
    \State $x_2=lp\_call(\constraints[x_1][1..k][\{(k,i), (k+1,j)\}], \min{\distance{x_2\!-\!x_1}{\infty}})$
      \If{ $x_2$ is a valid input} \Return{$x_1,x_2$}

      \EndIf
    \EndFor
    \State \Return{\_, \_}
  \end{algorithmic}
\end{algorithm}%\paragraph{DS test suite generation}%DS method catches the testing cases when the activation direction of the decision neuron changes, but%it is affected by value, instead of activation sign, changes of condition neurons. For each neuron%$p_{k,i}$, the $ds\_get\_input\_pair$ below returns a pair of inputs that satisfies this condition.

The functions that find input pairs subject to the other test requirements
largely follow the same structure as the SS Cover case, with the addition
of distance change or value change constraints.  Note that when implementing
test generation for $cov^d_{DS}$ and $cov^{d,h}_{DV}$, there is no need to
go through every neuron pair, as these metrics do not require the
individual change of a particular condition neuron as long as the overall
behavior of all condition neurons of the decision neuron fall into certain
distance change (and they do not exhibit sign change).

  \caption{$get\_input\_pair$ with the first argument being $\covered{DS}{d}{}$}
  \begin{flushleft}
    \textbf{INPUT:} $\covered{DS}{d}{}$, neuron $n_{k,i}$\\
    \textbf{OUTPUT:} input pair $t_1, t_2$
  \end{flushleft}
  \begin{algorithmic}[1]
    \For{each $x_1 \in data\_set$}
    \State $x_2=lp\_call(\constraints[x_1][1..k-1](\{(k,i),\distancefunction\}), \min{\distance{x_2\!-\!x_1}{\infty}})$
      \If{ $x_2$ is a valid input} \Return{$x_1,x_2$}
      \EndIf
    \EndFor
    \State \Return{\_, \_}
    \label{algo:ds-get-input-pair}
  \end{algorithmic}
\end{algorithm}%Then, similar as in the SS case, by applying Algorithm \ref{algo:ds-get-input-pair} on each neuron, we are%able to obtain a test suite that covers DS requirements, and a set of adversarial examples found during the%test generation. %\paragraph{SV test suite generation}\begin{algorithm}[!htp]
  \caption{$get\_input\_pair$ with the first argument being $\covered{SV}{\valuefunction}{}$}
  \begin{flushleft}
    \textbf{INPUT:} $\covered{SV}{\valuefunction}{}$, neuron pair $n_{k,i},n_{k+1,j}$\\
    \textbf{OUTPUT:} input pair $x_1, x_2$
  \end{flushleft}
  \begin{algorithmic}[1]
    \For{each $x_1 \in data\_set$}
    \State $x_2\!=\!lp\_call(\constraints[x_1][1..k](\{(k,i), (k\!+\!1,j), \valuefunction\}), \min{\distance{x_2\!-\!x_1}{\infty}})$
      \If{ $x_2$ is a valid input} \Return{$x_1,x_2$}
      \EndIf
    \EndFor
    \State \Return{\_, \_}
    \label{algo:sv-get-input-pair}
  \end{algorithmic}
\end{algorithm}%\paragraph{DV test suite generation}\begin{algorithm}[!htp]
  \caption{$get\_input\_pair$  with the first argument being $\covered{DV}{d,\valuefunction}{}$}
  \begin{flushleft}
    \textbf{INPUT:} $\covered{DV}{d,\valuefunction}{}$, neuron $p_{k,i}$\\
    \textbf{OUTPUT:} input pair $t_1, t_2$
  \end{flushleft}
  \begin{algorithmic}[1]
    \For{each $x_1 \in data\_set$}
    \State $x_2=lp\_call(\constraints[x_1][1..k](\{(k+1,j), \distancefunction\}), \min{\distance{x_2\!-\!x_1}{\infty}})$
      \If{ $x_2$ is a valid input} \Return{$x_1,x_2$}
      \EndIf
    \EndFor
    \State \Return{\_, \_}
    \label{algo:dv-get-input-pair}
  \end{algorithmic}
\end{algorithm}

In our implementation, every time the $get\_input\_pair$ is called we
randomly select 40 correctly labeled inputs to construct the $data\_set$. 
The objective $\min{\distance{x_2\!-\!x_1}{\infty}}$ is used in all LP
calls, to find good adversarial examples with respect to the safety
requirement.  Moreover, we use
\valuefunction = \frac{v_{k+1,j}[x_2]}{v_{k+1,j}[x_1]}\geq \sigma
\end{equation}%
with $\sigma=2$ for $cov^g_{SV}$ and $\sigma=5$ for $cov^{d,g}_{DV}$.  We
admit that such choices are experimental.  Meanwhile, for generality and to
speed up the experiments, we leave the distance function $h$ unspecified. 
Providing a specific $\distancefunction$ may require more LP calls to find a
$x_2$ (because $\distancefunction$ is an additional constraint) but the
resulting $x_2$ can be better with respect to $\distancefunction$.

We note that $|\neuronpairs(\networks)|= \sum_{k=2}^{K} s_k \cdot s_{k-1}$. 
Therefore, when the size of the network is large, the generation of a test
suite may take a  long time.  To work around this, we may consider an
alternative definition of $\neuronpairs(\networks)$ by letting
$(n_{k,i},n_{k+1,j})\in \neuronpairs(\networks)$ only when the weight
$w_{k+1,i,j}$ is one of the $\kappa$ largest among $\{|w_{k+1,i',j}|~|~i'\in
[1\dots s_{k}]\}$.  The rationale behind is that condition neurons do not
equally affect their decision, and those with higher (absolute) weights are
likely to have a larger influence.

\section{Experiments}%We have implemented our test case generation algorithms in Section \ref{sec:test-gen} within %\emph{Deep Learning Testing (DLT)} software tool. IBM CPLEX solver is used to solve the linear programming call.

We use the well-known MNIST \emph{Handwritten Image Dataset} to train a~set
of 10 fully connected DNNs to perform classification.  Each DNN has an input
layer of $28\times 28=784$ neurons and an output layer of $10$ neurons.  The
number of hidden layers for each DNN is randomly sampled from the set of $\{3,
4, 5\}$ and at each hidden layer, the number of neurons are uniformly
selected from $20$ to $100$.  Every DNN is trained until an accuracy of at
least $97.0\%$ is reached on the MNIST validation data.  Details on the
structure of the DNNs are given in Table~\ref{tab:results}.

All implementations and the data discussed in this section are available
online\footnote{Available from github link \url{https://github.com/theyoucheng/deepcover}}.
Most experiments were conducted on a MacBook 
with 2.5\,GHz Intel Core i5 and 8\,GB of memory.

\subsection{Hypothesis 1: Neuron Coverage is Easy}

Neuron coverage is a simple test coverage metric proposed
in~\cite{PCYJ2017}.  It~was applied as one of the dual optimization
objectives to compute adversarial examples for DNNs.  Unfortunately, as a
testing criterion, neuron coverage is inadequate.  In particular, a test
suite with high neuron coverage is not sufficient to increase confidence in
the neural network in safety-critical domains.

To demonstrate this, for each DNN tested, we randomly pick $25$ images from
the MNIST test dataset.  For each selected image, to maximize the neuron
coverage, if an input neuron is not activated (i.e., its activation value
is equal to 0), we sample its value from $[0, 0.1]$.  Then we measure
the neuron coverage of the DNN by using the generated test suite of $25$
images.  As a result, for all $10$ DNNs, we obtain almost 100\% neuron
coverage.  Our simple experiment here demonstrates that it is
straight-forward to obtain a trivial test suite that has high neuron
coverage but does not provide any adversarial examples.

As stated in~\cite{ZHM1997}, the role of test adequacy is a contributory
factor in building confidence in the repeated cycle of testing, debugging,
modifying program, and then testing again.  The above experimental result
shows that neuron coverage does not serve well in this role.

\subsection{Hypothesis 2: Random Testing is Inefficient}\label{sec:random}

Previous research~\cite{ZHM1997} has observed that, while easy to perform,
random testing does not yield high confidence in software reliability, or
more formally, test adequacy is typically low.
Our experiments confirm this for the specific case of DNNs.

For each DNN, we first choose an image from the MNIST test dataset, denoted
by~$x$.  Subsequently, we randomly sample $10^5$ inputs in the region
bounded by $x\pm 0.1$, and we check whether an adversarial example exists
for the original image $x$.  Using this process, we have obtained
adversarial examples for only a single one of the $10$ DNNs; for the other
nine DNNs, we did not observe any adversarial examples among the $10^5$
randomly generated images.

More importantly, random testing does not provide information on the
adequacy level achieved.  Though it can be used to generate test cases for
simple criteria such as neuron coverage, it is not trivial to apply random
testing for complex metrics, such as the ones given in this paper, which
require independent sign change or value change of particular neuron
combinations.\footnote{However, we have no intention to discourage the use of
dynamic testing for DNNs, which has been under-investigated.  On the
contrary, we tend to expect that a more carefully designed dynamic testing method,
together with white-box techniques like algorithms in our paper, can improve
the performance and efficiency for testing DNNs.}\subsection{SS, SV, DS, and DV Cover}

We apply the test generation algorithms given in Section~\ref{sec:test-gen},
and we record the testing results for the SS, SV, DS, and DV covering
methods.  Besides the coverage $Mf$ for each method, we also measure the
percentage of adversarial examples among all test pairs in the test suite,
which is denoted as $AEf$ with
$f\in\{\covered{SS}{}{},\covered{DS}{d}{},\covered{SV}{\valuefunction}{},\covered{DV}{d,\valuefunction}{}\}$.
We further investigate the use of the ``top 10 weights'' simplification of
SS Cover, results of which are represented by $M{cov_{SS}^{w10}}$ and
$AE{cov_{SS}^{w10}}$.  For clarity, our experiments are classified into four
classes: \emph{\ding{192}~bug finding \ding{193}~DNN safety statistics
\ding{194}~testing efficiency \ding{195}~DNN internal structure
analysis}, and results will be labeled correspondingly.

\paragraph{DNN Bug finding \ding{192}}

The full results from our testing approaches are reported in
Table~\ref{tab:results}.  In our set-up, the input layer may contribute
a disproportionately large number of covered neuron pairs.  Thus,
in Table~\ref{tab:results}, we separate them from condition neurons
in the input layer.  As we are going to see later, coverage of neuron pairs
at front layers is easier than covering their deeper counterparts.

\definecolor{Gray}{gray}{0.9}\definecolor{LightCyan}{rgb}{0.88,1,1}\newcolumntype{g}{>{\columncolor{Gray}}r}\begin{table}[!htb]
  \begin{center}
    \def\arraystretch{1.2}    
\scalebox{0.85}{
    \begin{tabular}{|c|c|rg|rg|rg|rg|rg|} \hline
      & hidden layers & $M{cov_{SS}}$ & $AE{cov_{SS}}$ &  $M{cov_{DS}^d}$ & $AE{cov_{DS}^d}$ &  $M{cov_{SV}^g}$ & $AE{cov_{SV}^g}$ &  $M{cov_{DV}^{d,g}}$ & $AE{cov_{DV}^{d,g}}$ \\\hline
      $\networks_1$ & 67x22x63       & 99.7\% & 18.9\% & 100\% & 15.8\% & 100\% &6.7\% & 100\% & 21.1\%  \\\hline
      $\networks_2$ & 59x94x56x45    & 98.5\% &  9.5\% & 100\% & 6.8\% & 99.9\% & 3.7\% & 100\% & 11.2\% \\\hline
      $\networks_3$ & 72x61x70x77    & 99.4\% &  7.1\% & 100\% & 5.0\% & 99.9\% & 3.7\% & 98.6\% & 11.0\%     \\\hline
      $\networks_4$ & 65x99x87x23x31 & 98.4\% &  7.1\% & 100\% & 7.2\% & 99.8\% & 3.7\% & 98.4\% & 11.2\%  \\\hline
      $\networks_5$ & 49x61x90x21x48 & 89.1\% & 11.4\% & 99.1\%& 9.6\%& 99.4\%& 4.9\%& 98.7\%& 9.1\% \\\hline
      $\networks_6$ & 97x83x32       &100.0\% &  9.4\% & 100\%& 5.6\%& 100\%& 3.7\%& 100\%& 8.0\%  \\\hline
      $\networks_7$ & 33x95x67x43x76 & 86.9\% &  8.8\% & 100\%& 7.2\%& 99.2\%& 3.8\%& 96\%& 12.0\% \\\hline
      $\networks_8$ & 78x62x73x47    & 99.8\% &  8.4\% & 100\%& 9.4\%& 100\%& 4.0\%& 100\%& 7.3\% \\\hline
      $\networks_9$ & 87x33x62       &100.0\% & 12.0\% & 100\%& 10.5\%& 100\%& 5.0\%& 100\%& 6.7\% \\\hline
   $\networks_{10}$ & 76x55x74x98x75 & 86.7\% &  5.8\% & 100\%& 6.1\%& 98.3\%& 2.4\%& 93.9\%& 4.5\% \\\hline
    \end{tabular}
}
  \end{center}
  \caption{Results on 10 DNNs with the SS, DS, SV and SV covering methods}
  \label{tab:results}
\end{table}\begin{table}[]
\centering
\def\arraystretch{1.2}    
\scalebox{1}{
  \begin{tabular}{|c|c|r|rc|r|r|r|} \hline
  \multirow{2}{*}{} & \multirow{2}{*}{$\sum_{1<k\leq K}s_k$} &  \multirow{2}{*}{$|\neuronpairs(\networks)|$} & \multicolumn{5}{c|}{$|\testsuites|$}\\\cline{4-8}
                             & & & $cov_{SS}$ &  $(cov_{SS}^{w10})$  & $cov_{DS}^d$ & $cov_{SV}^g$  & $cov_{DV}^{d,g}$ \\ \hline
    $\networks_1$ & 162 & 3490 & 3478& (949)& 95 & 3490& 95\\\hline
    $\networks_{2}$ & 264 & 13780& 13569& (2037)& 205& 13779& 205\\\hline
    $\networks_{3}$ & 290 & 14822& 14739 & (2170)& 218& 14820& 215\\\hline
    $\networks_{4}$ & 315 & 18072& 17779 & (2409)& 250& 18028& 246\\\hline
    $\networks_{5}$ & 279 & 11857& 10567& (2120)& 228& 11780& 227\\\hline
    $\networks_{6}$ & 222 & 11027& 11027& (1250)& 125& 11027& 125\\\hline
    $\networks_{7}$ & 324 & 16409& 14264& (2593)& 291& 16275& 280\\\hline
    $\networks_{8}$ & 270 & 13263& 13235& (1917)& 192& 13263& 192\\\hline
    $\networks_{9}$ & 192 & 5537& 5537 & (1050)& 105& 5537& 105\\\hline
    $\networks_{10}$ & 388 & 23602& 20459& (2806)& 312& 23203& 293\\\hline
  \end{tabular}
}
\caption{Test suites generated for DNNs using different criteria}
\label{tab:tests}
\end{table}
The results in Table \ref{tab:results} are promising.
  \item Our test generation algorithms are effective, as they reach high coverage for all covering criteria.
  \item The covering methods designed are useful. This is supported by the fact that a significant portion of 
    adversarial examples have been identified.
\end{itemize}\paragraph{DNN safety analysis \ding{193}}

The coverage $Mf$ and adversarial percentage $AEf$ together provide
quantitative statistics to evaluate a DNN.  Generally speaking, a DNN that
has a high coverage ($Mf$) test suite with low adversarial example
percentage ($AEf$) is considered more robust.  At the same time, we can
quantify the \emph{adversarial quality}.

Because our test generation minimizes the distance when computing new
input pairs (from which adversarial examples are distinguished), to evaluate
the quality of obtained adversarial examples, we can plot a distance curve
to see how close the adversarial example is to the correct input.  Let us
take a further look at the results of SS Cover for the last three DNNs in
Table~\ref{tab:results}.  As illustrated in Figure~\ref{fig:ss-distance-map},
the horizontal axis measures the $L^{\infty}$
distance, and the vertical axis reports the accumulated percentage of
adversarial examples fall into this distance.  A more robust DNN will have
its shape in the small distance end (towards 0) lower, as the reported
adversarial examples are relatively farther from their original correct
inputs.  Intuitively, this means that more effort needs to be made in order
to fool a DNN from correct classification to a wrong output label.
Figure~\ref{fig:ads} exhibits several adversarial examples found during the testing
with different distances.  The value of $b$ is used to indicate the
closeness and instantiate the safety property we define in Section~\ref{sec:safety}.

\begin{figure}[!htb]
\centering
\includegraphics[width=0.6\columnwidth]{images/ss-distance-map}
\caption{SS Cover distance curves of 3 DNNs}
\label{fig:ss-distance-map}
\end{figure}\begin{figure}[!htb]
\captionsetup{justification=centering}
\centering
\begin{tabular}{cc}
	
  \subfloat[$9$ to $8$ with $b=0.02$]{
    \includegraphics[width=0.30\columnwidth]{images/9-8-d002}
    \label{fig:ad1}
  }
  &
  \subfloat[$8$ to $2$ with $b=0.05$]{
    \includegraphics[width=0.30\columnwidth]{images/8-2-d005}
    \label{fig:ad2}
  }
  \\
  \subfloat[$1$ to $7$ with $b=0.1$]{
    \includegraphics[width=0.30\columnwidth]{images/1-7-d01}
    \label{fig:ad3}
  }
  &
  \subfloat[$0$ to $9$ with $b=0.2$]{
    \includegraphics[width=0.30\columnwidth]{images/0-9-d02}
    \label{fig:ad4}
  }
\end{tabular}
\caption{Examples of adversarial examples}
\label{fig:ads}
\end{figure}\begin{figure}[!htb]
\centering
\includegraphics[width=0.6\columnwidth]{images/ss-top10}
\caption{SS Cover vs SS Cover with top 10 weights}
\label{fig:ss-top10}
\end{figure}\paragraph{SS Cover with top weights \ding{194}}

Detailed information for the generated test suite for each covering method
is reported in Table \ref{tab:tests}, which lists the number of neurons and
neuron pairs in a DNN, and the number of test pairs inside the test suite
under different criteria.  As expected, covering methods that request independent
sign change of condition neurons need to examine a much larger set of test
cases.

Figure~\ref{fig:ss-top10} shows the difference, on coverage and adversarial
percentage, between SS Cover and its simplification for the testing of 10
DNNs.  In general, the two are comparable.  This is very useful in practice,
as the ``top weights'' simplification mitigates the size of test suite
resulted, and it is thus able to behave as a faster pre-processing phase and
even alternative with comparable results for SS Cover.

\begin{figure}[!htb]
\captionsetup{justification=centering}
\centering
\begin{tabular}{cc}
	
  \subfloat[Layerwise coverage]{
    \includegraphics[width=0.49\columnwidth]{images/layerwise-ss-coverage}
    \label{fig:scc-layerwise-coverage}
  }
  &
  \subfloat[Adversarial examples at different layers]{
    \includegraphics[width=0.49\columnwidth]{images/layerwise-ss-bugs}
    \label{fig:scc-layerwise-bugs}
  }
\end{tabular}
\caption{SS Cover: layerwise results} 
\label{fig:ssc-layerwise}
\end{figure}\paragraph{Layerwise behavior \ding{195}}

Through our experiments, we believe it is also worth showing that different
layers of a DNN exhibit different behaviors in testing. 
Figure~\ref{fig:ssc-layerwise} reports the SS Cover results, collected in
adjacent layers.  In particular, Figure~\ref{fig:scc-layerwise-coverage}
gives the percentage of covered neuron pairs within individual adjacent
layers.  As we can see, when going deeper into the DNN, it can become harder
for the cover of neuron pairs.  Under such circumstances, to improve the
coverage performance, the use of larger $data\_set$ when generating test
pairs is needed.  Figure~\ref{fig:scc-layerwise-bugs} gives the percentage
of adversarial examples found at different layers.  Interestingly, it seems
that most adversarial examples be found around the middle layers of all DNNs
tested.

\begin{table}[]
\centering
\scalebox{1}{
  \begin{tabular}{|l|ccc|ccc|ccc|} \hline
    \multirow{2}{*}{         } & \multicolumn{3}{c|}{$\networks_8$} &  \multicolumn{3}{c|}{$\networks_9$} & \multicolumn{3}{c|}{$\networks_{10}$}\\\cline{2-10}
                             & \#vars & $|\constraints|$ & $t$ & \#vars & $|\constraints|$ & $t$ & \#vars & $|\constraints|$ & $t$ \\\hline
    $L2\textnormal{-}3$             & 864 & 3294 & 0.58 & 873 & 3312 & 0.57 & 862 & 3290 & 0.49 \\
    $L3\textnormal{-}4$             & 926 & 3418 & 0.84 & 906 & 3378 & 0.61 & 917 & 3400 & 0.71 \\
    $L4\textnormal{-}5$             & 999 & 3564 & 0.87 & 968 & 3502 & 0.86 & 991 & 3548 & 0.75 \\
    $L5\textnormal{-}6$             & 1046& 3658 & 0.91 & --& --   &  --    & 1089& 3744 & 0.82 \\
    $L6\textnormal{-}7$             & -- & -- & -- & -- & -- & -- & 1164 & 3894 & 0.94 \\ 
    \hline
  \end{tabular}
}
\caption{Number of variables and constraints, and time cost of each LP call in test generation}
\label{tab:lp-call}
\end{table}\paragraph{Cost of LP call \ding{194}}

Since LP encoding of the DNN (partial) activation pattern plays a key part
in our test generation, in this part we give details of the LP call cost,
even though linear programming is widely accepted as an efficient method. 
For every DNN, we select a set of neuron pairs, where each decision neuron
is at a different layer.  Then, we measure the number of variables and
constraints, and the time $t$ in seconds (averaged over 100
runs) spent on solving each LP call.  Results in Table
\ref{tab:lp-call} confirm that the LP model of a partial activation pattern
is indeed lightweight, and its complexity increases in the linear manner as
traversing into deeper layers of a DNN.

\subsection{Convolutional Neural Networks \ding{195}}%Criteria and testing techniques 

The testing approach in this paper can be generalized to Convolutional
Neural Networks (CNNs).  In a CNN, a convolutional layer consists of
multiple \emph{feature maps}.  The weights and bias that define a feature
map are often said to be \emph{filters}.  Given a decision neuron inside
some feature map, its activation is computed by a subset of precedent
neurons that are processed by the associated filter.  To investigate the
testing of CNNs, we trained two networks $\networks^c_1$ and
$\networks^c_2$, each having two convolutional layers followed by one fully
connected layer of 128 neurons.  $\networks^c_1$ (resp.~$\networks^c_2$) has
20 (resp.~2) filters for the first convolutional layer and 40 (resp.~4)
filters for the second convolutional layer.  The filters have size $5\times
5$ and every convolutional layer is augmented with a so-called max-pooling
layer of size $2\times 2$.  Overall, $\networks^c_1$ has 14208 hidden
neurons and $\networks^c_2$ is much smaller with 1536 hidden neurons.  This
is designed on purpose to conduct our testing approaches with different
scalabilities.

The emphasis of this part is on understanding the impact of filters in CNN
through testing, and results of SS Cover are analysed at feature map level. 
We use $f_{k,i}$ to denote the $i$-th feature map in the $k$-th
convolutional layer, and $M{cov_{SS}}^{f_{k,i}}_{f_{k+1,j}}$ represents the
SS coverage among neuron pairs such that the condition neuron and decision
neuron are from $f_{k,i}$ and $f_{k+1,j}$ respectively.  Correspondingly,
$AE{cov_{SS}}^{f_{k,i}}_{f_{i+1,j}}$ is for the adversarial percentage. 
Results in Table \ref{tab:cnn-results} show the detailed testing information
on several feature maps of the two CNNs.  The use of extra filters is
justified by our testing, as a much smaller number of adversarial examples
were found for $\networks^c_1$.

\begin{table}[!htb]
  \begin{center}
  \scalebox{1}{
    \begin{tabular}{|c|rg|rg|} \hline
      & $M{cov_{SS}}^{f_{1,1}}_{f_{2,1}}$ & $AE{cov_{SS}}^{f_{1,1}}_{f_{2,1}}$ & $M{cov_{SS}}^{f_{1,1}}_{f_{2,2}}$ & $AE{cov_{SS}}^{f_{1,1}}_{f_{2,2}}$\\\hline
       $\networks^c_1$ & 99.7\%& 1.6\%& 99.7\%& 1.5\%\\\hline
       $\networks^c_2$ & 100\%& 7.6\%& 100\%& 6.8\%\\\hline
    \end{tabular}
  }
  \end{center}
  \caption{Feature map wise SS Cover results}
  \label{tab:cnn-results}
\end{table}
defined either at the syntactic level (or code-level), such as statement
coverage, branch coverage, MC/DC coverage, path coverage,
etc.~\cite{ZHM1997,JH2011,SWMPHS2017}, or semantic level (or model level),
such as state and transition coverage~\cite{AO2008,NSVT2007}.  In the
following, we briefly review existing work on how to validate safety properties
of DNNs.

\paragraph{Existing Test Coverage Metrics}

Up to now, there are two proposals for test coverage criteria. 
In~\cite{PCYJ2017}, \emph{neuron coverage} is proposed, and it  
is applied in \cite{tian2017deeptest} to guide the testing of
DNN-driven autonomous cars. 
Given an input $x$, we can observe the activations of the hidden neurons 
and compute the set $ReLU(x)$ of hidden neurons whose ReLU activation 
function is activated. Let $P_H$ be the set of hidden neurons.  
Given a test suite $\testsuites$, its neuron coverage is
Cov_{neuron}(X) = |\bigcup\{ReLU(x)~|~x\in \testsuites\}| / |P_H| 
\end{equation}%
While neuron coverage bears some similarity with statement coverage, as we
explained in the paper, it is a very coarse criterion: it is easy to find a
test suite that achieves 100\% neuron coverage but the network still
allows trivial adversarial examples.  Thus, neuron coverage does not even
satisfy the expectations we usually have for a test suite with high statement
coverage for conventional software~\cite{PCYJ2017}. %%, i.e., although passing a test suite does not suggest a guarantee, the bugs should appear only in corner cases. %A gradient descent based approach is employed in \cite{PCYJ2017} to generate test cases. 

In~\cite{WHK2018}, the input space is discretised with
$\tau$-hyper-rectangles, and then one test case is generated for each
hyper-rectangle.  It is shown that, if $\tau$ is small enough (precisely
$\tau < 2\ell /\hbar$ for $\ell$ the minimum confidence gap and $\hbar$ the
Lipschitz constant of the network), then this testing approach can provide a
guarantee, i.e., it can find all adversarial examples, and is able to claim
safety when no adversarial example can be found.  Thus, safety coverage is a
strong criterion, but the generation of a test suite can be very expensive;
therefore, \cite{WHK2018} uses a Monte-Carlo Tree Search based approach to
generate test cases.  Assume that we have a test suite $\testsuites$ and for
each $x\in \testsuites$ it is associated with a hyper-rectangle $\eta_x$,
then we have the safety coverage defined as follows:
Cov_{safety}(X) = ||\bigcup\{ \eta_x~|~x\in \testsuites\}|| / ||\eta|| 
\end{equation}
where $||\eta||$ measures the size of the region $\eta$. 

\paragraph{Adversarial Example Generation of DNNs}

Most existing work, e.g.,
\cite{SZSBEGF2014,GSS2014,AJJ2014,SAOP2016,CW2016}, applies various
heuristic algorithms, generally using search algorithms based on gradient
descent or evolutionary techniques.  \cite{PMJFCS2015} construct a saliency
map of the importance of the pixels based on gradient descent and then
modify the pixels.  These approaches may be able to find adversarial
examples efficiently, but are \emph{not able to provide any guarantee} (like
verification) or \emph{any certain level of confidence} (like testing) about
the nonexistence of adversarial examples when the algorithm fails to find
one.

\paragraph{Automated Verification of DNNs}

There are two ways to perform safety verification for DNNs.  The first is
to reduce the problem into a constraint solving problem.  Notable work
includes, e.g., \cite{PT2010,katz2017reluplex}.
Moreover, 
determines whether an output value of a DNN is reachable from a given input
subspace, and reduce the problem to a MILP problem.  \cite{dutta2017output}
considers the range of output values from a given input subspace.  Their
approach interleaves local search (based on gradient descent) with global
search (based on reduction to MILP).  These approaches can only work with
small networks
with a few hundred hidden neurons. 

The second approach is to discretise the vector spaces of the input or hidden layers
and then to apply exhaustive search algorithms or Monte-Carlo tree search
algorithm on the discretised spaces.  The guarantees are achieved by
establishing local assumptions such as minimality of manipulations in
\cite{HKWW2017} and minimum confidence gap for Lipschitz networks in
\cite{WHK2018}.  This approach has been shown to be applicable to
state-of-the-art networks.

\section{Conclusions}\label{sec:concl}

We have proposed a set of novel test criteria for DNNs, developped for each
criterion an algorithm for test suite generation, and implemented the
algorithms in a software tool.  Our experiments on a set of DNNs trained on
the MNIST data set show promising results, indicating the effectiveness of
both the test criteria and the test case generation algorithms for testing
the DNNs.  Future work includes even more efficient test case generation
algorithms that do not require linear programming.


\bibliographystyle{unsrt}
\bibliography{all}

\end{document}


