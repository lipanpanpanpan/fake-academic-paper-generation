\listfiles
\documentclass{article}
\pdfoutput=1

\usepackage[final, nonatbib]{nips_2016}

\usepackage{paralist}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts

\usepackage{url}    
\usepackage{amsmath}
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{wrapfig}
\usepackage{color}
\newcommand{\todo}[1]{\textcolor{red}{\textbf{Fix:} \emph{#1}}}


\usepackage[square, sort, comma, numbers]{natbib}
\usepackage{amsmath,amssymb}
\usepackage[pdftex]{graphicx}
\usepackage{algorithm2e}

\begin{document}

\title{Stochastic Function Norm Regularization of Deep Networks}
\author{
  Amal Rannen Triki\thanks{ART is also affiliated with KU Leuven.}\\
  Dept.\ of Computational Science and Engineering\\
  Yonsei University\\
  Seoul, South Korea\\
  \texttt{amal.rannen@yonsei.ac.kr} 
  \And
  Matthew B.\ Blaschko\\
  Center for Processing Speech and Images\\
  Departement Elektrotechniek\\
  KU Leuven, Belgium\\   \texttt{matthew.blaschko@esat.kuleuven.be} \\
}
\maketitle              % typeset the title of the contribution

\begin{abstract}
Deep neural networks have had an enormous impact on image analysis. State-of-the-art training methods, based on weight decay and DropOut, result in impressive performance when a very large training set is available. However, they tend to have large problems overfitting to small data sets. Indeed, the available regularization methods deal with the complexity of the network function only indirectly. In this paper, we study the feasibility of directly using the $L_2$ function norm for regularization. Two methods to integrate this new regularization in the stochastic backpropagation are proposed. Moreover, the convergence of these new algorithms is studied. We finally show that they outperform the state-of-the-art methods in the low sample regime on benchmark datasets (MNIST and CIFAR10). The obtained results demonstrate very clear improvement, especially in the context of small sample regimes with data laying in a low dimensional manifold.  Source code of the method can be found at \url{https://github.com/AmalRT/DNN_Reg}.
\end{abstract}

\section{Introduction}

Deep Neural Networks (DNNs) have been shown to be a powerful tool in learning from large datasets in many domains. However, they require a large number of training samples in order to have reasonable generalization power. Indeed, this fact tends to limit their application to the type of images where it is easy to construct a large labeled database, such as natural images where collection can be done from image sharing websites and labels obtained at low cost via crowdsourcing. When only few labeled samples are available, such as in medical imaging, DNNs tend to overfit to the data and may not be competitive with methods for which known tractable methods for stronger regularization are available, e.g.\kernel methods.


Generally in learning theory, one wants to minimize the statistical risk $
\mathcal{R}(W) = \int \ell(f_W(x),y)dP(x,y)$, where $\ell$ is a loss a function and $P(x,y)$ is the data distribution of inputs and outputs, respectively. However, as $P$ is generally unknown, only the empirical risk $\mathcal{R}_N(W) = \frac{1}{N}\sum_{i=1}^N \ell(f_W(x_i),y_i)$ is accessible.
Statistical learning theory \cite{vapnik2013nature} gives conditions for the empirical risk to be a good approximation of the statistical risk, by giving a bound of the form:
\begin{equation}
\mathcal{R}(W) \le \mathcal{R}_N(W) + g(h) / \sqrt{N},
\end{equation}
where $g(h)$ is an increasing function of $h$, the Vapnikâ€“Chervonenkis dimension (VC dimension). This measure depends on the function set. When $g(h)$ is small, the minimizer of the empirical risk gives a small statistical risk even when $N$ is small. Through their activation function and their weights, DNNs index very efficiently a large function space with few parameters \cite{cohen2015expressive}. However, the VC dimension of this function set tends to be high. Indeed, many researchers have studied the VC-dimension of some simple neural networks \cite{bianchini2014complexity, karpinski1997polynomial, sontag1998vc}. It has been shown that, for example, for networks with only sigmoidal activation functions, the optimal bound on VC dimension is $\mathcal{O}(\rho^2)$, where $\rho$ is the number of weights. The authors of~\cite{bianchini2014complexity} also show that a deep network implement functions of higher complexity than a shallow network with the same number of weights (i.e.\with the same generalization power according to the VC-dimension bounds), proving one of the many advantages of DNNs. Nevertheless, the DNNs that have become popular recently tend to have a high number of weights (For example, the Oxford VGG networks have more that 110 million parameters~\cite{simonyan2014very}), which makes their generalization power decrease significantly for small databases. For this reason, an appropriate regularization is required. Training a DNN with regularization is solving the following problem:
\begin{equation}
\arg\min_{W} \mathcal{R}_N(W) + \lambda\Omega(f_W).
\label{eq:prob2}
\end{equation}
Much research has approached the idea of regularized DNN training. However, until now, this regularization tends to be poor compared for examples to regularization in kernel methods~\cite{Scholkopf:2001:LKS:559923}. The common idea behind regularization is the application of Occam's razor: if several solutions exist for a given problem, we favor the ``simplest'' solution.  Which notion of simplicity is employed typically is determined by prior knowledge or computational considerations. Two categories of regularization can be distinguished in the existing literature:
\begin{inparaenum}[(i)]
\item regularizing by controlling the magnitude of the weights $W$($\ell_1$ and $\ell_2$ regularization \cite{Goodfellow-et-al-2016-Book,moody1995simple});
\item regularizing by controlling the network architecture (DropOut \cite{hinton2012improving} and DropConnect \cite{wan2013regularization}). 
\end{inparaenum} 
\iffalse
To control the magnitude of the weights, the term $\Omega(f_W)$ in \eqref{eq:prob2} is replaced by another constraint operating on the vector of weights $W$. The most commonly used constraints are the $\ell_1$ and $\ell_2$\footnote{This method is known as weight decay.} norms of $W$ \cite{moody1995simple}. In this type of regularization, the main goal is to fit the training data, while keeping the weights small, which means a reasonably complex network. 
In the regularization based on topology control, rather than introducing a regularization term, some researchers have explored the possibility of keeping the problem \eqref{eq:prob1} while changing the architecture during the training process:
\begin{inparaenum}[(i)]
\item The first (and more popular) way to apply this idea is a method known as DropOut \cite{hinton2012improving}: At each step of training, half of the neuron activation functions are randomly set to zero. This makes the training procedure similar to model averaging, and improves the generalization of the resulting network. 
\item The second method is inspired from the DropOut method; rather than dropping activation functions, DropConnect sets a random part of the weights to zero \cite{wan2013regularization}.
\end{inparaenum}
Even if these methods have been shown to be efficient on data where a big database can be constructed, they fail when only few samples are available. Indeed, these regularization methods are poor and deal with the complexity of $f_W$ only indirectly.
\fi
In order to make the regularization stronger, we study in this paper the feasibility of using $L_2$ function norm:
\begin{equation}
\Omega(f_W) = \| f_W\|_2^2 =\int \langle f_W(x), f_W(x) \rangle dx.
\end{equation} 
As the exact value of this norm, and more importantly of its gradient, are not accessible, a good approximation is needed. Two approximations are considered.


The first idea is to consider the function norm with respect to the probability measure (we suppose for simplicity that all the distributions admit density functions): 
\begin{equation}
\Omega(f_W) =\int \langle f_W(x), f_W(x) \rangle dP(x),
\end{equation}
where $P$ is the marginal data distribution:
\begin{equation}
P(x):= \int P(x,y) dy.
\label{eq:proba_x}
\end{equation}
One can easily see that this is a valid function norm~\cite{galambos1995advanced}, and furthermore is just a weighted canonical function norm such that the regularization has a higher effect on the data that has the higher probability to be observed.


The second idea is to use a Markov-chain sampling method for numerical integration~\cite{andrieu2003introduction}. Here, the idea is to generate samples from a probability density proportional to $\| f_W(x)\|_2^2$ (vector $\ell_2$ norm), and to use them to approximate the integral by:
\begin{equation}
\int \langle f_W(x), f_W(x) \rangle dx \approx \|\alpha\|_1^{-1} \sum_{i=1}^n \alpha_{i} \langle f_W(x_i), f_W(x_i) \rangle
\end{equation}
for some positive weights $\alpha_i$ and samples $x_i$.
The generated samples will be concentrated around the higher values of $\| f_W(x)\|_2^2$, and the regularization will have a higher effect on these regions.


In Section~\ref{method}, we detail these two methods, and describe the algorithms we use in order to integrate them into gradient descent. In Section~\ref{experiment}, we describe experiments using MNIST and CIFAR databases, and report the results. Finally, in Section~\ref{discussion}, the results are  discussed.

\section{Methods and algorithms : Function norm regularization}
\label{method}
\subsection{Function norm with respect to a probability measure}
For this first method, our function norm, written another way is
\begin{equation}
\| f_W\|_{L_2(\mathbb{R}^d, dP)}^2 = \mathbb{E}_{x \sim P(x)} \left[ \langle f_W(x), f_W(x) \rangle \right]
\end{equation}
where $d$ is the dimension of  $x$ the inputs of the network, $\| .\|_{L_2(\mathbb{R}^d, dP)}$ denotes the $L_2$ norm with respect to the probability measure on domain $\mathbb{R}^d$.  For some set $\{x_1, \dots ,x_m\}$ drawn i.i.d.\from $P(x)$
\begin{equation}
\frac{1}{m} \sum_{i=1}^m  \langle f_W(x_i), f_W(x_i) \rangle
\end{equation}
is an unbiased estimate.  For samples outside the training set, this is a $U$-statistic of order 1, and has an asymptotic Gaussian distribution for which finite sample estimates converge quickly\cite{lee1990u}.
For these samples, only their images by $f_W$ are needed and not their labels, which is particularly interesting for applications such as medical imaging 
where labeling the samples require expert intervention and is usually highly expensive.

Therefore, the problem we aim to solve can be written as following (using \eqref{eq:proba_x}):
\begin{align}
\arg\min_{W} \ &  \mathbb{E}_{(x,y)\sim P(x,y)}[\ell(f_W(x),y)] + \lambda \mathbb{E}_{x\sim P_x}[ \langle f_W(x), f_W(x) \rangle]\nonumber \\ &\rightarrow  \arg\min_{W} \int \int \ell(f_W(x),y) P(x,y) dxdy + \lambda \int \langle f_W(x), f_W(x) \rangle P(x) dx \nonumber \\ &\rightarrow  \arg\min_{W}\int \int \ell(f_W(x),y) + \lambda \langle f_W(x), f_W(x) \rangle  P(x,y) dxdy \nonumber \\ &\rightarrow \arg\min_{W} \underbrace{\mathbb{E}_{(x,y)\sim P(x,y)} [\ell(f_W(x),y) + \lambda \langle f_W(x), f_W(x) \rangle]}_{=: C_1(W)} 
\end{align}

\subsection{Markov-chain sampling}
Our second idea is to use stochastic integration methods to approximate the integral of $\| f_W(x)\|_2^2$. A simple way to apply these methods is to draw samples from the uniform distribution in the domain of the function and then to approximate the integral by the average value of the function in these points. This type of methods is known as Monte-Carlo and Quasi-Monte-Carlo methods~\cite{caflisch1998monte}.


However, as we are working in a very high dimension, this type of numerical integration is not appropriate because it suffers from the \textit{curse of dimensionality} and results in a sample concentration independent of the characteristics of the function we want to integrate, i.e. $\| f_W(x)\|_2^2$. As this function is real and non-negative, we can consider sampling from a distribution which density is proportional to the function itself, using Markov-chain methods such as Gibbs~\cite{casella1992explaining}  and Metropolisâ€“Hastings~\cite{hastings1970monte} sampling. As Gibbs sampling requires to approximate the marginal distribution (which is not easily accessible in our case and most importantly not standard), and Metropolis-Hastings sampling is depending on the choice of the proposal distribution, we generate our samples using slice sampling \cite{neal2003}.


This method draw samples uniformly from the hypervolume under the n-dimensional graph  of the function by iterating the following steps (to sample from a function $f$):
\begin{inparaenum}[(i)]
\item Choose an initial sample $x_0$;
\item Draw a uniformly distributed sample $y \in (0, f(x_0))$;
\item Find an hyperrectangle, around $x_0$, that is contained in the slice $S =\lbrace x : f(x)>f(x_0)\rbrace$;
\item Draw the new samples $x_1$ uniformly from this hyperrectangle;
\item Go back to the second step and iterate until collecting the wanted number of samples.
\end{inparaenum}
In~\cite{neal2003}, many methods to define the hyperrectangle in the third step are proposed. Here we used the method called "stepping-out and shrinking-in". This method consist in finding a neighborhood of $x_0$, increasing it progressively until being outside the slice, and drawing samples from this increased neighborhood. If the drawn sample is outside the slice, it is used to shrink the neighborhood. 


The samples that are drawn using this procedure will have a concentration that depends on the variations of $\| f_W(x)\|_2^2$. They will be concentrated around the regions where its values are high. The effect of the \textit{curse of dimensionality} is then limited. Moreover, this means that the regularization will have a higher effect on the regions where the samples are more concentrated, i.e.\the regions where the values of the function are high, resulting in a less complex function. The integral will then be approximated by
$\frac{1}{M}\sum_{i=1}^M \| f_W(x_i)\|_2^2$,
which is an unbiased estimate of
$\mathbb{E}_{x\sim \tilde{P}(x)}[\| f_W(x)\|_2^2]$,
where $\tilde{P}(x)$ is proportional to $\| f_W(x)\|_2^2$. The normalization factor is actually equal to the integral we want to approximate.\footnote{With this distribution, we need to prove that the used expression is a proper norm. The proof will be added soon.}


Now, if we consider any distribution $\overline{P}(y)$, and  define $\tilde{P}(x,y) = \tilde{P}(x)\overline{P}(y)$, we can rewrite :
\begin{align}
\mathbb{E}_{x\sim \tilde{P}(x)} \ & [\| f_W(x)\|_2^2] = %\nonumber\\&
\mathbb{E}_{(x,y)\sim \tilde{P}(x,y)} [\| f_W(x)\|_2^2] = \nonumber\\&
\mathbb{E}_{(x,y)\sim P(x,y)}  \left[\frac{\tilde{P}(x,y)}{P(x,y)}\| f_W(x)\|_2^2\right] =
\mathbb{E}_{(x,y)\sim P(x,y)}  [\alpha(x,y)\| f_W(x)\|_2^2]
\label{eqSlice}
\end{align}
Note that $\mathbb{E}_{(x,y)\sim P(x,y)}[\alpha(x,y)] = 1$, with $P(x,y)$ the observed data distribution.\footnote{This expression is needed only for theoretical development. In practice, we will keep sampling from $\| f_W(x)\|_2^2$.}
Finally, the problem to solve can be written as:
\begin{equation}
\arg\min_W \underbrace{\mathbb{E}_{(x,y)\sim P(x,y)} [\ell(f_W(x),y) + \lambda\alpha(x,y)\| f_W(x)\|_2^2]}_{=: C_2(W)} %= \arg\min_W C_2(W).
\end{equation}

\subsection{Stochastic gradient descent and algorithms}
\subsubsection{On the convergence of stochastic gradient descent}
In general, the stochastic gradient descent to minimize an objective of the form~\cite{bottou1998online}:
\begin{equation}
C(W) = \mathbb{E}_{(x,y) \sim P(x,y)}(Q(x,y,W))
\label{obj}
\end{equation}
is operated with an update of the form:
\begin{equation}
W_{t+1} = W_t - \gamma_t H(\mathbf{x_t,y_t},W_t)
\end{equation}
where
\begin{equation}
\mathbb{E}_{(\mathbf{x,y}) \sim P(\mathbf{x,y})}[H(\mathbf{x,y},W)] = \nabla_WC(W).
\label{eqCondH}
\end{equation}

For both of the cases we consider, the objective to minimize has the same form as~\eqref{obj}. In our methods,
\begin{inparaenum}[(i)]
\item for the first case, $H(\mathbf{x_t,y_t},W_t)$ is replaced by $\nabla_W \left(\ell(f(x_{t}),y_{t}) + \lambda \| f(r_{t})\|_2^2)\right)$ where $r_t$ is not the same as $x_t$ but drawn from the same distribution; 
\item for the second case, $H(\mathbf{x_t,y_t},W_t)$ is replaced by $\nabla_W \left(\ell(f(x_{t}),y_{t}) + \lambda \| f(r_{t})\|_2^2)\right)$ where $r_t$ is drawn from a density proportional to $\| f(x))\|_2^2$, or equivalently, according to~\eqref{eqSlice}, $\nabla_W \left(\ell(f(x_{t}),y_{t}) + \lambda \alpha(r_t,z_t)\|f(r_{t})\|_2^2)\right)$ where $(r_t,z_t)$ is drawn from the data distribution.
\end{inparaenum}
If we suppose that we can permute integration and derivation, it is straightforward to show that the two updates obey to the condition \eqref{eqCondH}.

Much research considers the question of the convergence of this procedure. Many of them are based on the results of~\cite{robbins1985convergence}. Nemirovsky \textit{et al}~\cite{nemirovski2009robust} showed that if $C(W)$ is convex, and if its gradient is Lipshitz continuous and bounded, then the optimal convergence rate is $\mathcal{O}(\frac{1}{t})$ and is obtained for $\gamma_t = \mathcal{O}(\frac{1}{t})$. In this case, the speed of decrease of $\gamma_t$ has the best compromise between the speed of convergence and the control of the variance introduced by using one sample rather than all samples to approximate the gradient.

Bottou~\cite{bottou1998online, bottou2010large, bottou2012stochastic} has shown this property holds for less constraining conditions on C. In~\cite{bottou1991stochastic}, he has studied in particular the type of objective that one wants to minimize in neural networks. These objectives are not convex and may admit several minimums. He shows that as long as: 
\begin{inparaenum}[(i)]
\item $\exists C_{min}, \forall W, C(W)>C_{min}$,
\item $\mathbb{E}_{(x,y)\sim P(x,y)}[H(x,y,W)] = \nabla_WC(W)$,
\item $\mathbb{E}_{(x,y)\sim P(x,y)}[H(x,y,W)^TH(x,y,W)] \le A + BC(W), A,B \geq 0$,
\item $\sum_t \gamma_t = \infty$ and $\sum_t \gamma_t^2 < \infty,$
\end{inparaenum}
$C(W_t)$ converges and $\nabla_WC(W_t)$ converges to 0, using SGD.  We know that the second condition holds for both of the cases we study. Thus, for our methods, knowing that the two other conditions on $C$ and $H$ hold for $C_0(W) = \mathbb{E}_{(x,y) \sim P(x,y)}[\ell(f_W(x),y)]$ and $H_0(x_t,y_t,W_t) = \nabla_W\ell(f_{W_t}(x),y_t)$ (which are the classical settings for neural networks), we need to prove that they still hold after adding the regularization term. Let $C_{min,0}, A_0$ and $B_0$ be the parameters that ensures the first and third conditions for $C_0$ and $H_0$.

\textbf{First case:} In this case, we can rewrite the objective as :
\begin{align}
C_1(W) &= C_0(W) + \lambda \mathbb{E}_{(x,y) \sim P(x,y)}[\| f_W(x)\|_2^2]\nonumber\\ H_1(x_t,y_t,W_t) &= H_0(x_t,y_t,W_t) + \lambda\nabla_W\| f_{W_t}(x_t)\|_2^2
\end{align}
Thus : 
\begin{equation}
H_1(x_t,y_t,W_t) = H_0(x_t,y_t,W_t) + 2\lambda J_Wf_{W_t}(x_t)^Tf_{W_t}(x_t).
\end{equation}
Then, as $\lambda \ge 0$ and $\mathbb{E}_{(x,y) \sim P(x,y)}[\| f_W(x)\|_2^2] \ge 0$, it follows that $C_1(W) > C_{min,0}$. In addition, we have:
\begin{align}
\mathbb{E}[H_1(x,y,W)^T&H_1(x,y,W)] = \mathbb{E}[H_0(x,y,W)^TH_0(x,y,W)] \nonumber\\&+ 2\lambda \mathbb{E}[H_0(x,y,W)^TJ_Wf_{W}(x)^Tf_{W}(x) + f_{W}(x)^TJ_Wf_{W}(x)H_0(x,y,W)] \nonumber\\&+ 4\lambda^2 \mathbb{E}[f_{W}(x)^TJ_Wf_{W}(x)J_Wf_{W}(x)^Tf_{W}(x)]
\end{align}
We know that the first term is bounded above by $A_0 + B_0C_0(W)$. Let us suppose that $H_0, f_W$ and $J_Wf_W$ are bounded. This is a reasonable hypothesis as we want to control the $L_2$ norm of $f_W$. Moreover, when $J_Wf_W$ is bounded, the condition on $H_0$ is verified for many popular loss functions such as the cross-entropy loss. All our experiments are conducted with this loss. Thus, we can find $A_1$ that bounds the second term, and $A_2$ such that the third term is bounded by $4\lambda^2A_2\mathbb{E}[f_{W}(x)^Tf_{W}(x)]$. Finally, it suffices to find $\lambda$ such that $4\lambda^2A_2 - B_0$ is positive, which will finish the proof of the third condition for $C_1$ and $H_1$.

\textbf{Second case:} In this case, we can rewrite the objective as :
\begin{align}
C_2(W) &= C_0(W) + \lambda \mathbb{E}_{(x,y) \sim P(x,y)}[\alpha(x,y)\| f_W(x)\|_2^2]\nonumber\\ H_2(x_t,y_t,W_t) &= H_0(x_t,y_t,W_t) + \lambda\nabla_W\alpha(x_t,y_t)\| f_{W_t}(x_t)\|_2^2
\end{align}
Note that $\alpha$ depends also on $W$. However, it is positive and its expected value is equal to 1. Thus, the first condition on $C_0$ holds for $C_2$, and the same sketch of proof for the third condition for $C_1$ holds for $C_2$.

Thus, if we consider a learning rate $\gamma_t = \mathcal{O}(\frac{1}{t})$, it obeys to the fourth condition and ensures the convergence of SGD for both of the considered objectives if $\lambda$ is tuned correctly. In practice, this parameter is tuned empirically.

\subsubsection{Algorithm}
During training, the algorithm~\ref{alg1} is used for each epoch (for $N$ training samples and $M$ regularization samples):\footnote{This algorithm is designed such that the network structure includes $\ell$ as a final layer.} Rather than using the SGD with single samples (online learning), we use a mini-batch approach which is most popular and used by the library we use for programming (MatConvNet). In this algorithm, we denote the network $N_{tr}$ and $N_{reg}$ for training and regularization and we note $N_*.L$ their layers. The training network differs from the regularization network only by its last layer that computes the loss function. $B_S$ denotes the batch size, and $B_N$ the number of patches, $\lambda$ is the regularization variable, ``mode" can be either ``train" or ``test", $R_{tr}$ and $R_{reg}$ contain the output of the network with the same subscript and $e$ designs the error. For the regularization step, backpropagation is initialized with the value of the output of the forward pass. The difference between the two proposed methods is in the definition of the regularization data (see~\eqref{eqChoice}).




\section{Experiments and results}
\label{experiment}
To test our algorithms, two series of experiments using two different databases are considered: MNIST and CIFAR10. The experiments are conducted using MatConvNet \cite{vedaldi15matconvnet}. For both of the databases, the convolutional neural network LeNet is used. The training algorithms provided by the cited library are modified according to the described algorithms. Both of the algorithms are used with weight decay. The results of our algorithms are compared with:
\begin{inparaenum}[(i)]
\item Training with only weight decay
\item Training with DropOut (with different rates) + weight decay. 
\end{inparaenum}
More extensive experimental results can be found in the supplementary material, but qualitatively follow the results reported here.

 
\subsection{MNIST}
In this paragraph, we report the results of the tests conducted with our algorithms on MNIST. The  MNIST data base is composed of 60,000 samples for training and 10,000 samples for test. These experiments where conducted using only a 2.4 GHz Intel Core 2 Duo processor.

\begin{center}
\begin{minipage}{.7\linewidth}
\begin{algorithm}[H]
 \KwData{$N_{tr}, B_S, B_N, \lambda$, mode}
 \KwResult{$N_{tr}$}
 $N_{reg}.L = N_{tr}.L$(1:end-1);
 $R_{tr} = [\ ], R_{reg} = [\ ], e = [\ ]$\;
 $B_{S,tr} = B_S\times\frac{n}{n+m}, B_{S,reg} = B_S - B_{S,tr}$\;
 \For{i=1:$B_N$}{
   \eIf{mode = 'train'}{
  $(X,y)\leftarrow B_{S,tr}$ training samples\;
   $R_{tr}$ := Forward + Backward ($N_{tr},X$)\;
   $e$ := [$e$, ErrorFunction($R_{tr}$, y)]\;
   Accumulate gradients\;
   $N_{reg}.L = N_{tr}.L$(1:end-1))\;
   \begin{equation}
   X \leftarrow \begin{cases}
B_{S,reg} \text{regularization samples
(only Data) if using data distribution;}\;\\
B_{S,reg} \text{samples from slice sampling if using slice sampling}\;
\end{cases}
\label{eqChoice}
\end{equation}\;
   $R_{reg}$ := Forward + Backward ($N_{reg},X$)\;
    Accumulate gradients for $N_{reg}$ using $\lambda$\;
    $N_{tr}.L$(1:end-1)) = $N_{reg}.L$\;
   }{
   $(X,y)\leftarrow B_S$ test samples\;
   $R_{tr}$ := Forward ($N_{tr},X$)\;
   $e$ := [$e$, ErrorFunction($R_{tr}$, y)]\;
  }
 }
 \caption{Epoch procedure}
 \label{alg1}
\end{algorithm}
\end{minipage}
\end{center}

\subsubsection{Sampling from the data distribution}
In this set of experiments, we test our first algorithm and the two reference methods listed above using a decreasing number of samples of MNIST in the training data set. The remaining training samples are used for regularization. 


Figure~\ref{reg-im-MNIST} shows the evolution of Top1 error during training (the x-axis represents the number of epochs) for weight decay, weight decay+ DropOut with the rate 50\%, weight decay+ DropOut with the rate 25\% and approximate function norm using data distribution. This error is displayed for the  test samples.
\begin{figure}[ht!]
\centering
\includegraphics[trim = 2.5cm 7.5cm 2cm 7.5cm,clip=true,width=0.3\textwidth]{images/mnist-12000.pdf} \hfill 
\includegraphics[trim = 3cm 8.3cm 2.5cm 8.5cm,clip=true,width=0.3\textwidth]{images/mnist-600.pdf} \hfill
\includegraphics[trim = 3cm 8.3cm 2.5cm 8.5cm,clip=true,width=0.3\textwidth]{images/mnist-100.pdf}
\caption{MNIST - Left: 12,000 samples - Center: 600 samples - Right: 100 samples}
\label{reg-im-MNIST}
\end{figure}


\subsubsection{Comparison of the two sampling methods}
\begin{wrapfigure}{R}{9cm}
\centering
\includegraphics[trim = 1cm 7.5cm 1cm 8cm,width=0.5\textwidth]{images/mnist-100-sampling-compare.pdf}
\caption{MNIST - Sampling method comparison -  Test Top1-error evolution in time}
\label{mnist-compare}
\end{wrapfigure}
In this test, we aim to compare the two proposed algorithms. We consider for this test the problem of classification using 100 samples.In Figure~\ref{mnist-compare}, we display the test Top1 error evolution in function of the training time. The number (1:N) indicates the ratio (Training samples:Regularization samples). Table~\ref{mnist-correct} shows the accuracy obtained on the test set at the end of the training. As a reference, note that the best accuracy obtained when all the training data is used is 99.06\%.


\begin{table}[ht!]
\centering
\begin{tabular}{|c|c||c|c|c|c|c|}
\hline
Training & Regularization & Weight & WD &WD & Function norm  & Function norm\\
& & decay &  + DO(25\%)& + DO(50\%)  &(data dist.)&(slice sampling)\\
\hline
\hline
12,000 & 48,000 & 98.26 & 97.65 & 95.82 & 97.99 &  97.99 (1:4)\\
1,200 & 58,800 & 92.60 & 89.93 & 85.52 & 95.15 & 92.70 (1:4)\\
600 & 59,400 & 88 & 83.12 & 73.97 & 92.84 & 90.47 (1:10)\\
100 & 59,900 & 30.27 & 18.75 & 13.36 & 80.53 & 79.74 (1:40)\\
\hline
\end{tabular}
\caption{MNIST - Correct classification rate (\%) for different methods}
\label{mnist-correct}
\end{table}

\subsection{CIFAR}
In this paragraph, we report the results of our algorithms when applied to CIFAR 10. The used CIFAR database is composed of 50,000 samples for training and 10,000 samples for test. These experiments where conducted using GPU, on a machine equipped with a 4 core CPU (4.00 GHz) and a GPU GeForce GTX 750 Ti. In this set of experiments, we test our two algorithms, weight decay and weight decay + DropOut(10\% and 50\%) with a decreasing number of samples of CIFAR in the training data set. The remaining training samples are used for regularization. Figure~\ref{reg-im-CIFAR} shows the Top1 error for the test set as a function of the number of epochs using respectively 10,000, 500 and 100 training samples. Figure~\ref{cifar500} shows the evolution of the test Top1-error with the training time using 500 samples for training. Table~\ref{cifar-correct} shows the accuracy obtained on the test set at the end of the training. As a reference, note that the best accuracy obtained when all the training data is used is 80.38\%.


\begin{figure}[ht!]
\includegraphics[trim = 1.5cm 7.5cm 1cm 7.5cm,clip=true,width=0.3\textwidth]{images/cifar-10000.pdf} \hfill 
\includegraphics[trim = 1.5cm 7.5cm 1cm 7.5cm,clip=true,width=0.3\textwidth]{images/cifar-500.pdf} \hfill
\includegraphics[trim = 1.5cm 7.5cm 0cm 7cm,clip=true,width=0.3\textwidth]{images/cifar-100.pdf}
\caption{CIFAR - Left: 10,000 samples - Center: 500 samples - Right: 100 samples}
\label{reg-im-CIFAR}
\end{figure}
\begin{table}[ht!]
\centering
\begin{tabular}{|c|c||c|c|c|c|c|}
\hline
Training & Regularization & Weight & WD &WD & Function norm  & Function norm\\
& & decay &  + DO(10\%)& + DO(50\%)  &(data dist.)&(slice sampling)\\
\hline
\hline
10,000 & 40,000 & 71.53 & 69.35 & 40.79 & 71.90&  72.02 (1:4)\\
5,000 & 45,000 & 67.27 & 64.59 & 28.08 & 68.26 & 68.21 (1:4)\\
1,000 & 49,000 & 52.57 & 47.99& 12.03 & 50.75 & 54.38 (1:10)\\
500 & 49,500 & 43.64 & 37.16 & 11.57 & 39.96 & 48.47 (1:10)\\
\hline
\end{tabular}
\caption{CIFAR- Correct classification rate (\%) for different methods}
\label{cifar-correct}
\end{table}

\begin{wrapfigure}{R}{7cm}
\centering
\includegraphics[trim = 1cm 7.5cm 1cm 8cm,width=0.5\textwidth]{images/cifar-500-bis.pdf}
\caption{CIFAR - 500 samples - Test Top1-error evolution in time}
\label{cifar500}
\end{wrapfigure}


\section{Discussion}
\label{discussion}
\subsection{MNIST}
The MNIST experiments show the feasibility and the efficiency of a regularization based on function norm. Indeed, Figure~\ref{reg-im-MNIST} shows that the function norm regularization with sampling from the data distribution behaves at least as well as the state-of-the art methods when a high number of samples is used, and outperforms them when we decrease the number of samples. Figure~\ref{mnist-compare} shows that the two proposed methods converge to similar errors, but with different computation time. Indeed, in this case, the data distribution and the network are simple enough to ensure equally high accuracy for the approximation of the function norm using the data distribution or the slice sampling.  Both of the proposed methods resulted in an accuracy as high as 80\% with only 100 training samples.


As mentioned above, for the case of MNIST, the main difference between the sampling methods is the computation time. In this point of view, using the data distribution is the fastest, as it needs only to read data from memory while the slice sampling approach require the generation of new samples. However, these methods are preferred to the first algorithm in the situations where the number of available samples is small, such as in medical imaging, where the data collection itself (even without labeling) may be expensive. 

\subsection{CIFAR}
The experiments conducted on CIFAR10 show that our idea of regularization is also feasible for more complicated data and a more complicated network. The curves and the accuracy rates show that the performance of our method is always higher than the state-of-the-art methods (at least for the tested sizes of database). Our methods gave an accuracy close to the best accuracy obtained using 50,000 samples for training, with only 10,000 samples. It is also more robust to the decrease of the number of samples. Even if the obtained results with this method 500 and 100 sample are not optimal, the error is still lower than with the other methods. 


Another important observation is that the slice sampling seems more robust than using the data distribution for this database. This may be explained by the fact that CIFAR's distribution is more complicated, and the available samples for regularization may be not enough to capture all the variations of the distribution. However, in all the cases, the slice sampling captures well the characteristics of the function variations. 

\section{Conclusion}
In this paper, two methods to introduce function norm regularization in neural network training have been developed. We have demonstrated both theoretically and empirically that their integration into stochastic backpropagation converges. Their efficiency have been showed on the  MNIST data set and on the more complicated data of CIFAR10. Our experiments suggest that the developed methods are of high quality on small databases that lie in low dimensional manifolds. For more complicated data with a small number of samples, the results of our methods outperform the state-of-the-art regularization methods available in the literature. 


\subsection*{Acknowledgements}

This   work   is   partially   funded   by   Internal   Funds
KU  Leuven,  the European Commission through FP7-MC-CIG
334380, and the Research Foundation - Flanders (FWO) through project number G0A2716N.

\bibliographystyle{abbrv}
\bibliography{biblio} 

 






















\appendix

\section{MNIST}

In these appendices we provide additional experimental results.

\subsection{Sampling from the data distribution}
In this paragraph, we show the test Top1 and Top5-errors obtained by applying the function norm regularization using the data distribution, weight decay and weight decay + Dropout to MNIST data of decreasing sizes. Figure~\ref{sup-reg-im-MNIST-e5} shows the test Top5-error when 12,000, 600 and 100 samples are used. The corresponding Top1-error is showed in the main text. Figure~\ref{sup-reg-im-MNIST} shows the test Top1 and Top5 errors using 1,200 and 6,000 samples. Figure~\ref{t-vs-err-im-MNIST} shows the evolution of the test Top1-error with the training time using 600 and 100 samples.


\begin{figure}[ht!]
\includegraphics[trim = 2.5cm 8cm 2cm 8cm,width=0.3\textwidth]{images/mnist-12000-e5.pdf} \hfill 
\includegraphics[trim = 2.5cm 8cm 2cm 8cm,width=0.3\textwidth]{images/mnist-600-e5.pdf} \hfill
\includegraphics[trim = 2.5cm 8cm 2cm 8cm,width=0.3\textwidth]{images/mnist-100-e5.pdf}
\caption{MNIST - Left: 12,000 samples - Center: 600 samples - Right: 100 samples}
\label{sup-reg-im-MNIST-e5}
\end{figure}


\begin{figure}[ht!]
\includegraphics[trim = 2.5cm 8cm 2cm 8cm,width=0.45\textwidth]{images/mnist-6000.pdf} \hfill 
\includegraphics[trim = 2.5cm 8cm 2cm 8cm,width=0.45\textwidth]{images/mnist-6000-e5.pdf} \\
\includegraphics[trim = 2.5cm 8cm 2cm 8cm,width=0.45\textwidth]{images/mnist-1200.pdf} \hfill
\includegraphics[trim = 2.5cm 8cm 2cm 8cm,width=0.45\textwidth]{images/mnist-1200-e5.pdf}
\caption{MNIST - Up: 6,000 samples - Down: 1,200 samples}
\label{sup-reg-im-MNIST}
\end{figure}

\begin{figure}[ht!]
\includegraphics[trim = 2.5cm 8cm 2cm 8cm,width=0.45\textwidth]{images/mnist-600-time.pdf} \hfill
\includegraphics[trim = 2.5cm 8cm 2cm 8cm,width=0.45\textwidth]{images/mnist-100-time.pdf}
\caption{MNIST - Left: 600 samples - Right: 100 samples}
\label{t-vs-err-im-MNIST}
\end{figure}




\subsection{Model selection (600 samples)}
In this experiment, we aim to show the effect of the choice of $\lambda$. We tested the function norm regularization using the data distribution with 8 different $\lambda$s decreasing by a factor of 10 from 50 to 0.000005. In Figure~\ref{lambda-e1}, we display the validation Top1-error. In Figure~\ref{lambda-e5}, we display the validation Top5-error. For both figures, we show in the left the curves corresponding to all the values. In the right, we remove the value 50 for which the procedure fails in order to have a better visualization for the other values. Among the test samples, 2000 are taken randomly in order to use them for validation. The curves are constructed using these samples. The 8000 other samples are used to compute the accuracies displayed in Table~\ref{acc-model}.
\begin{figure}[ht!]
\includegraphics[trim = 2cm 7.5cm 2cm 7.cm,width=0.45\textwidth]{images/mnist-model-err1-all.pdf}\hfill
\includegraphics[trim = 2.5cm 8cm 2cm 8cm,width=0.45\textwidth]{images/mnist-model-err1.pdf}
\caption{MNIST - Effect of $\lambda$ - Top1-error}
\label{lambda-e1}
\end{figure}

\begin{figure}[ht!]
\includegraphics[trim = 2cm 7.5cm 2cm 7.cm,width=0.45\textwidth]{images/mnist-model-err5-all.pdf} \hfill
\includegraphics[trim = 2cm 7.5cm 2cm 7.cm,width=0.45\textwidth]{images/mnist-model-err5.pdf}
\caption{MNIST - Effect of $\lambda$ - Top5-error}
\label{lambda-e5}
\end{figure}

\begin{table}[ht!]
\centering
\begin{tabular}{|c||c|c|c|c|c|c|c|c|}
\hline
$\lambda$ & 50 & 5 & 0.5 & 0.05& 5$\cdot 10^{-3}$& 5$\cdot 10^{-4}$& 5$\cdot 10^{-5}$ & 5$\cdot 10^{-6}$\\
\hline
Accuracy & 10,34 &	93,24 &	92,76&	92,45&	93&	92,72&	92,74&	92,65\\
\hline

\end{tabular}
\caption{MNIST - Correct classification rate (\%) for the function norm regularization using the data distribution with different $\lambda$s}
\label{acc-model}
\end{table}

This set of experiments shows that as long as $\lambda$ is low enough, the trained network has a good performance. Indeed, except for $\lambda$ = 50, all the other values result in curves for the Top1 and Top5 errors that are
hardly classifiable. Note that the value that gives the best accuracy on the test set (i.e. 5) does not correspond to the best Top1-error nor the best Top5-error.



\section{CIFAR}
In this paragraph, we show the test Top1 and Top5-errors obtained by applying the function norm regularization using the data distribution and slice sampling, weight decay and weight decay + Dropout to CIFAR10 data of decreasing sizes. 
Figure~\ref{sup-reg-im-CIFAR-e5} shows the test Top5-error when 10,000, 500 and 100 samples are used. The corresponding Top1-error is showed in the main text. Figure~\ref{sup-reg-im-CIFAR} shows the test Top1 and Top5 errors using 1,200 and 5,000 samples. Figure~\ref{t-vs-err-im-CIFAR} shows the evolution of the test Top1-error with the training time using 100 samples.
\begin{figure}[ht!]
\includegraphics[trim = 2.5cm 8cm 2cm 8cm,width=0.3\textwidth]{images/cifar-10000-e5.pdf} \hfill 
\includegraphics[trim = 2.5cm 8cm 2cm 8cm,width=0.3\textwidth]{images/cifar-500-e5.pdf} \hfill
\includegraphics[trim = 2.5cm 8cm 2cm 8cm,width=0.3\textwidth]{images/cifar-100-e5.pdf}
\caption{CIFAR - Left: 10,000 samples - Center: 500 samples - Right: 100 samples}
\label{sup-reg-im-CIFAR-e5}
\end{figure}


\begin{figure}[ht!]
\includegraphics[trim = 2.5cm 8cm 2cm 8cm,width=0.45\textwidth]{images/cifar-5000.pdf} \hfill 
\includegraphics[trim = 2.5cm 8cm 2cm 8cm,width=0.45\textwidth]{images/cifar-5000-e5.pdf} \\
\includegraphics[trim = 2.5cm 8cm 2cm 8cm,width=0.45\textwidth]{images/cifar-1000.pdf} \hfill
\includegraphics[trim = 2.5cm 8cm 2cm 8cm,width=0.45\textwidth]{images/cifar-1000-e5.pdf}
\caption{CIFAR - Up: 5,000 samples - Down: 1,000 samples}
\label{sup-reg-im-CIFAR}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[trim = 2.5cm 7.5cm 2cm 8cm,width=0.6\textwidth]{images/cifar-100-time.pdf}
\caption{CIFAR- 100 samples - Top1 error vs. training time}
\label{t-vs-err-im-CIFAR}
\end{figure}



\end{document}
