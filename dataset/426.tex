\documentclass[10pt, journal, compsoc]{IEEEtran}
\ifCLASSOPTIONcompsoc
    \usepackage[nocompress]{cite}
\else
    \usepackage{cite}
\fi
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{ragged2e}
\usepackage{xcolor}
\usepackage[caption=false]{subfig}

\begin{document}

\title{Effects of Degradations on Deep Neural Network Architectures}
\author{Prasun~Roy$^*$,~Subhankar~Ghosh$^*$,~Saumik~Bhattacharya$^*$~and~Umapada~Pal \\
Indian Statistical Institute Kolkata, India - 700108 \\
\thanks{$^*$These authors contributed equally in this paper.}
\thanks{$^{**}$The source code and datasets are available at: https://goo.gl/EZ43Lx}
}

\IEEEtitleabstractindextext{
\begin{abstract}
\justifying{
Deep convolutional neural networks (CNNs) have achieved many state-of-the-art results recently in different fields of computer vision. Different architectures are emerging almost everyday that produce fascinating results in machine vision tasks consisting million of images. Though CNN based models are able to detect and recognize large number of object classes, most of the times these models are trained with high quality images. Recently, an alternative architecture for image classification is proposed by Sabour et al. \cite{sabour2017dynamic} based on groups of neurons (capsules) and a novel dynamic routing protocol. The architecture has shown promising result by surpassing the performances of state-of-the-art CNN based models in some of the existing datasets. However, the behavior of capsule based models and CNN based models are largely unknown in presence of noise. As in most of the practical applications, it can not be guaranteed the input image is completely noise-free and does not contain any distortion, it is important to study the performance of these models under different noise conditions. In this paper, we select six widely used CNN architectures and consider their performances for image classification task for two different datasets under various image quality distortions. We show that the capsule network is more robust to several image degradations than the CNN based models. To the best of our knowledge, this is the first study on the performance of CapsuleNet (or CapsNet) and other state-of-the-art CNN architectures under different types of image degradations.
}
\end{abstract}

\begin{IEEEkeywords}
CapsuleNet, convolutional neural networks, image degradations.
\end{IEEEkeywords}
}

\maketitle

\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}
\IEEEPARstart{V}{isual} quality is an important parameter in machine vision problems. Though there are several no-reference image quality measures available in literature \cite{mittal2012no}, visual quality of an input image is a subjective quantity and traditionally we rely on human perception to conclude about it. However, computer vision algorithms work differently from human vision system (HVS), and the concept of image quality for computer vision problems does not always match with human perception. Convolutional neural networks (CNNs) depend on several sets of filter outputs to perform the final classification task. Thus, it is difficult to predict the outcome of a degraded input in an intuitive way and the classification accuracy largely depends on the model architecture and the nature of the degradation. In most of the cases, we train and validate a CNN model with high quality images with minimum noise. However, in practical applications, several different kinds of degradations can be introduced in the input image that can heavily affect the performances of CNN models. These image degradations can be generated in the scene due to poor image sensor, lighting conditions, focus, stabilization, exposure time etc. To overcome this problem, some researchers have suggested to include noisy data in the training itself \cite{dodge2016understanding}. Though, this technique produces better results than training only with high quality images, it is practically not possible to train a network with all probable degradation types that may appear in real scenes. Starting from multi-class classification tasks to generative models, CNNs are used in numerous computer vision algorithms. State-of-the-art CNN architectures such as ResNet50, Inception v3, DenseNet etc. \cite{howard2017mobilenets, simonyan2014very, he2016deep} have achieved exceptional results for large image classification task in ILSVRC 2010 challenge (ImageNet). Thus, the performances of different deep CNN architectures in classification task consisting of different challenging images are considered in this paper in presence of different image degradations. Interestingly, it has been shown recently that even a small amount of carefully selected noise might completely fool a well-trained complex CNN model, though such noise does not create any problem in visual recognition \cite{moosavi2016deepfool}. Though the probability of occurrence of such adversarial noises might be low, it is important to know the performances of different CNN architectures under different noise conditions to build more robust systems in future.

\vspace{0.7cm}

\begin{figure}
\includegraphics[width=\linewidth]{figures/dataset_samples.png}
\caption{Typical examples from the datasets that are used in this work: (a) Synthetic digits dataset; (b) Natural images dataset.}
\label{fig:dataset_samples}
\end{figure}

\subsection{Related Works}\label{sec:relatedwork}
In most of the recent applications, it is generally assumed that the CNN models accept high quality images. However in many cases, it is not possible to have good quality images in computer vision problems. Thus, several authors have recently proposed different architectures and preprocessing steps to work with low quality images. In \cite{ren2012coupled}, the authors used coupled kernel embedding to recognize faces in low resolution images that also suffer from degradations due to high compression. Zou et al. \cite{zou2012very} addressed the same problem of face recognition in low resolution by introducing discriminative constraints to achieve high quality images for recognition. In \cite{basu2017learning}, authors introduced a modified version of well-known MNIST dataset, that includes synthetically generated noisy handwritten images. The authors also proposed a novel framework that learns representations using probabilistic quadtrees and deep belief network. A noisy face database is developed in \cite{karam2015quality} to act as a benchmark in robust face recognition algorithms. However, in \cite{karam2015quality}, authors did not mention any model that can achieve robust recognition. Later, Tao et al. \cite{tao2016multi} proposed joint kernel sparse representation based classification algorithm that performs satisfactorily on this database. In \cite{ullman2016atoms}, Ullman et al. tried to show that human vision system and CNNs process an image differently by defining minimal recognizable configurations (MIRC) which are the smallest cropped version of an input image for which human being can still recognize a class or action perfectly. It was shown in \cite{ullman2016atoms} that even though the cropping action reduces the information content in an image, CNNs are still no match for HVS. In \cite{dodge2016understanding}, the authors rigorously analyzed the effect of image degradation under different noise conditions on the accuracy of CNNs in classification task. Though the authors include degradations like blur, compression, contrast etc., they did not include different common degradations like motion blur, salt and pepper noise etc. in their work. Moreover, CNN models have evolved greatly from the models that are considered in \cite{dodge2016understanding}. Particularly, the models like ResNet50 and CapsuleNet, that integrate different architectures along with conventional CNN layers to find more complex features from an input image. In this paper, we not only cover more number of degradations to study their effects on CNNs, we also include the state-of-the-art architectures to perform our experiments. We show that the recently proposed capsule network shows impressive robustness under certain noise conditions.

\begin{figure}

\captionsetup[subfigure]{labelformat=empty}
\textbf{(a)}\;\;\subfloat[$\sigma=0$]{\includegraphics[width=1.5cm, height=1.5cm]
{figures/noisy_images/gaussian_white/gaussian_white_0}}\hspace{1px}
\subfloat[$\sigma=0.25$]{\includegraphics[width=1.5cm, height=1.5cm]
{figures/noisy_images/gaussian_white/gaussian_white_64}}\hspace{1px}
\subfloat[$\sigma=0.50$]{\includegraphics[width=1.5cm, height=1.5cm]
{figures/noisy_images/gaussian_white/gaussian_white_128}}\hspace{1px}
\subfloat[$\sigma=0.75$]{\includegraphics[width=1.5cm, height=1.5cm]
{figures/noisy_images/gaussian_white/gaussian_white_192}}\hspace{1px}
\subfloat[$\sigma=1$]{\includegraphics[width=1.5cm, height=1.5cm]
{figures/noisy_images/gaussian_white/gaussian_white_256}}\hspace{1px}

\captionsetup[subfigure]{labelformat=empty}
\textbf{(b)}\;\;\subfloat[$\sigma=0$]{\includegraphics[width=1.5cm, height=1.5cm]
{figures/noisy_images/gaussian_color/gaussian_color_0}}\hspace{1px}
\subfloat[$\sigma=0.25$]{\includegraphics[width=1.5cm, height=1.5cm]
{figures/noisy_images/gaussian_color/gaussian_color_64}}\hspace{1px}
\subfloat[$\sigma=0.50$]{\includegraphics[width=1.5cm, height=1.5cm]
{figures/noisy_images/gaussian_color/gaussian_color_128}}\hspace{1px}
\subfloat[$\sigma=0.75$]{\includegraphics[width=1.5cm, height=1.5cm]
{figures/noisy_images/gaussian_color/gaussian_color_192}}\hspace{1px}
\subfloat[$\sigma=1$]{\includegraphics[width=1.5cm, height=1.5cm]
{figures/noisy_images/gaussian_color/gaussian_color_256}}\hspace{1px}

\captionsetup[subfigure]{labelformat=empty}
\textbf{(c)}\;\;\subfloat[$d=0$]{\includegraphics[width=1.5cm, height=1.5cm]
{figures/noisy_images/salt_and_pepper/salt_and_pepper_0_0}}\hspace{1px}
\subfloat[$d=0.25$]{\includegraphics[width=1.5cm, height=1.5cm]
{figures/noisy_images/salt_and_pepper/salt_and_pepper_0_25}}\hspace{1px}
\subfloat[$d=0.50$]{\includegraphics[width=1.5cm, height=1.5cm]
{figures/noisy_images/salt_and_pepper/salt_and_pepper_0_50}}\hspace{1px}
\subfloat[$d=0.75$]{\includegraphics[width=1.5cm, height=1.5cm]
{figures/noisy_images/salt_and_pepper/salt_and_pepper_0_75}}\hspace{1px}
\subfloat[$d=1$]{\includegraphics[width=1.5cm, height=1.5cm]
{figures/noisy_images/salt_and_pepper/salt_and_pepper_1_0}}\hspace{1px}

\captionsetup[subfigure]{labelformat=empty}
\textbf{(d)}\;\;\subfloat[$k_m=3$]{\includegraphics[width=1.5cm, height=1.5cm]
{figures/noisy_images/motion_blur/motion_blur_3}}\hspace{1px}
\subfloat[$k_m=11$]{\includegraphics[width=1.5cm, height=1.5cm]
{figures/noisy_images/motion_blur/motion_blur_11}}\hspace{1px}
\subfloat[$k_m=17$]{\includegraphics[width=1.5cm, height=1.5cm]
{figures/noisy_images/motion_blur/motion_blur_17}}\hspace{1px}
\subfloat[$k_m=25$]{\includegraphics[width=1.5cm, height=1.5cm]
{figures/noisy_images/motion_blur/motion_blur_25}}\hspace{1px}
\subfloat[$k_m=31$]{\includegraphics[width=1.5cm, height=1.5cm]
{figures/noisy_images/motion_blur/motion_blur_31}}\hspace{1px}

\captionsetup[subfigure]{labelformat=empty}
\textbf{(e)}\;\;\subfloat[$k_b=1$]{\includegraphics[width=1.5cm, height=1.5cm]
{figures/noisy_images/gaussian_blur/gaussian_blur_1}}\hspace{1px}
\subfloat[$k_b=13$]{\includegraphics[width=1.5cm, height=1.5cm]
{figures/noisy_images/gaussian_blur/gaussian_blur_13}}\hspace{1px}
\subfloat[$k_b=25$]{\includegraphics[width=1.5cm, height=1.5cm]
{figures/noisy_images/gaussian_blur/gaussian_blur_25}}\hspace{1px}
\subfloat[$k_b=37$]{\includegraphics[width=1.5cm, height=1.5cm]
{figures/noisy_images/gaussian_blur/gaussian_blur_37}}\hspace{1px}
\subfloat[$k_b=49$]{\includegraphics[width=1.5cm, height=1.5cm]
{figures/noisy_images/gaussian_blur/gaussian_blur_49}}\hspace{1px}

\captionsetup[subfigure]{labelformat=empty}
\textbf{(f)}\;\;\subfloat[$q=24$]{\includegraphics[width=1.5cm, height=1.5cm]
{figures/noisy_images/jpeg_quality/jpeg_quality_24}}\hspace{1px}
\subfloat[$q=18$]{\includegraphics[width=1.5cm, height=1.5cm]
{figures/noisy_images/jpeg_quality/jpeg_quality_18}}\hspace{1px}
\subfloat[$q=12$]{\includegraphics[width=1.5cm, height=1.5cm]
{figures/noisy_images/jpeg_quality/jpeg_quality_12}}\hspace{1px}
\subfloat[$q=6$]{\includegraphics[width=1.5cm, height=1.5cm]
{figures/noisy_images/jpeg_quality/jpeg_quality_6}}\hspace{1px}
\subfloat[$q=0$]{\includegraphics[width=1.5cm, height=1.5cm]
{figures/noisy_images/jpeg_quality/jpeg_quality_0}}\hspace{1px}

\caption{Visual samples of different image degradations: (a) Gaussian white noise, (b) Colored Gaussian noise, (c) salt and pepper noise, (d) motion blur, (e) Gaussian blur, (f) JPEG quality.}
\label{fig:degradation_samples}
\end{figure}

\begin{figure*}
\captionsetup[subfigure]{labelformat=empty}
\subfloat{\includegraphics[width=6cm]{figures/synthetic_digits_top_1/gaussian_white_top_1}}
\subfloat{\includegraphics[width=6cm]{figures/synthetic_digits_top_1/gaussian_color_top_1}}
\subfloat{\includegraphics[width=6cm]{figures/synthetic_digits_top_1/salt_and_pepper_top_1}}
\vspace{-3mm}\\
\subfloat[(a) Gaussian white noise]{\includegraphics[width=6cm]{figures/synthetic_digits_top_3/gaussian_white_top_3}}
\subfloat[(b) Colored Gaussian noise]{\includegraphics[width=6cm]{figures/synthetic_digits_top_3/gaussian_color_top_3}}
\subfloat[(c) Salt and pepper noise]{\includegraphics[width=6cm]{figures/synthetic_digits_top_3/salt_and_pepper_top_3}}
\vspace{3mm}\\
\subfloat{\includegraphics[width=6cm]{figures/synthetic_digits_top_1/motion_blur_top_1}}
\subfloat{\includegraphics[width=6cm]{figures/synthetic_digits_top_1/gaussian_blur_top_1}}
\subfloat{\includegraphics[width=6cm]{figures/synthetic_digits_top_1/jpeg_quality_top_1}}
\vspace{-3mm}\\
\subfloat[(d) Motion blur]{\includegraphics[width=6cm]{figures/synthetic_digits_top_3/motion_blur_top_3}}
\subfloat[(e) Gaussian blur]{\includegraphics[width=6cm]{figures/synthetic_digits_top_3/gaussian_blur_top_3}}
\subfloat[(f) JPEG Quality]{\includegraphics[width=6cm]{figures/synthetic_digits_top_3/jpeg_quality_top_3}}
\caption{Comparison of classification accuracies of different CNN architectures under different image degradations on synthetic digits dataset. In each sub-figures, the top figure shows top accuracy vs. respective degradation parameter and the bottom figure shows top-3 accuracy vs. respective degradation parameter.}
\label{fig:result_synthetic_digits}
\end{figure*}

\section{Experimental Setup and Dataset}\label{sec:methodology}
To understand the effects of different distortions on a classification task, we select some state-of-the-art CNN architectures that achieved impressive results in ImageNet challenge. We evaluate the models on a synthetic digits dataset and another natural images dataset.

\subsection{Dataset}
\subsubsection{Synthetic Digits Dataset (ISISD)}
Getting the inspiration from MNIST dataset, we build our own synthetic numeral dataset with 16 different English fonts. The numerals are randomly rotated with rotation angle -30 degree to 30 degree. Each digit has a random color and random font size ranging from 30 to 240. To increase the difficulty of the dataset, we arbitrarily select image patches from COCO dataset \cite{lin2014microsoft} and place them in the background of the digits. The dataset has 10 different English numeral classes and each class contains 1200 images. We use 10000 images for training and 2000 images for testing the CNN models.
\subsubsection{Natural Images Dataset (ISINI)}
State-of-the-art CNN models trained on the ImageNet dataset can correctly classify 1000 classes even in very complex environmental context. On the other hand, the behavior of capsule network is not well-explained for natural images like ImageNet or COCO dataset. Thus, we also built a dataset containing natural images to evaluate the performance of different CNNs including CapsuleNet for complex input images under various image degradations. The dataset contains 8 different classes- airplane, car, cat, dog, flower, fruit, motorbike and person having 727, 968, 885, 702, 843, 1000, 788 and 986 image samples respectively. Of these total 6899 images, we use 5724 images for training and 1175 images for testing the CNN models.

Fig. \ref{fig:dataset_samples} shows typical examples from synthetic digits dataset and natural images dataset.

\begin{figure*}
\captionsetup[subfigure]{labelformat=empty}
\subfloat{\includegraphics[width=6cm]{figures/natural_images_top_1/gaussian_white_top_1}}
\subfloat{\includegraphics[width=6cm]{figures/natural_images_top_1/gaussian_color_top_1}}
\subfloat{\includegraphics[width=6cm]{figures/natural_images_top_1/salt_and_pepper_top_1}}
\vspace{-3mm}\\
\subfloat[(a) Gaussian white noise]{\includegraphics[width=6cm]{figures/natural_images_top_3/gaussian_white_top_3}}
\subfloat[(b) Colored Gaussian noise]{\includegraphics[width=6cm]{figures/natural_images_top_3/gaussian_color_top_3}}
\subfloat[(c) Salt and pepper noise]{\includegraphics[width=6cm]{figures/natural_images_top_3/salt_and_pepper_top_3}}
\vspace{3mm}\\
\subfloat{\includegraphics[width=6cm]{figures/natural_images_top_1/motion_blur_top_1}}
\subfloat{\includegraphics[width=6cm]{figures/natural_images_top_1/gaussian_blur_top_1}}
\subfloat{\includegraphics[width=6cm]{figures/natural_images_top_1/jpeg_quality_top_1}}
\vspace{-3mm}\\
\subfloat[(d) Motion blur]{\includegraphics[width=6cm]{figures/natural_images_top_3/motion_blur_top_3}}
\subfloat[(e) Gaussian blur]{\includegraphics[width=6cm]{figures/natural_images_top_3/gaussian_blur_top_3}}
\subfloat[(f) JPEG Quality]{\includegraphics[width=6cm]{figures/natural_images_top_3/jpeg_quality_top_3}}
\caption{Comparison of classification accuracies of different CNN architectures under different image degradations on natural images dataset. In each sub-figures, the top figure shows top accuracy vs. respective degradation parameter and the bottom figure shows top-3 accuracy vs. respective degradation parameter.}
\label{fig:result_natural_images}
\end{figure*}

\subsection{Deep Neural Networks}\label{sec:deep_neural_networks}
In this paper we consider five CNN based architectures along with CapsuleNet to evaluate the respective performances. Though there are numerous different CNN architectures present in the literature, the networks that are tested here are the popularly known standard deep CNN architectures.\\
\begin{table}[t]
\centering
\caption{Summary of the models used}
\label{tab:cnnsummary}
\begin{tabular}{lrl}
\hline
\multicolumn{1}{c}{Model} & \begin{tabular}[c]{@{}l@{}} \#Trainable\\ Parameters\end{tabular} & \begin{tabular}[c]{@{}l@{}}Input Size\\ \end{tabular}\\
\hline
MobileNet    &  55.7M & $(224\times224\times3) $ \\
VGG16        &  41.5M & $(224\times224\times3) $ \\
VGG19        &  54.6M & $(256\times256\times3) $ \\
ResNet50     &  26.7M & $(224\times224\times3) $ \\
Inception v3 & 157.1M & $(299\times299\times3) $ \\
CapsuleNet   &  47.6M & $(104\times104\times3) $ \\
V-CapsNet    &  15.7M & $(256\times256\times3) $ \\
\hline
\end{tabular}
\end{table}
The first network that we considered for the experiment is MobileNet which is based on streamlined architecture consisting of depth-wise separable convolutional layers \cite{howard2017mobilenets}. Considering both depth-wise and point-wise convolutions as separate layers, this model has 28 layers. It maintains a trade-off between latency and accuracy to consume minimal computational resources. This architecture is preferable in mobile and embedded based vision application where the availability of resources is limited.\\
VGG16 \cite{simonyan2014very} is a deep architecture with 13 convolutional layers with very small convolutional filters and 3 fully connected layers. Including input, output, pooling and activations, there are 41 layers in this model. It outperformed many well-known CNN like Caffe Reference Model and AlexNet in ImageNet classification task.\\
VGG19 is even deeper architecture than VGG16 with similar architecture but 16 convolutional layers and 3 fully connected layers. Including input, output, pooling and activations, VGG19 has 47 layers.\\
\begin{figure*}
\captionsetup[subfigure]{labelformat=empty}
\subfloat[(a) Gaussian white noise]{\includegraphics[width=6cm]{figures/capsnet_comparisons/gaussian_white}}
\subfloat[(b) Colored Gaussian noise]{\includegraphics[width=6cm]{figures/capsnet_comparisons/gaussian_color}}
\subfloat[(c) Salt and pepper noise]{\includegraphics[width=6cm]{figures/capsnet_comparisons/salt_and_pepper}} \vspace{-3mm}\\
\subfloat[(d) Motion blur]{\includegraphics[width=6cm]{figures/capsnet_comparisons/motion_blur}}
\subfloat[(e) Gaussian blur]{\includegraphics[width=6cm]{figures/capsnet_comparisons/gaussian_blur}}
\subfloat[(f) JPEG Quality]{\includegraphics[width=6cm]{figures/capsnet_comparisons/jpeg_quality}}
\caption{Top-3 performance comparison of CapsuleNet architectures for two different parameter settings.}
\label{fig:result_capsnet_comparisons}
\end{figure*}
As most CNN based models go deeper, they become difficult to train for large datasets. It was found that instead of training the filtered outputs, it is easier to train on the residuals of the outputs\cite{he2016deep}. Following the concept, ResNet50 depends on a different kind of architecture than VGG to achieve better accuracy for ImageNet classification problem. ResNet50 typically contains fewer filters and lower complexity than VGG. In this paper, we consider ResNet50 architecture for the comparisons.\\
Inception architectures are built upon so called `inception modules' that contains different convolutional kernels stacked together along the depth dimension to get multi-scale features \cite{szegedy2016rethinking}. Here, we consider Inception v3 model that also includes the concept of residual training for better accuracy.\\
CapsuleNet is based on a relatively new neural network concept of `dynamic routing between capsules'. A capsule is a collection of neurons whose activity vector represents the instantiation of parameters of an object \cite{sabour2017dynamic}. CapsuleNet removes the maxpooling layers and relies on dynamic routing protocol between capsules to achieve translational invariance. The CapsuleNet architecture that has been used in this paper is different from the original architecture proposed in \cite{sabour2017dynamic}. Our model takes input image of dimension $104\times104$. Unlike the model of \cite{sabour2017dynamic}, our model has two generic convolutional layers along with a primary capsule layer and one classification capsule layer. The decoder module contains three fully connected layers and a reshaping operation after the last layer. The loss function and the routing protocol of the architecture are as described in \cite{szegedy2016rethinking}.\\
The details of the different architectures that are used in this paper are summarized in Table \ref{tab:cnnsummary}. For all the datasets used in this paper except for CapsuleNet, we train the models after initializing with pre-trained ImageNet weights for faster convergence. We also propose a modified capsule architecture based model named V-CapsNet that we will discuss in Sec. \ref{results}.

\subsection{Degradation Types}
For this work, we consider the following types of degradations.
\subsubsection{Gaussian noise}
Additive white Gaussian noise (AWGN) can be introduced if an image is transmitted through a channel or it is captured using a low quality senor \cite{dodge2016understanding}. To understand the effect of AWGN, we apply two different types of AWGN noise and colored Gaussian noise.\\
In AWGN, we generate a noise matrix that has same spatial dimension with the image and add the same noise with the three color channels.\\
In the second type, noises are added independently in three color channels and thus the noise can cause color artifacts. We call the second type of AWGN in this paper as `colored Gaussian noise'. It is important to note that the colored Gaussian noise considered in this paper is different from additive colored Gaussian noise. In both the cases, the AWGN have zero mean and standard deviation $\sigma > 0$. We change the variance in the experiment to see the effect of AWGN noises on different networks.
\subsubsection{Salt and pepper noise}
Salt and pepper is a typical impulse noise that can be observed in images due to sparse but intense disturbances. This noise replaces the original pixel values with random white and black pixels. We define a parameter $d$ that controls the noise density in the image. For example, $d=0.1$ indicates that the 10\% pixels in an image is degraded with salt and pepper noise. We vary the noise density from 0 to 1 to check its effect on a classification task.
\begin{figure*}
\captionsetup[subfigure]{labelformat=empty}
\subfloat[(a) Gaussian white noise]{\includegraphics[width=6cm]{figures/fusion_model_top_1/gaussian_white_top_1}}
\subfloat[(b) Colored Gaussian noise]{\includegraphics[width=6cm]{figures/fusion_model_top_1/gaussian_color_top_1}}
\subfloat[(c) Salt and pepper noise]{\includegraphics[width=6cm]{figures/fusion_model_top_1/salt_and_pepper_top_1}} \vspace{-3mm}\\
\subfloat[(d) Motion blur]{\includegraphics[width=6cm]{figures/fusion_model_top_1/motion_blur_top_1}}
\subfloat[(e) Gaussian blur]{\includegraphics[width=6cm]{figures/fusion_model_top_1/gaussian_blur_top_1}}
\subfloat[(f) JPEG Quality]{\includegraphics[width=6cm]{figures/fusion_model_top_1/jpeg_quality_top_1}}
\caption{Performance comparison of V-CapsNet architecture with VGG19 and CapsuleNet under different image degradation.}
\label{fig:result_fusion_model}
\end{figure*}
\subsubsection{Blur}
One of the most common degradations that can be observed in real scenes is blurring. In this paper, we consider two different types of blurs: motion blur and Gaussian blur.\\
Motion blur typically occurs due to poor stabilization of camera or movement of an object during exposure. Gaussian blur roughly approximate defocus blurring and blurs that may arise in different post-processing operations. In this paper, we consider only horizontal blur with kernel width $k_m$ signifying the number of pixels that contributes in the motion blurring. We vary $k_m$ from 1 to 31 with an interval of 2 and normalize the kernel accordingly to observe the effect of motion blur. To generate zero mean Gaussian blur, we vary the kernel size $k_b$from $3\times3$ to $51\times51$ with an interval of 2 to maintain odd kernel size and the standard deviation of the blur is calculated as $\sigma_b = 0.3*((k_b-1)*0.5 - 1) + 0.8$, where $k_b$ is the size of the square Gaussian kernel.
\subsubsection{Quality}
Often after capturing, raw image goes through multiple compression steps for storage or processing. Thus, to understand the effect of compression, we consider JPEG compression as a distortion type in our experiment. We use standard JPEG encoder and vary the JPEG quality level ($q$) from 30 to 0 in our experiments where a higher value in the quality level parameter indicates better visual quality of the compressed image with less compression.\\
In Fig. \ref{fig:degradation_samples}, we show the effects of different degradations that are used in this work. To apply a particular image degradation, we first apply the degradation on the input image and then resize the degraded image into input shape of a specific CNN model.
\begin{figure*}
\captionsetup[subfigure]{labelformat=empty}
\subfloat[(a) Gaussian white noise]{\includegraphics[width=6cm]{figures/ssim/gaussian_white_ssim}}
\subfloat[(b) Colored Gaussian noise]{\includegraphics[width=6cm]{figures/ssim/gaussian_color_ssim}}
\subfloat[(c) Salt and pepper noise]{\includegraphics[width=6cm]{figures/ssim/salt_and_pepper_ssim}} \vspace{-3mm}\\
\subfloat[(d) Motion blur]{\includegraphics[width=6cm]{figures/ssim/motion_blur_ssim}}
\subfloat[(e) Gaussian blur]{\includegraphics[width=6cm]{figures/ssim/gaussian_blur_ssim}}
\subfloat[(f) JPEG Quality]{\includegraphics[width=6cm]{figures/ssim/jpeg_quality_ssim}}
\caption{SSIM comparisons between the last convolution layers of basic CapsuleNet and V-CapsNet under different image degradations.}
\label{fig:result_ssim}
\end{figure*}

\section{Results}\label{results}
To understand the effects of different degradations, we apply the degradations on the input images and measure the top recognition accuracy and top-3 recognition accuracy of the six models considered in this paper. The output of the networks are the recognition probability of each class of the dataset. We include the top-3 accuracy measure as it is popular in many complex recognition tasks. Fig. \ref{fig:result_synthetic_digits} shows the accuracies of different models under the degradations.

It can be observed that for both AWGN and colored Gaussian, the recognition accuracies decrease as we increase the variance of the additive noises. VGG architectures look more robust against the additive noises than the other conventional CNN architectures. The decrement in recognition accuracy for CapsuleNet is notably small for Gaussian noises.

Salt and pepper noise badly affects MobileNet architecture. Though the recognition accuracy using CapsuleNet is slightly lower than the other models in absence of impulse noise, CapsuleNet starts to outperform all the models even when the noise density is more than 0.2. For both the accuracy measures, CapsuleNet outperforms all the state-of-the-art models in presence of salt-and-pepper noise.

Both motion blur and Gaussian blur degrade the performance of CNN models. However, VGG architectures are more robust to blurring than ResNet or Inception models for synthetic digits dataset. CapsuleNet is robust to both blurring degradations and outperforms MobileNet, Inception v3 and ResNet.

As we can see from Figs. \ref{fig:result_synthetic_digits}(f) and \ref{fig:result_natural_images}(f), JPEG quality does not affect the recognition performance of any model till the compression quality is 20. Beyond that the performance of ResNet and MobileNet degrade sharply. Inception, VGG and CapsuleNet architectures perform significantly well even when the JPEG quality is 0.

To understand the robustness of the capsule architecture, we perform the same test on two different CapsuleNet models - one with single routing iteration and with only marginal loss in the loss function and another with 3 routing iterations and reconstruction loss combined with marginal loss in the loss function. In both the cases, no significant change in accuracy under different image degradations was observed, though architecture with higher routing and reconstruction loss as regularizer performed better in most of the cases. In Fig. \ref{fig:result_capsnet_comparisons}, we compare the recognition accuracy for two different hyper parameter settings of the CapsuleNet. It can be observed that except for the additive noises, CapsuleNet with higher routing performs slightly better.

As the basic capsule architecture cannot achieve substantially high accuracy for real image dataset, we design a novel capsule-based architecture. The proposed architecture takes input shape $256\times256$ and the convolutional layers are same as VGG19 architecture up to its 1st convolutional layer of fifth block. The output of the last convolutional layer goes to primary capsule layer with a dropout of 80\% where we have 32 number of 8-dimensional capsules generated using convolution with kernel size 3 and stride 2. The final 16-dimensional capsule layer is same as in \cite{sabour2017dynamic}, but to reduce the number of parameters, we remove the decoder module of basic capsule architecture and minimize only the marginal loss. For the ease of discussion, we call this novel architecture as V-CapsNet (VGG19 + CapsuleNet) in rest of our discussion. Using the architecture, we achieve 99.83\% accuracy in the natural image dataset, which is almost 6.2\% higher than the accuracy of basic CapsuleNet model discussed in Sec. \ref{sec:deep_neural_networks}. However, even after this significant improvement in classification accuracy, we observe that in almost all the cases, V-CapsNet is more susceptible to degradations than the basic CapsuleNet architecture. The performance of V-CapsNet under different image degradations are shown in Fig. \ref{fig:result_fusion_model}. It is evident that though the novel V-CapsNet architecture achieves significantly higher accuracy than the conventional CapsuleNet architecture, V-CapsNet is more sensitive to image degradations than both basic CapsuleNet and VGG19.\\
While investigating the reason behind the resilience of basic CapsuleNet model over V-CapsNet under degradations, we observe that not only the capsule layers, but the shallowness of the basic CapsuleNet model shows robustness when degradation is present in the input image. To test our hypothesis, we take the output of last convolution layer of basic CapsuleNet and the output of last convolution layer of V-CapsNet and measures the change in the outputs independently when we introduce degradation in input image. To quantify the change in output, we normalize the output and measure mean structural similarity index (SSIM) \cite{wang2004image} of the output in presence of noise with respect to the output in absence of noise. It is observed that the structural information of the features extracted by basic CapsuleNet change much slower than the structural information of the features extracted by V-CapsNet. In other words, the features extracted by basic CapsuleNet is more robust to image degradation. Thus, the shallowness of the basic CapsuleNet might be one of the major reasons behind its robustness against different types of degradation. The SSIM comparison of the features extracted by basic CapsuleNet and V-CapsNet is shown in Fig. \ref{fig:result_ssim}. The two capsule architectures, i.e., basic CapsuleNet and V-CapsNet, that are used in this paper are shown in Fig. \ref{fig:capsnet_models}.
\begin{figure*}
\includegraphics[width=\linewidth]{figures/capsnet_models}
\caption{Two different capsule architectures used in this paper: (a) V-CapsNet; (b) basic CapsuleNet.}
\label{fig:capsnet_models}
\end{figure*}

\section{Conclusion}
It is evident that all the CNN architectures are susceptible to image degradations. It is interesting to observe that some shallower models like VGGs that achieve less accuracy in many classification tasks are more resilient to degradations. It is also important to notice that the new capsule architecture is remarkably robust against several image degradations, particularly against salt and pepper noise and blurring. We proposed a novel capsule network based architecture that achieves high accuracy in classification task. We also observe that having small number of convolution layers, basic CapsuleNet architecture is resilient to degradation and simply going deeper in the architecture in conventional way will affect the robustness. Thus, it is important to design CNN models that can achieve high accuracy without increasing the depth of the network. Though, we failed to achieve very high accuracy for recognition task using CapsuleNet where the dataset contains very large number of classes like ImageNet, CapsuleNet has shown promising results in all the cases. It will be important to come up with new networks in future that can provide robustness against image degradations maintaining high accuracy.

\section{Acknowledgement}
We would like to thank Nvidia\textsuperscript{\textregistered} for providing a Titan X GPU to our research group.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}


