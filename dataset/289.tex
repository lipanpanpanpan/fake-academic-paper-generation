\documentclass[10pt,onecolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{helvet}
\usepackage{courier}
\usepackage{bm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{multirow}%
\usepackage{amsmath, amssymb}
\usepackage{color}
\usepackage{mathrsfs}
\usepackage{amsthm}
\usepackage{epstopdf}
\usepackage{wrapfig}
\usepackage{picinpar}
\usepackage{url}


\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\DeclareUrlCommand\ULurl{%
  \renewcommand\UrlFont{\ttfamily\color{blue}}%
  \renewcommand\UrlLeft{\uline\bgroup}%
  \renewcommand\UrlRight{\egroup}}

\cvprfinalcopy % *** Uncomment this line for the final submission


\def\diag{\mbox{diag}}
\def\rank{\mbox{rank}}
\def\grad{\mbox{\text{grad}}}
\def\dist{\mbox{dist}}
\def\sgn{\mbox{sgn}}
\def\tr{\mbox{tr}}
\def\etal{{\em et al.\/}\, }
\def\card{{\mbox{Card}}}

\def\cD{ {\cal D } }
\def\cW{ {\cal W } }

\def\balpha{\mbox{{\boldmath $\alpha$}}}
\def\bbeta{\mbox{{\boldmath $\beta$}}}
\def\btheta{\mbox{{\boldmath $\theta$}}}
\def\bzeta{\mbox{{\boldmath $\zeta$}}}
\def\bgamma{\mbox{{\boldmath $\gamma$}}}
\def\bdelta{\mbox{{\boldmath $\delta$}}}
\def\bmu{\mbox{{\boldmath $\mu$}}}
\def\beps{\mbox{{\boldmath $\epsilon$}}}
\def\blambda{\mbox{{\boldmath $\lambda$}}}
\def\bnu{\mbox{{\boldmath $\nu$}}}
\def\bomega{\mbox{{\boldmath $\omega$}}}
\def\bfeta{\mbox{{\boldmath $\eta$}}}
\def\bsigma{\mbox{{\boldmath $\sigma$}}}
\def\bzeta{\mbox{{\boldmath $\zeta$}}}
\def\bphi{\mbox{{\boldmath $\phi$}}}
\def\bxi{\mbox{{\boldmath $\xi$}}}
\def\bvphi{\mbox{{\boldmath $\phi$}}}
\def\bdelta{\mbox{{\boldmath $\delta$}}}
\def\bvarsigma{\mbox{{\boldmath $\varsigma$}}}



\def\bXi{\mbox{{\boldmath $\Xi$}}}
\def\bTheta{\mbox{{\boldmath $\Theta$}}}
\def\bPi{\mbox{{\boldmath $\Pi$}}}
\def\bDelta{\mbox{{\boldmath $\Delta$}}}
\def\bPi{\mbox{{\boldmath $\Pi$}}}
\def\bPsi{\mbox{{\boldmath $\Psi$}}}
\def\bSigma{\mbox{{\boldmath $\Sigma$}}}

\def\mA{{\mathcal A}}
\def\mB{{\mathcal B}}
\def\mC{{\mathcal C}}
\def\mD{{\mathcal D}}
\def\mE{{\mathcal E}}
\def\mF{{\mathcal F}}
\def\mG{{\mathcal G}}
\def\mH{{\mathcal H}}
\def\mI{{\mathcal I}}
\def\mJ{{\mathcal J}}
\def\mK{{\mathcal K}}
\def\mL{{\mathcal L}}
\def\mM{{\mathcal M}}
\def\mN{{\mathcal N}}
\def\mO{{\mathcal O}}
\def\mP{{\mathcal P}}
\def\mQ{{\mathcal Q}}
\def\mR{{\mathcal R}}
\def\mS{{\mathcal S}}
\def\mT{{\mathcal T}}
\def\mU{{\mathcal U}}
\def\mV{{\mathcal V}}
\def\mW{{\mathcal W}}
\def\mX{{\mathcal X}}
\def\mY{{\mathcal Y}}
\def\mZ{{\mathcal{Z}}}

\def\bmW{\mbox{\boldmath${\mathcal W}$}}



\def\0{{\bf 0}}
\def\1{{\bf 1}}

\def\bA{{\bf A}}
\def\bB{{\bf B}}
\def\bC{{\bf C}}
\def\bD{{\bf D}}
\def\bE{{\bf E}}
\def\bF{{\bf F}}
\def\bG{{\bf G}}
\def\bH{{\bf H}}
\def\bI{{\bf I}}
\def\bJ{{\bf J}}
\def\bK{{\bf K}}
\def\bL{{\bf L}}
\def\bM{{\bf M}}
\def\bN{{\bf N}}
\def\bO{{\bf O}}
\def\bP{{\bf P}}
\def\bQ{{\bf Q}}
\def\bR{{\bf R}}
\def\bS{{\bf S}}
\def\bT{{\bf T}}
\def\bU{{\bf U}}
\def\bV{{\bf V}}
\def\bW{{\bf W}}
\def\bX{{\bf X}}
\def\bY{{\bf Y}}
\def\bZ{{\bf{Z}}}


\def\ba{{\bf a}}
\def\bb{{\bf b}}
\def\bc{{\bf c}}
\def\bd{{\bf d}}
\def\be{{\bf e}}
\def\bff{{\bf f}}
\def\bg{{\bf g}}
\def\bh{{\bf h}}
\def\bi{{\bf i}}
\def\bj{{\bf j}}
\def\bk{{\bf k}}
\def\bl{{\bf l}}
\def\bm{{\bf m}}
\def\bn{{\bf n}}
\def\bo{{\bf o}}
\def\bp{{\bf p}}
\def\bq{{\bf q}}
\def\br{{\bf r}}
\def\bs{{\bf s}}
\def\bt{{\bf t}}
\def\bu{{\bf u}}
\def\bv{{\bf v}}
\def\bw{{\bf w}}
\def\bx{{\bf x}}
\def\by{{\bf y}}
\def\bz{{\bf z}}

\def\hy{\hat{y}}
\def\hby{\hat{{\bf y}}}


\def\mmP{{\mathrm P}}
\def\mmB{{\mathrm B}}
\def\mmR{{\mathbb R}}
\def\mMLr{{\mM_{\leq r}}}
\def\mMr{{\mM_{r}}}
\def\tC{\tilde{C}}
\def\tk{\tilde{r}}
\def\tJ{\tilde{J}}
\def\tbx{\tilde{\bx}}
\def\tbK{\tilde{\bK}}
\def\tL{\tilde{L}}
\def\tbPi{\mbox{{\boldmath $\tilde{\Pi}$}}}
\def\tw{{\bf \tilde{w}}}





\def\pd{{\succ\0}}
\def\psd{{\succeq\0}}
\def\vphi{\varphi}
\def\trsp{{\sf T}}


\def\mRMD{{\mathrm{D}}}
\def\mRMD{{\mathrm{D}}}

\def\kui{\textcolor{black}}
\def\young{\textcolor{black}}
\def\javen{\textcolor{black}}
\def\anton{\textcolor{black}}
\def\mark{\textcolor{black}}

\def\citep{\cite}
\def\citet{\cite}

\newtheorem{coll}{Corollary}
\newtheorem{deftn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{ass}{Assumption}


\def\diag{\mbox{diag}}
\def\rank{\mbox{rank}}
\def\grad{\mbox{\text{grad}}}
\def\dist{\mbox{dist}}
\def\sgn{\mbox{sgn}}
\def\tr{\mbox{tr}}
\def\etal{{\em et al.\/}\, }
\def\card{{\mbox{Card}}}
\def\st{\mbox{s.t. }}
\def\SexyName{AuxNet\xspace}


\author{Yong Guo$^1$~~~ Mingkui Tan$^1$~~~ Qingyao Wu$^1$ Jian Chen$^1$~~~ Anton Van Den Hengel$^2$~~~ Qinfeng Shi$^2$\\
$^1$South China University of Technology, $^2$The University of Adelaide\\
{\tt\small \{guoyongcs, tanmingkui\}@gmail.com, \{qyw, ellachen\}@scut.edu.cn} \\
{\tt\small \{anton.vandenhengel, javen.shi\}@adelaide.edu.au}
}


\begin{document}

\title{The Shallow End: Empowering Shallower Deep-Convolutional Networks\\ through Auxiliary Outputs}


\maketitle

\begin{abstract}
The depth is one of the key factors behind the great success of convolutional neural networks (CNNs), with the gradient vanishing issue having been largely addressed by various nets, \eg ResNet. However, when the depth goes very deep, the supervision information from the loss function will vanish due to the long backpropagation path,  especially for those shallow layers. This means that intermediate layers receive less supervision information and will lead to redundancy in models. As a result, the model becomes very redundant and the over-fitting issue may happen.



To address this, we propose a model, called \SexyName, by introducing auxiliary outputs at intermediate layers. Different from existing approaches, we propose a Multi-path training method to propagate not only gradients but also sufficient supervision information from multiple auxiliary outputs. The proposed AuxNet with multi-path training method  gives rise to more compact networks which outperform their very deep equivalent (i.e. ResNet). For example, AuxNet with 44 layers performs better than the ResNet equivalent with 110 layers on several benchmark data sets, \ie CIFAR-10, CIFAR-100 and SVHN.





\end{abstract}

\section{Introduction} \label{sec:introduction}
Since 2012 when AlexNet won the first place in the ImageNet competition~\cite{krizhevsky2012imagenet}, convolutional neural networks (CNNs)~\cite{lecun1989backpropagation}, have been producing state-of-the-art results in many of the most challenging vision tasks including image classification~\cite{krizhevsky2012imagenet,Lee2015,he2015deep}, face recognition~\cite{schroff2015facenet,sun2015deeply}, semantic segmentation~\cite{long2015fully} and object detection~\cite{ren2015faster,Zagoruyko2016}.
Moreover, CNNs  have become the  workhorse of many learning tasks and real-world applications beyond computer vision, such as natural language understanding and speech recognition~\cite{lecun2015deep}.

Recent studies~\cite{simonyan2014very,srivastava2015training,szegedy2015going,Lee2015} have demonstrated that the depth of neural networks is crucial for their representation ability with limited units~\cite{srivastava2015training}.
However,  in practice, a deeper network often does not necessarily lead to better performance mainly due to gradient vanishing issue~\cite{glorot2010understanding,romero2014fitnets,srivastava2015highway,he2015deep}.
It is shown in \cite{he2015deep} that a plain network with large depth  often {\em under-fits}, instead of over-fitting, the training data, leading to a poor test accuracy.
More critically,  when the number of layers is very large, the network performance  may be severely degraded, due to the training difficulty caused by \emph{gradient vanishing} or \emph{exploding}~\cite{glorot2010understanding,romero2014fitnets,srivastava2015highway,he2015deep}.

Recently proposed deep residual network (ResNet) has largely addressed the gradient vanishing issue by introducing shortcut connections~\cite{he2015deep}. However, when the depth become very large, the overfitting issue may still happen due to what we call \emph{supervision information
vanishing}. This means that intermediate layers fail to improve their discriminative power as much as the increasing depth accordingly, which consequently result in severe internal redundancy in models and degradation in generalization ability~\cite{he2015deep,DBLP:journals/corr/HuangSLSW16}.
Especially, when there are insufficient training data available, the over-fitting is inevitable~\cite{he2015deep}. Nevertheless, how to choose a suitable depth for the model remains an open question. Moreover, the number of model parameters increases dramatically  with depth, leading to a dramatic increase in inference cost. However, in real-world applications, there is a demand for more computationally efficient models, and particularly for models capable of fast inference~\cite{ba2014deep}.

To address the supervision information vanishing issue and obtain compact models accordingly, we introduce auxiliary losses at intermediate layers to provide additional supervision information. In this way, all the auxiliary losses make up a joint loss which acts as a kind of regularization to help training deep networks, like DSN~\cite{Lee2015} and GoogLeNet~\cite{szegedy2015going}. However, treating the multiple losses jointly still has some limitations. First, the importance of losses are required to be adjusted during the training to overcome the training difficulty (See ~\cite{Lee2015}).  Second, the supervision information vanishing will still happen, and the performance often improve marginally  (See more details in Section~\ref{sec:bpcomparision}).




In this paper, we propose to address the above issues by devising a so called \SexyName, in which multiple weighted auxiliary losses (or outputs) are introduced to intermediate layers.  Different from existing methods such as  DSN~\cite{Lee2015} and GoogLeNet~\cite{szegedy2015going}, we propose a \emph{{Multi-path}} backpropagation (BP) method to train \SexyName.
Due to the Multi-path BP, the supervision vanishing can be effectively addressed.





The contributions  and novelty of this paper are summarized as follows.
\begin{itemize}
\vspace{-0.5em}
\item We devise a Multi-path BP method to train deep networks with auxiliary losses. Essentially, it applies one forward propagation for all losses but conducts the gradient BP for each loss separately in series. In this way, the supervision information vanishing issue can be effectively addressed, leading to greatly improved generalization performance than Joint BP methods adopted in DSN~\cite{Lee2015} and GoogLeNet~\cite{szegedy2015going}.
\vspace{-0.5em}
\item By providing sufficient supervision information with Multi-path BP, the proposed \SexyName reduces the internal model redundancy and gives rise to a more compact model (i.e., with fewer layers but better performance). For example, AuxNet with \textbf{44} layers performs better than the ResNet equivalent with \textbf{110} layers on several benchmark data sets, \ie, CIFAR-10, CIFAR-100 and SVHN.
\vspace{-0.5em}
\item All the losses in \SexyName remain effective during the training. Therefore, it inherently produces multiple models of different depths simultaneously. Surprisingly, these models have very good prediction power. This is beneficial for model selection. Given training data, models with large depth can be over-fitted. In this sense, the intermediate models may generalize better. In this sense, we propose a simple \emph{Safe Prediction} scheme by taking all the intermediate models to make a safe prediction and thus improve the prediction performance.

\end{itemize}




\section{Related Studies}\label{sec:related_Studies}
Employing auxiliary classifiers to aid in model training has been investigated in DeepID models for face recognition~\cite{sun2015deeply,sun2015deepid3}, GoogLeNet~\cite{szegedy2015going}, and deep supervised nets (DSN)~\cite{Lee2015,wang2015training}. In~\cite{sun2015deeply},  the supervisory signals are connected to each convolution layer; while in~\cite{sun2015deepid3} they are added to each pooling layer.

In GoogLeNet, two auxiliary classifiers (with weights equal to 0.3) are connected to intermediate layers to address the problem of vanishing gradients. In DSN~\cite{Lee2015}, each convolution layer, followed by a \emph{mlpconv} layer, is associated with a classifier. To avoid the over-fitting issue, they keep the losses for a number of epochs, and discard all but the final loss to finish the rest epochs. However, our method does not discard any loss, which helps to generate multiple models at the same time, with the ensuing benefits for model selection. In the training, both of them construct a joint objective function and apply only one backpropagation in each iteration, which is very different form our Multi-path BP. Moreover, these networks cannot be very deep (less than 25 layers). Even with auxiliary classifiers, the issue of supervision information vanishing can still happen for these methods.
In BranchyNet~\cite{teerapittayanon2016branchynet}, auxiliary outputs are regarded as side branches for early exit. These outputs, not for training, but allow test samples to exit from shallow layers to achieve fast inference.

Very recently, some attempts have been made to improve the discriminative power of networks.
He \etal investigated that the pre-activation” function could further improve the ResNet and make the training of 1000-layers model possible. In StochResNet~\cite{DBLP:journals/corr/HuangSLSW16}, a subset of layers are randomly dropped and bypassed in training~\cite{DBLP:journals/corr/HuangSLSW16}. However, all the layers are kept when doing the inference, thus the testing exhibits the same complexity as the original ResNet.
Zhang \etal~\cite{zhang2016residual} propose a new residual-network architecture called ResNet of ResNet, which add shortcut connections between multilevel blocks in ResNet and achieve better performance. In DenseNet~\cite{huang2016densely}, any two layers are directly connected during feed-forward and the feature maps of all preceding layers are used as inputs into all subsequent layers, which is very different from the additivity of feature in ResNet.
Although it keeps approximately the same number of parameters as ResNet, both the training and inference efficiency suffer due to the extremely increasing feature space.  In~\cite{jia2016improving}, Jia studies the effect of orthogonal weight matrices during training and constrains them within a feasible set using a bounding solution, specifically, Singular Value Bounding (SVB) for convolution layers and Bounded Batch Normalization (BBN) for batch normalization layers.




\begin{figure}[t]
\begin{center}
   \includegraphics[width=0.7\columnwidth, height=0.6\columnwidth]{auxresnet.pdf}
\end{center}
   \caption{Architecture of the proposed \SexyName. $fc$ indicates a fully connected layer, $\{\xi_k\}_{k=1}^{K}$ denotes losses w.r.t. auxiliary outputs, $\xi$ denotes the loss w.r.t. the final output, $N$ denotes the width of network and $\{\gamma_k\}_{k=1}^K$ denotes the weights on the auxiliary losses.}
\label{fig:ausresnet}
\vspace{-1em}
\end{figure}
\section{Proposed Architecture}\label{sec:model_structure}
In this section, we first discuss the issue of supervision information vanishing based on ResNet~\cite{he2015deep}, and then present the details of the proposed \SexyName.
\subsection{Supervision Information Vanishing}\label{sec:SIV}
Without loss of generality, we study ResNet~\cite{he2015deep} which
conducts \emph{forward inference} by
 \begin{equation}\label{eq:residual}
 \by_{l} = \lambda_l {\bx_l} + \mF({\bx_l},{{\bmW}_l}), ~~~\bx_{l + 1} = h(\by_{l}),
 \end{equation}
where $h$ is a nonlinear activation function and $\mF$ is the transformation function (e.g.,  the convolution operation~\cite{romero2014fitnets})
parameterized by $\bmW_l$. The model is reduced to a plain network when $\lambda_l=0$. For convenience, we assume $h(\bx) = \bx$.
Assuming $h(\bx) = \bx$, and applying the recursive rule in Eqn.~(\ref{eq:residual}), we achieve a simplified relation:
${\bx_L} = \lambda_l{\bx_l} + \sum_{i = l}^{L - 1} { \mF(\bx_i,\bmW_i) }$~\cite{he2016identity}.


Let $\xi$ be the loss function. For backpropagation, by applying the chain rule,  the gradient of $\xi$ w.r.t. $\bx_l$  for any layer $l$  can be written as
\begin{equation}\label{eq:gradient_resnet}
{{\partial_{\bx_l} \xi }} = {\partial_{\bx_{L}} \xi }  \prod\limits_{i = l}^{L - 1} \Big( {1 + \partial_{\bx_i}  {\mF({\bx_i},{{\bmW}_i})}} \Big),
\end{equation}
Here, the term ${\partial_{\bx_{L}} \xi }$ reflects the {\em supervision information} directly obtained from the loss $\xi$; While $\prod_{i = l}^{L - 1} ( {1 + \partial_{\bx_i}  {\mF({\bx_i},{{\bmW}_i})}} )$ transforms $\partial_{\bx_{L}} \xi$, and it can be very large at shallow layers with long path backpropagation, due to the additivity property of shortcut connections.

Now, we can measure the ratio of supervision information over the norm of gradient ${{\partial_{\bx_l} \xi }}$   for any layer $l$ by
$$
\rho_l = \frac{||\partial_{\bx_{L}} \xi||}{||\partial_{\bx_{l}} \xi||} \geq \underline{\rho} =  1 \Big/ \left\|\prod_{i = l}^{L - 1}( {1 + \partial_{\bx_i}  {\mF({\bx_i},{{\bmW}_i})}} )\right\|,
$$
where $\underline{\rho}$ is a lower bound of $\rho_l$ and $||\cdot||$ is some norm.

Apparently, given a large depth $L$, the lower bound $\underline{\rho}$ vanish rapidly for a small $l$, which means that the layer $l$ cannot receive sufficient information from the loss. Therefore, the ratio $\rho$ may also vanish rapidly for very shallow layers. Then, the \emph{{supervision information vanishing}} happens and consequently the supervision information reaching shallower layers may thus not be sufficient.










\subsection{\SexyName with Multiple Outputs}
To address the issue of supervision information vanishing, rooted in ResNet, we introduce additional $K$ auxiliary outputs (each associated with a loss function) to some intermediate blocks. The losses are weighted by $\{\gamma_k\}_{k=1}^K$. The general architecture, called \SexyName, is shown in Figure \ref{fig:ausresnet}. Note that \SexyName has the same forward propagation rule to ResNet.


In \SexyName, including the final output, it contains $K+1$ outputs in total, each for the same classification task. For convenience, hereafter we use \SexyName-$L$-$\widehat{K}$ to denote \SexyName with $L$ layers and $\widehat{K}$ outputs (where $\widehat{K} = K+1$). Besides ResNet, the above scheme can be applied to many existing CNNs. For example, one may extend the width of network by a factor of $N$ to improve the representation power of each block (See Wide ResNets~\cite{zagoruyko2016wide}).



\textbf{{Setting of $\{\gamma_k\}$}}.
The values of the $\{\gamma_k\}$ are  important to the performance of \SexyName.  Since features at deeper layers should have more discriminative power, the losses at deeper layers should be more important. Moreover, in practice, if $\gamma_k<0.01$, the effect of $\xi_k$ is negligible, we thus set $\gamma_k \geq 0.01$. In this sense, we suggest choosing $\gamma_k = \max(0.01, (\frac{L_k}{L_K})^\nu)$, where $\nu\geq 0$ reflects the decay rate of $\{\gamma_k\}$ w.r.t. $k$, and $L_k$ is the index of layer to which the $k$-th loss is applied. If setting $\nu = 0$, it follows $\gamma_k = 1, \forall k$, and that every loss is equally weighted.  However, this is too aggressive since shallow layers are not very discriminative, and a large weights may hamper the performance. If setting $\nu > 0$, the weights of losses at shallower layers will be smaller, which can improves overall performance. In practice, we suggest choosing $1\leq \nu \leq 2$ (See supplementary file for more discussions).

The positions of the auxiliary outputs are also important. In general, very shallow layers represent low-level structures which often have weak representation power even for a well trained shallow model~\cite{zeiler2014visualizing}. Therefore, the outputs should not be added to very shallow layers. For example, the first layer represents low-level feature after one convolution layer, thus it has very little representation ability. If adding an output to this layer, the loss and the gradients can be always very large, meaning that the training may not converge (See supplementary materials). Moreover, adding too many losses may cause severe over-fitting, which may hamper the generalization ability of intermediate layers and result in great drop in terms of testing error (see details in Section~\ref{exp:num_outputs}). Therefore, we do not add outputs to each block or even each layer. This is very different from the approach adopted in DSN~\cite{Lee2015}.







\begin{figure*}[t]
\centering

\subfigure[Joint backpropagation]{
\includegraphics[width = 0.32\columnwidth, height= 0.2\columnwidth]{jointbp.pdf}\label{fig:jointbp}
}
\subfigure[Pair-wise backpropagation]{
\includegraphics[width = 0.32\columnwidth, height= 0.2\columnwidth]{pairwisebp.pdf}\label{fig:pairwisebp}
}
\subfigure[Multi-path backpropagation]{
\includegraphics[width = 0.32\columnwidth, height= 0.2\columnwidth]{multiwaybp.pdf}\label{fig:multiwaybp}
}

\caption{Demonstration of three different BP methods for \SexyName. }\label{fig:BP}
\vspace{-1em}
\end{figure*}

\section{Training Methods for \SexyName} \label{sec:backpropagation}
In this section, we first show Joint gradient BP, which has been used in DSN~\cite{Lee2015} and GoogLeNet~\cite{szegedy2015going} for \SexyName, and discuss its limitations. We then propose our Pair-wise and Multi-path BP methods. The overall schemes of the three BP approaches are shown in Figure \ref{fig:BP}.

\subsection{Joint Gradient Backpropagation}
Let ${\xi}_k$ be the loss of the $k$-th output and ${\xi}$ be the loss of the final output. Joint gradient BP considers minimizing the following joint objective function as in~\cite{Lee2015}
\begin{equation} \label{eq:companion_obj}
{\cal{L}} = {\xi} + \sum\limits_{k = 1}^K {{\gamma _k}{\xi}_k}.
\end{equation}
Let $\bg(\bx_l)$ be the gradient induced by all losses w.r.t. $\bx_l$.
For any shallow layer $l$ (where $L_{k-1} < l \leq L_k$), the gradient $\bg(\bx_l)$ of the joint loss ${\cal{L}}$ w.r.t. $\bx_l$  can be computed by
\begin{equation}\label{eq:joint_gradient}
\bg_L(\bx_l) \!\!=\!\!  \frac{{\partial \xi }}{{\partial {\bx_l}}}  +  \sum_{j=k}^K\gamma_j\frac{{\partial \xi_j }}{{\partial {\bx_l}}}, \mathrm{~for~}
k = 1, ..., K,
\end{equation}
where
\begin{equation}\label{eq:sub}
\frac{{\partial \xi_j }}{\partial {\bx_l}} = {\partial_{\bx_{L_j}} \xi_j}  \prod_{i = l}^{L - 1} ( {1 + \partial_{\bx_i}  {\mF({\bx_i},{{\bmW}_i})}} . )\end{equation}

From Eqn.~(\ref{eq:sub}), $\bg_L(\bx_l)$ is closely related to the depth $L$. In particular, for any shallow layer $l$ (where $L_{k-1} < l \leq L_k$), the supervision information is mainly obtained from the nearest loss (\ie the $k$-th loss).
Similar in Section \ref{sec:SIV}, we can measure the ratio of supervision information  $||{\partial_{\bx_{L_k}} \xi_k}||$ over the whole gradient $||\bg_L(\bx_l)||$ by
\begin{equation}\label{eq:joint_ratio}
\rho_l = {||{\partial_{\bx_{L_k}} \xi_k}||}/{||\bg_L(\bx_l))||}.
\end{equation}
When $L$ is very large, $||\prod_{i = l}^{L - 1} ( {1 + \partial_{\bx_i}  {\mF({\bx_i},{{\bmW}_i})}}||$ can be very large, and $||\bg(\bx_l)||$ can be very large with high probability.\footnote{We also empirically demonstrate this in Figure \ref{fig:grad}.}
As a result, the supervision information may still vanish when the network goes deeper. 

\subsection{Pair-wise Gradient Backpropagation}
In this method, we sequentially conduct forward propagation and backpropagation for each output (shown in Figure~\ref{fig:pairwisebp}).  For any epoch $t$ in SGD, suppose we deal with the outputs in an order of $k = 1, 2, ..., K+1$, where $K+1$ denotes the index of the final output. For the $k$-th output, we first compute the features $\{\bx^{k}_l\}$ and the loss $\xi_k$ regarding this output based on the model updated by the $(k-1)$-th output, denoted by $\bmW_t^{k-1}$ for simplicity.  {Here, when $k=1$, we have $\bmW_t^{0} = \bmW_{t-1}^{K+1}$.} After that, we compute the gradient $\bg_k(\bx_l)$ for the $k$-th output
at any layer $l$ that is lower than or equal to $L_k$ by
\begin{equation}\label{eq:pair-wise_gradient}
\bg_k(\bx^{k}_{l}) = \gamma_k \frac{\partial \xi_k }{ \partial \bx^{k}_{L_k} }  \cdot \frac{\partial \bx^{k}_{L_k} }{ \partial \bx^{k}_{L_k-1} }  ...\frac{\partial  \bx^{k}_{l+1}  }{\partial \bx^{k}_{l} }, ~~\mathrm{for}~~ l\leq L_k,
\end{equation}
or it can be simplified as
\begin{equation}\label{eq:pair-wise-grad}
\bg_k(\bx^{k}_{l}) = \gamma_k {\partial_{\bx_{L_k}^k} \xi_k }  \prod\limits_{i = l}^{L_k - 1} \Big( {1 + \partial_{\bx_i^k}  {\mF({\bx_i^k},{{\bmW}_i^k})}} \Big),
\end{equation}
Similar to Eqn. \ref{eq:joint_ratio}, we can measure the ratio of supervision information over $||\bg_k(\bx^{k}_{l})||$ for any layer $l$ ($l\leq L_k$) by
\begin{small}
$$\rho_l = \frac{||\gamma_k{\partial_{\bx_{L_k}^k} \xi_k }||}{||\bg_k(\bx^{k}_{l})||} \geq \underline{\rho} =  1 \Big/ \left\|\prod\limits_{i = l}^{L_k - 1} \Big( {1 + \partial_{\bx_i^k}  {\mF({\bx_i^k},{{\bmW}_i^k})}} \Big)\right\|.$$\end{small}
 Obviously, for Pair-wise gradient BP, in contrast to ResNet and Joint gradient BP, the ratio $\rho_l$ is determined by $L_k$ rather than $L$. As a result, for any layer $l$, sufficient supervision information can be obtained from the nearest output (\ie, $\xi_k$).


In Pair-wise gradient BP, for each loss,
{the features and loss are updated before each forward-backpropagation pair, which may incur two \emph{limitations}.}
\textbf{First}, after each backpropagation, the loss for the next output shall decrease with very high probability, which will decrease the magnitude of gradient. This will incur degraded performance. \textbf{Second}, forward propagation for each output takes additional cost. To amend these, we propose the following BP method, called \emph{Multi-path gradient} backpropagation.




\subsection{Multi-path Gradient Backpropagation}
We apply one forward propagation for updating all losses and the features for all layers. Then, keeping the features and losses unchanged, we conduct gradient backpropagation to update $\bmW^{k}$ for each loss separately  instead of summing them up, as shown in Figure~\ref{fig:multiwaybp}.
Let $\{\bx_l\}$ be the features of layer $l$ which are fixed for all outputs. In this case, for the $k$-th output, the gradient ${{\partial \xi_k }}/{{\partial {\bx_l}}}$ for any layer $l$ that is lower or equal to $L_k$ shall be computed by
\begin{eqnarray} \label{eq:multipath_individual_gradient}
\bg_k(\bx_{l}) = \gamma_k \frac{\partial \xi_k }{ \partial \bx_{L} }  \cdot \frac{\partial \bx_{L} }{ \partial \bx_{L-1} }  ...\frac{\partial  \bx_{l+1}  }{\partial \bx_{l} }, ~~\mathrm{for}~~ l\leq L_k
\end{eqnarray}
The difference of (\ref{eq:multipath_individual_gradient}) from (\ref{eq:pair-wise_gradient}) is that the features are kept unchanged for all the backpropagations.

The ratio of supervision information over $||\bg_k(\bx_{l})||$ for any layer $l$ ($l\leq L_k$) can be computed by
\begin{small}{$$\rho_l = \frac{||\gamma_k{\partial_{\bx_{L_k}} \xi_k }||}{||\bg_k(\bx_{l})||} \geq \underline{\rho} =  1 \Big/ \left\|\prod\limits_{i = l}^{L_k - 1} \Big( {1 + \partial_{\bx_i}  {\mF({\bx_i},{{\bmW}_i^k})}} \Big)\right\|.$$}\end{small}
Similar to Pair-wise BP, the lower bound $\underline{\rho_l}$ for the ration is determined by $L_k$ rather than $L$. As a result, for any layer $l$, sufficient supervision information for intermediate layers can be obtained from the nearest output (\ie, $\xi_k$). Therefore, the issue of supervision information vanishing is addressed.

Lastly, in terms of training cost, AuxNet with Multi-path BP is about ($K/2+1$) times of ResNet, since it conducts one forward propagation and ($K+1$) backpropagations at each iteration. Testing time for AuxNet is the same as ResNet when they have the same depth. However, AuxNet often produces
more compact network than ResNet, thus the testing for AuxNet can be much faster.

















\begin{figure}[t]
\centering

\subfigure[]{
\includegraphics[trim = 2mm 0mm 12mm 1mm,
clip, width = 0.43\columnwidth, height= 0.36\columnwidth]{supervision_information_ratio.pdf}\label{fig:ratio}
}
\subfigure[]{
\includegraphics[trim = 6mm 0mm 8mm 1mm,
clip, width = 0.43\columnwidth, height= 0.36\columnwidth]{cifar10_bp_test_new.pdf}\label{fig:ratio_testing_error}
}


\caption{Performance comparison of \SexyName-56-5 on CIFAR-10 with different BP methods. (a) Supervision information changes w.r.t. layers; (b) Evolution of testing error.}\label{fig:grad}
\vspace{-0.8em}
\end{figure}


\subsection{Comparisons of Backpropagation Methods }\label{sec:bpcomparision}
We empirically study the supervision information vanishing issue and compare different BP methods for \SexyName with 56 layers and 4 auxiliary outputs (at layers 15, 25, 35, 45) on CIFAR-10. ResNet with 56 layers is adopted as the baseline.

In Figure \ref{fig:ratio}, we compute and record the supervision information ration $\rho_l$ (not the lower bound $\underline{\rho}$) at the
final training epoch of SGD for ResNet, Joint BP, Pair-wise BP and Multi-path BP, respectively. In Figure \ref{fig:ratio_testing_error}, we monitor the testing error evolution of different methods.

From Figure \ref{fig:ratio}, $\rho_l$ for ResNet indeed decreases rapidly as $l$ reduces (reaching shallow layers). This makes intermediate layers difficult to train, and leads to model redundancy. For \SexyName with Joint BP, it shows improvement over ResNet, but the ratio still diminishes very fast when reaching shallower layers. Conversely, for \SexyName with  Pair-wise BP and Multi-path BP, the supervision ratio remains reasonably large even to the first layer. From Figure \ref{fig:ratio_testing_error}, \SexyName with Joint BP  shows much better performance than ResNet; while Multi-path BP shows much better performance than Joint BP. For Pair-wise BP, since it updates the loss for each output, the performance is severely degraded. %Due to page limitation,  we only study \SexyName with  Multi-path gradient BP in our later experiments.












\begin{figure*}[t]
\centering

\subfigure[Input images]{
\includegraphics[trim = 0mm 0mm 0mm 0mm,
clip, width = 0.19\columnwidth, height= 0.31\columnwidth]{predplot1.pdf}\label{}
}
\hspace{-5ex}
\subfigure[output-67]{
\includegraphics[trim = 0mm 0mm 0mm 0mm,
clip, width = 0.2\columnwidth, height= 0.31\columnwidth]{predplot2.pdf}\label{}
}
\hspace{-3ex}
\subfigure[output-79]{
\includegraphics[trim = 0mm 0mm 0mm 0mm,
clip, width = 0.2\columnwidth, height= 0.31\columnwidth]{predplot3.pdf}\label{}
}
\hspace{-3ex}
\subfigure[output-91]{
\includegraphics[trim = 0mm 0mm 0mm 0mm,
clip, width = 0.2\columnwidth, height= 0.31\columnwidth]{predplot4.pdf}\label{}
}
\hspace{-3ex}
\subfigure[output-101]{
\includegraphics[trim = 0mm 0mm 0mm 0mm,
clip, width = 0.2\columnwidth, height= 0.31\columnwidth]{predplot5.pdf}\label{}
}

\caption{Prediction results AuxNet with 101 layers and 4 outputs on ImageNet validation data.  \textbf{Column (a)}: testing images with ground truth label; \textbf{Column (b)}: The top 5 prediction probabilities of the output at the $67$th layer; \textbf{Column (c)}: The top 5 prediction probabilities of the output at the $79$th layer; \textbf{Column (d)}: The top 5 prediction probabilities of the output at the $91$th layer; \textbf{Column (e)}: The top 5 prediction probabilities of the output at the $101$th layer. The red bar denotes the prediction probability of the ground truth class.}\label{fig:safeprediction}
\label{fig:testcase}
\end{figure*}

\section{Safe Prediction with Multiple  Models}\label{sec:safe}
One advantage of AuxNet is that, it inherently produces multiple models of different depths simultaneously.
In general, the discriminative power of deeper models are superior to the shallower ones.  This may not be necessarily true in practice. In Figure~\ref{fig:testcase}, for the first two images, deeper models shows better discriminative power, but for the third image, the final model make a fake prediction while its shallower counterparts have correctly predicted the ground truth label. Moreover, given limited data, the CNNs may easily over-fit if the depth is too large. In this case, the shallow models may generalize much better. Nevertheless, how to choose an appropriate model is still an open question in deep learning.

An natural question is, considering the shallow models may already correctly predicted the ground truth label, is there any way to combine all the intermediate models to attain a safe prediction? If so, the difficulty of model selection can be largely avoided.


Here, we address by proposing a method to select the prediction with the highest confidence among all the outputs.
Let $\hat{\by}$ denote the obtained softmax probability from a specific output.  To evaluate the quality of prediction for the sample $\bx$, we use \textbf{entropy} as the measure:
\begin{equation}
Entropy({\hat \by}; \bx) =  - \left\langle {\hat \by,\log \hat \by} \right\rangle ,
\end{equation}
where $\langle \cdot, \cdot\rangle$ denotes the inner product of two vectors. The smaller the entropy is, the more confident the prediction will be, referring to the prediction with the smallest entropy in each test case in Figure~\ref{fig:testcase}. Note that in Figure~\ref{fig:testcase}, the correct decision for an example is made by the model with the least entropy. Motivated by this, we first make prediction with all the models and then select the one with the least entropy as the final decision:
\begin{equation}
\hat \by = \mathop {\arg \min }\limits_{{\hat \by_i}} ~~Entropy({\hat \by_i};{\bx_i}), ~~~ i=1, \cdots, K+1.
\end{equation}
In this way, although deep models may over-fit, by taking advantage of all the outputs, we can select the output with the highest confidence for the final decision, which is at least as good as the prediction with the final model. We thus make a safer prediction.


\section{Experiments}\label{sec:exp}
We implement \SexyName based on Torch.\footnote{Torch is from \url{http://torch.ch}. We will release the code and models of \SexyName soon.}
Several state-of-the-art deep learning models are adopted as baselines, including FitNet~\cite{romero2014fitnets},
DSN~\cite{Lee2015}, Frac.Pool~\cite{graham2014fractional}, Highway Network~\cite{srivastava2015highway}, StochResNet~\cite{DBLP:journals/corr/HuangSLSW16}, ResNet~\cite{he2016identity}, Wide Residual Network(Wide ResNet)~\cite{zagoruyko2016wide}, ResNet of ResNet~\cite{zhang2016residual}, DenseNet~\cite{huang2016densely} and SVB-Wide ResNet~\cite{jia2016improving}.

We conduct comparisons on four benchmark data sets: CIFAR-10~\cite{krizhevsky2009learning}, CIFAR-100~\cite{krizhevsky2009learning}, the Street View House Number (SVHN)~\cite{netzer2011reading} and ImageNet-2012~\cite{russakovsky2015imagenet}.
For CIFAR-10, it contains 10 classes of 32x32 natural color images, each with 5,000 training samples
and 1,000 testing samples. CIFAR-100 contains 100 classes. Each class has 500 training samples and 100 testing samples.
SVHN contains house number images of 32x32 pixels, which includes 73237 digits for training, 26032 digits for testing, and 531131 digits as extra training data.
ImageNet contains 1,000 classes with 1.28 million training images and 50k testing images.

We use the same experimental setting for \SexyName as that for ResNet~\cite{he2015deep}. We train AuxNet using the proposed Multi-path BP with SGD using a mini-batch size of 128 on 2 GPUs (64 each). The momentum for SGD is 0.9, and the learning rate starts from 0.1 and is divided by 10 at 40\% and 60\% of total epochs respectively.  The model parameters are initialized as in~\cite{he2015delving}. For AuxNet, each intermediate output can be used for prediction. However, for convenience, we first report the results using \textbf{last output}. We will study the prediction ability of intermediate models and safe prediction in Section~\ref{exp:inter_models} and Section~\ref{exp:ensemble}, respectively.







\subsection{Results on CIFAR-10 and CIFAR-100}\label{exp:main_results}
\noindent \emph{\textbf{Comparisons with ResNet}}.
We perform 400 SGD epochs for the training, and use the same data augmentation as in~\cite{Lee2015}, such as translation and horizontal flipping.
We first compare  the testing error evolution of 3 ResNets with (44, 56, 110)-layers  and 4 AuxNets of different settings. From Figure \ref{fig:cifar10test} and Table~\ref{tab:cifar-resnet}, we have the following observations.
\textbf{First}, \SexyName-44-2, \SexyName-44-3, \SexyName-56-2, and \SexyName-56-5 all consistently outperform their ResNet counterparts with the same depth.
\textbf{Second}, with much fewer layers, \SexyName-56-2 and \SexyName-56-5 yield better results than the deeper ResNet with 110 layers.
\textbf{Third}, \SexyName-44-3 and \SexyName-56-5 with more auxiliary outputs achieve larger improvement in terms of testing error than \SexyName-44-2 and \SexyName-56-2, respectively. All these observations strongly demonstrate the effectiveness of \SexyName trained by Multi-path BP.

\begin{figure}[t]
\centering
\subfigure[Testing error]{
\includegraphics[width = 0.47\columnwidth, height= 0.36\columnwidth]{best_result_cifar10_test.pdf}\label{fig:cifar10test}
}
\subfigure[Training error]{
\includegraphics[width = 0.47\columnwidth, height= 0.36\columnwidth]{best_result_cifar10_train.pdf}\label{fig:cifar10train}
}

\caption{Testing error and training error evolution of ResNet and different \SexyName on CIFAR-10. Taking \SexyName-44-2 for example, it denotes a \SexyName network with 44 layers and 1 auxiliary output (i.e., 2 outputs in total). The numbers in brackets indicate the indices of layers with added outputs.}
\label{fig:cifar}
\end{figure}

\begin{table}[t]
  \centering
  \caption{Comparisions with ResNet on CIFAR-10 and CIFAR-100.}
    \begin{tabular}{c|c|c|c|c}
    \hline
    \multirow{2}[0]{*}{network} & \multirow{2}[0]{*}{depth} & \multirow{2}[0]{*}{outputs position} & \multicolumn{2}{c}{error (\%)} \\
    \cline{4-5}
        &   &   & CIFAR-10 & CIFAR-100 \\
    \hline
    ResNet-20~\cite{he2016identity} & 20    & \{20\}  & 7.76 & 31.12 \\
    ResNet-32~\cite{he2016identity} & 32    & \{32\}  & 6.81 & 29.74 \\
    ResNet-44~\cite{he2016identity} & 44    & \{44\}  & 6.37 & 28.85 \\
    ResNet-56~\cite{he2016identity} & 56    & \{56\}  & 6.08 & 28.46 \\
    ResNet-110~\cite{he2016identity} & 110   & \{110\} & 5.86 & 27.41 \\
    \hline
    \SexyName-20-2 & 20    & \{17, 20\} & 7.72 & 30.44 \\
    \SexyName-32-2 & 32    & \{25, 32\} & 6.73 & 28.24 \\
    \SexyName-44-2 & 44    & \{35, 44\} & 6.12 & 27.46 \\
    \SexyName-44-3 & 44    & \{25, 35, 44\} & 5.85 & 27.19 \\
    \SexyName-56-2 & 56    & \{45, 56\} & 5.77 & 26.83 \\
    \SexyName-56-5 & 56    & \{15, 25, 35, 45, 56\} & 5.53 & 26.62 \\
    \SexyName-110-5 & 110   & \{58, 73, 85, 97, 110\}   &   5.41    &  26.48 \\
    \hline
    \end{tabular}
  \label{tab:cifar-resnet}%
\end{table}

\noindent \emph{\textbf{Comparisons with the state-of-the-arts}}.
Now we report a comprehensive comparison of \SexyName with other baselines on both CIFAR-10 and CIFAR-100 data sets in terms of testing error.
Note that as shown in Figure~\ref{fig:ausresnet}, we can increase the network width by a factor of $N$ to improve the representation ability of each layer.
From Table \ref{tab:cifar}, we have the following new observations. \textbf{First}, our \SexyName with auxiliary outputs achieves better or comparable performance with ResNet and other baselines. Specifically, \SexyName-56-5 yields 5.53\% and 26.62\% error on CIFAR-10 and CIFAR-100, respectively, which are much better than its ResNet-56 counterpart and ResNet-110 with far more layers. This demonstrates that \SexyName indeed is capable of reducing the model redundancy.
\textbf{Second}, by increasing the layer width, \SexyName succeeds to greatly improve the performance, i.e., \SexyName-28-2 (10x wider) yields 3.77\% and 19.69\% on CIFAR-10 and CIFAR-100 respectively.
Because of the success of DenseNet~\cite{huang2016densely} and Singular Value Bounding (SVB)~\cite{jia2016improving}, we further apply the Multi-path BP to train \SexyName with dense layers and SVB. \textbf{Finally}, we further improve the performance and \SexyName-SVB-28-2/10 achieve the best performance simultaneously on CIFAR-10 (3.32\%) and CIFAR-100 (17.87\%).
These observations demonstrate that, by leveraging the power of auxiliary outputs, \SexyName trained by Multi-path BP improves the performance of ResNet by a large margin.

\begin{table}[h]
  \centering
  \caption{Classification error on CIFAR-10 and CIFAR-100. Note that \kui{\{$\cdot$/$N$\}} indicates a N-fold increase in network width, e.g. \SexyName-26-2/10 is 10 times wider than the baseline.}
    \begin{tabular}{c|c|c|c|c}
    \hline
    \multirow{2}[0]{*}{network} & \multirow{2}[0]{*}{depth} & \multirow{2}[0]{*}{outputs position} & \multicolumn{2}{c}{error (\%)} \\
    \cline{4-5}
        &   &   & CIFAR-10 & CIFAR-100 \\
    \hline
    FitNet~\cite{romero2014fitnets} & 19 & \{19\}  & 8.39 & 35.04 \\
    DSN~\cite{Lee2015} & 11 & \{1, 5, 9, 11\}  & 7.97 & 34.57 \\
    Frac.Pool, 1 test~\cite{graham2014fractional} & 15 & \{15\}  & 4.50 & 31.20 \\
    Highway Network~\cite{srivastava2015highway} & 19 & \{19\}  & 7.60 & 32.24 \\
    StochResNet-110~\cite{DBLP:journals/corr/HuangSLSW16} & 110 & \{110\} & 5.23 & 24.58 \\
    DenseNet~\cite{huang2016densely} & 100 & \{100\} & 3.74 & 19.25 \\
    ResNet of ResNet/4~\cite{zhang2016residual} & 58 & \{58\} & 3.77 & 19.73 \\
    Wide ResNet/10~\cite{zagoruyko2016wide} & 28 & \{28\} & 4.17 & 20.50 \\
    SVB-Wide ResNet/10~\cite{jia2016improving} & 28 & \{28\} & 3.58 & 18.32 \\
    ResNet-110~\cite{he2016identity} & 110   & \{110\} & 5.86 & 27.41 \\
    ResNet-1001~\cite{he2016identity} & 1001   & \{1001\} & 4.62 & 22.71 \\
    \hline
    \SexyName-28-2/10 & 28   & \{21, 28\} & 3.77 & 19.69 \\
    \SexyName-Dense-100-4 & 100  & \{72, 81, 90, 100\} & 3.53 & 19.13 \\
    \SexyName-SVB-28-2/10    & 28  & \{21, 28\} & \textbf{3.32} & \textbf{17.87} \\
    \hline
    \end{tabular}
  \label{tab:cifar}%
\end{table}

\subsection{Results on SVHN}
We divide the images by 255 for scaling without data augmentation and perform training for 200 epochs. From Table \ref{tab:svhn}, \SexyName-56-2 with 56 layers significantly improves the performance of ResNet-110, which is a deeper structure (yielding 1.84\% in error rate). When using dropout, \SexyName-56-3 obtains 1.58\% error rate, which is the best published result on SVHN to our knowledge. Dropout here works as a regularizer to reduce overfitting, and improves the performance of both \SexyName and ResNet. Nevertheless, \SexyName performs consistently better than ResNet with and without dropout.

\begin{table}[h]
  \centering
  \caption{Testing error on SVHN. $^\dagger$ denotes results with dropout.}
    \begin{tabular}{c|c|c}
    \hline
    network & outputs position & error (\%) \\
    \hline
    FitNet~\cite{romero2014fitnets}      & \{19\}  & 2.42 \\
    DSN~\cite{Lee2015}      & \{1, 5, 9, 11\} & 1.92 \\
    StochResNet~\cite{DBLP:journals/corr/HuangSLSW16}      & \{152\} & 1.75 \\

    ResNet-44~\cite{he2016identity}   & \{44\}    & 2.15 \\
    ResNet-56~\cite{he2016identity}   & \{56\}    & 2.06 \\
    ResNet-110~\cite{he2016identity}  & \{110\}  & 1.97 \\

    ResNet-44$^\dagger$~\cite{he2016identity}   & \{44\}    & 1.91 \\
    ResNet-56$^\dagger$~\cite{he2016identity}   & \{56\}    & 1.70 \\
    ResNet-110$^\dagger$~\cite{he2016identity}   & \{110\}    & 1.65 \\
    \hline
    \SexyName-44-2 & \{35, 44\}    & 1.96 \\
    \SexyName-56-2 & \{56, 45\}    & 1.84 \\
    \SexyName-44-2$^\dagger$ & \{35, 44\}    & 1.75 \\
    \SexyName-56-2$^\dagger$ & \{45, 56\}    & 1.63 \\
    \SexyName-56-3$^\dagger$ & \{35, 45, 56\}    & \textbf{1.58} \\
    \hline
    \end{tabular}
  \label{tab:svhn}%
\end{table}%

\subsection{Results on ImageNet}
The experimental setting is the same as that given in~\cite{he2015deep}. We scale the learning rate by a factor of 0.5 to train \SexyName with 90 epochs.
Table~\ref{tab:imagenet1000} shows the results of top-1 and top-5 errors on ImageNet-2012 validation set.

\begin{table}[htbp]
  \centering
  \caption{Validation error on ImageNet-2012 (\textbf{10-crop} testing).}
    \begin{tabular}{c|c|c|c}
    \hline
    \multirow{2}[0]{*}{network} & \multirow{2}{*}{outputs position} & \multicolumn{2}{c}{error (\%)} \\
    \cline{3-4}
        &   & top-1 & top-5 \\
    \hline
    VGG-16~\cite{simonyan2014very} & \{16\}  & 28.07 & 9.33 \\
    GoogLeNet~\cite{szegedy2015going} & \{22\}  & - & 9.15 \\
    PReLU-net~\cite{he2015delving} & \{22\}  & 24.27 & 7.38 \\
    \hline
    ResNet-34~\cite{he2015deep} & \{34\}  & 24.76 & 7.35 \\
    ResNet-50~\cite{he2015deep} & \{50\}  & 22.85 & 6.71 \\
    ResNet-101~\cite{he2015deep} & \{101\}  & 21.75 & 6.05 \\
    Inception-ResNet~\cite{DBLP:journals/corr/SzegedyIV16} & \{136\}  & 18.77 & 4.13 \\ \hline \hline
    \SexyName-34-2 & \{29, 34\} & 24.61 & 7.18 \\
    \SexyName-50-2 & \{40, 50\} & 22.77 & 6.27 \\
    \SexyName-50-4 & \{22, 31, 40, 50\} & 22.34 & 6.07 \\
    \SexyName-101-4 & \{67, 79, 91, 101\} & 20.95 & 5.25 \\
    Inception-\SexyName-4 & \{98, 110, 122, 136\} & \textbf{18.69} & \textbf{4.05} \\
    \hline
    \end{tabular}
  \label{tab:imagenet1000}%
\end{table}

Table~\ref{tab:imagenet1000} shows that \SexyName achieves better performance than ResNet.
For example, \SexyName-34-2 and \SexyName-50-2 has outperformed ResNet with the same number of layers. When adding more auxiliary outputs, the \SexyName-50-4 with 4 outputs in total further improve the performance to 22.43\% and 6.07\% on top-1 error and top-5 error respectively.
For Inception-\SexyName with much more parameters, to save training time, we employed the trained Inception-ResNet model as initialization when performing Multi-path BP to train with auxiliary outputs. We use the same setting as those for ResNets except for the learning rate that starts from 0.0045 (0.1 times of that in~\cite{DBLP:journals/corr/SzegedyIV16}). As a result, we achieve the best performance of 18.69\% and 4.05\% on top-1 error and top-5 error respectively.






\section{More Analysis on \SexyName}\label{sec:analysis}



\subsection{Prediction Ability of Intermediate Models}\label{exp:inter_models}
The \SexyName training process inherently generates multiple models of differing depths.
In Table~\ref{tab:intermediate}, we compare the prediction performance of intermediate models generated by each loss of \SexyName-56-5
against that of the corresponding layer of ResNet-56. We observe that each intermediate model of \SexyName-56-5 outperforms its ResNet comparator and even often outperform their full-depth ResNet counterparts.
By comparing these results with Table \ref{tab:cifar}, we see that
the \SexyName-56-5 intermediate model obtained at \textbf{output-45} shows much better performance ($5.67\%$ error)
than ResNet-44 (6.37\%), ResNet-56 (6.08\%), ResNet-110 (5.86\%), as well as \SexyName-44-3 (5.85\%).
This demonstrates not only that \SexyName improves the discriminative power of intermediate layers, but also that it provides the opportunity for a form of model selection, and a means of avoiding over-fitting.

In Table \ref{tab:intermediate}, the third column records the number of model parameters. Fewer model parameters imply faster inference, which is important for real-world applications. For example, the \SexyName-56-5 model at \textbf{output-45} has 0.48M parameters, which is nearly half the number in ResNet-56 (0.85M), and 1/3 of that in ResNet-110 (1.7M).

\begin{table}[htbp]
  \centering
  \caption{Testing error of intermediate models of \SexyName-56-5 and ResNet-56 on CIFAR-10.}
    \begin{tabular}{c|c|c|c|c}
    \hline
    \multirow{2}[0]{*}{model} & \multirow{2}[0]{*}{\#layers} & \multirow{2}[0]{*}{\#params} & \multicolumn{2}{c}{error (\%)} \\
    \cline{4-5}
          &       &       & \SexyName & ResNet \\
    \hline
    model-15 & 15    & 0.03M & 50.35 & 63.01 \\
    model-25 & 25    & 0.09M & 18.94 & 45.07 \\
    model-35 & 35    & 0.18M & 9.23  & 34.01 \\
    model-45 & 45    & 0.48M & \textbf{5.67}  & 13.71 \\
    model-56 & 56    & 0.85M & \textbf{5.53}  & 6.56 \\
    \hline
    \end{tabular}
  \label{tab:intermediate}
\end{table}

\subsection{Demonstration of Safe Prediction}\label{exp:ensemble}

With safe prediction, we always select the output with the highest confidence as the final decision to correct potential wrong predictions.
Compared to the original prediction method in which the last output makes predictions for all the samples,
each output in safe prediction is responsible for making predictions for a subset of samples with high confidences.
Based on the advantage of the discriminative ability of each output on specific data subset, we achieve slightly better results on the benchmark data sets, as shown in Table~\ref{tab:ensemble}.
Moreover, we see that a large part of samples can be predicted at shallow outputs with high quality, e.g., 63\% (the first two outputs) on SVHN and 39\% (the first three outputs) on ImageNet.
This observation indicates that it is not necessary to go through the whole network to perform inference, especially for those samples that can be predicted with high confidences at shallow outputs.


\begin{table}[htbp]
  \centering
  \caption{Comparison of safe prediction and the original prediction method of \SexyName. Here the original method denotes the prediction using the last output.}
    \begin{tabular}{c|c|c|c|c}
    \hline
    \multirow{2}[0]{*}{model} & \multirow{2}[0]{*}{outputs} & \multicolumn{1}{c|}{\multirow{2}[0]{*}{method}} & \multicolumn{1}{c|}{\multirow{2}[0]{*}{prediction ratio (\%)}} & \multicolumn{1}{c}{\multirow{2}[0]{*}{error (\%)}} \\
          &       &       &       &  \\
    \hline
    \SexyName-SVB-28-2/10 & \multirow{2}[0]{*}{\{21, 28\}} & original & \{0, 100\}     & 3.34 \\
    on CIFAR-10      &       & safe & \{32, 68\}     & 3.31 \\
    \hline
    \SexyName-SVB-28-2/10 & \multirow{2}[0]{*}{\{21, 28\}} & original & \{0, 100, 0\}     & 17.87 \\
    on CIFAR-100      &       & safe & \{13, 87\}     & 17.81 \\
    \hline
    \SexyName-56-3 & \multirow{2}[0]{*}{\{35, 45, 56\}} & original & \{0, 0, 100\}     & 1.58 \\
    on SVHN      &       & safe & \{12, 51, 34\}     & 1.58 \\
    \hline
    \SexyName-Inception-4 & \multirow{2}[0]{*}{\{98, 110, 122, 136\}} & original & \{0, 0, 0, 100\}     & 18.69 \\
    on ImageNet      &       & safe & \{5, 13, 21, 61\}     & 18.66 \\
    \hline
    \end{tabular}%
  \label{tab:ensemble}%
\end{table}%

\subsection{Combatting Overfitting Issue}


For ResNet, when the depth goes deeper, the model has only limited performance gain (\eg ResNet-110 has almost two times the number of parameters than ResNet-56 but only yields 0.2\% accuracy gain, as shown in Figure~\ref{tab:cifar}).  In Figure~\ref{fig:cifar10train}, compared to the training of ResNet, \SexyName with Multi-path BP reaches slightly higher training error at convergence, but produces lower testing error. This phenomenon is observed on models with any number of layers, which indicates that \SexyName is less prone to overfitting. To some extents, the performance gain of \SexyName can be the side-effect of combatting overfitting issue.


\subsection{Effects on Numbers of Outputs}\label{exp:num_outputs}
In this experiment, we test the performance of \SexyName for various numbers of intermediate outputs on the CIFAR-10 data set.
The results are given in Table \ref{tab:num_outputs}. We see that
\SexyName-56-2 and \SexyName-56-5 perform significantly better than ResNet-56.
However, adding too many outputs does not necessarily improve the performance. For example,  \SexyName-56-10 with 10 outputs performs much worse than  \SexyName-56-2, \SexyName-56-5 and ResNet-56.

\begin{table}[h]
  \centering
  \caption{Testing error for varying number of outputs on CIFAR-10.}
    \begin{tabular}{c|c|c}
    \hline
    network & outputs position & error (\%) \\
    \hline
    ResNet-56~\cite{he2016identity}     & \{56\}  & 6.08 \\
    \SexyName-56-2      & \{45, 56 \} & 5.77 \\
    \SexyName-56-5      & \{15, 25, 35, 45, 56\} & \textbf{5.53} \\
    \hline
    \multirow{2}{*}{\SexyName-56-10}
    & \{13, 17, 21, 25, 29,&
    \multirow{2}{*}{7.96}\\
    & 33, 37, 41, 45, 56\} & \\

    \hline
    \end{tabular}
  \label{tab:num_outputs}%
\end{table}%


\subsection{Outputs at Very Shallow Layers}\label{exp:shallowoutput}
While auxiliary outputs can help a simpler model to achieve the same accuracy as a deeper ResNet model, adding auxiliary outputs to very shallow layers may severely hampers the prediction performance since the features of shallow layers may not be sufficiently discriminative. To demonstrate this, we investigate two \SexyName models by adding single output to very shallow layers, and record the results.
From Table \ref{tab:shallow_layers}, adding outputs at very shallow layer, e.g. 5 out of ResNet-20 and ResNet-44, severely influences the prediction performance. That means, in practice, it is necessary to avoid adding auxiliary outputs at very shallow layers.

\begin{table}[t]
  \centering
  \caption{Effects of auxiliary outputs at very shallow layers on CIFAR-10 test set.}
    \begin{tabular}{c|c|c}
    \hline
    network & outputs position & error (\%) \\
    \hline
    ResNet-20~\cite{he2016identity} & \{20\} & 7.76 \\
    \SexyName-20-2 & \{5, 20\} & 9.64 \\
    \SexyName-20-2 & \{17, 20\} & 7.58 \\
    \hline
    ResNet-44~[6] & \{44\} & 6.37 \\
    \SexyName-44-2 & \{5, 44\} & 7.73 \\
    \SexyName-44-2 & \{15, 44\} & 6.75 \\
    \SexyName-44-2 & \{35, 44\} & 6.12 \\
    \hline
    \end{tabular}
  \label{tab:shallow_layers}%
\end{table}%






\subsection{Effect of Weighting Scalar $\nu$}\label{exp:nu}
We conducted experiments to investigate how to weight outputs to achieve better performance. Under the setting of $\{\gamma_k\}$ described in Section 3.1, we compare different value of $\nu$ to adjust weights. From Table \ref{tab:weight}, the equally weighted outputs ($\nu=0$) severely hampers the performance. If $\nu \geq 1$, the weights of shallower outputs become smaller due to their decreasing discriminative powers. Especially for $\nu=1$ or $\nu=2$, the $n$-layer \SexyName behaves better than the original $n$-layer ResNet. For a larger $\nu=5$, the weights decay so aggressively that the effects of auxiliary outputs are negligible.

\begin{table}[h]
  \centering
  \caption{Effect of weighting scalar $\nu$ on CIFAR-10 test set.}
    \begin{tabular}{c|c|c}
    \hline
    network & $\nu$ & error (\%) \\
    \hline
    ResNet-56~\cite{he2016identity} & - & 6.08 \\
    \hline
    \multirow{4}{*}{\SexyName-56-5} & 0 & 8.43 \\
    & 1 & 5.90 \\
    & 2 & 5.53 \\
    & 5 & 6.03 \\
    \hline
    \end{tabular}%
  \label{tab:weight}%
\end{table}

\subsection{Effects of the Parameter $\lambda$}\label{exp:lambda}
The choice of the parameter $\lambda$ in the shortcut mapping
is critical to the performance of residual networks. When $\lambda = 0$, ResNet is reduced to the plain network. We study the influence of $\lambda$ on shortcuts within a residual unit of ResNet and \SexyName. The values of $\lambda$ considered are \{0.5, 0.95, 1\}.  From Table \ref{tab:lambda}, the \SexyName consistently outperforms ResNet with different $\lambda$ settings. We also observe that the performance of \SexyName is robust across different values of $\lambda$, and the default parameter setting $\lambda=1.0$ gives good results in our experiments, whilst ResNet is more sensitive.

\begin{table}[h]
  \centering
  \caption{Classification error on CIFAR-10 test set based on ResNet-56, with different $\lambda$ applied to $\bx_l$.}
    \begin{tabular}{c|c|c|c}
    \hline
    $\lambda$ & network & \# outputs & error (\%) \\
    \hline
    \multirow{2}[3]{*}{0.5} & ResNet-56~\cite{he2016identity} & 1   & 9.25 \\
          & \SexyName-56-5 & 5  & 6.87 \\
    \hline
    \multirow{2}[3]{*}{0.95} & ResNet-56~\cite{he2016identity} & 1   & 6.32 \\
          & \SexyName-56-5 & 5   & 6.14 \\
    \hline
    \multirow{2}[3]{*}{1} & ResNet-56~\cite{he2016identity} & 1    & 6.08 \\
          & \SexyName-56-5 & 5  & 5.53 \\
    \hline
    \end{tabular}%
  \label{tab:lambda}%
\end{table}


\section{Conclusion}
This paper investigates the backpropagation method for deep networks with auxiliary outputs. When the network goes to very deep, shallow layers tend to receive insufficient supervision information due to the long backpropagation path away from the output layer, thus resulting in great model redundancy. To address it, we developed a model called \SexyName and propose the Multi-path backpropagation to train the network with multiple losses. In particular, we conducts one forward propagation for all losses but multiple backpropagations in each iteration. The proposed method reduces the internal redundancy and yields significant performance gains in several benchmark data sets. Moreover, because of the fact that it optimizes multiple intermediate models, \SexyName also offers the opportunity for a form of model selection.

\clearpage
\small
\bibliographystyle{ieee}

\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{ba2014deep}
J.~Ba and R.~Caruana.
\newblock Do deep nets really need to be deep?
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2654--2662, 2014.

\bibitem{glorot2010understanding}
X.~Glorot and Y.~Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, volume~9, pages 249--256, 2010.

\bibitem{graham2014fractional}
B.~Graham.
\newblock Fractional max-pooling.
\newblock{\em arXiv preprint arXiv:1412.6071}, 2014.

\bibitem{he2015delving}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, pages 1026--1034, 2015.

\bibitem{he2015deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock{\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2016.

\bibitem{he2016identity}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Identity mappings in deep residual networks.
\newblock{\em European Conference on Computer Vision}, 2016.

\bibitem{huang2016densely}
G.~Huang, Z.~Liu, K.~Q. Weinberger, and L.~van~der Maaten.
\newblock Densely connected convolutional networks.
\newblock{\em arXiv preprint arXiv:1608.06993}, 2016.

\bibitem{DBLP:journals/corr/HuangSLSW16}
G.~Huang, Y.~Sun, Z.~Liu, D.~Sedra, and K.~Q. Weinberger.
\newblock Deep networks with stochastic depth.
\newblock{\em CoRR}, abs/1603.09382, 2016.

\bibitem{jia2016improving}
K.~Jia.
\newblock Improving training of deep neural networks via singular value
  bounding.
\newblock{\em arXiv preprint arXiv:1611.06013}, 2016.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky and G.~Hinton.
\newblock Learning multiple layers of features from tiny images, 2009.

\bibitem{krizhevsky2012imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1097--1105, 2012.

\bibitem{lecun2015deep}
Y.~LeCun, Y.~Bengio, and G.~Hinton.
\newblock Deep learning.
\newblock{\em Nature}, 521(7553):436--444, 2015.

\bibitem{lecun1989backpropagation}
Y.~LeCun, B.~Boser, J.~S. Denker, D.~Henderson, R.~E. Howard, W.~Hubbard, and
  L.~D. Jackel.
\newblock Backpropagation applied to handwritten zip code recognition.
\newblock{\em Neural Computation}, 1(4):541--551, 1989.

\bibitem{Lee2015}
C.-Y. Lee, S.~Xie, P.~Gallagher, Z.~Zhang, and Z.~Tu.
\newblock Deeply-supervised nets.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, 2015.

\bibitem{long2015fully}
J.~Long, E.~Shelhamer, and T.~Darrell.
\newblock Fully convolutional networks for semantic segmentation.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 3431--3440, 2015.

\bibitem{netzer2011reading}
Y.~Netzer, T.~Wang, A.~Coates, A.~Bissacco, B.~Wu, and A.~Y. Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock 2011.

\bibitem{ren2015faster}
S.~Ren, K.~He, R.~Girshick, and J.~Sun.
\newblock Faster r-cnn: Towards real-time object detection with region proposal
  networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  91--99, 2015.

\bibitem{romero2014fitnets}
A.~Romero, N.~Ballas, S.~E. Kahou, A.~Chassang, C.~Gatta, and Y.~Bengio.
\newblock Fitnets: Hints for thin deep nets.
\newblock{\em arXiv preprint arXiv:1412.6550}, 2014.

\bibitem{russakovsky2015imagenet}
O.~Russakovsky, J.~Deng, H.~Su, J.~Krause, S.~Satheesh, S.~Ma, Z.~Huang,
  A.~Karpathy, A.~Khosla, M.~Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock{\em International Journal of Computer Vision}, 115(3):211--252,
  2015.

\bibitem{schroff2015facenet}
F.~Schroff, D.~Kalenichenko, and J.~Philbin.
\newblock Facenet: A unified embedding for face recognition and clustering.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 815--823, 2015.

\bibitem{simonyan2014very}
K.~Simonyan and A.~Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock{\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{srivastava2015highway}
R.~K. Srivastava, K.~Greff, and J.~Schmidhuber.
\newblock Highway networks.
\newblock{\em International Conference on Machine Learning}, 2015.

\bibitem{srivastava2015training}
R.~K. Srivastava, K.~Greff, and J.~Schmidhuber.
\newblock Training very deep networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2377--2385, 2015.

\bibitem{sun2015deepid3}
Y.~Sun, D.~Liang, X.~Wang, and X.~Tang.
\newblock Deep{ID3}: Face recognition with very deep neural networks.
\newblock{\em arXiv preprint arXiv:1502.00873}, 2015.

\bibitem{sun2015deeply}
Y.~Sun, X.~Wang, and X.~Tang.
\newblock Deeply learned face representations are sparse, selective, and
  robust.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 2892--2900, 2015.

\bibitem{DBLP:journals/corr/SzegedyIV16}
C.~Szegedy, S.~Ioffe, and V.~Vanhoucke.
\newblock Inception-v4, inception-resnet and the impact of residual connections
  on learning.
\newblock{\em CoRR}, abs/1602.07261, 2016.

\bibitem{szegedy2015going}
C.~Szegedy, W.~Liu, Y.~Jia, P.~Sermanet, S.~Reed, D.~Anguelov, D.~Erhan,
  V.~Vanhoucke, and A.~Rabinovich.
\newblock Going deeper with convolutions.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 1--9, 2015.

\bibitem{teerapittayanon2016branchynet}
S.~Teerapittayanon, B.~McDanel, and H.~Kung.
\newblock Branchynet: Fast inference via early exiting from deep neural
  networks.
\newblock ICPR, 2016.

\bibitem{wang2015training}
L.~Wang, C.-Y. Lee, Z.~Tu, and S.~Lazebnik.
\newblock Training deeper convolutional networks with deep supervision.
\newblock{\em arXiv preprint arXiv:1505.02496}, 2015.

\bibitem{zagoruyko2016wide}
S.~Zagoruyko and N.~Komodakis.
\newblock Wide residual networks.
\newblock{\em arXiv preprint arXiv:1605.07146}, 2016.

\bibitem{Zagoruyko2016}
S.~Zagoruyko, A.~Lerer, T.-Y. Lin, P.~O. Pinheiro, S.~Gross, S.~Chintala, and
  P.~Doll\'{a}r.
\newblock A multipath network for object detection.
\newblock Technical report, Facebook AI Research, 2016.

\bibitem{zeiler2014visualizing}
M.~D. Zeiler and R.~Fergus.
\newblock Visualizing and understanding convolutional networks.
\newblock In {\em European Conference on Computer Vision}, pages 818--833.
  Springer, 2014.

\bibitem{zhang2016residual}
T.~X. H. Y. X. G. L. L.~T. Zhang~Ke, Sun~Miao.
\newblock Residual networks of residual networks: Multilevel residual networks.
\newblock{\em arXiv preprint arXiv:1608.02908}, 2016.

\end{thebibliography}



\end{document}


