\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{authblk}


\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{1192} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

\title{Deep Depth Inference using Binocular and Monocular Cues}

\renewcommand\Authands{ and }

\author[1]{Xinqing Guo}
\author[2]{Zhang Chen}
\author[2]{Siyuan Li}
\author[1]{Yang Yang}
\author[2]{Jingyi Yu}
\affil[1]{University of Delaware, Newark, DE, USA. \texttt{\{xinqing, yyangwin\}@udel.edu}}
\affil[2]{ShanghaiTech University, Shanghai, China. \texttt{\{chenzhang, lisy1, yujingyi\}@shanghaitech.edu.cn}}

\maketitle


\begin{abstract}
Human visual system relies on both binocular stereo cues and monocular
focusness cues to gain effective 3D perception. In computer vision,
the two problems are traditionally solved in separate tracks. In this
paper, we present a unified learning-based technique that simultaneously uses
both types of cues for depth inference. Specifically, we use a pair of
focal stacks as input to emulate human perception. We first construct
a comprehensive focal stack training dataset synthesized by
depth-guided light field rendering. We then construct three individual
networks: a FocusNet to extract depth from a single focal
stack, a EDoFNet to obtain the extended depth of field (EDoF)
image from the focal stack,  and a StereoNet to conduct stereo
matching. We then integrate them into a unified solution to
obtain high quality depth maps. Comprehensive experiments show that
our approach outperforms the state-of-the-art in both accuracy and
speed and effectively emulates human vision systems.


\end{abstract}

\section{Introduction}
Human visual system relies on a variety of depth cues to gain 3D perception. The most important ones are binocular, defocus, and motion cues. Binocular cues such as stereopsis, eye convergence, and disparity yield depth from binocular vision through exploitation of parallax. Defocus cue allows depth perception even with a single eye by correlating variation of defocus blurs with the motion of the ciliary muscles surrounding the lens.  Motion parallax also provides useful input to assess depth, but arrives over time and depends on texture gradients.

Computer vision algorithms such as stereo matching \cite{scharstein02, brown03} and depth-from-focus/defocus \cite{nayar92, nayar94, malik07, favaro05, favaro07} seek to directly employ binocular and defocus cues which are available without scene statistics. Recent studies have shown that the two types of cues complement each other to provide 3D perception \cite{held12}. In this paper, we seek to develop learning based approaches to emulate this process.

To exploit binocular cues, traditional stereo matching algorithms rely on feature matching and optimization to maintain the Markov Random Field property: the disparity field should be smooth everywhere with abrupt changes at the occlusion boundaries. Existing solutions such as graph-cut, belief propagation~\cite{kolmogorov02, sun03}, although effective, tend to be slow. In contrast, depth-from-focus (DfF) exploits differentiations of sharpness at each pixel across a focal stack and assigns the layer with highest sharpness as its depth. Compared with stereo, DfF generally presents a low fidelity estimation due to depth layer discretization. Earlier DfF techniques use a focal sweep camera to produce a coarse focal stack due to mechanical limitations whereas more recent ones attempt to use a light field to synthetically produce a denser focal stack.

Our solution benefits from recent advance on computational photography and we present an efficient and reliable learning based technique to conduct depth inference from a focal stack pair, emulating the process of how human eyes work. We call our technique binocular DfF or B-DfF. Our approach leverages deep learning techniques that can effectively extract features learned from large amount of imagery data. Such a deep representation has shown great promise in stereo matching \cite{zbontar15, zagoruyko15, luo16}. Little work, however, has been proposed on using deep learning for DfF or more importantly, integrating stereo and DfF. This is mainly due to the lack of fully annotated DfF datasets.

\begin{figure}[t]
\begin{center}
   \includegraphics[width=0.99\linewidth]{DFF_teaser.pdf}
\end{center}
\vspace{-8pt}
   \caption{\emph{BDfFNet} integrates \emph{FocusNet}, \emph{EDoFNet} and \emph{StereoNet} to predict high quality depth map from binocular focal stacks.}
\label{fig:teaser}
\end{figure}
\begin{figure*}[t]
\begin{center}
   \includegraphics[width=0.9\linewidth]{dataset_sample.pdf}
\end{center}
\vspace{-8pt}
   \caption{A binocular focal stack pair consists of two horizontally rectified focal stacks. The upper and lower triangles show corresponding slices focusing at respective depths. Bottom shows the ground truth color and depth images. We add Poisson noise to training data, a critical step for handling real scenes.}
\label{fig:dataset_sample}
\end{figure*}

We first construct a comprehensive focal stack dataset. Our dataset is based on the highly diversified dataset from \cite{mayer16}, which contains both stereo color images and ground truth disparity maps. Then we adopt the algorithm from \emph{Virtual DSLR} \cite{yang16} to generate the refocused images. \cite{yang16} uses color and depth image pair as input for light field synthesis and rendering, but without the need to actually create the light field. The quality of the rendered focal stacks are comparable to those captured by expensive DSLR camera. Next, we propose three individual networks: (1) \emph{FocusNet}, a multi-scale network to extract depth from a single focal stack (2) \emph{EDoFNet}, a deep network consisting of small convolution kernels to obtain the extended depth of field (EDoF) image from the focal stack and (3) \emph{StereoNet} to obtain depth directly from a stereo pair. The EDoF image from \emph{EDoFNet} serves to both guide the refinement of the depth from \emph{FocusNet} and provide inputs for \emph{StereoNet}. We also show how to integrate them into a unified solution \emph{BDfFNet} to obtain high quality depth maps. Fig. \ref{fig:teaser} illustrates the pipeline.


We evaluate our approach on both synthetic and real data. To physically implement B-DfF, we construct a light field stereo pair by using two Lytro Illum cameras. Light field rendering is then applied to produce the two focal stacks as input to our framework. Comprehensive experiments show that our technique outperforms the state-of-the-art techniques in both accuracy and speed. More importantly, we believe our solution provides important insights on developing future sensors and companion 3D reconstruction solutions analogous to human eyes.


\section{Related Work}
Our work is closely related to depth from focus/defocus and stereo. The strength and weakness of the two approaches have been extensively discussed in \cite{schechner00, vaish06}.

\noindent\textbf{Depth from Focus/Defocus}
Blur carries information about the object's distance. Depth from Focus/Defocus (DfF/DfD) recovers scene depth from a collection of images captured under varying focus settings. In general, DfF \cite{nayar92, nayar94, malik07} determines the depth by analyzing the most in-focus slice in the focal stack, while DfD \cite{favaro05, favaro07} infers depth based on the amount of the spatially varying blur at each pixel. To avoid ambiguity in textureless region, Moreno-Noguer \emph{et al.} \cite{Moreno-Noguer07} used active illumination to project a sparse set of dots onto the scene. The defocus of the dots offers depth cue, which could be further used for realistic refocusing. \cite{hasinoff09} combined focal stack with varying aperture to recover scene geometry. Moeller \emph{et al.} \cite{moeller15} applied an efficient nonconvex minimization technique to solve DfD in a variational framework. Suwajanakorn \emph{et al.} \cite{suwajanakorn15} proposed the DfF with mobile phone under uncalibrated setting. They first aligned the focal stack, then jointly optimized the camera parameters and depth map, and further refined the depth map using anisotropic regularization.

A drastic difference of these methods to our approach is that they rely on hand-crafted features to estimate focusness or blur kernel, whereas in this paper we leverage neural network to learn more discriminative features from focal stack and directly predict depth at lower computational cost.

\noindent \textbf{Learning based Stereo}
Depth from stereo has been studied extensively by the computer vision community for decades. We refer the readers to the comprehensive survey for more details \cite{scharstein02, brown03}. Here we only discuss recent methods based on Convolutional Neural Network (CNN).

Deep learning benefits stereo matching at various stages. A number of approaches exploit CNN to improve the matching cost. The seminal work by {\v Z}bontar and LeCun~\cite{zbontar15} computed a similarity score from patches using CNN, then applied the traditional cost aggregation and optimization to solve the energy function. Han \emph{et al.}\cite{han2015matchnet} jointly learned feature representations and feature comparison functions in a unified network, which improved on previous results with less storage requirement. Luo \emph{et al.} \cite{luo16} speeded up the matching process by using a product layer, and treated the disparity estimation as a multi-class classification problem. ~\cite{chen2015deep, zagoruyko15, liu2016euclidean, park2016look} conducted similar work but with different network architecture. Alternatively, CNN can also help predict the confidence of disparity map to remove outliers. Seki and Pollefeys \cite{seki2016patch} leveraged CNN for stereo confidence measure, and incorporated predicted confidence into Semi-Global Matching by adjusting its parameters. In order to automatically generate the dataset for learning based confidence measure, Mostegel \emph{et al.} \cite{mostegel16} checked the consistency of multiple depth maps of the same scene obtained with the same stereo approach, and collected labeled confidence map as the training data.

End-to-end network architectures have also been explored. Mayer \emph{et al.}~\cite{mayer16} adopted and extended the architecture of the FlowNet~\cite{dosovitskiy2015flownet}, which consists of a contractive part and an expanding part to learn depth at multiple scales. They also created three synthetic datasets to facilitate the training process. Kn{\"o}belreiter \emph{et al.}~\cite{knobelreiter2016end} learned unary and pairwise cost of stereo using CNNs, then posed the optimization as a conditional random field (CRF) problem. The hybrid CNN-CRF model was trained in image's full resolution in an end-to-end fashion.

Combining DfF/DfD and stereo matching has also been studied, although not within the learning framework. Early work~\cite{klarquist95, subbarao97} attempted to utilize the depth map from the focus/defocus to reduce the search space for stereo and solve the correspondence problem more efficiently. \cite{Rajagopalan04} simultaneously recovered depth and restored the original focused image from a defocused stereo pair. Recently, Tao \emph{et al.}~\cite{tao13} analyzed the epipolar image (EPI) from light field camera to infer depth. They found that the horizontal variances after vertical integration of the EPI encodes defocus cue, while vertical variances encodes disparity cue. The two cues were then jointly optimized in an MRF framework. To obtain high resolution depth in a semi-calibrated manner, Wang \emph{et al.}~\cite{wang16} proposed a hybrid camera system that consists of two calibrated auxiliary cameras and an uncalibrated main camera. They first transfered the depth from auxiliary cameras to the viewpoint of the main camera by rectifying three images simultaneously, and further improved the depth map along occlusion boundaries using defocus cue.

Aforementioned approaches leave the combination and optimization of focus and disparity cue to postprocessing. In contrast, we resort to extra layers of network to infer the optimized depth with low computational cost and efficiency.




\section{Dual Focal Stack Dataset}
With fast advances of the data driven methods, numerous datasets have been created for various applications. However, by far, there are limited resources on focal stacks. To this end, we generate our dual focal stack dataset based on FlyingThings3D from \cite{mayer16}. FlyingThings3D is an entirely synthetic dataset, consisting of everyday objects flying along randomized 3D paths. Their 3D models and textures are separated into disjointed training and testing parts. In total, the dataset contains about 25,000 stereo images with ground truth disparity. To make the data tractable, we select stereo frames whose largest disparity is less than 100 pixels, then we normalize the disparity to $0\sim1$.

Takeda \emph{et al.} \cite{takeda2013fusing} demonstrate that in stereo setup, the disparity $d$ and the diameter of the circle of confusion $c$ have a linear relationship:
\begin{equation}
\frac{d}{c} = \frac{l}{D}
\end{equation}
where $l$ is the baseline length and $D$ is the aperture size. Based on above observation, we adopt the \emph{Virtual DSLR} approach from \cite{yang16} to generate synthetic focal stacks. \emph{Virtual DSLR} requires color and disparity image pair as inputs, and outputs refocused images with quality comparable to those captured from regular, expensive DSLR. The advantage of their algorithm is that it resembles light field synthesis and refocusing but does not require actual creation of the light field, hence reducing both memory and computational load. In addition, their method takes special care of occlusion boundaries to avoid color bleeding and discontinuity commonly observed in brute-force blur-based defocus synthesis.
To better explain their approach, we list the formulation as below:

\begin{equation}
C_p = \frac{|s-s_p|}{s_p}D = sD|\frac{1}{z_p} - \frac{1}{z_s}|,
\end{equation}

To simulate a scene point $p$ with depth $z_p$ projected to a circular region on sensor, we assume the focal length $f$, an aperture size $D$, sensor to lens distance $s$,and the circular region diameter $C_p$. Here $z_s = (1/f - 1/s)^{-1}$ and $s_p = (1/f - 1/z_p)^{-1}$ according to the thin lens law. The diameter of the circular region $C_p$ measures the size of blur kernel and it is linear to the absolute difference of the inverse of the distances $z_p$ and $z_s$. For the scope of this paper, we use only circular apertures, although more complex ones can easily be synthesized. To emulate the pupil of the eye in varying lighting conditions, we randomly select the size of the blur kernel for each stereo pair, but limit the largest diameter of the blur kernel to 31 pixels. We also evenly separate the scene into 16 depth layers and render a refocused image for each layer. After generating the focal stacks, we add poisson noise to the images to simulate the real image captured by a camera. This turns out to be critical in real scene experiments, as described in section \ref{section:result_FocalStack}. Finally, we split the generated dual focal stacks into 750 training data and 70 testing data. Figure \ref{fig:dataset_sample} shows two slices from the dual focal and their corresponding color and depth image.



\section{B-DfF Network Architecture}

\label{section:NetworkArchitectures}

Convolutional neural networks are very efficient at learning non-linear mapping between the input and the output. Therefore, we aim to take an end-to-end approach to predict a depth map. \cite{simonyan14} shows that a deep network with small kernels is very effective in image recognition tasks. Although a small kernel has limited spatial support, a deep network by stacking multiple layers of such kernels could substantially enlarge the receptive field while reducing the number of parameters to avoid overfitting. Therefore, a general principle in designing our network is to use deep architecture with small convolutional kernels.

As already mentioned, the input to our neural network are two rectified focal stacks. To extract depth from defocus and disparity, our solution is composed of three individual networks. We start in section \ref{section:multiscale} by describing the \emph{FocusNet}, a multi-scale network that estimates depth from a single focal stack. Then in section \ref{section:allinfocus} we further enhance the result by the extended depth of field images from \emph{EDoFNet}. Finally we combine \emph{StereoNet} and \emph{FocusNet} in \ref{section:stereo} to infer high quality depth from binocular focal stacks.


\subsection{FocusNet for DfF/DfD}
\label{section:multiscale}

\begin{figure}[t]
\begin{center}
   \includegraphics[width=0.99\linewidth]{network_multiscaleNets.pdf}
\end{center}
\vspace{-8pt}
   \caption{\emph{FocusNet} is a multi-scale network for conducting depth-from-focus.}
\label{fig:network_dfdMultiScale}
\end{figure}

\begin{figure}[t]
\begin{center}
   \includegraphics[width=1\linewidth]{n3.pdf}
\end{center}
\vspace{-8pt}
   \caption{Left: \emph{EDoFNet} consists of 20 layers of convolutional layers to form an extended depth-of-field (EDoF) image from focal stack. Right: \emph{FocusNet-v2} combines \emph{FocusNet} and \emph{EDoFNet} by using the EDoF image to refine the depth estimation.}
\label{fig:network_allinfocus}
\end{figure}


\begin{figure*}[t]
\begin{center}
   \includegraphics[width=0.85\linewidth]{hourglass_network_small.pdf}
\end{center}
\vspace{-8pt}
   \caption{(a) \emph{StereoNet} follows the Hourglass network architecture which consists of the max pooling layer (yellow), the deconvolution layer (green) and the residual module (blue). (b) shows the detailed residual module.}
\label{fig:network_hourglass}
\end{figure*}


Motivated by successes from multi-scale networks, we propose \emph{FocusNet}, a multiscale network to extract depth from a single focal stack. Specifically, \emph{FocusNet} consists of four branches of various scales. Except the first branch, other branches subsample the image by using different strides in the convolutional layer, enabling aggregation of information over large areas. Therefore, both the high-level information from the coarse feature maps and the fine details could be preserved. At the end of the branch, a deconvolutional layer is introduced to upsample the image to its original resolution. Compared with the traditional bicubic upsampling, deconvolution layer automatically learns upsampling kernels that are better suited for the application. Finally, we stack the multi-scale features maps together, resulting in a concatenated per-pixel feature vector. The feature vectors are further fused by layers of convolutional networks to predict the final depth value.

An illustration of the network architecture is shown in Fig. \ref{fig:network_dfdMultiScale}. We use $3\times3$ kernels for most layers except those convolutional layers used for downsampling and upsampling, where a larger kernel is used to cover more pixels. The spatial padding is also applied for each convolution layer to preserve the resolution. Following \cite{simonyan14}, the number of feature maps increases as the image resolution decreases. Between the convolutional layers we insert PReLU layer \cite{he15} to increase the network's nonlinearity. For the input of the network we simply stack the focal stack images together along the channel's dimension.


\subsection{Guided Depth Refinement by EDoF Image}
\label{section:allinfocus}

There exist many approaches \cite{ferstl13, hui16} to refine/upsample depth image with the guidance of an intensity image. The observation is that homogeneous texture regions often correspond to homogeneous surface parts, while depth edges often occur at high intensity variations. With this in mind, we set out to first extract the EDoF image from the focal stack, then guide the refinement of the depth image. Several methods \cite{kuthirummal11, suwajanakorn15} have been proposed to extract the EDoF image from the focal stack. However, the post processing is suboptimal
in terms of computational efficiency and elegance. Thus, we seek to directly output an EDoF image from a separate network, which we termed \emph{EDoFNet}.

\emph{EDoFNet} is composed of 20 convolutional layers, with PRelu as its activation function. The input of the \emph{EDoFNet} is the focal stack, the same as the input of \emph{FocusNet}, and the output is the EDoF image. With the kernel size of $3\times3$, a 20 layer convolutional network will produce a receptive field of $41\times41$, which is larger than the size of the largest blur kernel. Fig. \ref{fig:network_allinfocus} shows the architecture of \emph{EDoFNet}.

Finally, we concatenate the depth image from \emph{FocusNet} and the EDoF image from the \emph{EDoFNet}, and fuse them by using another 10 layer convolutional network. We call the new network \emph{FocusNet-v2}. The architecture of \emph{FocusNet-v2} is illustrated in Fig. \ref{fig:network_allinfocus}.


\subsection{StereoNet and BDfFNet for Depth from Binocular Focal Stack}
\label{section:stereo}
Given the EDoF stereo pair from the \emph{EDoFNet}, we set out to estimate depth from stereo using another network, termed \emph{StereoNet}. For stereo matching, it is critical to consolidate both local and global cues to generate precise pixel-wise disparity.
To this end, we propose \emph{StereoNet} by adopting the Hourglass network architecture~\cite{newell16}, as shown in Fig. \ref{fig:network_hourglass}. The advantage of this network is that it can attentively evaluate the coherence of features across scales by utilizing large amount of residual modules~\cite{he2016deep}. The network composes of downsampling part and upsampling part. The downsampling part consists of a series of max pooling interleaved with residual modules while the upsampling part is a mirrored architecture of the downsampling part, with max pooling replaced by deconvolution layer for upsampling. Between any pair of corresponding max pooling and upsampling, there is a connection layer comprising of a residual module. Elementwise addition follows to add processed lower-level features to higher-level features. In this way, the network learns a more holistic representation of input images. Prediction is generated at the end of the upsampling part. One round of downsampling and upsampling part can be viewed as one iteration of predicting, whereas additional rounds can be stacked to refine initial estimates. For \emph{StereoNet}, we use two rounds of downsampling and upsampling parts as they already give good performance, while further rounds improve marginally at the cost of more training time. Note that the weights are not shared in the two rounds.

Different from~\cite{newell16}, we do not downsample input images before the first downsampling part. This stems from the difference in problem settings: our solution aims for pixel-wise precision while~\cite{newell16} only requires structured understanding of images. Throughout the network, we use small convolution filters ($3\times3$ or $1\times1$). After each pair of downsampling and upsampling parts, supervision is applied using the same ground truth disparity map. The final output is of the same resolution as the input images.

Finally, we construct \emph{BDfFNet} by concatenating the results from \emph{StereoNet}, \emph{FocusNet-v2} and \emph{EDoFNet}, and adding more convolutional layers. The convolutional layers serve to find the optimal combination from focus cue and disparity cue.

\section{Implementation}
\label{section:implementation}

\noindent\textbf{Optimization}
Given the focal stack as input and ground truth color/depth image as label, we train all the networks end-to-end. In our implementation, we first train each network individually, then fine-tune the concatenated network with the pre-trained weights as initialization. Because \emph{FocusNet} and \emph{FocusNet-v2} contains multiple convolutional layers for downsampling,
the input image needs to be cropped to the nearest number that is multiple of 8 for both height and width. We use the mean square error (MSE) with $l_2$-norm regularization as the loss for all models, which leads to the following objective function

\begin{equation}
\label{eqn: lossFunction}
    \min_{\theta}\frac{1}{N}\sum_{i=1}^N\left\| F(S^{i};\theta)- D^{i} \right\rVert_{2}^{2} + \frac{\lambda}{2}\left\| \theta \right\rVert_{2}^{2}
\end{equation}

where $S^i$ and $D^i$ are the $i$-th focal stack and depth image, $F(S^{i};\theta)$ is the function represented by the network and $\theta$ are the learned weights. Although there are works \cite{zhao17} suggesting the mean absolute error (MAE) might be a better loss function, our experiment shows that results from MAE are inferior to MSE.

Following \cite{ioffe15}, we apply batch normalization after the convolution layer and before PRelu layer. We initialize the weights using the technique from \cite{he15}. We employ MXNET \cite{chen15} as the learning framework and train and test the networks on a NVIDIA K80 graphic card. We make use of the Adam optimizer \cite{kingma15} and set the weight decay = 0.002, $\beta1$ = 0.9, $\beta2$ = 0.999. The initial learning rate is set to be 0.001. We first train each sub-network of \emph{BDfFNet} separately and then combine them for further training. All the networks are trained for 80 epoches.

\noindent\textbf{Data augmentation and preprocessing}
For \emph{FocusNet} and \emph{EDoFNet}, the size of the analyzed patches determines the largest sensible blur kernel size. Therefore, we randomly crop a patch of size $64\times64$ from the image, which contains enough contextual information to extract the depth and EDoF image. For \emph{StereoNet}, a larger patch of size $256\times256$ is used to accommodate the large disparity between stereo images. To facilitate the generalization of the network, we augment the data by flipping the patches horizontally and vertically. All the data augmentations are performed on the fly at almost no extra cost. Finally, the range of all images are normalized to $0\sim1$.

\begin{figure}[t]
\begin{center}
   \includegraphics[width=1.0\linewidth]{DFF_result_EDoF.pdf}
\end{center}
\vspace{-8pt}
   \caption{Results of our \emph{EDoFNet}. First row shows two slices of the focal stack focusing at different depth. Second and third row show the EDoF and ground truth image respectively.}
\label{fig:result_EDoF}
\end{figure}
\section{Experiments}
\label{section:experiments}

\begin{figure*}[h]
\begin{center}
   \includegraphics[width=0.9\linewidth]{result_Focus_v3.pdf}
\end{center}
\vspace{-8pt}
   \caption{Comparisons on \emph{FocusNet} vs. \emph{FocusNet-v2}, i.e., without and with the guide of an all-focus image.}
\label{fig:result_DfF}
\end{figure*}

\subsection{Extract the EDoF Image from Focal Stack}
We train \emph{EDoFNet} on a single focal stack of 16 slices. Although the network has simple structure, the output EDoF image features high image quality. Our network also runs much faster than conventional methods based on global optimization: on the resolution of $960\times540$ it runs at 4 frames per second. Fig. \ref{fig:result_EDoF} shows the result of \emph{EDoFNet}. Compared with ground truth image, the produced EDoF image is slightly blurry. However, given a very noisy focal stack as input, the resultant EDoF image gets rid of large part of the noise. Our experiments also show that it suffices to guide the refinement of depth image and be used as the input of \emph{StereoNet}.



\subsection{Depth Estimation from Focal Stack}
\label{section:result_FocalStack}

As mentioned in \ref{section:allinfocus}, to construct \emph{FocusNet-v2}, we first train \emph{FocusNet} and \emph{EDoFNet} respectively, then concatenate their output with more fusion layers and train the combination. Fig. \ref{fig:result_DfF} shows the result of both \emph{FocusNet}
and \emph{FocusNet-v2}. We observe that \emph{FocusNet} produces results with splotchy artifact, and depth bleeds across object's boundary. However,
\emph{FocusNet-v2} utilizes the EDoF color image to assist depth refinement, alleviating the artifacts and leading to clearer
depth boundary. It is worth noting that we also trained a network that
has identical structure to \emph{FocusNet-v2} from scratch, but the result is of inferior quality. We suspect this is due to the good initialization provided by the pre-trained model.

We compare our results with \cite{suwajanakorn15} and \cite{moeller15} using the data provided by the authors of \cite{suwajanakorn15}. We select 16 images from their focal stack for DfF. Fig. \ref{fig:result_compareVariation} illustrates the results. Our \emph{FocusNet-v2} is capable of predicting disparity value with higher quality, while using significantly less time (0.9 second) than \cite{suwajanakorn15} (10 mins) and \cite{moeller15} (4 seconds).

We also train the \emph{FocusNet-v2} on a clean dataset without poisson noise. It performs better on synthetic data, but exhibits severe noise pattern on real images, as shown in Fig. \ref{fig:result_datasetWoNoise}. The experiment confirms the necessity to add noise to the dataset for simulating real images.

\begin{figure}[t]
\begin{center}
   \includegraphics[width=0.99\linewidth]{result_compareVariational_v2.pdf}
\end{center}
\vspace{-8pt}
   \caption{Comparisons on depth estimation from a single focal stack using our \emph{FocusNet-v2} (last column) vs. \cite{suwajanakorn15} (second column) and \cite{moeller15} (third column). \emph{FocusNet-v2} is able to maintain smoothness on flat regions while preserving sharp occlusion boundaries.}
\label{fig:result_compareVariation}
\end{figure}

\begin{figure}[t]
\begin{center}
   \includegraphics[width=0.5\linewidth]{compareDatasetWoNoise_v2.pdf}
\end{center}
\vspace{-8pt}
   \caption{Results from \emph{FocusNet-v2} trained by the clean dataset without poisson noise.}
\label{fig:result_datasetWoNoise}
\end{figure}

\subsection{Depth Estimation from Stereo and Binocular Focal Stack}
\label{section:stereoAndBDfF}

Figure \ref{fig:result_BDfF} shows the results from \emph{StereoNet} and \emph{BDfFNet}. Compared with \emph{FocusNet-v2}, \emph{StereoNet} gives better depth estimation. This is expected since \emph{StereoNet} requires binocular focal stacks as input, while \emph{FocusNet-v2} only use a single focal stack. However, \emph{StereoNet} exhibits blocky artifacts and overly smoothed boundary. In contrast, depth prediction from \emph{BDfFNet} features sharp edges. The depth in flat surface region is also smoother compared to \emph{FocusNet-v2}.

Table \ref{tab:result} describes the mean absolute error (MAE) and running time of all models on $960\times540$ image.

\begin{figure*}[t]
\begin{center}
   \includegraphics[width=0.9\linewidth]{DFF_result_Stereo_BDfF_v2.pdf}
\end{center}
\vspace{-8pt}
   \caption{Comparisons on results only using \emph{StereoNet} vs. the composed \emph{BDfFNet}. BDfFNet produces much sharper boundaries while reducing blocky artifacts.}
\label{fig:result_BDfF}
\end{figure*}

\begin{table}[b]
\begin{center}
 \begin{tabular}{|c|c |c |c |c|}

    \hline
    \& \emph{FocusNet}   & \emph{FocusNet-v2} & \emph{StereoNet} & \emph{BDfFNet} \\ [0.5ex]
    \hline
    MAE & 0.045  & 0.031 & 0.024 & 0.021 \\
    Time(s) & 0.6  & 0.9 & 2.8 & 9.7 \\

 \hline

\end{tabular}
\caption{MAE and running time of models.}
\label{tab:result}
\end{center}
\end{table}

\subsection{Real Scene Experiment}
\label{section:realScene}
We further conduct tests on real scenes. To physically implement B-DfF, we construct a light field stereo pair by using two Lytro Illum cameras, as illustrated in Fig. \ref{fig:result_dualLytro}. Comparing with stereo focal sweeping, the Lytro pair can conduct high quality post-capture refocusing without the need for accurate synchronized mechanical control on focal length. In our experiment the two light field cameras share the same configuration including the zoom and focus settings. The raw images are preprocessed using Light Field Toolbox \cite{dansereau13}. Finally we conduct refocusing using shift-and-add algorithm \cite{ng05} to synthesize the focal stack.

\begin{figure*}[h]
\begin{center}
   \includegraphics[width=0.9\linewidth]{DFF_result_realScene_v2.pdf}
\end{center}
\vspace{-8pt}
   \caption{Comparisons of real scene results from \emph{FocusNet-v2}, \emph{StereoNet} and \emph{BDfFNet}.}
\label{fig:result_realScene}
\end{figure*}

Figure \ref{fig:result_realScene} shows the predicted depth from \emph{FocusNet-v2}, \emph{StereoNet} and \emph{BDfFNet}. Results show that \emph{BDfFNet} benefits from both \emph{FocusNet-v2} and \emph{StereoNet} to offer smoother depth with sharp edges. The experiments also demonstrate that models learned from our dataset could be transferred to predict real scene depth.

\begin{figure}[t]
\begin{center}
   \includegraphics[width=0.7\linewidth]{dualLytro.pdf}
\end{center}
\vspace{-8pt}
   \caption{To emulate our B-DfF setup, we combine a pair of Lytro Illum cameras into a stereo setup.}
\label{fig:result_dualLytro}
\end{figure}




\section{Discussions and Future Work}
\label{section:conclusion}

Our deepeye solution exploits efficient learning and computational light field imaging to infer depths from a focal stack pair. Our technique mimics human vision system that simultaneously employs binocular stereo matching and monocular depth-from-focus. Comprehensive experiments show that our technique is able to produce high quality depth estimation orders of magnitudes faster than the prior art. In addition, we have created a large dual focal stack database with ground truth disparity.

Our current implementation limits the input size of our network to be focal stacks of 16 layers. In our experiments, we have shown that it is able to produce high fidelity depth estimation under our setup. To handle denser focal stacks, one possibility is to concatenate all images in the stack as a 3D $(XYS)$ focal cube or volume \cite{zhou12}, where $X$ and $Y$ are  the width and height and $S$ is the index of a layer. We can then downsample the $XS$ slice along $S$ dimension to 16 slices using light field compression or simplification techniques such as tensor \cite{wanner12} and triangulation \cite{yu13}. Another important future direction we plan to explore is to replace one of the two focal stacks to be an all-focus image. This would further reduce the computational cost for constructing the network but would require adjusting the architecture. Finally, aside from computer vision, we hope our work will stimulate significant future work in human perception and the biological nature of human eyes.

\section{Acknowledgments}

This project was supported by the National Science Foundation under grant CBET-1706130.





{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}


