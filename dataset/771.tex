
\documentclass[journal]{IEEEtran}


\usepackage{cite}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{subfig}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{romannum}
\usepackage{caption}
\usepackage{tabularx}
\usepackage{tabulary}
\usepackage{array}
\usepackage{tabu}

\usepackage{caption}
\captionsetup[subfigure]{labelformat=parens,labelsep=space,font=small}

\ifCLASSINFOpdf
\else
\fi

\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}


\title{Deeply Self-Supervised Contour Embedded\\ Neural Network Applied to Liver Segmentation}
\author{Minyoung Chung, Jingyu Lee, Minkyung Lee, Jeongjin Lee$^{\ast}$, and Yeong-Gil Shin%
\thanks{\textit{Asterisk indicates corresponding author.}}%
\thanks{M. Chung is with the Department of Computer Science and Engineering, Seoul National University, Korea (e-mail: chungmy@cglab.snu.ac.kr).}%
\thanks{J. Lee is with the Department of Computer Science and Engineering, Seoul National University, Korea (e-mail: leejingyu@cglab.snu.ac.kr).}%
\thanks{M. Lee is with the Department of Computer Science and Engineering, Seoul National University, Korea (e-mail: mklee317@cglab.snu.ac.kr}%
\thanks{*J. Lee is with the Department of Computer Science and Engineering, Soong-sil University, Korea (e-mail: leejeongjin@ssu.ac.kr).}%
\thanks{Y. Shin is with the Department of Computer Science and Engineering, Seoul National University, Korea (e-mail: yshin@snu.ac.kr).}}



\maketitle

\begin{abstract}
\textit{Objective:} In this study, we propose a neural network-based liver segmentation algorithm and evaluate its performance using abdominal computed tomography (CT) images. 
\textit{Methods:} We develop a fully convolutional network to overcome the volumetric image segmentation problem. To guide a neural network to accurately delineate the target liver object, we deeply supervised our network by applying the adaptive self-supervision scheme to derive the essential contour which acts as a complement with the global shape. The discriminative contour, shape, and deep features are internally merged for the segmentation results. 
\textit{Results and Conclusion:} We used 160 abdominal CT images for training and validation. The quantitative evaluation of our proposed network is performed through eight-fold cross-validation. The result showed that our method, which uses the contour feature, successfully segmented the liver more accurately and showed better generalization performance than any other state-of-the-art methods without expanding or deepening the neural network. The proposed approach can be easily extended to other imaging protocols (e.g., magnetic resonance imaging) or other target organ segmentation problems without any modifications of the framework.
\textit{Significance:} In this work, we introduce a new framework to guide a neural network to learn complementary contour features. Our proposed neural network demonstrates that the guided contour features can significantly improve the performance of the segmentation task.


\end{abstract}

\begin{IEEEkeywords}
Convolutional neural network, contour embedded network, liver segmentation, self-supervising network.
\end{IEEEkeywords}


\IEEEpeerreviewmaketitle


\section{Introduction}

\IEEEPARstart{L}{iver} segmentation plays a crucial role in liver structural analysis, volume measurement, and clinical operations (e.g., surgical planning). For clinical usage, accurate segmentation of a liver is one of the key components of automated radiological diagnosis systems. Manual or semi-automatic segmentation of liver is a very impractical task owing to its large shape variability and unclear boundaries. Unlike other organs, ambiguous boundaries with heart, stomach, pancreas, and fat make liver segmentation difficult. Thus, for a computer-aided diagnosis (CAD) system, fully automatic and accurate segmentation of liver will play an important role in medical imaging.\par
Many methods have been proposed to segment a liver \cite{lim2006automatic, rusko2007fully, suzuki2010computer, lee2007efficient, zhang2010automatic, okada2007automated, ling2008hierarchical, heimann2009comparison, campadelli2009liver, van2007automatic}. The simplest and most intuitive approaches to perform liver segmentation are thresholding and region growing \cite{lim2006automatic, rusko2007fully}. Active contour model approaches \cite{suzuki2010computer, lee2007efficient} were also presented mainly using intensity distributions. However, such a local intensity-based approach easily fails owing to the large variability of shapes and intensity contrasts. Shape-prior-based methods such as active shape model, statistical shape model, and registration-based methods \cite{ling2008hierarchical, zhang2010automatic, okada2007automated,  heimann2007statistical, kainmuller2007shape, wimmer2009generic, van2007automatic} have been presented to overcome such difficulties. Shape-based methods are more successful than simple intensity-based methods owing to embedded shape priors. However, the shape-based methods suffer from limited prior information because of the difficulty of embedding all inter-patient organ shapes. Thus, the number of training statistical models directly affects the model matching performance.\par
In recent years, deep neural networks (DNNs) have been widely used for many imaging applications \cite{simonyan2014very, he2016deep, szegedy2017inception, long2015fully, noh2015learning, badrinarayanan2017segnet, fu2017stacked, dong2016image, jegou2017one}. For imaging applications, a convolutional neural network (CNN) is the most effectively used network with respect to image classification \cite{simonyan2014very, he2016deep, szegedy2017inception}, segmentation \cite{long2015fully, noh2015learning, badrinarayanan2017segnet, fu2017stacked, jegou2017one}, and enhancement \cite{dong2016image, burger2012image}. Various active studies have successfully applied CNNs to medical image segmentation \cite{ronneberger2015u, cciccek20163d, chen2017voxresnet, milletari2016v, chen2017dcan, kamnitsas2017efficient, havaei2017brain, dou20173d, jegou2017one, chen2018deeplab, oktay2018anatomically, gibson2018automatic}. U-net \cite{ronneberger2015u} applied contracting and expanding paths together with skip connections, which successfully combined both low and high-level features. U-net is not suitable for volumetric image segmentation because it is a fully convolutional network (FCN) based on 2D images. A 2D network architecture cannot leverage complex 3D anatomical information. 3D U-net \cite{cciccek20163d} attempted to overcome the limitation of the original U-net architecture to extract 3D contextual information via 3D convolutions with sparse annotation. However, 3D U-net had limitations of slice-based annotations. In \cite{milletari2016v}, a full 3D-CNN-based U-net-like architecture has been presented to segment volumetric medical images using a dice coefficient loss metric to overcome the class imbalance problem. Deep contour-aware network \cite{chen2017dcan} attempted to depict clear contours explicitly with a multi-task framework. VoxResNet \cite{chen2017voxresnet} has been presented to perform brain tissue segmentation using a voxelwise residual network. The authors used a residual learning mechanism \cite{he2016deep} to classify each voxel. Subsequently, they used an auto-context algorithm \cite{tu2010auto} to further refine voxelwise prediction results. Deeply supervised networks \cite{lee2015deeply} have been presented to hierarchically supervise multiple layers to segment medical images \cite{dou20173d}. Deep supervision \cite{dou20173d} allowed effective and fast learning and regularization of the network. The authors also applied a fully connected conditional random field model as a post-processing step to refine the segmentation results \cite{dou20173d}. In \cite{oktay2018anatomically}, incorporation of global shape information with neural networks has been presented. The authors attempted to construct convolutional autoencoder networks to learn anatomical shape variations from training images \cite{oktay2018anatomically}.\par
In this study, we propose a deeply self-supervising CNN with adaptive contour features. Instead of learning explicit ground-truth contour features such as in \cite{chen2017dcan}, we guide a neural network to learn complementary contour region that can aid accurate delineation of the target liver object. The main reason for learning partially significant contour is that, unlike other segmentation problems (e.g., gland), the contour of a liver is hard to be obtained accurately even with DNNs owing to its ambiguous boundaries. Learned partial contours are later fused with a global shape prediction to derive final segmentation. The network can be interpreted as a contour embedded shape estimation which uses three discriminative features: shape, contour, and deep features. Similar to \cite{gibson2018automatic}, we designed our base network architecture as densely connected V-net \cite{milletari2016v} structure. The number of parameters and layers are effectively reduced using a densely connected network architecture \cite{huang2017densely} and separable convolutions while preserving the network capability. Finally, we use the learned DNN for automatic segmentation of liver from CT images.\par
The remainder of this paper is organized as follows. In Section {\Romannum{2}}, we review several CNN models that are closely related to our method. We describe our proposed method in Section {\Romannum{3}}. The experimental results, discussion, and conclusion are presented in Sections {\Romannum{4}}, {\Romannum{5}}, and {\Romannum{6}}, respectively.
\newline



\section{RELATED WORK}
In this section, we briefly review CNN mechanism and provide three major related works that contribute key steps to our method: V-net \cite{milletari2016v}, deeply supervising networks (DSNs) \cite{lee2015deeply, dou20173d}, and densely connected convolutional networks \cite{huang2017densely}.

\subsection{V-net}
V-net \cite{milletari2016v} is a volumetric FCN for medical image segmentation. The U-net architecture was extended \cite{ronneberger2015u} to volumetric convolution (i.e., 3D convolution) and adopted U-net-like downward and upward transitions (i.e., convolutional reduction and de-convolutional expanding of feature dimensions; for more details, refer to the original paper \cite{milletari2016v}) were adopted together with many skip connections via an element-wise summation scheme. The dice loss was presented first to overcome the class imbalance problem.

\subsection{Deeply Supervising Network}
A DSN \cite{lee2015deeply} has been proposed to supervise a network to a deep level. Accordingly, a loss function penetrates through multiple layers in a DNN. The deeply supervising scheme makes intermediate features highly discriminative so that the final classifier can easily be a better discriminative classifier for the output. Another aspect of DSN is that training difficulty owing to exploding and vanishing gradient problems can be alleviated by direct and deep gradient flows. In \cite{dou20173d}, a 3D deep supervision mechanism has been adapted to volumetric medical image segmentation. The authors exploited explicit supervision to hidden layers and those auxiliary losses were integrated to the final loss with the last output layer to back-propagate gradients.

\begin{table}[!t]%[b]%[!t]
\renewcommand{\arraystretch}{1.5}
\caption{TABLE OF NOTATIONS}
\label{table:symbols}
\centering
\begin{tabular}{c||c||c}
Symbol & Definition & Expression\\
\hline
\(\Omega\) & Image domain & \(\Omega\in\mathbb{R}^3\)\\
\(I\) & Input 3D image & normalized to [0..1]\\
\(\Gamma\) & Binary ground truth label & \\
\(\Gamma_d\) & Down-sampled \(\Gamma\) & by the factor of 2\\
\(\Gamma_c\) & Binary ground truth contour label & \\
\(C\) & Convolution & \\
\(C^-\) & De-convolution & Inverse convolution \\
\(s\) & Stride value of convolution & for each dimension\\
\(p\) & Zero-padding value & for each dimension\\
\(d\) & Dilation value of convolution & for each dimension\\
\(B_{i,j}\) & \(j^{th}\) DenseBlock-(k,n) at \(i^{th}\) level & Fig. \ref{fig:denseblock}\\
\(F_{i,j}\) & Output features of \(B_{i,j}\) & \\
\(D_i\) & Down transition layer for level \(i\) & \(2^3\) \(C\), \(s=2\), \(p=0\)\\
\(U_i\) & Up transition layer for level \(i\) & \(2^3\) \(C^-\), \(s=2\), \(p=0\)\\
\(T_c\) & Transformation for \(\Gamma_c\) & Fig. \ref{fig:transition_deep}\\
\(T^i_s\) & Transformation for \(\Gamma_d\) & Fig. \ref{fig:transition_deep}, \(i\in{0,1}\)\\
\(F_c\) & Output features of \(T_c\) & \\
\(F^i_s\) & Output features of \(T^i_s\) & \\
\(T^i_o\) & Out transition layer & Fig. \ref{fig:transition_out}, \(i\in{0,1}\)\\
\(F^i_o\) & Output features of \(T^i_o\) & \\
\(W\) & Weights of the network & \\
\end{tabular}
\end{table}

\subsection{Densely Connected Convolutional Network}
Densely connected convolutional network (DenseNet) \cite{huang2017densely} connects each layer to every other layer in a feed-forward fashion. The main advantage of the presented architecture is that the gradient directly flows to deep layers, accelerating the learning procedure. Feature reuse also strongly contributes to a substantial reduction of the number of parameters. This structure can be viewed as an implicit deep supervision network similar to the explicit version \cite{lee2015deeply}. The \(l^{th}\) layer obtains the concatenation of all outputs of the preceding layers \cite{huang2017densely}:
\newline
\begin{equation}
    x_l=H_l(x_0, x_1, ..., x_{l-1}),
\label{eq:densenet}
\end{equation}\newline
where \(x_l\) denotes the output of the \(l^{th}\) layer, \([x_0, x_1, ..., x_{l-1}]\) refers to the concatenation of the feature-maps produced in the previous layers, and \(H_l\) denotes a non-linear transformation at the \(l^{th}\) layer (e.g., composition of convolution and non-linear activation function). The feature-reusing scheme of DenseNet, which causes reduction of parameters, is an effective feature for the 3D volumetric neural network because the volume data lack GPU memory for DNNs.

\begin{figure*}[h!bt]
    \centering
    \includegraphics[width=\linewidth]{figures/network.PNG}
    \caption{Our proposed volumetric network architecture (best viewed in color). Stacked DenseBlock-(k,n) units form a base architecture with many skip connections. The red (i.e., circled arrows) and blue lines (i.e., squared arrows from DenseBlocks) indicate down and up transition layers (i.e., \(D_i\) and \(U_i\)), respectively. The orange lines (i.e., dotted squared arrows) indicate up sampling layers with linear interpolation scheme. The red and blue dotted boxes represent contour and shape transitions, respectively. The two transitions are deeply supervised by contour and ground-truth images. The final output prediction is achieved by successive out transition layers that combine the deep features. All images are displayed as 2D for simplicity.}
    \label{fig:network}
\end{figure*}



\section{METHODOLOGY}

The base architecture of the network is composed of several contracting, expanding paths, and skip connections similar to V-net \cite{milletari2016v}. The key part of our network is two different deep-supervisions embedded in the network: contour and shape transition layers (i.e., the red and blue dotted boxes in Fig. \ref{fig:network}). Deeply supervised contour and shape features are sequentially concatenated for the final segmentation result. There are three different non-linear modules in our model: DenseBlock-(k,n) (Fig. \ref{fig:denseblock}), deep, and out transition layers (Fig. \ref{fig:transition}). Each module comprises convolution, batch normalization \cite{ioffe2015batch}, rectified linear unit (ReLU) non-linearity \cite{nair2010rectified}, and skip connections. Details of the architecture, non-linear modules, and deep supervisions are described in the following subsections. The basic symbols for notations are presented in Table \ref{table:symbols}.



\subsection{Network Architecture}
The base network uses DenseBlock-(k,n) as a non-linear module and performs several contracting (i.e., down transition), expanding (i.e., up transition) paths, and concatenating skip connections. For down transition layers, \(D_i\) (i.e., down sampling feature dimensions; circled red lines in Fig. \ref{fig:network}), we get \(F_{i,0}\) as input and down samples the feature map by the factor of 2 for each dimension via convolutions with stride 2. We preserve the number of feature of \(F_{i,0}\). For up transition layers, \(U_i\) (i.e., up sampling feature dimensions; squared blue lines in Fig. \ref{fig:network}), we use de-convolution (i.e., transposed convolution) with restoring the number of features as the same as the skip connected upper layer for feature summation. Each up transitioned layers are summed with previous feature outputs (i.e., skip connection in Fig. \ref{fig:network}) and passes through a DenseBlock-(k,n) unit (i.e., \(B_{i,1}\), where \(1\leq i\leq2\)). The feature outputs of lower layers, \(F_{2,1}\) and \(F_{1,1}\), are up-scaled (i.e., orange lines in Fig. \ref{fig:network}) and concatenated with \(F_{0,1}\) for further propagation of \(0^{th}-level\) layers. After passing \(B_{0,2}\), contour and shape features are sequentially concatenated to the out transition layers (Fig. \ref{fig:network}).\par
The final prediction of the network is made by integrating the three major features: 1) deep features from the base network (i.e., stack of DenseBlock-(k,n)), 2) contour features from the contour transition branch (i.e., the red dotted box in Fig. \ref{fig:network}), and 3) shape features from the shape transition branch (i.e., the blue dotted box in Fig. \ref{fig:network}). The two deep transition layers are deeply supervised for each feature extractions.

\subsection{Non-linear Modules}
Let unit block, \(B_{i,j}\), be the DenseBlock-(k,n) transformation at level \(i\) (i.e., down transitioned level starting with \(0\)) and \(j^{th}\) block at level \(i\). \(B_{i,j}\) is composed of non-linear transformation series: convolution, batch normalization, and ReLU non-linear activation function (Fig. \ref{fig:denseblock}). These transformations are densely connected for feature reuse. Unlike in the original paper \cite{huang2017densely}, we introduce depth-wise separable convolutions \cite{chollet2017xception} in the densely connected block rather than bottleneck layers \cite{szegedy2016rethinking} or compression layers \cite{huang2017densely} for more efficient use of parameters. We write the output features of each \(B_{i,j}\) layer as \(F_{i,j}\).\par

Transition layers (Fig. \ref{fig:transition}) are also composed of non-linear transformation series such as DenseBlock-(k,n). In transition layers, however, separable convolutions are not used. Deep transition layers (Fig. \ref{fig:transition_deep}) perform down and up transitions (i.e., the red and blue circled arrows in Fig. \ref{fig:transition_deep}) similar to a small V-net-like architecture. The down transition layer down samples the feature map by the factor of 2 for each dimension via convolutions with stride 2. Conversely, up transition layer up samples the feature map by using de-convolution (i.e., transposed convolution). By contracting and expanding paths, the deep transition layer can achieve more multi-scaled features (i.e., higher receptive field) for each contour and shape feature extraction. Out transition layers simply forward the feature maps with dense connections followed by $1\times1\times1$ convolution (Fig. \ref{fig:transition_out}). There are two out transition layers in the network for integrating features at the final stage.\par

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/denseblock2.PNG}
    \caption{Densely connected block component (i.e., DenseBlock-(k,n)). The number of feature input for each separable convolution is $N$ and the features are separated by groups containing $n$ features. \(k\) indicates the number of features produced by $1\times1\times1$ convolution applied to concatenated features. The number of total features of a single DenseBlock-(k,n) becomes \(3k\). Separable convolutions are applied to all DenseBlock-(k,n) units.}
    \label{fig:denseblock}
\end{figure}

\begin{figure}[t]
    \centering
    \subfloat[Deep transition layers (i.e., contour and shape).]{\includegraphics[width=3.5in]{figures/deep_transition.png}%
    \label{fig:transition_deep}}
    \vfil
    \subfloat[Out transition layers.]{\includegraphics[width=3.5in]{figures/out_transition.png}%
    \label{fig:transition_out}}
    \caption{Transition layers for (a) contour (\(T_c\)), shape (\(T^i_s\)), and (b) out (\(T^i_o\)) transition layers. The blue box indicates a series of convolution, batch normalization, and non-linear activation. Gray box indicates a single convolution layer. Each kernel size, stride ($s$), padding ($p$), and dilation value ($d$) is specified.}
    \label{fig:transition}
\end{figure}

\subsection{Deeply Supervised Transition Layers}
We applied two different deep supervision mechanism in our model: shape and contour transitions (i.e., the blue and red dotted boxes in Fig. \ref{fig:network}). Shape supervision is applied to \(F_{1,1}\) with shape transition layers (i.e., \(T^i_s\); Fig. \ref{fig:transition_deep}). Two identical transitions were applied separately for learning complementary residuals. The effectiveness of the residual connection is evaluated in the experiment Section \Romannum{4}. Contour transition layer (i.e., \(T_c\); Fig. \ref{fig:transition_deep}) is applied to low-level features for contour supervision. For contour transition, \(F_{0,0}\) feature map is fed to \(T_c\) for contour responses.\par

For deep supervision of contour (i.e., \(F_c\)), we dynamically modified the ground-truth contour image, \(\Gamma_c\), for every iteration (paired blue arrow in Fig. \ref{fig:network}):
\newline
\begin{equation}
    \Tilde{\Gamma_c}=\Gamma_c \otimes (\bold{y}_p),
\label{eq:closs1}
\end{equation}
\newline
where \(\otimes\) is an element-wise multiplication operator and \(\bold{y}_p\) is a binary image with respect to the threshold value, \(p\):
\newline
\begin{equation}
    \bold{y}_p(x)=
    \begin{cases}
    1, & \text{if}\ \bold{y}(x)<p \\
    0, & \text{otherwise},
    \end{cases}
\label{eq:closs2}
\end{equation}
\newline
where \(\bold{y}\) is the output probability prediction of our network for a given iteration. That is, we automatically erased the ground-truth contours (i.e., foreground voxels in \(\Gamma_c\)) if our network successfully delineated the corresponding labels at the output. This adaptive self-supervision procedure aid contour transition layer to effectively delineate the misclassified contour region with respect to low-level features (e.g., edge). The discriminative feature of contour transition is later combined with shape prediction for final liver object delineation.

\subsection{Overall Loss Function}
Let vectors \(\bold{x}=\{x_i\in\textit{R}, i\in\Omega\}\) and \(\bold{y}=\{y_i\in\{0,1\}, i\in\Omega\}\) represent the input image and ground-truth label, respectively. The task of the given learning system is to model conditional probability distribution, \(P(\bold{y}|\bold{x})\). To effectively model the probability distribution, we learn our network model to map the segmentation function \(\phi(\bold{x}):I(\bold{x})\longrightarrow\{0,1\}\) by minimizing the following loss function:
\newline
\begin{equation}
\begin{split}
    \textit{L}_p(\bold{x}, \bold{y};W)=&\mathcal{D}(F^1_o, \Gamma)+\alpha\mathcal{D}(F^0_s-F^1_s, \Gamma_d)+\\
    &\beta\chi(F_c, \Tilde{\Gamma_c})+\gamma\|W\|_2^2,
\label{eq:loss_total}
\end{split}
\end{equation}
\newline
where \(\mathcal{D}\) indicates the dice loss \cite{milletari2016v}, \(\chi\) indicates softmax-cross-entropy loss,
\newline
\begin{equation}
    \chi(\bold{x}, \bold{y})=\sum_i(-w_i log(\frac{exp(\bold{x}[y_i])}{\sum_j{exp(\bold{x}[j])}})).
\label{eq:loss_xentropy}
\end{equation}
\newline
\(\alpha, \beta\), and \(\gamma\) in (\ref{eq:loss_total}) are weighting parameters. The \(w_i\) parameter in (\ref{eq:loss_xentropy}) is class balancing weights. The output of the network is obtained by applying softmax to the final output feature maps.\par




\subsection{Data Preparation and Augmentation}
We acquired 160 subjects in total: 90 subjects from a publicly available dataset\footnote{DOI:http://doi.org/10.5281/zenodo.1169361} in \cite{gibson2018automatic}, 20 subjects from MICCAI-Sliver07 dataset \cite{heimann2009comparison}, 20 subjects from 3Dircadb\footnote{https://www.ircad.fr/research/3dircadb}, and additional 30 annotated subjects with the help of clinical experts in the field. In the dataset, the slice thickness ranged from 0.5-5.0mm and pixel sizes ranged from 0.6-1.0mm.\par
For the training dataset, we resampled all abdominal computed tomography images by $128\times128\times64$. We pre-processed the image using fixed windowing values: level=10 and width=700 (i.e., clipped the intensity values under $-340$ and over $360$). After re-scaling, we normalized the input images into the range [0..1] for each voxel. On-the-fly random affine deformations were subsequently applied to that dataset for each iteration with 80\% probability. Finally, we performed the cutout image augmentation \cite{devries2017improved} with 80\% probability. We did not constrain the position of the cutout mask with respect to the boundaries. We applied a randomly sized zero mask with the range \(L/5\leq l \leq L/4\), where \(l\) and \(L\) are the lengths of the mask and the image in each dimension, respectively. To the best of our knowledge, this is the first study to apply cutout \cite{devries2017improved} augmentation to image segmentation problem. We presented the effect of the cutout augmentations in the experiment Section \Romannum{4}.

\subsection{Learning the Network}
'Xavier' initialization \cite{glorot2010understanding} is used for initializing all the weights of the proposed network. While training the network, we fixed the loss parameters as \(\alpha=1\), \(\beta=1\), and \(\gamma=0.1\) in (\ref{eq:loss_total}). We set the parameter \(p\) as 1 until 100 epochs, and decayed by multiplying 0.9 for every 10 epoch until 0.5 (i.e., the minimum value of \(p\)). For the dense block unit, we used DenseBlock-(16,4) (i.e., \(k=16, n=4\)). We used the Adam optimizer with batch size 4 and learning rate 0.001. We decayed the learning rate by multiplying 0.1 for every 50 epoch. We trained the network for 300 epochs using an Intel i7-7700K desktop system with 4.2 GHz processor, 32 GB of memory, and Nvidia Titan Xp GPU machine. It took 10h to complete all training procedures.
\par



\section{EXPERIMENTS AND RESULTS}

In our experiments, we evaluated the learning curves and results of the proposed network by comparing them with those of other FCN-based models. We used DSN \cite{dou20173d}, VoxResNet \cite{chen2017voxresnet}, DenseVNet \cite{gibson2018automatic}, and our proposed network, CENet for performance evaluation.

\begin{figure}[t]%[h!tb]%[b]
    \centering
    \subfloat[eight-fold (i.e., 140/20) cross-validation.]{\includegraphics[width=\linewidth]{figures/LC_final_8_fold.png}%
    \label{fig:learningcurve_8fold}}
    \vfil
    \centering
    \subfloat[10/150 cross-validation.]{\includegraphics[width=\linewidth]{figures/LC_final_sfold_wcutout.png}%
    \label{fig:learningcurve_sfold_wcutout}}
    \vfil
    \centering
    \subfloat[10/150 cross-validation without cutout augmentations.]{\includegraphics[width=\linewidth]{figures/LC_final_sfold_wocutout.png}%
    \label{fig:learningcurve_sfold_wocutout}}
    \vfil
    \caption{Learning curves of DSN \cite{dou20173d}, VoxResNet \cite{chen2017voxresnet}, DenseVNet \cite{gibson2018automatic}, and CENet with multiple cross-validations: (a) used 140 images for training and 20 images for validation (i.e., eight-fold cross-validation). (b) and (c) used 10 images for training and 150 images for validation. (c) shows the learning curve without cutout \cite{devries2017improved} augmentation.}
    \label{fig:learningcurve}
\end{figure}

\begin{figure*}[tb]
    \centering
        \begin{minipage}[b]{3.5in}
            \captionsetup[subfigure]{labelformat=parens,labelsep=space,font=small}
            \centering
                \vfil
                \includegraphics[width=1.11in]{figures/livers/responses/full/1/BTCV0228.png}
                \includegraphics[width=1.11in]{figures/livers/responses/full/1/BTCV02_Side26.png}
                \includegraphics[width=1.11in]{figures/livers/responses/full/1/BTCV02_Back.png}
                \vfil
                \includegraphics[width=1.11in]{figures/livers/responses/full/2/BTCV0438.png}
                \includegraphics[width=1.11in]{figures/livers/responses/full/2/BTCV04_Side34.png}
                \includegraphics[width=1.11in]{figures/livers/responses/full/2/BTCV04_Back40.png}
                \vfil
                \includegraphics[width=1.11in]{figures/livers/responses/full/3/MICCAI0307.png}
                \includegraphics[width=1.11in]{figures/livers/responses/full/3/MICCAI03_Side08.png}
                \includegraphics[width=1.11in]{figures/livers/responses/full/3/MICCAI03_Back12.png}
                \vfil
                \includegraphics[width=1.11in]{figures/livers/responses/full/4/MICCAI2001.png}
                \includegraphics[width=1.11in]{figures/livers/responses/full/4/MICCAI20_Side02.png}
                \includegraphics[width=1.11in]{figures/livers/responses/full/4/MICCAI20_Back04.png}
                \vfil
                \includegraphics[width=1.11in]{figures/livers/responses/full/5/PANC0214.png}
                \includegraphics[width=1.11in]{figures/livers/responses/full/5/PANC02_Side18.png}
                \includegraphics[width=1.11in]{figures/livers/responses/full/5/PANC02_Back22.png}
            \captionof{subfigure}{Fully-supervised contour feature maps.}
            
        \end{minipage}
        \hfil
        \begin{minipage}[b]{3.5in}
            \centering
                \vfil
                \includegraphics[width=1.11in]{figures/livers/responses/self/1/BTCV0230.png}
                \includegraphics[width=1.11in]{figures/livers/responses/self/1/BTCV02_Side25.png}
                \includegraphics[width=1.11in]{figures/livers/responses/self/1/BTCV02_Back24.png}
                \vfil
                \includegraphics[width=1.11in]{figures/livers/responses/self/2/BTCV0439.png}
                \includegraphics[width=1.11in]{figures/livers/responses/self/2/BTCV04_Side33.png}
                \includegraphics[width=1.11in]{figures/livers/responses/self/2/BTCV04_Back41.png}
                \vfil
                \includegraphics[width=1.11in]{figures/livers/responses/self/3/MICCAI0306.png}
                \includegraphics[width=1.11in]{figures/livers/responses/self/3/MICCAI03_Side09.png}
                \includegraphics[width=1.11in]{figures/livers/responses/self/3/MICCAI03_Back11.png}
                \vfil
                \includegraphics[width=1.11in]{figures/livers/responses/self/4/MICCAI2000.png}
                \includegraphics[width=1.11in]{figures/livers/responses/self/4/MICCAI20_Side03.png}
                \includegraphics[width=1.11in]{figures/livers/responses/self/4/MICCAI20_Back05.png}
                \vfil
                \includegraphics[width=1.11in]{figures/livers/responses/self/5/PANC0215.png}
                \includegraphics[width=1.11in]{figures/livers/responses/self/5/PANC02_Side20.png}
                \includegraphics[width=1.11in]{figures/livers/responses/self/5/PANC02_Back21.png}
            \captionof{subfigure}{Self-supervised contour feature maps.}
        \end{minipage}
        \vfil
       
        \includegraphics[width=4.5in]{figures/prob_grad.PNG}
        
    \caption{Contour feature (i.e., $F_c$) visualizations after full training: (a) without self-supervision and (b) with self-supervision (i.e., (\ref{eq:closs1})). The self-supervised contour feature map (b) is much sparser than that of full-supervision and later used as a strong contour features. The ground-truth surface is used for visualizing the distribution of the contour feature. The softmax value of $F_c$ is normalized into the range [0..1].}
    \label{fig:responses}
\end{figure*}


        
        
        
        
    
        
        

\subsection{Learning Curve}
As shown in Fig. \ref{fig:learningcurve}, a learning curve with dice loss is plotted. All hyperparameters (such as learning rate and optimizer) were set as specified in the original studies. We first designed eight-fold cross-validation for performance evaluation (i.e., 140 training images and 20 validation images). The plot in Fig. \ref{fig:learningcurve_8fold} indicates that our proposed network achieved the most successful training result. The other networks found it severely difficult to minimize the validation errors. The quantitative results are presented in Tables \ref{table:results} and \ref{table:results_stat}. We have additionally designed a special experimental setting with 10 training images and 150 validation images (Figs. \ref{fig:learningcurve_sfold_wcutout} and \ref{fig:learningcurve_sfold_wocutout}). We believe that this experimental setting approximately proxies the real-life deep learning problem and shows an extremely generalized regularization analysis. The overall validation errors increased in a special cross-validation with 10 training images (Fig. \ref{fig:learningcurve_sfold_wcutout}). Meanwhile, our proposed network did not over-fit (i.e., lowest generalization error) to the training images as compared to other networks. Fig. \ref{fig:learningcurve_sfold_wocutout} shows the worse generalization curve without cutout augmentation \cite{devries2017improved} indicating that cutout augmentation greatly aids the network training to be generalized. In all training experiments, our proposed network made the fastest convergence, showed the lowest loss value, and resulted in best generalization.



        
        
        
        
        
        


\subsection{Contour and Shape Feature Layers}

We have visualized the output feature map of $T_c$ (i.e., $F_c$) in Fig. \ref{fig:responses}. The contour feature map of a fully-supervised network (i.e., using ground-truth contour supervision without modification (\ref{eq:closs1})) was activated within all contour region (Fig. \ref{fig:responses}a). The one strong indication from Fig. \ref{fig:responses}a is that even with full training, the network failed to extract full contour features accurately (i.e., part of low softmax responses on the ground-truth contour region). Meanwhile, with a self-supervised network, the contour feature map was activated in local contour regions that can further aid the accuracy of the segmentation (Fig. \ref{fig:responses}b). As shown in Fig. \ref{fig:responses}b, contour transition layer successfully learned discriminative contours excluding ambiguous regions that can be better delineated by global shape prediction (i.e., $F^0_s-F^1_s$; Fig. \ref{fig:residuals}). The quantitative evaluation between the two methods is presented in the following section.\par
The effects of residuals in shape transition layers are shown in Fig. \ref{fig:residuals}. Both \(T^0_s\) and \(T^1_s\) learns complementary features (Figs. \ref{fig:residual1} and \ref{fig:residual2}) for accurate shape delineation by subtraction scheme.


\begin{figure}[!tb]
    
    \centering
        \subfloat[\(F^0_s\).]{\includegraphics[width=1.05in]{figures/residual1.png}
        \label{fig:residual1}}\hfil
        \subfloat[\(F^1_s\).]{\includegraphics[width=1.05in]{figures/residual2.png}
        \label{fig:residual2}}\hfil
        \subfloat[\(F^0_s-F^1_s\).]{\includegraphics[width=1.05in]{figures/residual.png}
        \label{fig:residual}}
    \caption{Visualization of shape feature maps (i.e., \(F^i_s\)) after full training. (a) \(F^0_s\), (b) \(F^1_s\), and (c) shows the subtraction result of \(F^i_s\) features.}
    \label{fig:residuals}
\end{figure}

\begin{table*}[t]%[b]%[!t]
\renewcommand{\arraystretch}{1.7}
\captionsetup{justification=centering, labelsep=newline}
\caption{QUANTITATIVE EVALUATION OF EIGHT-FOLD CROSS-VALIDATION WITH MEDIAN METRIC}
\label{table:results}
\begin{tabularx}{\textwidth}{X||>{\centering\arraybackslash}X|>{\centering\arraybackslash}X|>{\centering\arraybackslash}X|>{\centering\arraybackslash}X|>{\centering\arraybackslash}X}
Methods & DSC & HD [mm] & ASSD [mm] & Sensitivity & Precision\\
\hline
DSN \cite{dou20173d} & \(0.94\) & \(5.60\) & \(1.60\) & \(0.95\) & \(0.94\)\\
VoxResNet \cite{chen2017voxresnet} & \(0.93\) & \(6.71\) & \(1.78\) & \(0.92\) & \(0.96\)\\
DenseVNet \cite{gibson2018automatic} & \(0.92\) & \(10.13\) & \(2.27\) & \(\textbf{0.97}\) & \(0.88\)\\
CENet & \(\textbf{0.96}\) & \(\textbf{3.99}\) & \(\textbf{1.20}\) & \(\textbf{0.97}\) & \(\textbf{0.97}\)\\
CENet-A & \(\textbf{0.96}\) & \(4.97\) & \(1.23\) & \(\textbf{0.97}\) & \(0.96\)\\
CENet-C & \(\textbf{0.96}\) & \(4.56\) & \(\textbf{1.21}\) & \(\textbf{0.97}\) & \(0.96\)\\
CENet-S & \(\textbf{0.96}\) & \(4.21\) & \(\textbf{1.19}\) & \(0.96\) & \(\textbf{0.97}\)\\
CENet-R & \(\textbf{0.96}\) & \(5.25\) & \(1.27\) & \(0.96\) & \(0.96\)\\
\end{tabularx}
\end{table*}

\begin{table}[t]%[b]%[!t]
\renewcommand{\arraystretch}{1.7}
\captionsetup{justification=centering, labelsep=newline}
\caption{MEAN AND STANDARD DEVIATION OF EIGHT-FOLD CROSS-VALIDATION}
\label{table:results_stat}
\begin{tabularx}{\linewidth}{c||X|X|X|X}
Metric & DSN \cite{dou20173d} & VoxResNet \cite{chen2017voxresnet} & DenseVNet \cite{gibson2018automatic} & CENet\\
\hline
DSC & \(0.94\pm0.02\) & \(0.93\pm0.02\) & \(0.92\pm0.02\) & \(\textbf{0.96}\pmb{\pm}\textbf{0.01}\)\\
HD & \(7.49\pm4.21\) & \(7.99\pm4.04\) & \(11.47\pm6.60\) & \(\textbf{5.25}\pmb{\pm}\textbf{2.70}\)\\
ASSD & \(1.77\pm0.51\) & \(1.93\pm0.48\) & \(2.53\pm0.59\) & \(\textbf{1.17}\pmb{\pm}\textbf{0.30}\)\\
S & \(0.93\pm0.04\) & \(0.91\pm0.04\) & \(\textbf{0.97}\pmb{\pm}\textbf{0.02}\) & \(\textbf{0.96}\pmb{\pm}\textbf{0.03}\)\\
P & \(0.94\pm0.02\) & \(0.95\pm0.02\) & \(0.88\pm0.03\) & \(\textbf{0.96}\pmb{\pm}\textbf{0.02}\)\\
\end{tabularx}
\end{table}

\begin{figure}
    \centering
        \vfil
        \subfloat[DSC.]{\includegraphics[width=\linewidth]{figures/plot_dice.png}
        \label{fig:plot_dice}}
        \vfil
        \subfloat[95\% HD in mm.]{\includegraphics[width=\linewidth]{figures/plot_hd95.png}
        \label{fig:plot_hd95}}
        \vfil
        \subfloat[ASSD in mm.]{\includegraphics[width=\linewidth]{figures/plot_mbd.png}
        \label{fig:plot_mbd}}
        \vfil
        \subfloat[Sensitivity.]{\includegraphics[width=\linewidth]{figures/plot_sensitivity.png}
        \label{fig:plot_s}}
        \vfil
        \subfloat[Precision.]{\includegraphics[width=\linewidth]{figures/plot_precision.png}
        \label{fig:plot_p}}
        \vfil
        \subfloat{\includegraphics[width=2.5in]{figures/legend.PNG}}
    \caption{Box plots of segmentation metrics for eight-fold performance evaluations.}
    \label{fig:plot}
\end{figure}

\subsection{Quantitative Evaluations}


We evaluated the segmentation results using dice similarity coefficient (DSC), 95\% Hausdorff distance (HD), average symmetric surface distance (ASSD), sensitivity (S), and precision (P). The DSC is defined as
\begin{equation}
    DSC(X,Y)=\frac{2|X \cap Y|}{|X|+|Y|},
\end{equation}
\noindent
where \(|\cdot|\) is the cardinality of a set. Let \(\textbf{S}_X\) be a set of surface voxels of a set \(X\), the shortest distance of an arbitrary voxel \(p\) is defined as \cite{heimann2009comparison}:
\begin{equation}
    d(p, \textbf{S}_X)=\min_{s_X \in \textbf{S}_X}{||p-s_X||}_2.
\label{eq:d}
\end{equation}
\noindent
HD is thus given by \cite{heimann2009comparison}:
\begin{equation}
    HD(X,Y)=\max\{\max_{s_X \in \textbf{S}_X}{d(s_X, \textbf{S}_Y)}+\max_{s_Y \in \textbf{S}_Y}{d(s_Y, \textbf{S}_X)}\}.
\label{eq:hd}
\end{equation}
\noindent
Defining the distance function as
\begin{equation}
    D(\textbf{S}_X, \textbf{S}_Y)=\Sigma_{s_X \in \textbf{S}_X} d(s_X, \textbf{S}_Y),
\end{equation}
the ASSD can be defined as \cite{heimann2009comparison}:
\begin{equation}
    ASSD(X,Y)=\frac{1}{|\textbf{S}_X|+|\textbf{S}_Y|}(D(\textbf{S}_X, \textbf{S}_Y)+D(\textbf{S}_Y, \textbf{S}_X)).
\end{equation}
Sensitivity and precision are defined as follows:
\begin{equation}
    S=\frac{TP}{TP+FN},
\label{eq:sensitivity}
\end{equation}
\begin{equation}
    P=\frac{TP}{TP+FP}
\label{eq:precision}
\end{equation}
\noindent
where \(TP\), \(FN\), and \(FP\) are the numbers of true positive, false negative, and false positive voxels, respectively. In (\ref{eq:hd}), 95\% of voxels were calculated in (\ref{eq:d}) to exclude 5\% outlying voxels. This is for a generalized evaluation of distance without portal vein variations (Fig. \ref{fig:visualization}). Eight-fold cross-validation is used to obtain the quantitative results in Tables \ref{table:results} and \ref{table:results_stat}. Visual box plot of Table \ref{table:results_stat} is presented in Fig. \ref{fig:plot}. Our proposed CENet showed the best segmentation results within all evaluations. Especially, DenseVNet failed to segment liver accurately because of two significant reasons: 1) the network resolution is too low and 2) shape prior has weak representative power. Thus, with excessively coarse dimensions of an image, the segmentation result suffers from the accurate delineation of an object in the original domain. Furthermore, \(12^3\) resolution of shape prior is too small and training images must be accurately and manually cropped to fully utilize the learned shape prior. There is no specific metric presented in the original paper \cite{gibson2018automatic} to crop testing images automatically.\par

\begin{figure}[tb]
    \centering
        \subfloat[Ground truth.]{
        \includegraphics[width=1.12in]{figures/livers/front/groundtruth.png}
        \includegraphics[width=1.12in]{figures/livers/back/groundtruth.png}
        \includegraphics[width=1.12in]{figures/livers/bottom/groundtruth.png}
        \label{fig:liver_gt}}
        \vfil
        
        \subfloat[DSN \cite{dou20173d}.]{
        \includegraphics[width=1.12in]{figures/livers/front/dsn.png}
        \includegraphics[width=1.12in]{figures/livers/back/dsn.png}
        \includegraphics[width=1.12in]{figures/livers/bottom/dsn.png}
        \label{fig:liver_dsn}}
        \vfil
        
        \subfloat[VoxResNet \cite{chen2017voxresnet}.]{
        \includegraphics[width=1.12in]{figures/livers/front/voxres.png}
        \includegraphics[width=1.12in]{figures/livers/back/voxres.png}
        \includegraphics[width=1.12in]{figures/livers/bottom/voxres.png}
        \label{fig:liver_voxresnet}}
        \vfil
        
        \subfloat[DenseVNet \cite{gibson2018automatic}.]{
        \includegraphics[width=1.12in]{figures/livers/front/densev.png}
        \includegraphics[width=1.12in]{figures/livers/back/densev.png}
        \includegraphics[width=1.12in]{figures/livers/bottom/densev.png}
        \label{fig:liver_densevnet}}
        \vfil
        
        \subfloat[CENet.]{
        \includegraphics[width=1.12in]{figures/livers/front/e2c.png}
        \includegraphics[width=1.12in]{figures/livers/back/e2c.png}
        \includegraphics[width=1.12in]{figures/livers/bottom/e2c.png}
        \label{fig:liver_CE}}
        \vfil
        
        \subfloat{
        \includegraphics[width=\linewidth]{figures/color_grad.PNG}
        }
        
    \caption{Visualizations of eight-fold segmentation results. The surface color is visualized with respect to distance to the ground-truth surface. Visualized surfaces are smoothed via a curvature flow smoothing method \cite{smoothTriangMesh} at the original image resolution.}
    \label{fig:visualization}
\end{figure}

We have extended our experiments with our network variants: CENet without self-supervised contour learning (i.e., using full ground-truth contour $\Gamma_c$ instead of adaptively modified $\Tilde{\Gamma_c}$; CENet-A), without contour transition layer (i.e., removing the red box in Fig. \ref{fig:network}; CENet-C), without shape transition layer (i.e., removing the blue box in Fig. \ref{fig:network}; CENet-S), and without the residual shape estimation layer (i.e., removing the black box in Fig. \ref{fig:network}; CENet-R). In case of CENet-R, two shape transition layers (i.e., \(T^0_s\)) are sequentially stacked for the shape estimation. The accuracy of our network variants was slightly less than that of the original CENet. DSC, sensitivity, and precision scores of the variants were preserved while distance errors (i.e., 95\% HD, ASSD) slightly increased. CENet-S showed the lowest distance errors among the variants, while CENet-R showed the highest. This indicates that the residual shape estimation process is critical for an accurate shape estimation. When using the CENet-R network, the \(F^0_s\) feature was similar to that shown in Fig. \ref{fig:residual1} which leads to an inaccurate output result. Without residuals, we might have to design more complex and deep transition layers for shape estimation, which might lead to over-fitting. The result of CENet-C indicates that the contour transition part plays a key role for accurate delineation of an object. However, CENet-A performed worse than CENet-C with respect to HD and ASSD measurements, indicating that enforcing the network to learn full ground-truth contour image has a negative effect on the performance.\par
The visual result of an example liver subject is presented in Fig. \ref{fig:visualization}. As it is clearly visualized, our proposed CENet successfully segmented liver with proper guidelines of contour and accurate shape estimation. All the networks including the one proposed by us, found it difficult to segment portal vein entry region accurately. However, we observed that our training database (i.e., clinically annotated ground-truth images) had serious internal variation in the portal vein entry region. Some clinicians included the vessels while some excluded the major entry vessel region. We suggest that a concurrent and integrated liver and vascular system segmentation framework be built in the future so that the variability of annotations can be overcome. In the case of DenseVNet, inaccurate shape prior seriously affected the final output, as shown in Fig. \ref{fig:liver_densevnet}.


\section{DISCUSSION}
Segmentation of organs in medical imaging is still a very challenging problem. The edge feature is unquestionably the most important feature for accurate object segmentation in the perspective of contour delineation. However, the full contour is hard to be identified in various cases, such as unclear boundaries and false edges in contrast-enhanced vessels. Even with the strong capacity of the neural network, it is very difficult to classify ambiguous regions. Thus, the proposed network avoided learning full contour features that are unnecessary to our objective. The proposed method guided (i.e., self-supervised) the neural network to learn sparse but essential contour that can be a great complementary feature to be later fused with the global shape estimation. We have used two major neural network branches: contour and shape estimations. This network might be seen as that similar to a multi-task learning framework. However, we do not enforce the network to explicitly inference multiple tasks. Our network internally guided weights to make object contour features without supervising the entire contour image. We self-supervised the network with modified contour images. The main underlying principle of the proposed network is to make contour delineation pass concentrate on the missing contour part of an object (i.e., fine details of an object that are easy to be misclassified using end-to-end learning). There are two important reasons for using our proposed method: 1) Even with a powerful deep neural network, unclear boundaries are very challenging to be discriminated as a contour. 2) Contour regions in unclear boundaries can be well delineated by global shapes. Finally, we have merged three strong discriminative features (i.e., shape, contour details, and deep features) for the best segmentation results. The proposed network can be intuitively interpreted as a robust contour guided shape estimation.\par

Our proposed network can be easily extended to other imaging protocols (e.g., magnetic resonance imaging) or other target organ segmentation problems without any network modifications. For effective modification of our proposed network to other applications, we recommend modifying parameters of the dense block (Fig. \ref{fig:denseblock}) and \(p\) (i.e., the threshold value to determine the misclassified voxels). The parameters \(k\) and \(n\) in the dense block adjust the complexity of the network and \(p\) adjusts the workload of contour transition (i.e., \(T_c\)). The higher the value of \(p\), the larger the contour region will be required to be delineated in the contour estimation pass. In our experiments (i.e., liver segmentation), the parameter \(p\) was not sensitive to the presented results.\par


\section{CONCLUSION}
In this work, we designed an FCN for image segmentation with a self-supervised contour-guiding scheme. Our proposed network combined the shape and contour features to accurately delineate the target object. Contour features were learned to delineate the complementary contour region in a self-supervising scheme. We divided the network into two big branches for shape and complementary contour estimations. Our proposed network showed that the critical and partial contour features, rather than fully-supervised contour, can effectively improve the performance of the segmentation result. Deep contour self-supervision is automatically performed by the output of the network. The building block of our network was a densely connected block with separable convolutions, which made the network more compact and representative. Our proposed network successfully performed liver segmentation without deepening or widening the neural network, unlike those in the state-of-the-art methods. Much fewer parameters compared to other models resulted in good regularization effect.




\ifCLASSOPTIONcaptionsoff
  \newpage
\fi


\bibliographystyle{IEEEtran}
\bibliography{myBiB}



\end{document}




