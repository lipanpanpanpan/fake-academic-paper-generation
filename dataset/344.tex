
\documentclass{article}

\usepackage{microtype}
\usepackage{times}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{subfigure}
\usepackage{color}
\usepackage{url}
\usepackage{pdfsync}
\usepackage{booktabs} % for professional tables

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}
\def\etal{\emph{et al.}}
\def\ie{\emph{i.e.}}
\def\eg{\emph{e.g.}}

\graphicspath{{Figs/}}

\usepackage[accepted]{icml2018}


\begin{document}

\twocolumn[
\icmltitle{Latent Dirichlet Allocation in Generative Adversarial Networks}



\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Lili Pan}{to}
\icmlauthor{Shen Cheng}{to}
\icmlauthor{Jian Liu}{to}
\icmlauthor{Yazhou Ren}{to}
\icmlauthor{Zenglin Xu}{to}
\end{icmlauthorlist}

\icmlaffiliation{to}{University of Electronic Science and Technology of China, Chengdu, China}


\icmlcorrespondingauthor{Zenglin Xu}{zenglin@gmail.com}


\vskip 0.3in
]



\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution

\begin{abstract}
Mode collapse is one of the key challenges in the training of Generative Adversarial Networks(GANs).
Previous approaches have tried to address this challenge either by changing the loss of GANs, or by modifying optimization strategies.
We argue that it is more desirable if we can find the underlying structure of real data and build a structured generative model to further get rid of mode collapse.
To this end, we propose Latent Dirichlet Allocation based Generative Adversarial Networks (LDAGAN), which have high capacity of modeling complex image data.
Moreover, we optimize our model by combing varitional expectation-maximization (EM) algorithm with adversarial learning.
Stochastic optimization strategy ensures the training process of LDAGAN is not time consuming.
Experimental results demonstrate our method outperforms the existing standard CNN based GANs on the task of image generation.
\end{abstract}

\section{Introduction}
\label{sec:Intro}

Generative Adversarial Networks (GANs)~\cite{goodfellow2014generative} refer to a new type of learning algorithms that can map a random noise to a target distribution in the way of adversarial training.
GANs have gained considerable research interests recently for two main reasons.
On one hand, they open a door to learn a complex generative model for which no likelihood function is specified, but only a generative procedure~\cite{uehara2016generative,mohamed2017learning,nowozin2016f,tran2017deep}.
On the other hand, the ability to generate realistic images makes GANs achieve successes in many applications in computer vision, such as image and video generation~\cite{tran2017disentangled,bansal2018recycle,yang2017learning}, image-to-image translation~\cite{zhu2017unpaired,yi2017dualgan,liu2017unsupervised}, text-to-image generation~\cite{reed2016generative,zhang2017stackgan},  object detection~\cite{li2017perceptual}, and so on.

\begin{figure}[t]
  \centering
    \includegraphics[width=0.45\textwidth]{Fig1.pdf}
     \caption{Illustration of LDA based image generation process and its three corresponding components: (\romannumeral1) mode specific generators $G_k\left(\mathbf{z}',\bm{\theta}_k\right)$, (\romannumeral2) sub-modal distributions $p_{g_k}\left(\mathbf{x}\right)$, and (\romannumeral3) model distribution $p_g\left(\mathbf{x}\right)$. Here, $\bm{\pi}$ is mode distribution which has a Dirichlet prior.}
     \label{fig:1}
\end{figure}

Despite the successes, one well-recognized challenge in the training of GANs is the problem of mode collapse~\cite{salimans2016improved,nguyen2017dual,miyato2018spectral,tran2018dist,tolstikhin2017adagan}.
Many approaches have been proposed to tackle this problem with varying degrees of success.
Most of them can be roughly categorized into two categories: single-generator based and multiple-generators based, depending on the number of generators employed.
Single-generator based approaches, for example~\cite{nguyen2017dual,miyato2018spectral,salimans2016improved,tran2018dist}, try to modify the objective of GANs, or optimization strategies to prevent mode collapse.
They usually neglect the underlying data structure, rendering some modes dropping in the generation process.
Different from single-generator based approaches, multiple-generators based approaches, for example~\cite{hoang2018mgan,tolstikhin2017adagan,arora2017generalization}, use a mixture of generators to cover more modes of real data. Despite the successful applications in some scenarios, there lacks a systematic and effective way to mixing different generators -- simply mixing generators together may limit the model capacity.


To address this challenge,
this paper proposes to introduce Latent Dirichlet Allocation (LDA)~\cite{blei2003latent}, also known as topic model, into GANs (see Fig.~\ref{fig:1}), leading to a new model called LDAGAN.
The motivation of LDAGAN is two folds: (\romannumeral1) the mode distribution for each sample could be distinct with each other, thus using LDA to explicitly model the structured data generation process is a natural choice, and (\romannumeral2) deep neural networks in combination with LDA can result in a structured and implicit generative model which has high capacity of modelling complex image data.
Given the strong representation power of LDAGAN, a natural question in the inference step then arises: \emph{how to solve a three-level hierarchical deep Bayesian model in the framework of adversarial learning?}
To make efficient inference, we take an important step by utilizing variational expectation-maximization (EM) algorithm in the adversarial training.



The main contributions of this paper are summarized as follows:
(\romannumeral1) We introduce LDA into GANs to build a structured generative model, which is sufficiently flexible to model highly complex image data and to address the problem of mode collapse.
(\romannumeral2) To solve our model, perhaps remarkably, we combine variational EM algorithm with adversarial learning.  In particular, we propose a stochastic optimization strategy, coupled with parameter sharing network architecture, to ensure our LDAGAN can be learned efficiently.
(\romannumeral3) We achieve state-of-the-art performance on CIFAR-10~\cite{krizhevsky2009learning}, CIFAR-100~\cite{krizhevsky2009learning} and ImageNet~\cite{russakovsky2015imagenet} datasets. For example, our method has achieved a value of $28.9$ for Fr\'{e}chet Inception Distance on the ImageNet dataset, which is currently the best reported result with standard CNN architecture in literature to our knowledge.



\section{Related Work}
\label{sec:Previous}

This section reviews previous work on variants of Generative Adversarial Networks.
\begin{figure}
  \centering
    \includegraphics[width=0.45\textwidth]{Fig2.pdf}
     \caption{Graphical models for GANs and Mixture GANs. Random variables are denoted by open circles while deterministic parameters are denoted by smaller solid circles. The observed variables are denoted by shading the corresponding node. $\mathbf{z}'$ and $\mathbf{z}$ represent the input noise variables and latent mode of each generated sample respectively. Generators are parameterized by $\bm{\theta}$.}
     \label{fig:GANs}
\end{figure}

\subsection{Generative Adversarial Networks}
\label{sec:GANs}


Generative Adversarial Networks learn a generator $G$ that transforms input noise variables to target distribution.
Conventionally, $G$ is a nonlinear function, specified by a deep network with parameters $\bm{\theta}$.
Given the distribution of input noise variables $\mathbf{z}'$, the generator's distribution $p_g\left(\mathbf{x}\right)$ can be written as:
\begin{gather}
\mathbf{x} = G\left(\mathbf{z}'; \bm{\theta}\right);\quad \mathbf{z}'\sim p\left(\mathbf{z}'\right) \nonumber \\
p_g\left(\mathbf{x}\right)
=
\int
p\left(\mathbf{x}|\mathbf{z}', \bm{\theta}\right) p\left(\mathbf{z}'\right)
d\mathbf{z}'.
\label{eq:Back1}
\end{gather}
Here, the joint distribution is given by $p\left(\mathbf{x}|\mathbf{z}', \bm{\theta}\right) p\left(\mathbf{z}'\right)$, and the generator's distribution is obtained by integrating the joint distribution over all possible values of $\mathbf{z}'$.
Fig.~\ref{fig:GANs}a illustrates the graphical model for GANs.
Goodfellow \etal, in~\cite{goodfellow2014generative}, demonstrated the optimal solution for $\bm{\theta}$ is obtained when the distance between $p_g\left(\mathbf{x}\right)$ and $p_{data}\left(\mathbf{x}\right)$ is zero.
However, in fact, a single generator's distribution is not so flexible to cover all modes of real data, especially when $p_{data}\left(\mathbf{x}\right)$ is highly complex and multimodal.
The low model capacity may be one of the primary reasons to cause mode collapse/dropping.



One main difficulty in training GANs is how to avoid mode collapse whilst affording an efficient evaluation.
To account for this, many important variants of GANs have been proposed.
Salimans \etal~\cite{salimans2016improved} introduced several techniques into GANs training, including feature matching, minibatch discrimination, historical averaging and virtual batch normalization, to avoid mode collapse.
D2GAN optimizes both the KL and reverse KL divergences of real and generator's distributions simultaneously to overcome mode collapse~\cite{nguyen2017dual}.
WGAN leverages a smooth metric (\ie~Wasserstein distance) to measure the distance between two probability distributions to improve the stablity of GANs training~\cite{arjovsky2015wasserstein}.
WGAN-GP replaces the weight clipping in WGAN with penalizing the gradient norm of the interpolated samples to achieve more stable performance~\cite{gulrajani2017improved}.
WGAN-GP+TURR uses a two time-scale update rule for training GANs to guarantee it converges to a stationary local Nash equilibrium~\cite{heusel2017gans}.
Furthermore, in~\cite{heusel2017gans}, Fr\'{e}chet inception distance was proposed for evaluating mode collapse more effectively.
Recently, Miyato \etal~\cite{miyato2018spectral} have applied spectral normalization to stabilize training the discriminator of SNGANs, rendering generated samples more diverse.
Although these improvements in GANs may overcome mode collapse somewhat, their performance tends to be unsatisfactory when real data distributions are highly complex.


\subsection{Mixture Generative Adversarial Networks}
\label{sec:MixGANs}


To account for the drawback of single generator based GANs, in~\cite{hoang2018mgan,tolstikhin2017adagan,arora2017generalization}, mixture GANs train multiple generators to capture different mode of data.
When mixed, these generators have the potential to cover more modes of real data.
For example, AdaGAN and boosting-inspired GANs learn to generate samples of some modes with one generator and then remove samples of the same modes in training set to train a next generator~\cite{tolstikhin2017adagan}.
To simplify this process, Mix-GAN and MGAN suppose all generators together induce a mixture of sub-modal distributions~\cite{hoang2018mgan,arora2017generalization}.
The graphical model for mixture GANs is shown in Fig.~\ref{fig:GANs}b.
Here, the binary latent variables $\mathbf{z}$ indicate the underlying mode of each sample, where $\mathbf{z}$ has a multinomial prior $\text{Mult}\left(\bm{\pi}\right)$ and $\bm{\pi}=\left[\pi_1,...,\pi_K\right]$.
The generator's distribution then takes the form:
\begin{equation}
p_g\left(\mathbf{x}\right)
=
\sum_{\mathbf{z}} p\left(\mathbf{z}|\bm{\pi}\right) p_g\left(\mathbf{x}|\mathbf{z}\right),
\label{eq:Back2}
\end{equation}
where
\begin{equation}
p_g\left(\mathbf{x}|\mathbf{z}\right) = \int
p\left(\mathbf{x}|\mathbf{z}, \mathbf{z}', \bm{\theta}\right) p\left(\mathbf{z}'\right)
d\mathbf{z}',
\label{eq:Back3}
\end{equation}
and $p_g\left(\mathbf{x}|\mathbf{z}\right)$ is the generator's distribution conditioned on mode $\mathbf{z}$, namely sub-modal distribution.
Denoting the sub-modal distributions and mixing coefficients in Eq.~(\ref{eq:Back2}) respectively by $p_{g_k}\left(\mathbf{x}\right)$ and $\pi_k$, the model distribution $p_g\left(\mathbf{x}\right)$ can be written as linear superposition of sub-modal distributions in the form $\sum_{k=1}^K\pi_k p_{g_k}\left(\mathbf{x}\right)$.
Here,  $K$ is the total number of generators, each of which is $G_k$ and parameterized with $\bm{\theta}_k$, and $\bm{\theta}=\left\{\bm{\theta}_1,...,\bm{\theta}_K\right\}$.
Since the model distribution $p_g\left(\mathbf{x}\right)$ is a mixture of sub-modal distributions, it is prone to be more complex and flexible than a single generator's distribution.
This permits $p_g\left(\mathbf{x}\right)$ to become more close to complex real data distribution, making mixture GANs generate more diverse samples


Although mixture GANs seem to further get rid of problems like mode collapse, they exhibit two drawbacks.
Firstly, mixing weights (\ie~mode distribution) $\bm{\pi}$ in mixture GANs, for example in MGAN and MixGAN~\cite{hoang2018mgan,arora2017generalization}, are fixed.
Such a fixed mixing scheme limits the flexibility of model distribution $p_g\left(\mathbf{x}\right)$, probably leading to an undesired large divergence between real and model distributions.
Besides, $\bm{\pi}$ sometimes is predefined, such as in~\cite{hoang2018mgan}, which constraints the model capacity, and thus causes mode dropping.
Secondly, some mixture GANs~\cite{hoang2018mgan} plausibly encourage mode diversity of generated samples, resulting in intra-class mode dropping.



\section{Latent Dirichlet Allocation in GANs}
\label{sec:LDAGAN}
\begin{figure}
  \centering
    \includegraphics[width=0.21\textwidth]{Fig3.pdf}
     \caption{Graphical model for LDAGAN. It is a three-level hierarchical deep Bayesian model. Latent variables $\bm{\pi}$ describe mode distribution which have a Dirichlet prior $\text{Dir}(\bm{\alpha})$. $\mathbf{z}'$ and $\mathbf{z}$ represent the input noise variables and latent mode of each sample respectively. The parameters $\bm{\theta}=\left\{\bm{\theta}_1,...,\bm{\theta}_K\right\}$ are associated with $K$ generators $G=\left\{G_1,...,G_K\right\}$. }
     \label{fig:LDA-GAN}
\end{figure}

Rather than mixing generators simply, we consider here the use of Latent Dirichlet Allocation (LDA)~\cite{blei2003latent} to build a structured generative model, which has high capacity of modelling complex image data.
Specifically, we define the following data generation process:
\begin{enumerate}
\item Choose mode distribution $\bm{\pi}\sim\text{Dir}(\bm{\alpha})$.
\item Choose a mode $\mathbf{z}\sim\text{Mult}(\bm{\pi})$.
\item Choose $\mathbf{z}'$ from the prior distribution $p\left(\mathbf{z}'\right)$, and generate a sample $\mathbf{x}=G_{k}\left(\mathbf{z}'; \bm{\theta}_k\right)$ if $z_k=1$.
\end{enumerate}
The graphical model representing this process is illustrated in Fig.~\ref{fig:LDA-GAN}.
Here, $z_k=1$ indicates which mode was chosen for generating sample $\mathbf{x}$, where $z_k\in\left\{0,1\right\}$ and $\sum_k z_k=1$.
That is, for each input noise $\mathbf{z}'$, we choose a mode specific generator $G_k$ parameterized by $\bm{\theta}_k$ for generation.
Then, each $G_k$ transforms all possible input noises $\mathbf{z}'$ to target samples and yields a sub-modal distribution $p_{g_k}\left(\mathbf{x}\right)$, also denoted by $p_g\left(\mathbf{x}|\mathbf{z}\right)$.
Summing and integrating the joint distribution, given by $p\left(\bm{\pi}|\bm{\alpha}\right)p\left(\mathbf{z}|\bm{\pi}\right) p_g\left(\mathbf{x}|\mathbf{z}\right)$, over $\mathbf{z}$ and $\bm{\pi}$, gives the following model distribution:
\begin{equation}
p_g\left(\mathbf{x}\right)
=
\int p\left(\bm{\pi}|\bm{\alpha}\right)\left(\sum_{\mathbf{z}} p\left(\mathbf{z}|\bm{\pi}\right) p_g\left(\mathbf{x}|\mathbf{z}\right) \right)d\bm{\pi},
\label{eq:LDAGAN1}
\end{equation}
where $\bm{\alpha}$ is the parameters of the Dirichlet prior with components $\alpha_k>0$.
\noindent The $K$-dimensional mode distribution parameters $\bm{\pi}$ have the following probability density:
\begin{equation}
p\left( \bm{\pi}|\bm{\alpha} \right)
=
\frac{\Gamma\left( \sum_{k=1}^K\alpha_k \right)}
{\prod_{k=1}^K\Gamma\left( \alpha_k \right)}
\pi_1^{\alpha_1-1}... \pi_K^{\alpha_K-1},
\label{eq:LDAGAN2}
\end{equation}
where $\Gamma\left(\cdot\right)$ is the Gamma function.
Since LDA is a three-level hierarchical Bayesian model, the mode distribution $\bm{\pi}$ changes to be variables and could take different values for different samples.
Owing to this, when compared with mixture model, LDA is an attractive choice to describe structured data as it gives much more flexibility.
This enables the model distribution $p_g\left(\mathbf{x}\right)$ (see Eq.~(\ref{eq:LDAGAN1})) has the potential to be as close as possible to $p_{data}\left(\mathbf{x}\right)$.
Introducing LDA into GANs is one of the main contributions of this paper.


The objective for learning $\bm{\theta}$ and $\bm{\alpha}$ is not explicit as there is no likelihood function being specified, only a generating procedure.
However, adversarial learning opens a door to solve this problem.
$\bm{\theta}$ and $\bm{\alpha}$ can be optimized through minimizing the divergence between model and real distributions.
In the work~\cite{mohamed2017learning}, Mohamed \etal~revealed training a discriminator in GANs is equivalent to training a good estimator to measure the distance between two distributions.
As such, we use a discriminator $D\left(\mathbf{x}; \bm{\phi}\right)$, a function bounded in $\left[0,1\right]$ with parameters $\bm{\phi}$, to output the probability of $\mathbf{x}$ belonging to real data, denoted by $p\left(y=1|\mathbf{x},\bm{\phi}\right)$.
Here, the binary variable $y$ indicates whether $\mathbf{x}$ is real or fake.
We hope $D$ to maximize the probability of assigning the correct label to both the real samples and samples generated from $\mathbf{z}'$.
Meanwhile, we hope $\bm{\alpha}$ and $\bm{\theta}$ to minimize $\log \left( 1-p\left( y=1 | \mathbf{z}',\bm{\theta},\bm{\alpha},\bm{\phi} \right)\right)$.
Then, the objective function of LDAGAN takes the form:
\begin{multline}
\min_{\bm{\theta}, \bm{\alpha}}
\max_{\bm{\phi}}
\mathbb{E}_{\mathbf{x} \sim p_{data}\left(\mathbf{x}\right)}
\left[ \log p\left( y=1 | \mathbf{x}, \bm{\phi} \right) \right]\\
+\mathbb{E}_{\mathbf{z}' \sim p\left(\mathbf{z}'\right)}
\left[ \log \left( 1-p\left( y=1 | \mathbf{z}',\bm{\theta},\bm{\alpha},\bm{\phi} \right)\right) \right].
\label{eq:LDAGAN3}
\end{multline}
\noindent Compared with traditional GANs, LDAGAN has the advantage that it learns the underlying data structure, along with generators simultaneously.
Since one of the main problems of GANs is the ignorance of data structure in generation, then LDAGAN can be expected to produce more diverse samples and overcome mode collapse further.


\section{Learning}
\label{sec:Learning}

This section describes the learning of the discriminator, generators and Dirichlet parameters $\bm{\alpha}$ in LDAGAN.
\subsection{Discriminators}
\label{sec:Discriminator}

We extract the terms only containing $\bm{\phi}$ from Eq.~(\ref{eq:LDAGAN3}) to construct the discriminative loss:
\begin{multline}
\label{eq:Learning-D1}
\max_{\bm{\phi}}
\mathbb{E}_{\mathbf{x} \sim p_{data}\left(\mathbf{x}\right)}
\mathbb{E}\left[ \log p\left( y=1 | \mathbf{x},\bm{\phi} \right) \right] \\
+\mathbb{E}_{\mathbf{z}' \sim p\left(\mathbf{z}'\right)}
\mathbb{E}\left[ \log\left( 1-p\left( y=1 | \mathbf{z}',\bm{\theta},\bm{\alpha},\bm{\phi} \right) \right) \right],
\end{multline}
where $p\left( y=1 | \mathbf{z}',\bm{\theta},\bm{\alpha},\bm{\phi} \right)$ is a marginal probability, representing the probability of the sample generated from $\mathbf{z}'$ being real.
It is obtained by integrating joint distribution over $\bm{\pi}$ and summing over $\mathbf{z}$:
\begin{equation}
\int p\left(\bm{\pi} | \bm{\alpha}\right)
\left(\sum_{\mathbf{z}}
p\left(\mathbf{z}|\bm{\pi}\right)
p\left(y=1|\mathbf{z}, \mathbf{z}', \bm{\theta}, \bm{\phi}\right)
\right)d\bm{\pi}.
\label{eq:Learning-D2}
\end{equation}
Obviously, the joint distribution is given as a product of conditionals in the form $p\left(\bm{\pi} | \bm{\alpha}\right)p\left(\mathbf{z}|\bm{\pi}\right)p\left(y=1|\mathbf{z}, \mathbf{z}', \bm{\theta}, \bm{\phi}\right)$.
$p\left(y=1|\mathbf{z}, \mathbf{z}', \bm{\theta}, \bm{\phi}\right)$ here denotes, given the underlying mode (\ie~$z_k=1$), the probability of the synthetic sample being real.
We utilize $ D\left(G_k\left(\mathbf{z}'\right)\right)$ to score this probability.
Substituting Eq.~(\ref{eq:Learning-D2}) back into the discriminative loss, the parameters $\bm{\phi}$ can be optimized after sampling since the integration over $\bm{\pi}$ is analytically intractable.
One note that despite we employs $K$ generators in our LDAGAN, we use only one discriminator.
This is because one discriminator helps to keep the balance of generators in training.

\subsection{Generators and $\bm{\alpha}$}
In this section, we turn to the optimization of generators and the Dirichlet parameters $\bm{\alpha}$.

\subsubsection{Generative Loss}
\label{sec:GeLoss}

Considering the terms related to generation in Eq.~(\ref{eq:LDAGAN3}), minimizing $\log\left(1-p\left(y=1|\mathbf{z}',\bm{\theta},\bm{\alpha}, \bm{\phi}\right)\right)$ with respect to $\bm{\theta}$ and $\bm{\alpha}$ is equivalent to maximizing $\log p\left(y=1|\mathbf{z}',\bm{\theta},\bm{\alpha}, \bm{\phi}\right)$ with respect to them.
Then, we rewrite the generative loss in the form:
\begin{equation}
\max_{\bm{\theta},\bm{\alpha}}
\mathbb{E}_{\mathbf{z}' \sim p \left( \mathbf{z}' \right)}
\left[ \log p\left( y=1 | \mathbf{z}',\bm{\theta},\bm{\alpha}, \bm{\phi} \right) \right],
\label{eq:Learning-G1}
\end{equation}
which means maximizing the likelihood of the samples generated from $\mathbf{z}'$ being real.
The maximum can be achieved if, and only if, our generative model finds the underlying structure of real data correctly and each generator models data of each mode appropriately.
In fact, maximizing the above likelihood is nontrivial for two reason:
(\romannumeral1) it is a deep likelihood, and
(\romannumeral2) it includes discrete and continuous latent variables $\mathbf{z}$ and $\bm{\pi}$.
In such a case, we theoretically propose to \emph{use the so called variational EM algorithm to maximize the deep likelihood}.



\subsubsection{Variational EM Algorithm}
\label{sec:EM}

The EM algorithm provides an useful way to find maximum likelihood solutions for probabilistic models having latent variables.
In general, it consists of three items, namely: variational distribution, E-step optimization and and M-step optimization.


\noindent\textbf{Variational Distribution:}
On the basis of the mean-field approximation, we define a variational distribution $q\left(\bm{\pi},\mathbf{z}|\bm{\gamma},\bm{\omega}\right)$ over the latent variables $\bm{\pi}$ and $\bm{z}$:
\begin{equation}
\label{eq:Learning-G2}
q\left(\bm{\pi}, \mathbf{z}|\bm{\gamma},\bm{\omega}\right)
= q\left(\bm{\pi}|\bm{\gamma}\right)
q\left(\mathbf{z}|\bm{\omega}\right),
\end{equation}
\noindent where $\bm{\gamma}$ is the Dirichlet parameters and $\bm{\omega}$ is the multinomial parameters.
Furthermore, we decompose the log likelihood function in Eq.~(\ref{eq:Learning-G1}) into the sum of lower bound function and KL divergence:
\begin{multline}
\log p\left(y=1|\mathbf{z}',\bm{\theta},\bm{\alpha}, \bm{\phi}\right)
=
L\left( \bm{\gamma},\bm{\omega};\bm{\alpha},\bm{\theta} \right) \\
+ \text{KL}\left( q\left( \bm{\pi},\mathbf{z} | \bm{\gamma},\bm{\omega} \right)
|| p\left( \bm{\pi}, \mathbf{z}|y=1, \mathbf{z}',\bm{\theta},\bm{\alpha},\bm{\phi} \right) \right).
\label{eq:Learning-G3}
\end{multline}
Here $L\left( \bm{\gamma},\bm{\omega};\bm{\theta},\bm{\alpha} \right)$ is the lower bound function on $\log p\left(y=1|\mathbf{z}',\bm{\theta},\bm{\alpha}, \bm{\phi}\right)$, which is a function of variational parameters $\bm{\gamma}$ and $\bm{\omega}$, and also a function of the parameters $\bm{\theta}$ and $\bm{\alpha}$.
The detailed derivation of this decomposition can be found in Appendix A.
It is easily verified that the lower bound $L\left( \bm{\gamma},\bm{\omega};\bm{\theta},\bm{\alpha} \right)$ is maximized when the KL divergence vanishes.
From this perspective, according to Eq.~(\ref{eq:Learning-G3}), the variational distribution can be viewed as an approximation to the posterior distribution $p\left( \bm{\pi}, \mathbf{z}|y=1, \mathbf{z}',\bm{\theta},\bm{\alpha},\bm{\phi} \right)$.



\noindent\textbf{E-Step Optimization:}
The variational EM algorithm is a two-stage iterative optimization algorithm.
The E-step involves maximizing the lower bound with respect to the variational parameters $\bm{\gamma}$ and $\bm{\omega}$.
When model parameters $\bm{\theta}$ and $\bm{\alpha}$ are fixed, computing the derivatives of $L\left( \bm{\gamma},\bm{\omega};\bm{\theta},\bm{\alpha} \right)$ with respect to $\omega_k$ and $\gamma_k$, and setting them equal to zeros yields the following formulations for variational parameters updating (see Appendix A),
\begin{equation}
\label{eq:Learning-G4}
\omega_k\propto
D\left( G_k\left( \mathbf{z}'\right) \right)\exp\left( \Psi\left(\gamma_k\right) - \Psi \left( \sum\nolimits_{j=1}^K \gamma_j\right) \right),
\end{equation}
\begin{equation}
\label{eq:Learning-G5}
\gamma_k
=
\alpha_k
+
\omega_k,
\end{equation}
where $\Psi\left(\cdot\right)$ is known as the digamma function, and $\omega_k$ is the $k^{th}$ elements of $\bm{\omega}$ that should be normalized to make $\sum_{k=1}^K\omega_k=1$.
Here, $D\left(G_k\left(\mathbf{z}'\right)\right)$ denotes the likelihood term, which reflects the probability of the sample generated from $\mathbf{z}'$, given underlying mode $\mathbf{z}$ (\ie~$z_k=1$), being real.
$\exp\left( \Psi\left(\gamma_k\right) - \Psi \left( \sum\nolimits_{j=1}^K \gamma_j\right)\right)$ is related to the prior, where $\Psi\left(\gamma_k\right) - \Psi \left( \sum\nolimits_{j=1}^K \gamma_j \right) = \mathbb{E}_q\left[\log \pi_k \right]$.
As such, the multinomial update can be though of as a posterior multinomial according to Bayes' theorem.
Similarly, the Dirichlet update, shown in Eq.~(\ref{eq:Learning-G5}), can be viewed as a posterior Drichlet.
Then, updating $\omega_k$ and $\gamma_k$ alternatively until some convergence criterion is met,
results in optimal $\bm{\omega}$ and $\bm{\gamma}$ which maximize the above lower bound.
One note that the variational parameters vary as a function of $\mathbf{z}'$, and thus we rewrite them in the form $\bm{\gamma}\left(\mathbf{z}'\right)$ and $\bm{\omega}\left(\mathbf{z}'\right)$.



\noindent\textbf{M-Step Optimization:} In the subsequent M-step, with $\bm{\gamma}$ and $\bm{\omega}$ fixed, we maximize the expected lower bound $\mathcal{L}\left( \bm{\gamma},\bm{\omega};\bm{\theta},\bm{\alpha} \right)$ with respect to $\bm{\theta}$ and $\bm{\alpha}$.
It should be emphasized that the symbol $\mathcal{L}$ here denotes the expectation of the lower bound $L$, that is, $L$ is only associated with one sample $\mathbf{z}'$, while $\mathcal{L}$ is the expectation averaging over all samples.



Maximizing $\mathcal{L}\left( \bm{\gamma},\bm{\omega};\bm{\theta},\bm{\alpha} \right)$ with respect to $\bm{\theta}_k$ yields:
\begin{equation}
\max_{\bm{\theta}_k}
\mathbb{E}_{\mathbf{z}' \sim p_{\mathbf{z}'}}
\left[
\omega_k\left( \mathbf{z}' \right)
\log D\left( G_k\left( \mathbf{z}' \right) \right)
\right],
\label{eq:Learning-G6}
\end{equation}
where $\bm{\theta}_k$ is the parameters of generator $G_k$, and $\omega_k\left( \mathbf{z}' \right)$ is a posterior approximation.
$\omega_k\left( \mathbf{z}' \right)$ approximates the posterior probability of generated sample, under the assumption of being real, being generated by $G_k$.
Therefore, an appealing intuitive explanation for Eq.~(\ref{eq:Learning-G6}) is that each sample, when optimizing $G_k$, is weighted, guiding each generator to give more considerations to the 'good' samples.
Moreover, $G_k$ share their parameters with each other except the first layer, largely reducing the parameter number and improving training efficiency.

The terms in $\mathcal{L}\left( \bm{\gamma},\bm{\omega};\bm{\theta},\bm{\alpha} \right)$ related to $\bm{\alpha}$ takes the form:
\begin{multline}
\mathcal{L}_{\bm{\alpha}}
=
\log \Gamma \left( \sum\nolimits_{j=1}^K \alpha_j \right)
-\sum_{k=1}^K \log \Gamma \left( \alpha_k \right)
+
\mathbb{E}_{\mathbf{z}' \sim p\left(\mathbf{z}'\right)}
\\
\left[
\sum_{k=1}^K
\left( \alpha_k-1 \right)
\left( \Psi\left(\gamma_k\left(\mathbf{z}'\right)\right)
-\Psi\left(\sum\nolimits_{j=1}^K\gamma_j\left(\mathbf{z}'\right)\right) \right)
\right].
\label{eq:Learning-G7}
\end{multline}
Taking the derivative with respect to $\bm{\alpha}$ and using gradient ascending, we will finally obtain the optimal solution for $\bm{\alpha}$.


\section{Stochastic Optimization}
\label{Stoch}

\begin{algorithm}[t]
   \caption{Minibatch stochastic optimization for LDAGAN.}
   \label{alg:Learning}
\begin{algorithmic}[1]

   \REQUIRE Initialize $\bm{\theta}$, $\bm{\alpha}$, and $\bm{\phi}$.
   \FOR{number in training iterations}
   \STATE Sample $M$ generated examples $\left\{\mathbf{x}'_m\right\}_{m=1}^M$.
   \STATE Sample $M$ real examples $\left\{\mathbf{x}_m\right\}_{m=1}^M$.
   \STATE Update $D$ by ascending gradient:
   \STATE{$\nabla_{\bm{\phi}}\frac{1}{M}\sum_{m=1}^M\left\{ \log D\left(\mathbf{x}_m\right) + \log \left(1-D\left(\mathbf{x}'_m\right)\right)\right\}$}.
   \STATE Sample $M$ noise samples $\left\{\mathbf{z}'_m\right\}_{m=1}^M$.
   \REPEAT
      \STATE Calculate $\bm{\omega}\left(\mathbf{z}'_m\right)$ in Eq.~(\ref{eq:Learning-G4}), $m=1,..,M$.
      \STATE Calculate $\bm{\gamma}\left(\mathbf{z}'_m\right)$ in Eq.~(\ref{eq:Learning-G5}), $m=1,..,M$.
   \UNTIL $\bm{\omega}\left(\mathbf{z}'_m\right)$ and $\bm{\gamma}\left(\mathbf{z}'_m\right)$ converge
   \STATE Update $\left\{G_k\right\}_{k=1}^K$ by ascending gradient:
   \STATE{$\nabla_{\bm{\theta}_k}\frac{1}{M}\sum_{m=1}^M\left\{\omega_k\left(\mathbf{z}'_m \right) \log D\left( G_k\left( \mathbf{z}'_m \right)\right)\right\}$}.
   \STATE Update $\bm{\alpha}$ by ascending gradient of Eq.~(\ref{eq:Learning-G7}).
   \ENDFOR
\end{algorithmic}
\end{algorithm}


The learning of LDAGAN is a two step alternative optimization procedure: (\romannumeral1) update discriminator, and (\romannumeral2) use EM algorithm to maximize a deep likelihood with respect to $\bm{\theta}$ and $\bm{\alpha}$.
Such a procedure appears to be reasonable, but it is unclear how to incorporate the EM algorithm into this adversarial learning framework.
Inspired by stochastic variational inference~\cite{hoffman2013stochastic}, we propose to update $\bm{\theta}$ and $\bm{\alpha}$ as well as variational parameters based on only a subset of the training data, namely stochastic minibatch optimization.
At each iteration, we take only one step in the gradient direction.
The complete learning procedure is outlined in Algorithm~\ref{alg:Learning}.
This differs from standard GANs optimization only in that the variational parameters as well as Dirichlet parameters $\bm{\alpha}$ need to be updated at each iteration.
The related calculation is not time consuming, and hence the whole procedure should be fast.
We empirically show this in Sec.~\ref{sec:Comp}





\section{Experiments}
\label{sec:Experiments}
We carried out experiments on both synthetic data and real-world data.

\subsection{Synthetic Data}
We first evaluated the generative performance of LDAGAN on a synthetic dataset with 3 types of data.
The fist type of training data was sampled from a 2D mixture of 8 isotopic Gaussians, where the mixing weights are fixed to be 0.125.
The means of all Gaussians uniformly distribute on a circle, the center and radius of which are $\mathbf{0}$ and 2.0, and the covariance matrices of all Gaussians are all $0.08\mathbf{I}$.
The second type of training data was drawn from the distribution of a LDA generative model, where $\bm{\pi}$ had a Dirichlet prior with $\bm{\alpha} = [8,4,...,8,4]$.
All the sub-modal distributions are the same Gaussians as above.
The third type of training data is similar to the second, but consists of different Gaussians.
The means of all Gaussians distribute on a circle with a much smaller radius $0.5$ and the covariance matrices of all Gaussians are $0.02\mathbf{I}$.
Each type of training data is shown in different columns of Fig.~\ref{fig:Synth}.


\begin{figure}
  \centering
    \includegraphics[width=0.48\textwidth]{Fig4.pdf}
     \caption{The generative performance comparison of LDAGAN and other GANs on synthetic data. Red points denote real data and blue points denote data generated by GANs. In MGAN and LDAGAN, points in different color mean data generated by different generators.}
     \label{fig:Synth}
\end{figure}

On this dataset we compared four types of GANs: (\romannumeral1)  GANs, (\romannumeral3)  MGAN, and (\romannumeral4)  LDAGAN proposed in Sec.~\ref{sec:LDAGAN}.
For MGANs, we employed 8 generators, each of which was designed to have an input layer with 256 units and two fully connected hidden layers with 128 ReLU units.
The network architectures of discriminator and classifier in MGAN were constructed following~\cite{hoang2018mgan}.
For LDAGAN, we also employed 8 generators and each had the same input layer as MGAN, but only one fully connected hidden layer with 128 ReLU units.
The Dirichlet parameters $\bm{\alpha}$ were all initialized to be 2.


Visible results of these experiments can be found in Fig.~\ref{fig:Synth}.
It shows the fitting results of 512 samples generated by LDAGAN and other baseline methods.
A consistent trend behind this is that LDAGAN captures data modes more precisely than MGAN and other GANs.
As discussed in Sec.~\ref{sec:Previous} and Sec.~\ref{sec:LDAGAN}, this is due to its ability to learn underlying structure of real data and optimize generators based on the learned structure.
On the other hand, the model capacity of LDAGAN is much higher than other GANs, enabling it to better fit data.
Because MGAN simply mix generators together and encourage mode diversity of synthetic samples, incurring intra-class mode dropping problem.
Similarly, single generator based GANs and WGAN-GP fails to account for capturing all modes of data, leading to mode collapse.


\subsection{Real-Word Data}

Testing the performance of LDAGAN on real word data is more meaningful as it gives a better indication on how well the method generalizes.
To further evaluate the effectiveness of LDAGAN, we tested it on large-scale real word datasets.

\begin{figure*}
  \centering
    \subfigure[CIFAR-10]{\includegraphics[width=0.3\textwidth]{CIFAR10.png}}
    \hspace{.15in}
    \subfigure[CIFAR-100]{\includegraphics[width=0.3\textwidth]{CIFAR100.png}}
    \hspace{.15in}
    \subfigure[ImageNet]{\includegraphics[width=0.3\textwidth]{ImageNet.png}}
     \caption{Images (with size $32\times32$) generated by different generators of LDAGAN. Each row corresponds to one generator (\ie~mode). \textbf{(a):} Trained on CIFAR-10. The images generated by the same generator have highly similarity, for example, the 'car' images in the 2rd row, the 'dog' images in the 4th row, and the 'ship' images in the last row.  \textbf{(b):} Trained on CIFAR-100. Obvious image similarity can be found in the same row, such as the 2rd and 7th rows. \textbf{(c):} Trained on ImageNet.}
     \label{fig:Struc}
\end{figure*}

\subsubsection{Datasets}
\label{sec:Datasets}

We used 3 challenging real-word datasets to demonstrate the effectiveness of our proposed LDAGAN.
The details of the three datasets are described as follows:

\noindent\textbf{CIFAR-10}: It has 60,000 labeled $32\times32$-sized RGB natural images in 10 classless.
The 10 classes include: airplane, automobile, bird, cat, deer, dog, frog, horse, ship and truck.
There are 50000 training images.

\noindent\textbf{CIFAR-100}: It is just like the CIFAR-10, but it has much more diverse classes.
It has 100 classes containing 600 images each.
There are 50000 training images.

\noindent\textbf{ImageNet}:  It contains over 1.4 million images of 1000 classes.
It is the largest,  most diverse, and most significant visual dataset at present.
To conduct fair comparison with the baselines, we resized images to $32\times32$.


\subsubsection{Evaluation Metrics}
\label{sec:EvaMetrics}

On the above three datasets, we used inception score and Fr\'{e}chet inception distance for performance evaluation.


\noindent\textbf{Inception Score}: For quantitative evaluation, we used the Inception score as in~\cite{salimans2016improved}.
\begin{equation}
\mathrm{IS} \left(G\right)
=
\exp
\left(
\mathbb{E}_{\mathbf{x}\sim p_g\left(\mathbf{x}\right)}
KL\left(p\left(y|\mathbf{x}\right) || p\left(y\right)\right)
\right),
\end{equation}
where $p\left(y\right)$ is the marginal class distribution, and $p\left(y|\mathbf{x}\right)$ is the conditional class distribution.
If the generated images contain clear objects and have a high diversity, the corresponding inception score (IS) results in a large value.
The inception scores were calculated for 10 partitions of 50,000 randomly generated samples for each model in our experiments.
We recorded the average inception score.

\textbf{Fr\'{e}chet Inception Distance}: The drawback of inception score is that it does not
compare the statistics of real samples and generated samples.
Fr\'{e}chet Inception Distance (FID) proposed in~\cite{heusel2017gans} compares the distributions of inception embeddings of real data distribution $p_{data}\left(\mathbf{x}\right)$ and model distribution $p_g\left(\mathbf{x}\right)$.
The distribution of inception embeddings are all modeled as Gaussians with the mean and covariance $\left(\mathbf{m}_r, \mathbf{C}_r\right)$ and $\left(\mathbf{m}_g,\mathbf{C}_g\right)$.
The distance measure is:
\begin{multline}
d^2\left(\left(\mathbf{m}_r, \mathbf{C}_r\right),\left(\mathbf{m}_g, \mathbf{C}_g\right)\right)
=
\|\mathbf{m}_r-\mathbf{m}_g\|_2^2  \\
+
\text{Tr}\left(\mathbf{C}_r + \mathbf{C}_g - 2\left(\mathbf{C}_r\mathbf{C}_g\right)^{\frac{1}{2}}\right).
\end{multline}
Large FID value indicates more realistic and diverse generated images.
It has the capability of detecting the intra-class mode dropping.

\subsubsection{Model Architecture}
\label{sec:Architec}

Our generator and discriminator architectures follow the design of DCGANs.
Moreover, all generators share parameters except the first layer.
This parameter sharing scheme helps to balance the learning of generators since we have only one discriminator.
Considering the declining problem of active neurons, we fixed the batch normalization center to be zero for all layers in the generator networks as in~\cite{hoang2018mgan}.
Please see Appendix B for the details.
\begin{table*}[h]
\caption{Inception scores on different datasets.}
\label{Tab:IS}
\centering
\addtolength{\tabcolsep}{-0pt}{\begin{tabular}{lccc}
\hline
Model & CIFAR-10 & CIFAR-100 & ImageNet\tabularnewline
\hline
\hline
 Real data    &     $11.24$    &   $14.90$     &    $25.78$     \tabularnewline

WGAN~\cite{arjovsky2015wasserstein}          &  $3.82$        &        -              &    -     \tabularnewline

MIX+WGAN~\cite{arora2017generalization}      &  $4.04$        &         -             &    -     \tabularnewline

BEGAN~\cite{berthelotSM2017began}         &    $5.62$      &         -             &    -     \tabularnewline

DCGAN~\cite{radford2015unsupervised}         &  $6.40$        &         -             &  $7.89$    \tabularnewline

D2GAN~\cite{yi2017dualgan}         &  $7.15$        &         -             & $8.25$ \tabularnewline

WGAN-GP~\cite{gulrajani2017improved}       &  $6.68$        &         -             &    -     \tabularnewline

SNGAN~\cite{miyato2018spectral}         &   $7.42$       &         -             &    -     \tabularnewline


MGAN~\cite{hoang2018mgan}         &   $7.30$       &    $\mathbf{7.67}$     &   $7.30$   \tabularnewline

LDAGAN       &$\mathbf{7.46}$ &       $7.50$           &   $\mathbf{8.37}$  \tabularnewline
 \hline
\end{tabular}}
\end{table*}

\subsubsection{Parameter and Hyperparameter Settings}
\label{sec:Para}

The hyperparameters of LDAGANs includes: the number of generators $K$ and the minibatch size.
The number of generators was set to be 10 on CIFAR-10 and CIFAR-100, and 20 on ImageNet.
We also trained models with 10, 27, 30 generators on ImageNet, and observed the best performance was achieved by the model with 20 generators (see  Appendix D).
We used a minibatch size of 24 for each generator on CIFAR-10 and CIFAR-100, and 12 on ImageNet.
The learning rate was set to be 0.0001.
The Dirichlet parameters $\bm{\alpha}$ were initialized to be 8.
Due to the importance of parameter sharing scheme in LDAGAN, we investigated how sharing scheme impacted its performance.
The best inception score and FID were exhibited when we removed the parameter sharing in the first hidden layer.
When we removed parameter sharing in more layers, these two measures became worse.
The reason behind this may be parameter sharing, to a certain extent, allows a trade-off between flexibility and balance of generators.
More details can be found in Appendix C.



\subsubsection{Underlying Mode Finding}
\label{sec:Struc}

Example images generated by LDAGAN can be found in Fig.~\ref{fig:Struc}.
The results show a consistent phenomenon that images in one row, which are generated by the same generator, have highly similarity.
For exapmle, in Fig.~\ref{fig:Struc}a, we observe the 'car' images in the second row, the 'dog' images in the fourth row, and the 'ship' images in the last row.
As discussed in Sec.~\ref{sec:LDAGAN}, this is due to LDAGAN's capability to find the underlying structure (\ie~mode) of real data and guide multiple generators to fit the structured data.
The similar phenomenon can also be found in both Fig.~\ref{fig:Struc}b and Fig.~\ref{fig:Struc}c.

\subsubsection{Comparison}
\label{sec:Comp}


On the real-word datasets, we compared LDAGAN with some other state-of-the-art GANs.
For MGAN, we set the number of generators to be 10 and set all the other hyper-parameters following~\cite{hoang2018mgan}.
For SNGANs and DistGAN, we reported the result on CNN architecture for fair comparison\footnote[1]{Note that, we used CNN based network architecture and standard loss in LDAGAN. Therefore, we compared with SNGANs and DistGANs having the same architecture and loss here. As reported in~\cite{miyato2018spectral}, SNGANs adopting ResNet and hinge loss performance even better, where the inception score and FID are 8.22 and 21.7 respectively.}.
For LDAGAN, we set the parameters and hyper-parameters as described in Sec.~\ref{sec:Para}.
One might consider discriminator's output likelihood is often inaccurate and unstable, but in fact this only appears obviously early in learning.
We fixed the variational parameters $\bm{\omega}$ and $\bm{\gamma}$ in the beginning epochs, and kept them updating after that (see Appendix E).

\noindent\textbf{Inception Score Comporison:} Results of the IS comparison in Tab.~\ref{Tab:IS} show a comparable performance of LDAGAN.
On CIFAR-10 and ImageNet, we achieved an inception score of 7.46 and 8.37, which outperformed MGAN and other single-generator based GANs.
This owes to the multiple mode-specific generators in LDAGAN, where each is an expert for image generation of that mode.
For MGAN, we ran the codes released by the author, but had not obtain the IS 8.33 and 9.32 on CIFAR-10 and ImageNet.
Thus, we reported the best results we obtained by running the released codes in Tab.~\ref{Tab:IS}.
Moreover, we report here the IS of LDAGAN with standard loss and CNN architecture for fair comparison with MGAN.
We believe its performance will be promoted a lot by adopting hinge loss and ResNet.
The two have shown much better performance in the works~\cite{tran2018dist,miyato2018spectral} recently.
As we discussed in Sec.\ref{sec:EvaMetrics}, IS only reflects the quality and diversity of synthetic images, but omits to compare the statistics of model and real distributions.


\begin{table*}[h]
\caption{Fr\'{e}chet Inception Distance on different datasets.}
\label{Tab:FID}
\centering
\addtolength{\tabcolsep}{-0pt}{\begin{tabular}{lccc}
\hline
Model & CIFAR-10 & CIFAR-100 & ImageNet\tabularnewline
\hline
\hline

DCGAN~\cite{radford2015unsupervised}           &       $37.7$     &         -       &   37.4  \tabularnewline

DCGAN+TTUR~\cite{heusel2017gans}      &       $36.9$     &         -       &    -    \tabularnewline

WGAN-GP~\cite{gulrajani2017improved}         &       $29.3$     &         -       &    -    \tabularnewline

WGAN-GP+TTUR~\cite{heusel2017gans}    &       $24.8$     &         -       &    -    \tabularnewline



MGAN~\cite{hoang2018mgan}           &       $26.7$      &     $32.9$      &    $43.9$  \tabularnewline

LDAGAN         & $\mathbf{24.3}$   & $\mathbf{28.8}$ & $\mathbf{28.9}$ \tabularnewline
 \hline
\end{tabular}}
\end{table*}


\noindent\textbf{Fr\'{e}chet Inception Distance Comporison:} For better comparison, we performed a qualitative analysis of FID for different GANs.
The results in Tab.~\ref{Tab:FID} show a consistent tendency that LDAGAN's FID maintains stable even on especially complex datasets, such as ImageNet, while MGAN's FID drops dramatically.
On ImageNet, LDAGAN has a 34.2\% FID decrease over MGAN.
This is due to LDAGAN's high model capacity of generating extremely complex image data.
Because MGAN simply mixes generators and has the intra-class mode dropping problem (see Sec.~\ref{sec:MixGANs}), it has inferior performance to LDAGAN.



\noindent\textbf{Training Time Comparison:}
Finally, we analyze the computational time of MGAN and LDAGAN for 1 epoch.
The time shown in Fig.~\ref{fig:Time} is the average over 200 epochs.
On CIFAR-10, the one epoch running time is 1.17 minutes for MGAN and 2.3 minutes for LDAGAN.
On CIFAR-100, these are not much different.
Although combing EM algorithm and adversarial training seems to be time consuming, our tanning time only increases 1 time over MGAN.
This demonstrates the stochastic optimization strategy proposed in Sec.~\ref{Stoch} and the parameter sharing scheme largely promotes training efficiency.

\begin{figure}[h]
  \centering
    \includegraphics[width=0.48\textwidth]{Fig5.pdf}
     \caption{Computational time for 1 epoch update.}
     \label{fig:Time}
\end{figure}




\section{Conclusion and Future Work}
\label{sec:Conclusion}
Latent Dirichlet allocation was introduced into generative adversarial networks in this work.
It helps to discover the underlying structure of real data, and make generators better fit data.
Moreover, EM algorithm was combined with adversarial technique to solve our model.
The stochastic optimization strategy was proposed in order to guarantee the learning efficiency of LDAGAN.
The proposed method was shown to outperform the existing GANs with standard CNN architecture for FID score.
Note that the superior results of LDAGAN were achieved based on the standard adversarial loss and the CNN architecture.
Our future work will involve improving the performance by utilizing the hinge loss and the ResNet architecture, and extending LDAGAN to more computer vision problems.

\bibliography{example_paper}
\bibliographystyle{icml2018}



\onecolumn
\appendix
\section{Variational EM algorithm}
\label{Appendix}

In this section, we derive the variational EM algorithm for efficient inference in the adversarial training,  described in Sec.~4.

\subsection{Variational Distribution}
\label{App:VaDistribution}

Based on mean-field approximation, we define a variational distribution which factorizes between latent variables $\bm{\pi}$ and $\bm{z}$ so that:
\begin{equation}
\label{eq:AVarDis1}
q\left( \bm{\pi}, \mathbf{z}|\bm{\gamma},\bm{\omega} \right)
=
q\left( \bm{\pi} | \bm{\gamma} \right)
q\left( \mathbf{z} | \bm{\omega} \right),
\end{equation}
where $\bm{\gamma}$ and $\bm{\omega}$ are the Dirichlet and multinomial parameters respectively.
It can be viewed as a surrogate for the posterior $p\left(\bm{\pi}, \mathbf{z}, y=1|\mathbf{z}',\bm{\theta}, \bm{\alpha},\bm{\phi}\right)$.


The log likelihood of a sample generated from $\mathbf{z}'$ being real is obtained by summing the joint distribution over all possible modes $\mathbf{z}$ and integrating over all mode distributions $\bm{\pi}$:
\begin{equation}
\label{eq:AVarDis2}
\log p\left( y=1 | \mathbf{z}',\bm{\theta},\bm{\alpha}, \bm{\phi} \right)
=
\log
\int
\sum_{\mathbf{z}}
p\left( \bm{\pi},\mathbf{z},y=1 | \mathbf{z}',\bm{\theta},\bm{\alpha},\bm{\phi} \right)
d\bm{\pi}.
\end{equation}
\noindent According to Jensen's inequality, the above log likelihood function has a lower bound:
\begin{align}
\label{eq:AVarDis3}
\log
\int
\sum_{\mathbf{z}}
\frac{p\left( \bm{\pi},\mathbf{z},y=1 | \mathbf{z}',\bm{\theta},\bm{\alpha},\bm{\phi} \right)
q\left( \bm{\pi},\mathbf{z} | \bm{\gamma},\bm{\omega} \right)}
{q\left( \bm{\pi}, \mathbf{z} | \bm{\gamma},\bm{\omega} \right)}
d\bm{\pi}
&\geq  \nonumber
\int
\sum_{\mathbf{z}}
q\left( \bm{\pi}, \mathbf{z}|\bm{\gamma},\bm{\omega} \right)
\log p\left( \bm{\pi},\mathbf{z},y=1 | \mathbf{z}',\bm{\theta},\bm{\alpha},\bm{\phi} \right)
d\bm{\pi}  \\
&-\int
\sum_{\mathbf{z}}
q\left( \bm{\pi},\mathbf{z} | \bm{\gamma},\bm{\omega} \right)
\log q\left( \bm{\pi},\mathbf{z} | \bm{\gamma},\bm{\omega} \right)
d\bm{\pi},
\end{align}
which is denoted as the right-hand side of Eq.~(\ref{eq:AVarDis3}), represented by $L\left(\bm{\gamma},\bm{\omega};\bm{\theta},\bm{\alpha}\right)$.
Thus, the log likelihood has the following decomposition:
\begin{equation}
\label{eq:AVarDis4}
\log p\left(y=1|\mathbf{z}',\bm{\theta},\bm{\alpha}, \bm{\phi}\right)
=
L\left( \bm{\gamma},\bm{\omega};\bm{\alpha},\bm{\theta} \right)
+ \text{KL}\left( q\left( \bm{\pi},\mathbf{z} | \bm{\gamma},\bm{\omega} \right)
|| p\left( \bm{\pi}, \mathbf{z} |y=1, \mathbf{z}',\bm{\theta},\bm{\alpha},\bm{\phi} \right) \right),
\end{equation}
which is the sum of the lower bound and the Kullback-Leibler divergence.


\subsection{E-Step Optimization}
In E-step, we maximize the lower bound $L\left(\bm{\gamma},\bm{\omega};\bm{\theta},\bm{\alpha}\right)$ with respect to variational parameters $\bm{\gamma}$ and $\bm{\omega}$.
It can be easily verified that the maximum of $L\left(\bm{\gamma},\bm{\omega};\bm{\theta},\bm{\alpha}\right)$ occurs when the KL divergence vanishes.
We expand the lower bound by using the factorizations of $p\left(\bm{\pi},\mathbf{z},y=1|\mathbf{z}',\bm{\theta},\bm{\alpha},\bm{\phi}\right)$ and $q\left(\bm{\pi}, \mathbf{z}|\bm{\gamma},\bm{\omega}\right)$:
\begin{equation}
\label{eq:AVarDis5}
L\left( \bm{\gamma},\bm{\omega}; \bm{\alpha},\bm{\theta} \right)
 =
\mathbb{E}_q \left[ \log  p\left( \bm{\pi} | \bm{\alpha} \right) \right]
+\mathbb{E}_q\left[ \log p\left( \mathbf{z} | \bm{\pi} \right) \right]
+\mathbb{E}_q\left[ \log p\left( y=1 | \mathbf{z},\mathbf{z}',\bm{\theta},\bm{\phi}\right) \right]
-\mathbb{E}_q\left[ \log  q\left( \bm{\pi} | \bm{\gamma} \right) \right]
-\mathbb{E}_q\left[ \log  q\left( \mathbf{z} | \bm{\omega} \right) \right],
\end{equation}
where $\mathbb{E}_q\left[\cdot\right]$ denotes the expectation with respect to variational distribution.
Each of the terms in the sum in Eq.~(\ref{eq:AVarDis5}) has the following expressions.
The first term is:
\begin{equation}
\label{eq:AVarDis6}
\mathbb{E}_q \left[ \log  p\left( \bm{\pi} | \bm{\alpha} \right) \right]
=
\log\Gamma\left( \sum\nolimits_{j=1}^K \alpha_j \right)
- \sum_{k=1}^K \log\Gamma\left(\alpha_k\right)
+\sum_{k=1}^K
\left( \alpha_k-1 \right)
\left( \Psi\left(\gamma_k\right)-\Psi\left(\sum\nolimits_{j=1}^K\gamma_j\right) \right),
\end{equation}
where $\Psi$ is the digamma function, representing the first derivative of the log Gamma function.
Similar, the remaining terms of Eq.~(\ref{eq:AVarDis5}) have the forms of:
\begin{align}
\mathbb{E}_q\left[ \log p\left( \mathbf{z} | \bm{\pi} \right) \right]
=&
\sum_{k=1}^K
\omega_k
\left( \Psi\left( \gamma_k \right) - \Psi\left( \sum\nolimits_{j=1}^K \gamma_j \right)\right),\label{eq:AVarDis7}\\
\mathbb{E}_q\left[ \log p\left( y=1 | \mathbf{z},\mathbf{z}',\bm{\theta},\bm{\phi}\right) \right]
=&
\sum_{k=1}^K
\omega_k
\log D\left( G_k\left( \mathbf{z}'\right) \right),\label{eq:AVarDis8}\\
\mathbb{E}_q \left[ \log  q\left( \bm{\pi} | \bm{\gamma} \right) \right]
=&
\log\Gamma\left( \sum\nolimits_{j=1}^K \gamma_j \right)
- \sum_{k=1}^K \log\Gamma\left(\gamma_k\right)\nonumber\\
&+\sum_{k=1}^K
\left( \gamma_k-1 \right)
\left( \Psi\left(\gamma_k\right)-\Psi\left(\sum\nolimits_{j=1}^K\gamma_j\right) \right),\label{eq:AVarDis9}\\
\mathbb{E}_q\left[ \log  q\left( \mathbf{z} | \bm{\omega} \right) \right]
=&
\sum_{k=1}^K
\omega_k
\log \omega_k.
\label{eq:AVarDis10}
\end{align}
Here, we use the fact that the expected value of the log of a single component under the Dirichlet have the following expression $\mathbb{E}_q\left[\log \pi_k\right] = \Psi\left(\bm{\gamma}_k\right)-\Psi\left(\sum_{j=1}^K \bm{\gamma}_j\right)$~\cite{blei2003latent}.


In the next two sections, we will show how to maximize the lower bound with respect to the variational Dirichlet and multinomial parameters $\bm{\gamma}$ and $\bm{\omega}$.


\subsubsection{Variational Dirichlet}
\label{App:VaDirichlet}


Picking out just those terms that only contain $\bm{\gamma}$ in $L\left(\bm{\gamma},\bm{\omega};\bm{\theta},\bm{\alpha}\right)$, we have:
\begin{align}
\label{eq:Appendix15}
L_{\bm{\gamma}}
&=
\sum_{k=1}^K
\left( \alpha_k-1 \right)
\left( \Psi\left(\gamma_k\right)-\Psi\left(\sum\nolimits_{j=1}^K\gamma_j\right) \right)
+\sum_{k=1}^K
\omega_k
\left(\Psi \left( \gamma_k \right)-\Psi \left( \sum\nolimits_{j=1}^K \gamma_j \right) \right) \nonumber \\
&-\log \Gamma \left( \sum\nolimits_{j=1}^K\gamma_j\right)
+\sum_{k=1}^K\log \Gamma \left( \gamma_k \right)
-\sum\nolimits_{k=1}^K
\left( \gamma_k-1 \right)
\left( \Psi\left( \gamma_k \right) -\Psi \left( \sum\nolimits_{j=1}^K \gamma_j\right) \right),
\end{align}
\noindent which simplifies to:
\begin{equation}
\label{eq:Appendix16}
L_{\bm{\gamma}}
=
\sum_{k=1}^K
\left( \Psi\left( \gamma_k \right) -\Psi \left( \sum\nolimits_{j=1}^K \gamma_j\right) \right)
\left( \alpha_k + \omega_k - \gamma_k \right)
-\log \Gamma \left( \sum\nolimits_{j=1}^K\gamma_j\right)
+\sum_{k=1}^K\log \Gamma \left( \gamma_k \right).
\end{equation}
\noindent The derivative of $L_{\bm{\gamma}}$ with respect to $\gamma_k$ can be expressed as:
\begin{equation}
\label{eq:Appendix17}
\frac{\partial L_{\bm{\gamma}}}
{\partial{\gamma_k}}
=
\Psi'\left( \gamma_k \right)
\left( \alpha_k + \omega_k - \gamma_k \right)
- \Psi'\left( \sum\nolimits_{j=1}^K \gamma_j \right)
\sum_{j=1}^K \left(\alpha_j + \omega_j - \gamma_j \right),
\end{equation}
where $\gamma_k$ is the $k^{th}$ element of $\bm{\gamma}$. Setting this derivative to zero yields a maximum at:
\begin{equation}
\label{eq:Appendix18}
\gamma_k
=
\alpha_k
+
\omega_k.
\end{equation}



\subsubsection{Variational Multinomial}
\label{App:VaMultinomial}

In this section, we show how to maximize the lower bound $L\left( \bm{\gamma},\bm{\omega}; \bm{\theta},\bm{\alpha} \right)$ with respect to the variational parameters $\bm{\omega}$.
One note there is a constrain on $\bm{\omega}$, which is $\sum_{k=1}^K\omega_k=1$.
We form the Lagrange by isolating the terms with respect to $\omega_k$, where $\omega_k$ is the $k^{th}$ element of $\bm{\omega}$, and adding the Lagrange multipliers,
\begin{equation}
\label{eq:Append.1.1-1}
L_{\omega_k}
=
\omega_k\left( \Psi \left( \gamma_k \right) - \Psi \left( \sum\nolimits_{j=1}^K \gamma_j \right) \right)
+ \omega_k \log D\left( G_k\left( \mathbf{z}' \right) \right)
- \omega_k \log \omega_k
+\lambda \left( \sum\nolimits_{j=1}^K \omega_j - 1\right).
\end{equation}
\noindent The derivatives of $L_{\omega_k}$ with respect to $\omega_k$ is given by:
\begin{equation}
\label{eq:Appendix13}
\frac{\partial L_{\omega_k}}{\partial \omega_k}
=
\Psi \left( \gamma_k \right) - \Psi \left( \sum\nolimits_{j=1}^K \gamma_j \right) + \log D\left( G_k\left( \mathbf{z}'\right) \right)
- \log \omega_k
- 1
+\lambda.
\end{equation}
Setting this derivative to zero yields the optimal value of variational parameter $\omega_k$:
\begin{equation}
\label{eq:Appendix14}
\omega_k
\propto
D\left( G_k\left( \mathbf{z}'\right) \right)\exp\left( \Psi\left(\gamma_k\right) - \Psi \left( \sum\nolimits_{j=1}^K \gamma_j\right) \right).
\end{equation}



\subsection{M-Step Optimization}
\label{App:PaEstimation}

In the subsequent M-step, the variational distribution $q\left( \bm{\pi}, \mathbf{z}|\bm{\gamma},\bm{\omega} \right)$ is fixed and the lower bound is maximized with respect to model parameters $\bm{\theta}$ and $\bm{\alpha}$.
In previous discussion, one note that we only consider the log likelihood for a single sample $\mathbf{z}'$ and the estimate of $\bm{\gamma}$ and $\bm{\omega}$ (see Eq.~(\ref{eq:Appendix14}) and Eq.~(\ref{eq:Appendix18})) are related to $\mathbf{z}'$.
We thus rewrite $\bm{\gamma}$ and $\bm{\omega}$ as functions of $\mathbf{z}'$, denoted by $\bm{\gamma}\left(\mathbf{z}'\right)$ and $\bm{\omega}\left(\mathbf{z}'\right)$.
Estimating model parameters $\bm{\theta}$ and $\bm{\alpha}$ should consider the lower bounds over all possible $\mathbf{z}'$, that is, the expectation of $L\left( \bm{\gamma},\bm{\omega}; \bm{\alpha},\bm{\theta} \right)$ with respect to $\mathbf{z}'$, described by $\mathcal{L}\left( \bm{\gamma},\bm{\omega}; \bm{\alpha},\bm{\theta}\right) = \mathbb{E}_{\mathbf{z}'\sim p\left(\mathbf{z}'\right)}\left[L\left( \bm{\gamma},\bm{\omega}; \bm{\alpha},\bm{\theta}\right)\right]$.


\subsubsection{Generators}
\label{App:Generators}

To optimize $\bm{\theta}_k$ associated with the $k^{th}$ generator, we isolate the terms in $\mathcal{L}\left( \bm{\gamma},\bm{\omega}; \bm{\alpha},\bm{\theta}\right)$ containing $\bm{\theta}_k$ and obtain:
\begin{equation}
\label{eq:Appendix19}
\mathcal{L}_{\bm{\theta}_k}
=
\mathbb{E}_{\mathbf{z}' \sim p\left(\mathbf{z}'\right)}
\left[
\omega_k\left( \mathbf{z}' \right)
\log D\left( G_k\left( \mathbf{z}'\right) \right)
\right],
\end{equation}
maximizing $\mathcal{L}_{\bm{\theta}_k}$ yields the following optimization problem:
\begin{equation}
\label{eq:Appendix20}
\max_{\bm{\theta}_k}
\mathbb{E}_{\mathbf{z}' \sim p \left( \mathbf{z}' \right)}
\left[
\omega_k\left( \mathbf{z}' \right)
\log D\left( G_k\left( \mathbf{z}' \right) \right)
\right].
\end{equation}
\subsubsection{Dirichlet}
\label{App:Dirichlet}

The terms in $\mathcal{L}\left( \bm{\gamma},\bm{\omega}; \bm{\alpha},\bm{\theta}\right)$ which contain $\bm{\alpha}$ are:
\begin{equation}
\label{eq:Appendix21}
\mathcal{L}_{\bm{\alpha}}
=
\log \Gamma \left( \sum\nolimits_{j=1}^K \alpha_j \right)
-\sum_{k=1}^K \log \Gamma \left( \alpha_k \right)
+
\mathbb{E}_{\mathbf{z}' \sim p_{\mathbf{z}'}}
\left[
\sum_{k=1}^K
\left( \alpha_k-1 \right)
\left( \Psi\left(\gamma_k\left(\mathbf{z}'\right)\right)
-\Psi\left(\sum\nolimits_{j=1}^K\gamma_j\left(\mathbf{z}'\right)\right) \right)
\right].
\end{equation}
Taking the derivative of $\mathcal{L}_{\bm{\alpha}}$ with respect to $\alpha_k$, we have:
\begin{equation}
\label{eq:Appendix22}
\frac{\partial \mathcal{L}_{\bm{\alpha}}}
{\partial \alpha_k}
=
\Psi\left(\sum\nolimits_{j=1}^K\alpha_j\right)-\Psi\left(\alpha_k\right)
+
\mathbb{E}_{\mathbf{z}' \sim p_{\mathbf{z}'}}
\left[
\Psi\left( \gamma_k\left( \mathbf{z}' \right) \right)
-\Psi\left( \sum\nolimits_{j=1}^K \gamma_j\left( \mathbf{z}' \right) \right)
\right].
\end{equation}
Finally, we use gradient ascent to update $\bm{\theta}$ and $\bm{\alpha}$.


\section{Network Architecture}
\label{App:NetArch}

\begin{figure}
  \centering
    \includegraphics[width=0.7\textwidth]{AFig1.pdf}
     \caption{Network architecture of LDAGAN, which has $K$ generators and 1 discriminator. Each generator has four convolutional layers, and parameter sharing occurs on the last three layers.}
     \label{fig:Synth}
\end{figure}

Our LDAGAN consists of $K$ generators and 1 discriminator, as illustrated in Fig.~\ref{fig:Synth}.
Each generator has four convolutional layers, and shares parameters except the input layer.
The parameter sharing scheme helps to keep the balance of generator's updating.
Besides, it dramatically reduces the number of parameters, and thus ensures the training process, compared with standard GANs, is not time consuming.


We constructed the network for LDAGAN according to the design of DCGANs~\cite{radford2015unsupervised} with some slight modifications.
The details our networks trained on the CIFAR-10, CIFAR-100, and ImageNet datasets can be found in Tab.~\ref{tab1} and Tab.~\ref{tab2}.
"BN" is short for batch normalization, and "Shared" is the short for parameter sharing.


\begin{table}
\small
\centering\caption{Network architecture and hyperparameters of LDAGAN on the CIFAR-10 and CIFAR-100 datasets.}
\centering{
\begin{tabular}{r l c c r c l c}%%%The number of columns has to be defined here
\hline
Unit & Operation & Kernel & Stride & Feature maps & BN & Activation & Shared\\ %%%% Table body
\hline
\hline
Generator& $\mathbf{z}'$ $\sim$ Uniform[-1,1] & & & 100 & & &\\
& Conv transposed & 4x4 & 1 & 128x4 & Yes & ReLU & No\\
& Conv transposed & 4x4 & 2 & 128x2 & Yes & ReLU & Yes\\
& Conv transposed & 4x4 & 2 & 128 & Yes & ReLU & Yes\\
& Conv transposed & 4x4 & 2 & 3 & No & Tanh & Yes\\
\hline
Discriminator & Conv & 5x5 & 2 & 128 & No & LReLU &\\
& Conv  & 5x5 & 2 & 128x2 & Yes & LReLU &\\
& Conv  & 5x5 & 2 & 128x4 & Yes & LReLU &\\
& Conv  & 4x4 & 1 & 1 & Yes & Sigmoid &\\
\hline
Number of generators& 10& & & & & &\\
Generator initialization  &$\mathcal{N}\left(\mu=0,\sigma=0.08\right)$ & & & & & &\\
Discriminator initialization  &$\mathcal{N}\left(\mu=0,\sigma=0.02\right)$ & & & & & &\\
Batch size of real data  &64 & & & & & &\\
Batch size for each generator  &12 & & & & & &\\
Leacky ReLU slope  &0.2 & & & & & &\\
Learning rate  &0.0001 & & & & & &\\
Optimizer  &Adam($0.5$, $0.999$) & & & & & &\\
\hline
\end{tabular}}
\label{tab1}
\end{table}

\begin{table}
\small
\centering\caption{Network architecture and hyperparameters of LDAGAN on the ImageNet dataset.}
\centering{
\begin{tabular}{r l c c r c l c}%%%The number of columns has to be defined here
\hline
Unit & Operation & Kernel & Stride & Feature maps & BN & Activation & Shared\\ %%%% Table body
\hline
\hline
Generator& $\mathbf{z}'$ $\sim$ Uniform[-1,1] & & & 100 & & &\\
& Conv transposed & 4x4 & 1 & 128x4 & Yes & ReLU & No\\
& Conv transposed & 5x5 & 2 & 128x2 & Yes & ReLU & Yes\\
& Conv transposed & 5x5 & 2 & 128 & Yes & ReLU & Yes\\
& Conv transposed & 5x5 & 2 & 3 & No & Tanh & Yes\\
\hline
Discriminator & Conv & 5x5 & 2 & 128 & No & LReLU &\\
& Conv  & 5x5 & 2 & 128x2 & Yes & LReLU &\\
& Conv  & 5x5 & 2 & 128x4 & Yes & LReLU &\\
& Conv  & 4x4 & 1 & 1 & Yes & Sigmoid &\\
\hline
Number of generators& 20& & & & & &\\
Generator initialization  &$\mathcal{N}\left(\mu=0,\sigma=0.08\right)$ & & & & & &\\
Discriminator initialization  &$\mathcal{N}\left(\mu=0,\sigma=0.02\right)$ & & & & & &\\
Batch size of real data  &64 & & & & & &\\
Batch size for each generator  &24 & & & & & &\\
Leacky ReLU slope  &0.2 & & & & & &\\
Learning rate  &0.0001 & & & & & &\\
Optimizer  &Adam($0.5$, $0.999$) & & & & & &\\
\hline
\end{tabular}}
\label{tab2}
\end{table}

\section{Parameter Sharing}
\label{App:ParShare}
We evaluated the effect of parameter sharing on the generative performance.
Inception scores (IS) and Fr\'{e}chet inception distance (FID) are two measures.
Tab.~\ref{tab:Sharing} shows the evaluation results on the CIFAR-10, CIFAR-100 and ImageNet datasets.
The results show a consistent tendency that the less the parameter sharing layers the worse the performance is.
Parameter sharing helps to keep the balance of generator's updating.
This makes it possible for discriminator to score the performance of different generators simultaneously.
This is a partial explanation of the performance dropping caused by adopting less parameter sharing layers.



\begin{table}[h]
\caption{The performances of LDAGAN using different parameter sharing schemes}
\centering
\small\addtolength{\tabcolsep}{-0pt}
\subtable[Inception scores on different datasets]{
{\begin{tabular}{lccc}
\hline
untied layer & CIFAR-10 & CIFAR-100 & ImageNet\tabularnewline
\hline
\hline
1         &  $7.46$        &        $7.57$              &    $8.34$     \tabularnewline

1-2      &  $6.13$        &         $6.910$             &    -     \tabularnewline
 \hline
\end{tabular}}
\label{tab:firsttable}
}
\qquad
\subtable[Fr\'{e}chet inception distance on different datasets]{
{\begin{tabular}{lccc}
\hline
untied layer & CIFAR-10 & CIFAR-100 & ImageNet\tabularnewline
\hline
\hline
1         &  $24.3$        &        $28.7$             &    $28.8$     \tabularnewline

1-2      &  $44.3$        &         $47.6$             &    -     \tabularnewline
 \hline
\end{tabular}}
\label{tab:secondtable}
}
\label{tab:Sharing}
\end{table}



\section{Different Generator Number}
\label{App:ParShare}
We tested the performances of LDAGAN with different generators on the CIFAR-10, CIFAR-100, and ImageNet datasets.
Since CIFAR-100 and ImageNet have much more image classes, more generators should be employed.
10, 20, and 30 generators were used on CIFAR-100, and 10, 20, 27 generators were used on ImageNet.
The quantitative results can be found in Tab.~\ref{tab:NumGenerator}, where the best performances are achieved when 10 and 20 generators are employed on CIFAR-100 and ImageNet respectively.


\begin{table*}[h]
\caption{The performances of LDAGAN with various number of generators}
\centering
\small\addtolength{\tabcolsep}{-0pt}
\subtable[Inception scores on different datasets]{
{\begin{tabular}{lccc}
\hline
number & CIFAR-10 & CIFAR-100 & ImageNet\tabularnewline
\hline
\hline
10         &  $7.46$        &        $7.50$              &    $8.20$     \tabularnewline

20      &  -       &         $7.57$             &    $8.21$     \tabularnewline

30/27        &    -      &         $6.91$             &    $8.34$     \tabularnewline
 \hline
\end{tabular}}
       \label{tab:firsttable}
}
\qquad
\subtable[Fr\'{e}chet Inception Distance on different datasets]{
{\begin{tabular}{lccc}
\hline
uncorrelated layer & CIFAR-10 & CIFAR-100 & ImageNet\tabularnewline
\hline
\hline
10         &  $24.3$        &        $28.8$              &    $36.1$     \tabularnewline

20      &  -        &         $29.8$             &    $28.9$     \tabularnewline

30/27        &    -      &         $35.26$             &    $31.5$     \tabularnewline
 \hline
\end{tabular}}
\label{tab:secondtable}
}
\label{tab:NumGenerator}

\end{table*}



\section{Parameter Updating}
\label{App:ParUp}

One assumption in LDAGAN is that discriminator can output a likelihood indicating how realistic the synthesized image is.
Such a likelihood is used to calculate variational parameters $\bm{\gamma}$ and $\bm{\omega}$.
However, the discriminator is sometimes inaccurate and unstable during training.
This seems to only appears obviously early in learning.
We thus fixed the variational parameters $\bm{\gamma}$ and $\bm{\omega}$ at the beginning epochs, and kept them updating after a certain epoch.


On CIFAR-10, we tested the performance of LDAGAN when updated $\bm{\gamma}$ and $\bm{\omega}$ after 0, 50 and 100 epochs.
The corresponding inception scores are 7.36, 7.38, and 7.46, and FIDs are 26.4, 25.7, and 24.3.
These results show LDAGAN has an improved performance if we update $\bm{\gamma}$ and $\bm{\omega}$ after a certain epoch.
On ImageNet, we analyzed the FID and IS of LDAGAN when updated $\bm{\gamma}$ and $\bm{\omega}$ after 0 and 4 epochs.
The inception scores are 8.37 and 8.21, and the FIDs are 29.0 and 28.9.
There does not exist significant difference between these two strategies.

\section{Sampling Details}
\label{App:Sampling}
The learning of discriminator refers to sampling.
As we described in Sec.~4, the discriminative loss has the from of:
\begin{equation}
\label{eq:Learning-D}
\max_{\bm{\phi}}
\mathbb{E}_{\mathbf{x} \sim p_{data}\left(\mathbf{x}\right)}
\mathbb{E}\left[ \log p\left( y=1 | \mathbf{x},\bm{\phi} \right) \right]
+\mathbb{E}_{\mathbf{z}' \sim p\left(\mathbf{z}'\right)}
\mathbb{E}\left[ \log\left( 1-p\left( y=1 | \mathbf{z}',\bm{\theta},\bm{\alpha},\bm{\phi} \right) \right) \right],
\end{equation}
where $p\left( y=1 | \mathbf{z}',\bm{\theta},\bm{\alpha},\bm{\phi} \right)$ is a marginal probability, which is obtained by integrating joint distribution over $\bm{\pi}$ and summing over $\mathbf{z}$:
\begin{equation}
\int p\left(\bm{\pi} | \bm{\alpha}\right)
\left(\sum_{\mathbf{z}}
p\left(\mathbf{z}|\bm{\pi}\right)
p\left(y=1|\mathbf{z}, \mathbf{z}', \bm{\theta}, \bm{\phi}\right)
\right)d\bm{\pi},
\label{eq:Learning-D2}
\end{equation}
$p\left(y=1|\mathbf{z}, \mathbf{z}', \bm{\theta}, \bm{\phi}\right)$ here denotes, given the underlying mode (\ie~$z_k=1$), the probability of the synthetic sample being real.
We utilize $ D\left(G_k\left(\mathbf{z}'\right)\right)$ to score this probability.
To solve $\bm{\phi}$, we should adopt ancestral sampling since the integration over $\bm{\pi}$ is analytically intractable.

Ancestral sampling, in fact, is somewhat time consuming.
On the CIFAR-10 dataset, the running time for 1 epoch with ancestral sampling is 5.10 minutes on a GTX1080Ti GPU.
To ensure the learning efficiency, we change ancestral sampling to randomly sampling fixed number of real and fake samples.
The details can be found in Tab.~\ref{tab:Sharing} and \ref{tab:NumGenerator}.
By virtue of this simplification, the running time for 1 epoch reduces to be 2.30 minutes.



\section{Training Process}
\label{App:TrainingPro}

On the CIFAR-10, CIFAR-100 and ImageNet datasets, the visualized training processes of LDAGAN are shown in Fig.~\ref{fig:TrainPro1}, Fig.~\ref{fig:TrainPro2} and Fig.~\ref{fig:TrainPro3}, respectively.


\begin{figure}[b]
  \centering
    \subfigure[Epoch 25]{\includegraphics[width=0.3\textwidth]{cifar10_fake_image_25.png}}
    \hspace{.15in}
    \subfigure[Epoch 50]{\includegraphics[width=0.3\textwidth]{cifar10_fake_image_50.png}}
    \hspace{.15in}
    \subfigure[Epoch 75]{\includegraphics[width=0.3\textwidth]{cifar10_fake_image_75.png}}
    \subfigure[Epoch 100]{\includegraphics[width=0.3\textwidth]{cifar10_fake_image_100.png}}
    \hspace{.15in}
    \subfigure[Epoch 125]{\includegraphics[width=0.3\textwidth]{cifar10_fake_image_125.png}}
    \hspace{.15in}
    \subfigure[Epoch 150]{\includegraphics[width=0.3\textwidth]{cifar10_fake_image_150.png}}
    \subfigure[Epoch 175]{\includegraphics[width=0.3\textwidth]{cifar10_fake_image_175.png}}
    \hspace{.15in}
    \subfigure[Epoch 200]{\includegraphics[width=0.3\textwidth]{cifar10_fake_image_200.png}}
    \hspace{.15in}
    \subfigure[Epoch 250]{\includegraphics[width=0.3\textwidth]{cifar10_fake_image_250.png}}
     \caption{Images (with size $32\times32$) generated by different generators of LDAGAN at different epochs. Each row corresponds to one generator (\ie~mode). The model is trained on CIFAR-10. }
     \label{fig:TrainPro1}
\end{figure}


\begin{figure*}
  \centering
    \subfigure[Epoch 25]{\includegraphics[width=0.3\textwidth]{cifar100_fake_image_25.png}}
    \hspace{.15in}
    \subfigure[Epoch 50]{\includegraphics[width=0.3\textwidth]{cifar100_fake_image_50.png}}
    \hspace{.15in}
    \subfigure[Epoch 75]{\includegraphics[width=0.3\textwidth]{cifar100_fake_image_75.png}}
    \subfigure[Epoch 100]{\includegraphics[width=0.3\textwidth]{cifar100_fake_image_100.png}}
    \hspace{.15in}
    \subfigure[Epoch 125]{\includegraphics[width=0.3\textwidth]{cifar100_fake_image_125.png}}
    \hspace{.15in}
    \subfigure[Epoch 150]{\includegraphics[width=0.3\textwidth]{cifar100_fake_image_150.png}}
    \subfigure[Epoch 175]{\includegraphics[width=0.3\textwidth]{cifar100_fake_image_175.png}}
    \hspace{.15in}
    \subfigure[Epoch 200]{\includegraphics[width=0.3\textwidth]{cifar100_fake_image_200.png}}
    \hspace{.15in}
    \subfigure[Epoch 250]{\includegraphics[width=0.3\textwidth]{cifar100_fake_image_250.png}}
     \caption{Images (with size $32\times32$) generated by different generators of LDAGAN at different epochs. Each row corresponds to one generator (\ie~mode). The model is trained on CIFAR-100. }
     \label{fig:TrainPro2}
\end{figure*}


\begin{figure*}
  \centering
    \subfigure[Epoch 1]{\includegraphics[width=0.45\textwidth]{imagenet_fake_1.png}}
    \hspace{.2in}
    \subfigure[Epoch 3]{\includegraphics[width=0.45\textwidth]{imagenet_fake_3.png}}
    \subfigure[Epoch 5]{\includegraphics[width=0.45\textwidth]{imagenet_fake_5.png}}
    \hspace{.2in}
    \subfigure[Epoch 8]{\includegraphics[width=0.45\textwidth]{imagenet_fake_8.png}}
     \caption{Images (with size $32\times32$) generated by different generators of LDAGAN at different epochs. Each row corresponds to one generator (\ie~mode). The model is trained on ImageNet. }
     \label{fig:TrainPro3}
\end{figure*}
\section{Generated Images}
\label{App:GenImg}

Some example images generated by LDAGAN trained on the CIFAR-10, CIFAR-100 and ImageNet datasets are shown in Fig.~\ref{fig:ExpImg1}, Fig.~\ref{fig:ExpImg2}, and Fig.~\ref{fig:ExampImg3}, resectively.
\begin{figure}[h]
  \centering
    \includegraphics[width=0.6\textwidth]{cifar10_good_image_200.png}
     \caption{Images generated by LDAGAN trained on the CIFAR-10 dataset.}
     \label{fig:ExpImg1}
\end{figure}

\begin{figure}[h]
  \centering
    \includegraphics[width=0.6\textwidth]{cifar100_good_image_200.png}
     \caption{Images generated by LDAGAN trained on the CIFAR-100 dataset.}
     \label{fig:ExpImg2}
\end{figure}

\begin{figure}[h]
  \centering
    \includegraphics[width=0.8\textwidth]{imagenet_good_image_9.png}
     \caption{Images generated by LDAGAN trained on the rescaled $32\times32$ ImageNet dataset.}
     \label{fig:ExampImg3}
\end{figure}










\end{document}



