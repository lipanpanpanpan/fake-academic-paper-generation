






\documentclass[journal, 12pt, onecolumn,draftclsnofoot]{IEEEtran}







\usepackage{color}
\usepackage{threeparttable}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{amsmath}
\interdisplaylinepenalty=2500
\usepackage{amssymb}
\usepackage{stfloats}
\usepackage{float}
\usepackage{lipsum}
\usepackage{multirow}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows, positioning}











\ifCLASSINFOpdf
\else
\fi








































\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{Detecting Damaged Buildings on Post-Hurricane Satellite Imagery Based on Customized Convolutional Neural Networks}

\author{Quoc Dung Cao
        and~Youngjun~Choe$^{*}$% <-this % stops a space
\thanks{$^{*}$Corresponding author (Postal mailing address: 3900 E Stevens Way NE,
Seattle, WA 98195, USA; Telephone: +1 (206) 543-1427; FAX: +1 (206) 685-3072; E-mail address: ychoe@u.washington.edu).}
\thanks{Quoc Dung~Cao and Youngjun~Choe are with the Department of Industrial and Systems Engineering, University of Washington, Seattle, WA, USA.}}% <-this % stops a space
















\maketitle

\begin{abstract}
After a hurricane, damage assessment is critical to emergency managers and first responders so that resources can be planned and allocated appropriately. One way to gauge the damage extent is to detect and quantify the number of damaged buildings, which is traditionally done through driving around the affected area. This process can be labor intensive and time-consuming. In this paper, utilizing the availability and readiness of satellite imagery, we propose to improve the efficiency and accuracy of damage detection via image classification algorithms. From the building coordinates, we extract their aerial-view windows of appropriate size and classify whether a building is damaged or not. We demonstrate the result of our method in the case study of 2017 Hurricane Harvey.
\end{abstract}

\begin{IEEEkeywords}
image classification, neural networks, emergency services, buildings %disaster, image classification, neural network, first response, emergency management
\end{IEEEkeywords}






\IEEEpeerreviewmaketitle



\section{Introduction}
\IEEEPARstart{W}{hen} a hurricane makes landfall, situational awareness is one of the most critical needs that emergency managers face before they can respond to the event. To assess the situation and damage, the current practice largely relies on emergency response crews and volunteers to drive around the affected area, which is also known as windshield survey. Another way to assess hurricane damage level is flood detection through synthetic aperture radar (SAR) images such as works at the Darthmouth Flood Observatory \cite{dfo}, or the damage proxy map to identify structures that may have been damaged at the Advanced Rapid Imaging and Analysis (ARIA) Project by Caltech and NASA \cite{aria}. SAR imagery is useful in terms of mapping different surface features, texture, or roughness pattern but is harder for laymen to interpret than optical sensor imagery. %It is also harder to collect SAR imagery at a large scale due to the requirement of specialized equipment. Hence, 
In this paper, we focus on using optical sensor imagery as a more intuitive way to analyze hurricane damage. From here onwards, we will refer to optical sensor imagery as `imagery'.

Recently, imagery produced by drones and satellites have started to help improve situational awareness from a bird's eye view, but the process still relies on human visual inspection which is generally time-consuming and unreliable during an evolving disaster. Computer vision techniques, therefore, prove to be particularly useful. Given the available imagery, our proposed method can automatically detect \textit{`Flooded/Damaged Building' vs `Undamaged Building'} on satellite imagery of an area affected by a hurricane. This could give the stakeholders useful information about the severity of the damage to plan for and organize the necessary resources. With decent accuracy and quick enough runtime, this process is expected to significantly reduce the time for building situational awareness and responding to hurricane-induced emergencies. 

The satellite imagery data used in the paper was captured by optical sensors with sub-meter resolution and preprocessed for orthorectification, atmospheric compensation, and pansharpening from the Greater Houston area before and after Hurricane Harvey in 2017 (Figure~\ref{fig:Harvey}). The damaged buildings were labeled by volunteers through crowd-sourcing. We then process, filter, and clean the dataset to ensure that it has higher quality and can be learned appropriately by the deep learning algorithm. 


\begin{figure}[h]{\centering
\includegraphics[height=2.5in]{Harvey2017.png} \\
\centering\caption{\small{Affected areas during Hurricane Harvey in 2017. The green dots are coordinates with damaged buildings/roads tagged by volunteers.}}
\label{fig:Harvey}
}
\end{figure}

Through this paper, we hope that other researchers can use the dataset to study and experiment with different uses of satellite imagery in disaster response. In addition, our framework can be applied to future hurricane events to improve damage assessment and resource planning. We also provide a pre-trained deep-learning architecture that achieves satisfactory result in terms of classification accuracy. It can facilitate transfer learning either in feature extraction, fine-tuning, or as a baseline model to speed up the learning process in future development/events with similar properties. 

The remaining of the paper is organized as follows. In Section ~\ref{sec:background}, we present a brief review of convolutional neural networks, machine learning-based damage detection work on post-hurricane satellite imagery, and challenges in processing satellite imagery. Section ~\ref{sec:method} describes our framework from collecting data to damage detection. Detailed implementation and discussion of results are shown in Section ~\ref{sec:case_study}. Finally, Section ~\ref{sec:conclusion} concludes our work and draws some future research directions. 













\section{Background}\label{sec:background}
\subsection{Convolutional neural network}
Object detection is a ubiquitous topic in computer vision, thanks to the development of convolutional neural network (CNN) \cite{cnn}. CNNs have proved to yield outstanding results over other algorithms in computer vision tasks such as natural language processing \cite{cnn-nlp}, object categorization \cite{cnn-object}, image classification \cite{cnn-image,cnn-imagenet}, or traffic sign recognition \cite{trafficSign}. Variations of CNN have been applied in remote sensing image processing tasks \cite{Zhang2016} such as aerial scene classification \cite{aid, aerial-label, scene-multiscale}, SAR imagery classification \cite{sar-cnn}, or object detection in unmanned aerial vehicle imagery \cite{Bazi2018}.

Structurally, CNN is a feed-forward network that is particularly powerful in extracting hierarchical features in images. The common structure of CNN has three components: the convolutional layer, the sub-sampling layer, and the fully connected layer as demonstrated in Figure~\ref{fig:cnn-archi}. 

\begin{figure*}[h]{\centering
\includegraphics[width=7in]{cnn-archi.png} \\
\centering\caption{A sample structure of convolutional neural networks, inspired by LeNet-5 architecture in \cite{cnn} ; C: Convolutional layer, S: Sub-sampling layer, F: Fully connected layer; 32@(148x148) means there are 32 filters to extract the features from the input image, and the original input size of 150x150 is reduced to 148x148 since no padding is added around the edges during $3\times3$ convolution operations so 2 edge rows and 2 edge columns are lost; 2x2 Max-pooling means the data will be reduced by a factor of 4 after each operation; Output layer has 1 neuron since the network outputs the probability of one class (`Flooded/Damaged Building'), and the other probability is just 1-$\mathbb{P}$r(`Flooded/Damaged Building').}
\label{fig:cnn-archi}
}
\end{figure*}

In the convolutional layer (C in Figure~\ref{fig:cnn-archi}), each element (or neuron) of the network in a layer receives information from a small region of the previous layer. A 3x3 convolutional filter will take a dot product of 9 weight parameters with 9 pixels (3x3 patch) of the input, transform the value by an activation function, and become a neuron value in the next layer. The same region can yield many information maps to the next layer through many convolutional filters. In Figure~\ref{fig:cnn-archi}, at convolutional layer C1, we have 32 filters that represent 32 ways to extract features from the previous layers and form a stack of 32 feature matrices. Another interesting property of CNN is its robustness to shifts and distortion of the input images \cite{shift-invariant}. This is crucial since in many datasets, the objects of interest are usually not positioned right at the center of the images and we want to learn the features, not their position. 

In the sub-sampling layer (S in Figure~\ref{fig:cnn-archi}), the network performs either local averaging or max pooling over a patch of the input. If the sub-sampling layer size is 2x2 such as S2, local averaging will produce the mean of the 4 nearby convoluted pixel values, whereas max pooling will produce the maximum number among them. Essentially, this operation reduces the input feature matrix to half its number of columns and rows, which helps to reduce the resolution by a factor of 4 and the network's sensitivity to shift and distortion. 

After the features are extracted and resolution reduced, the network will flatten the final stack of feature matrices into a feature vector and pass it through a sequence of fully connected layers (F in Figure~\ref{fig:cnn-archi}). Each subsequent layer's output neuron is a dot product between the feature vector and a weight vector, transformed by a non-linear activation function. In this paper, the last layer has only 1 neuron, which is the probability of a reference class (`Flooded/Damaged building'). 

As mentioned, the dot products are transformed by an activation function. This gives a neural network, with adequate size, the ability to model any function. Some common activation functions include sigmoid $f(x) = \frac{1}{1+e^{-x}}$, rectified linear unit (ReLU) $f(x) = max(0, x)$, or leaky ReLu $f(x) = max(\alpha x, x)$, with $0 < \alpha \ll 1$. There is no clear reason to choose any specific function over the others to improve performance of the network. However, using ReLU may speed up the training without affecting performance \cite{relu}.

\subsection{Machine learning-based damage detection on post-hurricane satellite imagery}

Within the area of remote sensing, object detection based on satellite imagery is a well-established research area. Nevertheless, few studies have investigated machine learning based damage detection on post-hurricane satellite imagery. A small project studied detecting \textit{flooded roads} by comparing pre-event and post-event satellite imagery \cite{Jack2017} but the method is not applicable to other types of damages. Two commercial vendors of satellite imagery also separately developed unsupervised algorithms to detect flooded area using spectral signature of impure water (which is not available from the pansharpened satellite images in our data) \cite{planet, gbd}. Before deep learning era, a method using a pattern recognition template set was applied to detect hurricane damages in \textit{multispectral} images \cite{Barnes2007} but the method is not applicable to our pansharpened images. 

\subsection{Challenges in damage detection from satellite imagery}
There are multiple challenges in damage detection from satellite imagery. First, the satellite imagery resolution of the object of interest is not as high as the various benchmark datasets commonly used to train neural networks (NNs) such as common objects (e.g ImageNet \cite{cnn-imagenet} or traffic signs \cite{trafficSign}. Dodge \& Karam studied the performance of NNs under quality distortions and highlighted that NNs could be prone to errors in blurry and noisy images \cite{Dodge2016}. Although our dataset is of relatively high quality, e.g., one of the satellites capturing the imagery is the GeoEye-1, which has 46cm panchromatic resolution \cite{geoeye}, it is still far from the resolution in animal or vehicle detection datasets. In fact, the labeling task is hard even with human visual inspection, which leads to another challenge. The volunteers' annotation could be erroneous. To limit this, the imagery provider has a proprietary system that computes the agreement score of each label. In this paper, we ignore this information to gather as many labels as possible and take the given labels as ground truth since limited size of training data could be a critical bottle-neck for models with many parameters to learn such as NNs. Third, there are some inconsistencies in image quality. Since the same region can be captured multiple times in different days, the same coordinate may have different quality and orthorectification, as shown in Figure~\ref{fig:orthorectification}. 

\begin{figure}[h]{\centering
\subfigure[Not orthorectified correctly]{\includegraphics[width=1.5in]{orthor1-1.png}}
\subfigure[Orthorectified correctly]{\includegraphics[width=1.5in]{orthor1-2.png}}
\subfigure[More blurry]{\includegraphics[width=1.5in]{orthor2-1.png}}
\subfigure[Less blurry]{\includegraphics[width=1.5in]{orthor2-2.png}}
\\
\caption{\small{Different orthorectification and processing quality  of the same location in different days}}
\label{fig:orthorectification}
}
\end{figure}
\section{Methodology}\label{sec:method}
In this section, we describe our end-to-end framework from collecting, processing, featurizing data to building the convolutional neural network to classify and detect damage. 

\subsection{Data description}
\begin{enumerate}
    \item The raw imagery data covering the Greater Houston area was captured in about four thousand strips ($\sim$ 400 million pixels ($\sim$ 1GB) with RGB bands per strip) in different days. Hence, some strips can overlap, leading to some images blacked out at the boundaries. In some days, the images are also covered fully or partially by clouds.
Figure~\ref{fig:strip} shows a typical strip in the data set and Figure~\ref{fig:bad_images} shows some examples of low quality images in 128x128 pixels that we choose to discard.


\begin{figure}[h]{\centering
\includegraphics[height=3.2in]{strip.png} \\
\centering\caption{\small{A typical strip of image}}
\label{fig:strip}
}
\end{figure}


\begin{figure}[h]{\centering
\subfigure[Blacked out partially]{\includegraphics[width=1.4in]{bad1.jpeg}}
\subfigure[Covered by cloud partially]{\includegraphics[width=1.4in]{bad2.jpeg}}
\subfigure[Covered by cloud mostly]{\includegraphics[width=1.4in]{bad3.jpeg}}
\subfigure[Covered by cloud totally]{\includegraphics[width=1.4in]{bad4.jpeg}}
\\
\caption{\small{Examples of 128x128-pixel low quality images}}
\label{fig:bad_images}
}
\end{figure}

    \item The raw data is in geoTIFF format
    \item \textit{Flooded/Damaged} buildings (or \textit{Damaged}) are annotated with labels and coordinates given in GeoJSON format. Using the coordinates, we extract the images of damaged buildings in JPEG format from the geoTIFF \textit{post-event} imagery. 
    \item \textit{Undamaged buildings} (or \textit{Undamaged}) are extracted in JPEG format directly from the geoTIFF \textit{pre-event} imagery at the same coordinates.
\end{enumerate}

\subsection{Damage annotation}\label{sec:annotation}
We present here a framework (Figure~\ref{fig:framework}) from raw data to damage annotation. Since there is no readily available data for model training, the first obvious step is to generate the data. We adopt a cropping window approach. Essentially, the building coordinates, which are either easily obtained publicly or available from crowd-sourcing projects, are used as the center of a window. The window is then cropped from the raw satellite imagery to create a data sample. The optimal window size will depend on various factors including the image resolution and building footprint sizes.. Too small windows may limit the background information contained in each sample, whereas too large ones may introduce unnecessary noise. We keep the window size as a tuning hyper-parameter in the model. A few sizes are considered such as 400x400, 128x128, 64x64, and 32x32. The cropped images are then manually filtered to ensure the high quality of the dataset. To let the model generalize well, we only discard obviously flawed images, as shown in Figure~\ref{fig:bad_images}. The clean images are then split into training, validation, and test sets and fed to a convolutional neural network for damage annotation as illustrated in Figure~\ref{fig:framework}. Validation accuracy is monitored to tune the necessary hyper-parameters and the window size.

\begin{figure}
\begin{center}
\tikzstyle{decision} = [diamond, draw, fill=blue!20, 
    text width=4.5em, text badly centered, node distance=3cm, inner sep=0pt]
\tikzstyle{block} = [rectangle, draw, fill=blue!20, 
    text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, rectangle, minimum width=1.5cm, minimum height=1cm, text centered, text width=1.5cm, draw=black, fill=red!20]
    



\begin{tikzpicture}[node distance = 2.8cm, auto, cloud/.style={draw,
fill=pink,
rectangle, 
minimum width={width("hyper-paramters")+2pt}}]
    \node[block] (input) {Raw imagery};
    \node[cloud, left of=input] (coordinate) [left = 1cm]{coordinates and labels};
    \node[cloud, right of=input] (size) [right = 1cm]{window size};
    \node[block, below of=input] (label) {Labeled images};
    \node[cloud, left of=label] (split) [left = 1cm] {train-validation-test split};
    \node[cloud, right of=label] (filter) {manual filter};
    \node[block, below of=label] (clean) {Clean dataset};
    \node[cloud, left of=clean] (hyper) [left = 1cm] {hyper-parameters};
    \node[decision, below of=clean] (decide) {Damage annotation};
    \path[line] (input) -- (label);
    \path[line] (label) -- (clean);
    \path[line] (clean) -- (decide);
    \path[line,dashed] (coordinate) -- (label);
    \path[line,dashed] (size) -- (label);
    \path[line,dashed] (split) -- (clean);
    \path[line,dashed] (filter) -- (clean);
    \path[line,dashed] (hyper) -- (decide);
    \path[line,dashed] (decide) -| node [near start] {update} (hyper);
        \path[line,dashed] (decide) -| node [near start] {update} (size);
\end{tikzpicture}
\end{center}

\caption{Damage annotation framework} \label{fig:framework}

\end{figure}

\subsection{Data processing}\label{sec:collection}
As mentioned above, the data generation starts from a building coordinate. Since there are many raw geoTIFF files containing the same coordinates, there are many duplicate images with different quality. This can potentially inflate the prediction accuracy as the same coordinate may both appear in the training and test sets. We maintain an unordered set of the available coordinates and make sure each coordinate yields a unique, ``good-quality" image in the dataset through a semi-automated process. We first automatically discard the totally blacked out images for each coordinate, and keep the first image we encounter that is not totally black. The remaining images are manually filtered to eliminate images that are partially black, and/or covered by clouds. 



\subsection{Data featurization}\label{sec:featurization}
Since we control the window size through physical distance, there could be round off errors when converting the distance to the number of pixels. When we featurize the images, we project them into the same feature dimension. For instance, 128x128 images are projected into 150x150 dimension. The images are then fed through a CNN to further extract the right set of features, such as edge extraction in Figure~\ref{fig:one_filter}.

\begin{figure}[h]{\centering
\subfigure[Original image (Damaged)]{\includegraphics[width=1.5in]{filter-0.png}}
\subfigure[After $1^{st}$ layer]{\includegraphics[width=1.5in]{filter-1.png}}
\subfigure[After $2^{nd}$ layer]{\includegraphics[width=1.5in]{filter-2.png}}
\subfigure[After $3^{rd}$ layer]{\includegraphics[width=1.5in]{filter-3.png}}
\\
\caption{\small{Information flow within one filter}}
\label{fig:one_filter}
}
\end{figure}

How to construct the most suitable CNN architecture is an ongoing research problem. The practice is usually starting with an existing architecture and fine-tune further from there. We experiment with a well-known architecture, VGG-16 \cite{vgg16}, and modify the first layer to suit our input dimension. VGG-16 can perform extremely well on the ImageNet dataset for object detection. 

However, realizing the crucial differences between common objects detection and damage building annotation tasks, we also build our own network from scratch with proper hyper-parameters. Our basis for determining the size and depth of a customized network is to monitor the information flow through the network and stop appropriately when there are too many ``dead" filters. Due to the nature of the rectified linear unit (ReLU) which is defined as $max(0,x)$, there will be many hard 0 in the hidden layer. Although sparsity in the layer will promote the model to generalize better, it can potentially cause the problem to gradient computation at 0 and hurt performance \cite{relu,leaky}. We see that in Figure ~\ref{fig:flow} after four convolutional layers, about 30\% of the filters are ``dead" and will not carry further information to the subsequent layers. This is a significant stopping criterion since we can avoid a deep network such as VGG-16 to save the computational time and safeguard satisfactory information flow in the network at the same time. 

We present our network architecture that achieves the best result in Table \ref{tab:architecture}. An example of how the number of parameters is calculated is as follows: in the first 2-D convolutional layer, there will be 32 convolutional filters, each of size (3x3), for each of the 3 RGB channels of the input image. Including 1 more bias parameter for each filter, we have: \\
\[
[(3 \times 3) \times 3 + 1] \times 32 = 896
\]
In our CNN structure, with four convolutional layers and two fully connected layers, there are already about $3.5$ million parameters to train for, given $67,500$ pixels as an input vector for each image. The VGG-16 structure (not shown here explicitly), with thirteen convolutional layers, has almost $15$ million trainable parameters, which can easily over-fit, be more resources-consuming and reduce generalization performance on the testing data. 

\begin{table}[t]
\caption{Convolutional neural network architecture which achieves the best result}
\begin{center}
\begin{tabular}{lllll}
\multicolumn{1}{c}{\bf Layer type}  &\multicolumn{1}{c}{\bf Output shape}
   &\multicolumn{1}{c}{\bf \begin{tabular}{@{}c@{}}Number of \\ trainable parameters \end{tabular}}

\\ \hline 
Input                       &3@(150x150)     &0  \\
2-D Convolutional 32@(3x3)        &32@(148x148)       &896   \\
2-D Max pooling (2x2)        &32@(74x74)       &0   \\
2-D Convolutional 64@(3x3)        &64@(72x72)       &18,496  \\
2-D Max pooling (2x2)         &64@(36x36)     &0   \\
2-D Convolutional 128@(3x3)        &128@(34x34)       &73,856   \\
2-D Max pooling (2x2)        &128@(17x17)       &0   \\
2-D Convolutional 128@(3x3)        &128@(15x15)       &147,584   \\
2-D Max pooling (2x2)        &128@(7x7)       &0   \\
Flattening      &1x6272       &0  \\
Dropout         &1x6272       &0  \\
Fully connected layer &1x512       &3,211,776  \\
Fully connected layer &1x1       &513  \\
\hline
\end{tabular}
\end{center}
Note:\\
Total number of trainable parameters: 3,453,121\\
C@($A\times B$) is interpreted as there are a total of C matrices of shape ($A\times B$) stacked on top of one another to form a three dimensional tensor.\\
2-D Max pooling layer with ($2\times 2$) pooling size means the input tensor's size will be reduced by a factor of 4. \\
\label{tab:architecture}
\end{table}


\begin{figure}[h]{\centering
\subfigure[After $1^{st}$ layer]{\includegraphics[width=2.5in]{flow1.png}}
\subfigure[After $2^{nd}$ layer]{\includegraphics[width=2.5in]{flow2.png}}
\subfigure[After $3^{rd}$ layer]{\includegraphics[width=2.5in]{flow3.png}}
\subfigure[After $4^{th}$ layer]{\includegraphics[width=2.5in]{flow4.png}}
\\
\caption{\small{Information flow in all filters after each layer}}
\label{fig:flow}
}
\end{figure}




\subsection{Image classification}\label{sec:classification}
Due to the limited availability of pre-event images and the presence of flawed images in the \textit{Damaged} and \textit{Undamaged} categories, we experience an unbalanced dataset with the majority class being \textit{Damaged}. As a result, the following method for splitting training, validation, and test datasets is adopted. We keep the training and validation sets balanced and leave the remaining data to construct 2 test sets, a balanced set and an unbalanced (with a ratio of 1:8) set. 

The first performance metric is the classification accuracy. Based on the unbalanced test set, the baseline performance is determined by annotating all buildings as the majority class \textit{Damaged} with $\frac{8}{9}=88.89\%$ accuracy. To be comprehensive, we also monitor the area under the receiver operating characteristic curve (AUC).









\section{Implementation and Result}\label{sec:case_study}
We train the neural network through the \textit{Keras} library with TensorFlow backend with a single NVIDIA K80 Tesla GPU with 64GB memory on a quad-core CPU machine. The network weights are initialized through Xavier initializer \cite{xavier}. The mini batch size for stochastic gradient descent optimizer is either 20 or 32.

After cleaning and manual filtering, we are left with 14,284 positive samples (\textit{Damaged}) and 7,209 negative samples (\textit{Undamaged}) of unique coordinates. 5,000 samples of each class are in the training set. 1,000 samples of each class are in the validation set. The rest of the data are reserved to form the test sets. 

Since it is computationally costly to train the CNN repeatedly, we do not tune all the hyper-parameters through a full grid search or full cross-validation. Only some reasonable combinations of the hyper-parameters are considered in a greedy manner. Among the parameters, the window size is truly a challenge. We do not try all the sizes with all hyper-parameters. We first implement a simple model with all the window sizes and find that 128x128 window yields an ideal result. 

We also implement a logistic regression (LR) on the featurized data to see how it compares to fully connected layers. LR under-performs in most cases but also achieves quite good accuracy. This illustrates that the image featurization through the network is very crucial to extract good features such that a simple algorithm can perform well enough on this data. 

For activation functions in CNN, a rectified linear unit (ReLU) is a common choice, thanks to its simplicity in gradient computation and prevention of vanishing gradient. As seen in  Figure ~\ref{fig:flow}, clamping the activation at $0$ could potentially cause a lot of filters to be dead. Therefore, we also consider using a leaky ReLU activation, with  $\alpha = 0.1$ in this case, based on the survey in \cite{leaky}. However, leaky ReLU does not improve the accuracy very much. 

To counter over-fitting, which is a recurrent problem of deep learning, we also adopt data augmentation in the training set through random rotation, horizontal flip, vertical and horizontal shift, shear, and zoom. This can effectively increase the number of training samples to ensure more generalization and achieve better validation and test accuracy (we do not perform data augmentation in the validation and test set). Furthermore, we also employ 50\% dropout and L2 regularization with $\lambda = 10^{-6}$ in the fully connected layer. These measures are shown to fight over-fitting effectively and significantly improve the validation accuracy in Figure~\ref{fig:dropout}.

\begin{figure}[h]
\hspace*{-0.4cm}{\centering
\subfigure[Over-fitting happens after a few epochs]{\includegraphics[trim= 0 0 0 21, clip, width=3.37in]{4-acc-transfer.png}}
\hspace*{0.3cm}\subfigure[Little sign of over-fitting]{\includegraphics[trim= 5 0 0 20, clip, width=2.84in]{2-acc-100epochs-aug-drop.png}}
\\
\caption{\small{Prevent over-fitting using data augmentation, drop-out, and regularization}}
\label{fig:dropout}
}
\end{figure}

As mentioned in Section ~\ref{sec:featurization}, we consider using a pre-built architecture VGG-16 (transfer learning) and building a fresh network. In Figure~\ref{fig:Transfer_vs_Custom}, we see that using a deeper and larger network we can achieve a high level accuracy earlier but accuracy pretty much plateaus (over-fitting happens) after a few epochs. Our simpler network can facilitate learning gradually, where accuracy keeps increasing to achieve better value, and take much less time to train. 


\begin{figure}[h]
\hspace*{-1cm}{\centering
\subfigure[Transfer learning using pre-built network]{\includegraphics[trim= 0 0 0 20.2, clip, width=3.676in]{8-everything-reulu-adam-acc.png}}
\subfigure[Custom network]{\includegraphics[trim= 0 0 0 19, clip,width=2.85in]{2-acc-100epochs-aug-drop-adam.png}}
\\
\caption{\small{Comparison between using a pre-built network and our custom network}}
\label{fig:Transfer_vs_Custom}
}
\end{figure}

We use two adaptive, momentum based optimizers RMSprop and Adam \cite{Adam} with initial learning rate of $10^{-4}$. Adam generally leads to about 1\% higher validation accuracy and it can be seen that Adam leads to less noisy learning (Figure~\ref{fig:Adam_vs_RMSprop}). 


\begin{figure}[h]{\centering
\subfigure[RMSprop]{\includegraphics[trim= 4 2 0 21, clip,width=2.9in]{2-acc-100epochs-aug-drop.png}}
\subfigure[Adam]{\includegraphics[trim= 0 0 3 19, clip,width=3.2in]{2-acc-100epochs-aug-drop-adam.png}}
\\
\caption{\small{Comparison between using RMSprop and Adam optimizers}}
\label{fig:Adam_vs_RMSprop}
}
\end{figure}

Table \ref{tab:accuracy} demonstrates the performance of various models. The best performing model is our customized model with data augmentation and dropout using Adam optimizer, which can achieve 97.08\% accuracy on the unbalanced test set. The AUC metric is also computed and shows a satisfying result of 99.8\% on the unbalanced test set. 

\begin{table*}[t]
\caption{Model performance}
\begin{center}
\begin{tabular}{lllll}
\multicolumn{1}{c}{\bf Model}  &\multicolumn{1}{c}{\bf Validation Accuracy}
  &\multicolumn{1}{c}{\bf \begin{tabular}{@{}c@{}}Test Accuracy \\ (Balanced) \end{tabular}}
   &\multicolumn{1}{c}{\bf \begin{tabular}{@{}c@{}}Test Accuracy \\ (Unbalanced) \end{tabular}}

\\ \hline %\\
CNN         &95.8\%      &94.69\%  &95.47\% \\
Leaky CNN   &96.1\%       &94.79\%  &95.27\%\\
CNN + DA + DO  &97.44\%      &96.44\%  &96.56\%\\
CNN + DA + DO (Adam) &\bf{98.06\%}     &\bf{97.29\%} &\bf{97.08\%}\\
Transfer + DO   &93.45\%      &92.8\%   &92.8\%\\
Transfer + DA + DO  &91.1\%       &88.49\%  &85.99\%\\
LR + L2 = 1    &93.55\%      &92.2\%   &91.45\%\\
Transfer + DA + FDO &96.5\%       &95.34\%  &95.73\%\\
Leaky+Transfer + DA + FDO +L2  &96.13\%        &95.59\% &95.68\%\\
Leaky+ Transfer + DA + FDO +L2 (Adam) &97.5\%  &96.19\% &96.21\%\\
\hline
\end{tabular}
\end{center}
\bigskip

Legend: CNN: Convolutional Neural Network; Leaky: Leaky ReLU activation function, else, default is ReLU; DA: Data Augmentation; LR: Logistic Regression; L2: L2 regularization; (Adam): Adam optimizer, else, default is RMSprop optimizer; DO: 50\% drop out only in the fully connected layer; FDO: Full drop out, i.e 25\% drop out after every max pooling layer and 50\% in the fully connected layer; Transfer: Transfer learning using VGG-16 architecture 
\label{tab:accuracy}
\end{table*}

\begin{figure}[h]{\centering
\subfigure[AUC of balanced test set]{\includegraphics[trim= 0 0 0 20, clip,width=3in]{AUC_balanced.png}}
\subfigure[AUC of unbalanced test set]{\includegraphics[trim= 0 0 0 19.8, clip,width=3in]{AUC_unbalanced.png}}
\\
\caption{\small{AUC for balanced and unbalanced test sets using our best performing model (CNN + DA + DO (Adam)) }}
\label{fig:AUC}
}
\end{figure}

Although the result is satisfactory, we also look at a few typical cases where the algorithm makes wrong classification to see if any intuition can be derived. Figure~\ref{fig:false_positive} shows some of the false positive cases. We hypothesize that the algorithm could predict the damage through flood water and/or debris detection. Under such hypothesis, the cars in the center of Figure~\ref{fig:false_positive}(a), the lake water in Figure~\ref{fig:false_positive}(b), the cloud covering the house in Figure~\ref{fig:false_positive}(c), and the trees covering the roof in Figure~\ref{fig:false_positive}(f) can potentially mislead the model. For the false negative cases in Figure~\ref{fig:false_negative}, it is harder to make sense out of the prediction. Even through visual inspection, we cannot see Figures~\ref{fig:false_negative}(a)(b) as being damaged. There could potentially be tagging mistakes by the volunteers. On the other hand, Figures~\ref{fig:false_negative}(e)(f) are clearly damaged but the algorithm misses them. 

\begin{figure}[h]{\centering
\subfigure[]{\includegraphics[trim= 0 0 0 20, clip,width=1.5in]{confused_by_cars.png}}
\subfigure[]{\includegraphics[trim= 0 0 0 20, clip,width=1.51in]{confused_by_roof.png}}
\subfigure[]
{\includegraphics[trim= 0 0 0 20, clip,width=1.52in]{confused_by_cloud.png}}
\subfigure[]
{\includegraphics[trim= 0 0 0 20, clip,width=1.55in]{confused_by_water.png}}
\subfigure[]
{\includegraphics[trim= 0 0 0 20, clip,width=1.55in]{confused_by_wate_roof.png}}
\subfigure[]
{\includegraphics[trim= 0 0 0 20, clip,width=1.5in]{confused_by_tree.png}}
\\
\caption{\small{False positive examples (label is Undamaged, prediction is Damaged)}}
\label{fig:false_positive}
}
\end{figure}

\begin{figure}[h]{\centering
\subfigure[]{\raisebox{0.5mm}{\includegraphics[trim= 0 0 0 20, clip,width=1.5in]{volunteer_mistake.png}}}
\subfigure[]{\includegraphics[trim= 0 0 0 20, clip,width =1.5in]{not_clear_damage_.png}}
\subfigure[]
{\includegraphics[trim= 0 0 0 20, clip,width=1.48in]{not_clear_damage_2.png}}
\subfigure[]
{\includegraphics[trim= 0 0 0 20, clip,width=1.48in]{not_clear_damage_3.png}}
\subfigure[]
{\includegraphics[trim= 0 0 0 20, clip,width=1.5in]{machine_mistake.png}}
\subfigure[]
{\includegraphics[trim= 0 0 0 21, clip,width=1.5in]{machine_mistakes.png}}
\\
\caption{\small{False negative examples (label is Damaged, prediction is Undamaged)}}
\label{fig:false_negative}
}
\end{figure}






\section{Conclusion and Future Research}\label{sec:conclusion}
We demonstrated that through deep learning, automatic detection of damaged buildings can be done satisfactorily. Although our data can be specific to the geographical condition and building properties in the Greater Houston area during Hurricane Harvey, the model can be further improved and generalized to other future disaster events in other regions if we can collect more positives samples from other past events and negative samples from other areas. 

For faster disaster response, we need a model that can work with low quality images generated on a particular day, especially near the hurricane landfall, which can be covered by cloud or imperfectly orthorectified. We will further investigate the model to see if it can be robust against such noise and distortion to reduce the amount of manual processing.


Through the inspection of false positive cases, we realized there could be a link to pixel-level classification to segment different damage types and levels. For instance, we can classify flooded buildings and wrecked buildings separately. More accurate classification may be achieved through more exact shapes of different types of damage. However, this requires a massive effort to label different damage shapes and types to train the model. 

We also wish to expand the current research to road damage annotation which could help plan effective transportation routes of food, medical equipment, or fuels to the disaster victims. 


\section*{Acknowledgement}
This work was partially supported by the National Science Foundation (NSF grant CMMI-1824681). We would like to thank DigitalGlobe for data sharing through their Open Data Program. We also thank Amy Xu, Aryton Tediarjo, Daniel Colina, Dengxian Yang, Mary Barnes, Nick Monsees, Ty Good, Xiaoyan Peng, Xuejiao Li, Yu-Ting Chen, Zach McCauley, Zechariah Cheung, and Zhanlin Liu in the Disaster Data Science Lab at the University of Washington, Seattle for their help with data collection and processing. 
\appendices
\section{Dataset and code used in this paper}
The dataset and code used in this paper are available at the first author's Github repository: \url{https://github.com/qcao10/DamageDetection}. The dataset is also available at the IEEE DataPort (DOI: 10.21227/284c-p879).\\





\ifCLASSOPTIONcaptionsoff
  \newpage
\fi





\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,tomnod_BIB}




\begin{IEEEbiography}[{\includegraphics[width=1.1in,height=1.25in,clip]{ProfessionalPhoto.png}}]{Quoc Dung Cao}
is a Ph.D. student at the University of Washington, where he is researching on
computer vision application to disaster management. His works involve analyzing satellite imagery to quantify damage level after a hurricane event. He is also investigating the model robustness against noise and distortion, which is often unavoidable in geo-information data. He also wishes to expand the result and methodology to road damage annotation which could help plan effective transportation routes of food, medical equipment, or fuels to the disaster victims.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1.1in,height=1.25in,clip]{20171210_163053-IEEE-bio-Choe.jpg}}]{Youngjun Choe}
received the B.Sc. degrees in Physics and Management Science from KAIST, Korea in 2010, and both the M.A. degree in Statistics and the Ph.D. degree in Industrial \& Operations Engineering from the University of Michigan, Ann Arbor, MI, USA in 2016. 

He is currently an Assistant Professor of Industrial \& Systems Engineering at the University of Washington, Seattle, WA, USA. His research centers around developing statistical methods to infer on extreme events using empirical and simulated data. 
\end{IEEEbiography}










\end{document}
