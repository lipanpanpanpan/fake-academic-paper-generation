\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}



\usepackage[final]{neurips_2018}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{amsmath,amssymb,bm}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{multirow,multicol,caption}
\usepackage{comment}
\usepackage{makecell}
\usepackage{hhline}
\usepackage{cellspace}

\usepackage{tikz}
\usetikzlibrary{arrows,shapes,positioning}
\usetikzlibrary{calc,decorations.markings}
\usetikzlibrary{decorations.pathreplacing}

\newtheorem{theorem}{Theorem}
\newtheorem{lem}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}{Remark}

\DeclareMathOperator*{\esssup}{ess\,sup}
\DeclareMathOperator*{\argmin}{argmin}

\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\xb}{\bm{x}}
\newcommand{\Xb}{\bm{X}}
\newcommand{\yb}{\bm{y}}
\newcommand{\zb}{\bm{z}}
\newcommand{\ub}{\bm{u}}
\newcommand{\wb}{\bm{w}}
\newcommand{\vb}{\bm{v}}
\newcommand{\hb}{\bm{h}}
\newcommand{\Ab}{\bm{A}}
\newcommand{\Qb}{\bm{Q}}
\newcommand{\Cb}{\bm{C}}
\newcommand{\Rb}{\bm{R}}
\newcommand{\sigb}{\bm{\sigma}}
\newcommand{\Ob}{\bm{O}}
\newcommand{\cb}{\bm{c}}
\newcommand{\Bb}{\bm{B}}
\newcommand{\Wb}{\bm{W}}
\newcommand{\Ub}{\bm{U}}
\newcommand{\Vb}{\bm{V}}
\newcommand{\bb}{\bm{b}}
\newcommand{\fb}{\bm{f}}
\newcommand{\Kb}{\bm{K}}
\newcommand{\sbb}{\bm{s}}
\newcommand{\Jb}{\bm{J}}
\newcommand{\ib}{\bm{i}}
\newcommand{\ob}{\bm{o}}
\newcommand{\Sb}{\bm{S}}
\newcommand{\Fb}{\bm{F}}
\newcommand{\gb}{\bm{g}}
\newcommand{\pb}{\bm{p}}
\newcommand{\rb}{\bm{r}}
\newcommand{\eps}{\epsilon}

\title{Nonlocal Neural Networks, Nonlocal Diffusion and Nonlocal Modeling}


\author{
  Yunzhe Tao \\
  School of Engineering and Applied Science\\
  Columbia University, USA\\
  \texttt{y.tao@columbia.edu} \\
   \And
   Qi Sun \\
  BCSRC \& USTC \\
  Beijing, China \\
   \texttt{sunqi@csrc.ac.cn} \\
   \AND
   Qiang Du \\
  School of Engineering and Applied Science\\
  Columbia University, USA\\
   \texttt{qd2125@columbia.edu} \\
   \And
   Wei Liu \\
   Tencent AI Lab \\
   Shenzhen, China \\
   \texttt{wl2223@columbia.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
Nonlocal neural networks \cite{wang2017non} have been proposed and shown to be effective in several computer vision tasks, where the nonlocal operations can directly capture long-range dependencies in the feature space. In this paper, we study the nature of diffusion and damping effect of nonlocal networks by doing spectrum analysis on the weight matrices of the well-trained networks, and then propose a new formulation of the nonlocal block. The new block not only learns the nonlocal interactions but also has stable dynamics, thus allowing deeper nonlocal structures. Moreover, we interpret our formulation from the general nonlocal modeling perspective, where we make connections between the proposed nonlocal network and other nonlocal models, such as nonlocal diffusion process and Markov jump process.
\end{abstract}

\section{Introduction}\label{sec:intro}
Deep neural networks, especially convolutional neural networks (CNNs) \cite{lecun1995convolutional} and recurrent neural networks (RNNs) \cite{elman1991distributed}, have been widely used in a variety of subjects \cite{lecun2015deep}. However, traditional neural network blocks aim to learn the feature representations in a local sense. For example, both convolutional and recurrent operations process a local neighborhood (several nearest neighboring neurons) in either space or time. Therefore, the long-range dependencies can only be captured when these operations are applied recursively, while those long-range dependencies are sometimes significant in practical learning problems, such as image or video classification, text summarization, and financial market analysis \cite{beran1995long,cont2005long,pipiras2017long,willinger2003long}.

To address the above issue, a nonlocal neural network \cite{wang2017non} has been proposed recently, which is able to improve the performance on a couple of computer vision tasks. In contrast to convolutional or recurrent blocks, nonlocal operations \cite{wang2017non} capture long-range dependencies directly by computing interactions between each pair of positions in the feature space. Generally speaking, nonlocality is ubiquitous in nature, and the nonlocal models and algorithms have been studied in various domains of physical, biological and social sciences \cite{ buades2005non, coifman2006diffusion, du2012analysis,silling2000reformulation,tadmor2015mathematical}.

In this work, we aim to study the nature of nonlocal networks, namely, what the nonlocal blocks have exactly learned through training on a real-world task. By doing spectrum analysis on the weight matrices of the stacked nonlocal blocks obtained after training, we can largely quantify the characteristics of the damping effect of nonlocal blocks in a certain network.

Based on the nature of diffusion observed from experiments, we then propose a new nonlocal neural network which, motivated by the previous nonlocal modeling works, can be shown to be more generic and stable. Mathematically, we can make connections of the nonlocal network to a couple of existing nonlocal models, such as nonlocal diffusion process and Markov jump process. The proposed nonlocal network allows a deeper nonlocal structure, while keeping the long-range dependencies learned in a well-preserved feature space.




\section{Background}\label{sec:bg}
Nonlocal neural networks are usually employed and incorporated into existing cutting-edge model architectures, such as the residual network (ResNet) and its variants, so that one can take full advantage of nonlocal blocks in capturing long-range features. In this section, we briefly review nonlocal networks in the context of traditional image classification tasks and make a comparison among different neural networks. Note that while nonlocal operations are applicable to both space and time variables, we concentrate merely on the spatial nonlocality in this paper for brevity.

\subsection{Nonlocal Networks}
Denote by $X=[X_1, X_2, \cdots, X_M]$ an input sample or the feature representation of a sample, with $X_i$ ($i=1,\cdots,M$) being the feature at position $i$. A nonlocal block \cite{wang2017non} is defined as
\begin{equation}\label{nl-bl-old}
Z_i = X_i + \frac{W_{Z}}{\mathcal{C}_i(X)} \sum_{\forall j} \omega(X_i,X_j)g(X_j)\,.
\end{equation}
Here $1\le i\le M$, $W_Z$ is the weight matrix, and $Z_i$ is the output signal at position $i$. Computing $Z_i$ depends on the features at possibly all positions $j$. Function $g$ takes any feature $X_j$ as input and returns an embedded representation. The summation is normalized by a factor $\mathcal{C}_i(X)$. Moreover, a pairwise function $\omega$ computes a scalar between the feature at position $i$ and those at all possible positions $j$, which usually represents the similarity or affinity. For simplicity, we only consider $g$ in the form of a linear embedding: $g(X_j)=W_g X_j$, with some weight matrix $W_g$. As is suggested in \cite{wang2017non}, the choices for the pairwise affinity function $\omega$ can be, but not limited to, (embedded) Gaussian, dot product, and concatenation.

When incorporating nonlocal blocks into a ResNet, a nonlocal network can be written as
\begin{equation}\label{nl-net}
Z^{k+1} := Z^k + \mathcal{F}(Z^k\, ;W^k)\,,
\end{equation}
where $W^k$ is the parameter set, $k=0,1,\cdots,K$ with $K$ being the total number of network blocks, and $Z^k$ is the output signal at the $k$-th block with $Z^0=X$, the input sample of the ResNet. On one hand, if a nonlocal block is employed, the $i$-th component of $\mathcal{F}$ will be
\begin{equation}\label{nl-op}
\left[\mathcal{F}(Z^k\, ;W^k)\right]_i = \frac{W_Z^k}{\mathcal{C}_i(Z^k)}\sum_{\forall j} \omega(Z^k_i,Z^k_j) (W_g^k Z^k_j)\,,
\end{equation}
where $1\le i\le M$ and $W^k=\{W^k_{Z},  W^k_{g}\}$ includes the weight matrices to be learned. In addition, the normalization scalar $\mathcal{C}_i(Z^k)$ is defined as
\begin{equation}
\mathcal{C}_i(Z^k)=\sum_{\forall j} \omega(Z^k_i,Z^k_j)\,,\quad \text{for }i=1,\cdots,M\,.
\end{equation}
On the other hand, when the $k$-th block is a traditional residual block of, \textit{e.g.}, the pre-activation ResNet \cite{he2016identity}, it contains two stacks of batch normalization (BN) \cite{ioffe2015batch}, rectified linear unit (ReLU) \cite{nair2010rectified}, and weight layers, namely,
\begin{equation}\label{resnet-f}
\mathcal{F}(Z^k; W^k) = W^k_2 f \left(W^k_{1} f(Z^k) \right)\,,
\end{equation}
where $W^k=\{W^k_{1},  W^k_{2}\}$ contains the weight matrices, and $f = \text{ReLU}\circ \text{BN}$ denotes the composition of BN and ReLU.

\subsection{A Brief Comparison with Existing Neural Networks}
The most remarkable nature of nonlocal networks is that the nonlocal network can learn long-range correlations since the summation within a nonlocal block is taken over all possible candidates in the current feature space. The pairwise affinity function $\omega$ plays an important role in defining nonlocal blocks, which in some sense determines the level of nonlocality. On one hand, if $\omega$ is always positive, then the feature space will be totally connected and every two features can interact with each other. On the other hand, if $\omega$ is chosen to be a Dirac-delta function $\delta_{ij}$, namely $\omega(X_i,X_j)=1$ for $i=j$ and is $0$ otherwise, then the nonlocal block gets localized and will act like a simplified residual block. 

Besides, as also mentioned in \cite{wang2017non}, the nature of nonlocal networks is different from other popular neural network architectures, such as CNN \cite{lecun1995convolutional} and RNN \cite{elman1991distributed}. The convolutional or recurrent operation usually takes the weighted sum over only nearest few neighboring inputs or the latest few time steps, which is still of a local sense in contrast with the nonlocal operation.



\begin{figure}
\centering
\subfigure[1 block]{
        \includegraphics[width=.32\linewidth]{old11.pdf}}
\\
\subfigure[2 blocks]{
        \includegraphics[width=.32\linewidth]{old21.pdf}
        \includegraphics[width=.32\linewidth]{old22.pdf}}
\subfigure[3 blocks]{
        \includegraphics[width=.32\linewidth]{old31.pdf}        		
        \includegraphics[width=.32\linewidth]{old32.pdf}
        \includegraphics[width=.32\linewidth]{old33.pdf}}
      \caption{Top 32 eigenvalues of the symmetric part of weight matrices with respect to nonlocal blocks. Figures (a), (b) and (c) correspond to the cases of adding 1, 2 and 3 nonlocal blocks to PreResNet-20, respectively. Y-axis represents the values.}
\label{fig:nl-old1}
\end{figure}



\section{The Damping Effect of Nonlocal Networks}\label{sec:model}
In this section, we first demonstrate the damping effect of nonlocal networks by presenting weight analysis on the well-trained network. More specifically, we train the nonlocal networks for image classification on the CIFAR-10 dataset \cite{krizhevsky2009learning}, which consists of 50k training images from 10 classes, and do the spectrum analysis on the weight matrices of nonlocal blocks after training. Based on the analysis, we then propose a more suitable formulation of the nonlocal blocks, followed by some experimental evaluations to demonstrate the effectiveness of the proposed model.

\subsection{Spectrum Analysis}
We incorporate nonlocal blocks into the 20-layer pre-activation ResNet (PreResNet) \cite{he2016identity} as stated in Eq. \eqref{nl-net}. Since our goal is to illustrate the diffusive nature of nonlocal operations, we add a different number of nonlocal blocks into a fixed place at the early stage, which is different from the experiments shown in \cite{wang2017non}, where the nonlocal blocks are added to different places along the ResNet. When employing the nonlocal blocks, we always insert them to right after the second residual block of PreResNet-20.

The inputs from CIFAR-10 are images of size $32\times32$, with  preprocessing of the per-pixel mean subtracted. %We train the model with a mini-batch size of 64, and follow \cite{he2016identity} to do the weight initialization \cite{he2015delving} and batch normalization \cite{ioffe2015batch}. 
In order to adequately train the nonlocal network, the training starts with a learning rate of $0.1$ that is subsequently divided by $10$ at 81 and 122 epochs (around 32k and 48k iterations). A weight decay of 0.0001 and momentum of 0.9 are also used. We terminate the training at 164 epochs. The model is with data augmentation \cite{lee2015deeply} and trained on a single NVIDIA Tesla GPU.

To see what the nonlocal blocks have exactly learned, we extract the weight matrices $W_Z$ and $W_g$ after training. Under the current experimental setting, the weight matrices are of dimensions $W_Z\in\mathbb{R}^{64\times 32}$ and $W_g\in\mathbb{R}^{32\times 64}$. Note that if we let $W=W_Z W_g$, Eq. \eqref{nl-op} can be rewritten as
\begin{equation}\label{nl-op2}
\left[\mathcal{F}(Z^k\, ;W^k)\right]_i = \frac{W^k}{\mathcal{C}_i(Z^k)}\sum_{\forall j} \omega(Z^k_i,Z^k_j) Z^k_j\,,
\end{equation}
where the sparse weight matrix $W^k\in\mathbb{R}^{64\times 64}$ has only 32 eigenvalues. We then denote the symmetric part of $W$ by
\begin{equation}
\widetilde{W} = \frac{W + W^\text{T}}{2}\,,
\end{equation}
so that the eigenvalues of  $\widetilde{W}^k$ are all real. Note that the effect of $W$ on the decay properties of the network is determined by the associated quadratic form, which is equivalent to that of the symmetric part of $W$, \textit{i.e.}, $\widetilde{W}$. Therefore, the eigenvalues of $\widetilde{W}^k$, especially those with the greatest magnitudes (absolute values), would describe the characteristics of the weights in nonlocal blocks.

\begin{figure}
\centering
\subfigure[Epoch 36]{
	\includegraphics[width=0.325\linewidth]{olds1.pdf}} % Epoch 36
\subfigure[Epoch 82]{
	\includegraphics[width=0.32\linewidth]{olds3.pdf}} % Epoch 82
\subfigure[Epoch 125]{
	\includegraphics[width=0.32\linewidth]{olds4.pdf}} % Epoch 125
\caption{Top 32 eigenvalues of the symmetrized weight matrices during training when adding 1 nonlocal block. Figures (a), (b) and (c) correspond to the results at Epoch 36, 82 and 125, respectively.}
\vskip -.5em
\label{fig:nl-old2}
\end{figure}

\begin{figure}
\centering
\subfigure[Training curves]{
	\includegraphics[width=0.48\linewidth]{log.pdf}}
\subfigure[Eigenvalues]{
	\includegraphics[width=0.48\linewidth]{old4.pdf}}
\caption{(a) The training curves of nonlocal networks with 1, 2, 3 and 4 nonlocal blocks. (b) Top 32 eigenvalues of the symmetric part of weight matrices when adding 4 nonlocal blocks.}
\vskip -.5em
\label{fig:nl-old3}
\end{figure}

Figure \ref{fig:nl-old1} shows the top 32 eigenvalues of the symmetric part of weight matrices after training when adding 1, 2 and 3 nonlocal blocks to PreResNet-20. One can observe that most of the eigenvalues are negative, especially the first several with the greatest magnitudes. Similar observations can be made along the training process. Figure \ref{fig:nl-old2} plots the top 32 eigenvalues of $\widetilde{W}$ at three intermediate epochs before convergence for the 1-block nonlocal network case. From Eq.~\eqref{nl-net} with $\mathcal{F}$ being the formulation of nonlocal blocks in Eq.~\eqref{nl-op2}, we can see that $Z^k$ tends to vanish regardless of the initial value when imposing small negative coefficients on multiple blocks. Namely, while nonlocal blocks are trying to capture the long-range correlations, the features tend to be damped out at the same time, which is usually the consequence of diffusion. A more detailed discussion can be found in Section \ref{sec:disc}.

However, the training becomes more difficult when employing more nonlocal blocks. Under the current training strategy, when adding 4 blocks, it does not converge after 164 epochs. Although reducing the learning rate or increasing the learning epochs will mitigate the convergence issue, the training loss decreases more slowly than the fewer blocks cases. Figure \ref{fig:nl-old3}(a) shows the learning curves of nonlocal networks with different nonlocal blocks, where we can see that the training loss for the 4-block network is much larger than the others. Note that for the 4-block case, we have reduced the learning rate by the ratio of 0.1 in order to make the training convergent. This observation implies that the original nonlocal network is not robust with respect to multiple nonlocal blocks, which is also shown in Figure \ref{fig:nl-old3}(b). For the 4-block case, we obtain many positive eigenvalues of the symmetrized weight matrices after training. In particular, some of the positive eigenvalues have large magnitudes, which leads to a potential blow-up of the feature vectors and thus makes training more difficult. %A simple numerical experiment is provided in the \textcolor{red}{supplementary} material for more intuitive illustrations of the instability with positive coefficients imposed on the nonlocal blocks. 


\subsection{A New Nonlocal Network}
Given the damping effect for the nonlocal network and the instability of employing multiple nonlocal blocks, we hereby suggest to modify the formulation in order to have a more well-defined nonlocal operation. Instead of Eq.~\eqref{nl-op} or Eq.~\eqref{nl-op2}, we regard the nonlocal blocks added consecutively to the same place as a \textit{nonlocal stage}, which consists of several nonlocal sub-blocks. The nonlocal stage takes input $X=[X_i]$ ($i=1,2,\cdots,M$) as the feature representation when computing the affinity within the stage. In particular, a nonlocal stage is defined as
\begin{equation}\label{nl-net-new}
Z^{n+1}_i := Z^n_i + \frac{W^n}{\mathcal{C}_i(X)}\sum_{\forall j} \omega(X_i,X_j)(Z^n_j-Z^n_i)\,,
\end{equation}
for $i=1,2,\cdots,M$, where $W^n$ is the weight matrix to be learned, $Z^0=X$, and $n=1,2,\cdots,N$ with $N$ being the number of stacking sub-nonlocal blocks in a stage. Moreover, $\mathcal{C}_i(X)$ is the normalization factor computed by
\begin{equation}
\mathcal{C}_i(X)=\sum_{\forall j} \omega(X_i,X_j)\,.
\end{equation}
There are a couple of differences between the two nonlocal formulations
in Eq.~\eqref{nl-bl-old} and Eq.~\eqref{nl-net-new}. On one hand, in a nonlocal stage in Eq.~\eqref{nl-net-new}, the affinity $\omega$ is pre-computed given the input feature $X$ and stays the same along the propagation within a stage, which reduces the computational cost while $\omega(X_i,X_j)$ can still represent the affinity between $Z^n_i$ and $Z^n_j$ to some extent. On the other hand, the residual part of  Eq.~\eqref{nl-net-new} is no longer a weighted sum of the neighboring features, but the difference between the neighboring signals and computed signal. In Section \ref{sec:relation}, we will study the connections between the proposed nonlocal blocks and several other existing nonlocal models to clarify the rationale of our model.



\subsection{Experimental Evaluation}
To demonstrate the difference between two nonlocal networks and the effectiveness of our proposed method, we present the empirical evaluation on CIFAR-10 and CIFAR-100. Following the standard practice, we present experiments performed on the training set and evaluated on the test set as validation. We compare the empirical performance of PreResNets incorporating into the original nonlocal blocks \cite{wang2017non} or the proposed nonlocal blocks in Eq.~\eqref{nl-net-new}.

\begin{table}[t]
\caption{Validation errors of different models based on PreResNet-20 over CIFAR-10.}
\label{table:results}
\begin{center}
\begin{tabular}{lc|cc}
\toprule
\multicolumn{2}{c}{Model} & Error (\%)  \\
\Xhline{1pt}
\multirow{9}{*}{The Same Place}& baseline & 8.19 \\
&2-block (original) & 7.83 \\
&3-block (original) & 8.28 \\
&4-block (original) & 15.02 \\
&2-block (proposed) & 7.74 \\ 
&3-block (proposed) & 7.62 \\
&4-block (proposed) & 7.37 \\
&5-block (proposed) & \textbf{7.29} \\ 
&6-block (proposed) & 7.55 \\ 
\Xhline{1pt}
\multirow{2}{*}{Different Places}& 3-block (original) & 8.07 \\
&3-block (proposed) & \textbf{7.33} \\
\Xhline{1pt}
\end{tabular}
\end{center}
\vskip -0.6em
\end{table}

Table~\ref{table:results} presents the validation errors of PreResNet-20 on CIFAR-10 with a different number of nonlocal blocks that are added consecutively to the same place or separately to different places as the experiments shown in \cite{wang2017non}. The best performance for each case is displayed in boldface. Note that all the models are trained by ourselves in order to have a fair comparison. We run each model 5 times and report the median. More experimental results with PreResNet-56 on CIFAR-100 are also provided in Table~\ref{table:results2}. 
Based on the results shown in the tables, we give some analysis as follows. 

\begin{table}[!tb]
\caption{Validation errors of different models based on PreResNet-56 over CIFAR-100.}
\label{table:results2}
\begin{center}
\begin{tabular}{lc|cc}
\toprule
\multicolumn{2}{c}{Model} & Error (\%)  \\
\Xhline{1pt}
\multirow{9}{*}{PreResNet-56}& baseline & 26.57  \\ 
&2-block (original)& 26.13 \\
&3-block (original)& 26.26 \\
&4-block (original)& 34.89 \\
&2-block (proposed) & 26.04 \\ 
&3-block (proposed) & 25.57 \\ 
&4-block (proposed) & 25.43\\ 
&5-block (proposed) & \textbf{25.29}  \\ 
&6-block (proposed) & 25.49\\ 
\Xhline{1pt}
\end{tabular}
\end{center}
\end{table}

\begin{figure}[!tb]
\centering
\subfigure[The 1st block]{
	\includegraphics[width=0.24\linewidth]{new41.pdf}}
\subfigure[The 2nd block]{
	\includegraphics[width=0.24\linewidth]{new42.pdf}}
\subfigure[The 3rd block]{
	\includegraphics[width=0.24\linewidth]{new43.pdf}}
\subfigure[The 4th block]{
	\includegraphics[width=0.23\linewidth]{new44.pdf}}
\caption{Top 32 eigenvalues of the symmetric part of weight matrices after training when adding 4 proposed nonlocal blocks to PreResNet-20.}
\vskip -0.55em
\label{fig:nl-new}
\end{figure}

First, for the original nonlocal network, since the damping effect cannot be preserved when adding more nonlocal blocks, the training becomes more difficult and thus the validation performs worse. In contrast, the proposed nonlocal network is more robust to the number of blocks. When employing the same number of nonlocal blocks, the proposed model consistently performs better than the original one. Moreover, the proposed model also reduces the computational cost because the affinity function can be pre-assigned in a nonlocal stage. Although the two formulations of nonlocal blocks appear similar, they are different in essence. Figure~\ref{fig:nl-new}  shows the top 32 eigenvalues of the symmetric part of weight matrices after training when adding 4 new nonlocal blocks to PreResNet-20, where we can see that most of the eigenvalues are positive. In the following section, we will show that the formulation of new nonlocal blocks is exactly the nonlocal analogue of local diffusion with positive coefficients. %The numerical experiment presented in the \textcolor{red}{supplementary} material also gives a preliminary illustration.

In addition, one can observe from Table~\ref{table:results} and Table~\ref{table:results2} that adding too many proposed nonlocal blocks reduces the performance. A potential reason is that all the nonlocal blocks in a stage share the same affinity function, which is pre-computed given the input features. Therefore, the kernel function cannot precisely describe the affinity between pairs of features after many sub-blocks. 

Note that in \cite{wang2017non}, the nonlocal blocks are always added individually in each fixed place of ResNets. We also compare the two nonlocal networks in the second part of Table~\ref{table:results}, where we in total add 3 nonlocal blocks to PreResNet-20 but into different residual blocks. Although in this case the two kinds of nonlocal blocks have the  same affinity kernel, the proposed nonlocal network still performs better than the original one, which implies that the proposed nonlocal blocks are more suitably defined with respect to the nature of diffusion.


\section{Connection to Nonlocal Modeling}\label{sec:relation}
In this section, we study the relationship between the proposed nonlocal network and a couple of existing nonlocal models, {\em i.e.}, the nonlocal diffusion systems and Markov chain with jump processes. Then we make a further comparison between the two formulations of nonlocal blocks.

\subsection{Affinity Kernels}
We make some assumptions here on the affinity kernel $\omega$ in order to make connections to other nonlocal modeling approaches. Let $\Gamma$ be the input feature space such that $X_i$'s are finite data samples drawn from $\Gamma$. Denote by $\mathcal{K}$ the kernel matrix such that $\mathcal{K}_{ij}=\omega(X_i, X_j)$. We first assume that $\mathcal{K}$ has a finite Frobenius norm, namely,
\begin{equation}
\|\mathcal{K}\|_F^2 = \sum_{\forall i,j} \big|\omega(X_i,X_j)\big|^2 < \infty\,.
\end{equation}
Then without loss of generality, we can assume that the kernel matrix $\mathcal{K}$ has sum 1 along its rows, {\em i.e.}, the normalization factor is assumed to be 1 for all $i$:
\begin{equation}
\mathcal{C}_i(X)=\sum_{\forall j}\omega(X_i,X_j)=1\,,\quad\text{for }i=1,\cdots,M\,.
\end{equation}
We further assume that the kernel function is symmetric and nonnegative within $\Gamma$, namely for all $X_i,X_j\in\Gamma$,
\begin{equation}\label{kernel-cond}
\omega(X_i,X_j)=\omega(X_j,X_i)\ \ \ \text{and}\ \ \  \omega(X_i,X_j)\ge 0\,.
\end{equation}
Note that for the instantiations given in \cite{wang2017non}, only the Gaussian function is symmetric. However, in the embedded Gaussian and dot product cases, we can instead embed the input features by some weight matrix as the parameter and then feed the pre-embedded features into the nonlocal stage and use the traditional Gaussian or dot product function as the affinity, namely, we replace the kernel by
\begin{equation}
\omega_\theta(X_i,X_j)=\omega\big(\theta(X_i),\theta(X_j)\big)\,,
\end{equation}
where $\theta$ is a linear embedding function. 


\subsection{Nonlocal Diffusion Process}
Define the following discrete nonlocal operator for $X=[X_1,\cdots,X_M]$ and $X_i\in\Gamma$:
\begin{equation}\label{nl-op-dis}
(\mathcal{L}^h Z)_i := \sum_{\forall j} \omega(X_i,X_j)(Z_j-Z_i)\,,
\end{equation}
where the superscript $h$ is the discretization parameter. Eq.~\eqref{nl-op-dis} can be seen as a reformulation of nonlocal operators in some previous works, such as the graph Laplacian \cite{chung1997spectral} (as well as its applications \cite{liu2009robust,liu2010large,liu2012robust,wu2018multi}), nonlocal-type image processing algorithms \cite{buades2005review,gilboa2007nonlocal}, and diffusion maps \cite{coifman2006diffusion}. All of them are nonlocal analogues \cite{du2012analysis} of local diffusions. %From the numerical examples presented in the supplementary material, we can also see that $\mathcal{L}^h$ acts like a diffusive process for positive $W^n$'s. 
Then the equation
\begin{equation}\label{nl-eq-dis}
Z^{n+1} = Z^{n} + \mathcal{L}^h Z^n
\end{equation}
describes a discrete nonlocal diffusion, where $Z$ satisfies that $Z^0 = X$ for $X_i\in\Gamma$. The above equation is equivalent to the proposed nonlocal network in Eq.~\eqref{nl-net-new} with positive weights.

In general, Eq.~\eqref{nl-eq-dis} is a time-and-space discrete form, where the superscript $n$ is the time step parameter, of the following nonlocal integro-differential equation:
\begin{equation}\label{nl-eq-con}
\left\{
\begin{aligned}
&\zb_t(\xb,t)-\mathcal{L} \zb(\xb)=0\,,\\
& \zb(\xb,0)=\ub(\xb)\,,
\end{aligned}
\right.
\end{equation}
for $\xb\in\Omega$ and $t\ge0$. Here $\ub$ is the initial condition, $\zb_t:=\partial\zb / \partial t$, and $\mathcal{L}$ defines a continuum nonlocal operator:
\begin{equation}
\mathcal{L}\zb(\xb) := \int_\Omega \rho(\xb,\yb)\big(\zb(\yb)-\zb(\xb)\big)d\yb\,,
\end{equation}
where the kernel
\begin{equation}
\rho(\xb,\yb):=\omega\big(\ub(\xb),\ub(\yb)\big)
\end{equation}
is symmetric and positivity-preserved. We provide in the supplementary material some properties of the nonlocal equation \eqref{nl-eq-con}, in order to illustrate the connection between Eq.~\eqref{nl-eq-con} and local diffusions, and demonstrate that the proposed nonlocal blocks can be viewed as an analogue of local diffusive terms. 

Since we assume that the kernel matrix $\mathcal{K}$ has a finite Frobenius norm, we have
\begin{equation}
\int_\Omega\int_\Omega |\rho(\xb,\yb)|^2 d\xb d\yb <\infty.
\end{equation}
Therefore, the corresponding integral operator $\mathcal{L}$ is a Hilbert-Schmidt operator so that Eq.~\eqref{nl-eq-con} and its time reversal are both well-posed and stable in finite time. The latter is equivalent to the continuum generalization of the nonlocal block with a negative coefficient (eigenvalue). Although $\mathcal{L}$ is a nonlocal analogue of the local Laplace (diffusion) operator, the nonlocal diffusion equation has its own merit. More specifically, the anti-diffusion in a local partial differential equation (PDE), {\em i.e.}, the reversed heat equation, is ill-posed and unstable in finite time. The stability property of the proposed nonlocal network is important, because it ensures that we can stack multiple nonlocal blocks within a single stage to fully exploit their advantages in capturing long-range features.


\subsection{Markov Jump Process}
The proposed nonlocal network in Eq.~\eqref{nl-net-new} also shares some common features with the discrete-time Markov chain with jump processes \cite{du2012analysis}. In this part, we assume, without loss of generality, that $Z_i$'s and $X_i$'s are scalars. In general, the component-wise properties can lead to similar properties of the vector field. Given a Markov jump process $\mathcal{Z}_t$ confined to remain in a bounded domain $\Omega\subset\mathbb{R}^d$, assume that $z(\xb,t)$ is the corresponding probability density function. Then a general master equation to describe the evolution of $z$ \cite{kenkre1973generalized} can be written as
\begin{equation}\label{mcj-cont}
z_t(\xb,t) = \int_\Omega \left[ \gamma(\xb',\xb,t)z(\xb',t)-\gamma(\xb,\xb',t)z(\xb,t)\right] d\xb'\,,
\end{equation}
where $\gamma(\xb',\xb,t)$ denotes the transition rate from $\xb'$ to $\xb$ at time $t$. Assume that the Markov process is time-homogeneous, namely $\gamma(\xb',\xb,t)=\gamma(\xb',\xb)$. Then in the discrete time form \cite{zhao2016generalized}, Eq.~\eqref{mcj-cont} is often reformulated as 
\begin{equation}\label{mcj-dis}
z(\xb,t)-z(\xb,t')=\int_\Omega \left[ p(\xb',\xb)z(\xb',t') -p(\xb,\xb')z(\xb,t')\right] d\xb'\,,
\end{equation}
where $p(\xb',\xb):=(t-t')\gamma(\xb',\xb)$ represents the transition probability of a particle moving from $\xb'$ to $\xb$. One can easily see that as $t'\to t$, the solution to Eq.~\eqref{mcj-dis} converges to the continuum solution to Eq.~\eqref{mcj-cont}. Note that 
$
\int_\Omega p(\xb,\xb') d\xb'=1
$, Eq.~\eqref{mcj-dis} reduces to
\begin{equation}
z(\xb,t)=\int_\Omega p(\xb',\xb)z(\xb',t') d\xb'\,.
\end{equation}
Suppose that there is a set of finite states drawn from $\Omega$, namely $\xb_1,\xb_2,\cdots,\xb_M$, and finite discrete time intervals, namely $t_1,t_2,\cdots, t_N$. Let $Z_i^n = z(\xb_i,t_n)$ with an initial condition $Z_i^0=X_i$. If we further introduce the kernel matrix $\mathcal{K}$, where $\mathcal{K}_{ij}:=\frac{1}{\mathcal{M}(\xb'_j)}\int_{\mathcal{M}(\xb'_j)} p(\xb_i, \xb')d\xb'$,  and the average of $p(\xb_i,\xb')$ over the grid mesh of $\xb'_j$. Then the kernel matrix $\mathcal{K}$ is a \textit{Markov matrix} (with each column summing up to 1) and
\begin{equation}
Z^{n+1}_i = \sum_{\forall j} \mathcal{K}_{ij} Z^n_j
\end{equation}
describes a discrete-time Markov jump process. The above system is equivalent to
\begin{equation}
Z^{n+1}_i - Z^{n}_i = \sum_{\forall j} \mathcal{K}_{ij} \left( Z^n_j - Z^n_i \right)\,,
\end{equation}
 which is exactly the proposed nonlocal stage regardless of the weight coefficients in front of the Markov operator. %Since we confine the particles within a bounded domain, the above equation can be viewed as a censored process and thus the probability should be conserved over $\Omega$, which can be easily checked by the fact that for all $n\ge 0$,

On the other hand, our proposed nonlocal network with negative weight matrices can be regraded as the reverse Markov jump process. Again, since we have the condition that
\begin{equation}
\|\mathcal{K}\|_F^2 < \infty\,,
\end{equation}
the Markov operator $\mathcal{K}$ is a Hilbert-Schmidt operator, which is bounded and implies that the Markov jump processes with both time directions are stable in finite time.


\subsection{Further Discussion}\label{sec:disc}
In this section, we have derived some properties of the proposed nonlocal network. Due to the nature of a Hilbert-Schmidt operator, we expect the robustness of our network. However, we should remark that whether the exact discrete formula in Eq.~\eqref{nl-net-new} has stable dynamics also depends on the weights $W^n$'s. This is ignored for simplicity when connecting to other nonlocal models. In practice, the stability holds as long as the weight parameters are small enough such that the \textit{CFL condition} is satisfied. 

In comparison, for the original nonlocal network, we define a discrete nonlinear nonlocal operator as
\begin{equation}\label{nl-op-2}
(\mathcal{\tilde{L}}^h Z)_i:= -\sum_{\forall j} \omega(Z_i,Z_j)Z_j\,.
\end{equation}
Then the flow of the original nonlocal network with small negative coefficients can be described as
\begin{equation}\label{discuss-nl-eq}
Z^{n+1} - Z^n = \mathcal{\tilde{L}}^h Z^n\,,
\end{equation}
with the initial condition $Z^0=X$. By letting $Z^{n+1}=Z^n$, the steady-state equation of Eq.~\eqref{discuss-nl-eq} can be written as
\begin{equation}
\mathcal{\tilde{L}}^hZ = 0\,.
\end{equation}
Since the kernel $\omega$ is strictly non-negative, the only steady-state solution is $Z\equiv 0$, which means that the output signals of original nonlocal blocks tend to be damped out (diffused) along iterations. However, 
 the original nonlocal operation with positive eigenvalues is unstable in finite time. While one can still learn long-range features in the network, stacking several original blocks will cast uncertainty to the model and thus requires extra work to study the initialization strategy or to fine-tune the parameters.

In applications, the nonlocal stage is usually employed and plugged into ResNets. From the viewpoint of PDEs, the flow of a ResNet can be seen as the forward Euler discretization of a dynamical system with respect to $\zb(t)$ \cite{weinan2017proposal}:
\begin{equation}\label{resnet-ds}
\frac{d\zb}{dt} = F\big(\zb, W(t)\big), \quad \zb(0) = X\,.
\end{equation}
The right-hand side $F$ for a PreResNet is given by
\begin{equation}
F\big(\zb,W(t)\big) = W_2(t) f \left (W_1(t) f(\zb) \right)\,,
\end{equation}
which can be regarded as a reaction term in a PDE because it does not involve any differential operator explicitly. From the standard PDE theory, incorporating the proposed nonlocal stage to ResNets is equivalent to introducing diffusion terms to the reaction system. As a result, the diffusion terms can regularize the PDE and thus make it more stable \cite{evans1997partial}.




\section{Conclusion}\label{sec:con}
In this paper, we studied the damping effect of the existing nonlocal networks in the literature by performing the spectrum analysis on the weight matrices of the well-trained networks. Then we proposed a new class of nonlocal networks, which can not only capture the long-range dependencies but also be shown to be more stable in the network dynamics and more robust to the number of nonlocal blocks. Therefore, we can stack more nonlocal blocks in order to fully exploit their advantages. In the future, we aim to investigate the proposed nonlocal network on other challenging real-world learning tasks.


\subsubsection*{Acknowledgments}
This work is supported in part by US NSF CCF-1740833, DMR-1534910 and DMS-1719699. Y. Tao wants to thank Tingkai Liu and Xinpeng Chen for their help on the experiments.

\begin{small}
\bibliography{nlnn}
\bibliographystyle{icml2018}
\end{small}

\appendix
\section*{Supplementary Material}
In the supplementary material, we aim to show some decay properties of the nonlocal equation
\begin{equation}\label{nl-eq-con}
\left\{
\begin{aligned}
&z_t(\xb,t)-\mathcal{L} z(\xb)=0\,,\\
& z(\xb,0)=u(\xb)\,,
\end{aligned}
\right.
\end{equation}
for $\xb\in\Omega\subset\mathbb{R}^d$ and $t\ge 0$. Here the nonlocal operator is defined as
\begin{equation}\label{nl-op}
\mathcal{L}z(\xb) := \int_\Omega \rho(\xb,\yb)\big(z(\yb)-z(\xb)\big)d\yb\,,
\end{equation}
where the kernel
\begin{equation}
\rho(\xb,\yb):=\omega(u(\xb),u(\yb))
\end{equation}
is symmetric and positivity-preserved. We should remark first that in all the proofs presented below, we always omit the time variable $t$ if not referred to. For simplicity, we always assume that $z(\xb)$ is a scalar function. 
We first derive some properties of the nonlocal operator $\mathcal{L}$.
\begin{proposition}
The nonlocal operator $\mathcal{L}$ defined in Eq.~\eqref{nl-op} admits the following properties:
\begin{itemize}
\item[(i)] If $z(\xb)$ is a constant function for all $\xb\in\Omega$, then $\mathcal{L}z\equiv 0$.
\item[(ii)] $\mathcal{L}z$ is mean zero for any function $z$, namely,
\begin{equation}
\int_\Omega \mathcal{L} z(\xb) d\xb = 0\,.
\end{equation}
\item[(iii)] $-\mathcal{L}$ is positive semi-definite, namely,
\begin{equation}
-\int_\Omega \mathcal{L} z(\xb) z(\xb) d\xb \ge 0\,,
\end{equation}
for any function $z$.
\end{itemize}
\end{proposition}
\begin{proof}
Property (i) is straightforward. Property (ii) is true by the symmetry of the kernel function, namely,
\[
\begin{aligned}
\int_\Omega\mathcal{L} z(\xb)d\xb&=\int_\Omega\int_\Omega \rho(\xb,\yb)(z(\yb)-z(\xb))d\yb d\xb\\
&=\frac{1}{2}\int_\Omega\int_\Omega \rho(\xb,\yb)(z(\yb)-z(\xb))d\yb d\xb +\frac{1}{2}\int_\Omega\int_\Omega \rho(\yb,\xb)(z(\xb)-z(\yb))d\yb d\xb\\
& = 0\,.
\end{aligned}
\]
For Property (iii), using the symmetry of $\rho$ again we can get
\[
\begin{aligned}
-\int_\Omega \mathcal{L}z(\xb) z(\xb)d\xb &= -\int_\Omega\int_\Omega \rho(\xb,\yb)(z(\yb)-z(\xb))z(\xb)d\yb d\xb \\
&=\frac{1}{2}\int_\Omega\int_\Omega \rho(\xb,\yb)(\zb(\yb)-\zb(\xb))^2 d\yb d\xb\,.
\end{aligned}
\]
Since $\rho(\xb,\yb)\ge 0$ for all $\xb,\yb\in\Omega$, the above is always non-negative.
\end{proof}
It is easy to see that the discrete nonlocal operator $\mathcal{L}^h$ defined in Section 4.2 of the main paper has the same properties in the discrete form. Now we can study the properties of the nonlocal integral-differential equation \eqref{nl-eq-con}. The following gives a formal theorem on this observation.

\begin{theorem}
The flow of Eq.~\eqref{nl-eq-con} admits the following properties:
\begin{itemize}
\item[(i)] The mean value is preserved, namely,
\begin{equation}
\int_\Omega z(t,\xb) d\xb = \int_\Omega u(\xb) d\xb,\quad \text{for all } t\ge 0\,.
\end{equation}
\item[(ii)] As $t\to\infty$, the solution converges to a constant, which is the mean value of the initial condition $u$, namely,
\begin{equation}
\lim_{t\to\infty} z(t,\xb) = \frac{1}{|\Omega|} \int_\Omega u(\xb)d\xb\,,
\end{equation}
where $|\Omega|$ is the measure of the space domain.
\end{itemize}
\end{theorem}
\begin{proof}
For Property (i), by taking the time derivative of the integral of $z$ over $\Omega$, we can get that
\[
\frac{d}{dt} \int_\Omega z(t,\xb)d\xb = \int_\Omega z_t(t,\xb) d\xb = \int_\Omega \mathcal{L} z(\xb)d\xb\,.
\]
From Property (ii) of Proposition 1, the above equations are equal to 0, which means that the mean value does not change over time.

For Property (ii), we first prove another property of energy decay, namely
\[
\frac{d}{dt}\int_\Omega z(t,\xb)^2d\xb \le 0\,.
\]
Indeed, by Property (iii) of Proposition 1, we have
\[
\begin{aligned}
\frac{d}{dt}\int_\Omega z(t,\xb)^2d\xb &= 2\int_\Omega z_t(t,\xb) z(t,\xb)d\xb\\
&=2\int_\Omega \mathcal{L} z(\xb) z(\xb)d\xb\\
&\le 0\,.
\end{aligned}
\]
Moreover, in the proof of Property (iii) of Proposition 1, we can see that the above estimate is strictly negative unless  $z(\xb)$ is a constant for any $\xb\in\Omega$. Then as $t\to\infty$, $z(t,\xb)$ must converge to a constant function, which has to be $\frac{1}{|\Omega|}\int_\Omega u(\xb)d\xb$ by Property (i) of this theorem.
\end{proof}

Another observation is that the variance of $z(t,\xb)$ is decreasing over iterations. The variance is defined as
\begin{equation}
\text{var}(z):=\frac{1}{|\Omega|} \int_\Omega \left( z(\xb)-\frac{1}{|\Omega|}\int_\Omega z(\yb)d\yb \right)^2 d\xb\,.
\end{equation}
Then we can derive the following theorem.
\begin{theorem}
Let $z(t,\xb)$ be the solution to Eq.~\eqref{nl-eq-con}, then
\begin{equation}
\frac{d}{dt} \text{var}(z) \le 0\,.
\end{equation}
\end{theorem}
\begin{proof}
Taking the time derivative of the variance, we can get
\[
\begin{aligned}
\frac{d}{dt} \text{var}(z) &= \frac{2}{|\Omega|} \int_\Omega \left( z(t,\xb)-\frac{1}{|\Omega|}\int_\Omega u(\yb)d\yb\right) z_t(t,\xb) d\xb\\
&= \frac{2}{|\Omega|}\left(\int_\Omega z(\xb) \mathcal{L}z(\xb)d\xb - \frac{1}{|\Omega|}\int_\Omega u(\yb)d\yb \int_\Omega \mathcal{L}z(\xb)d\xb\right)\\
&=\frac{2}{|\Omega|}\int_\Omega z(\xb) \mathcal{L}z(\xb)d\xb\\
&\le 0\,,
\end{aligned}
\]
where in the derivation we have used the fact that $\int_\Omega \mathcal{L}z(\xb)d\xb = 0$\,.
\end{proof}


We are also interested in the convergence rate of $z(t,\xb)$ to its mean value with respect to $t$. The following theorem tells us that the solution to Eq.~\eqref{nl-eq-con} decays to its mean value with an exponential rate.
\begin{theorem}
Suppose $u\in L^2(\Omega)$, \textit{i.e.}, square integrable. Then as $t\to\infty$, the solution $z$ to Eq.~\eqref{nl-eq-con} satisfies
\begin{equation}
\|z(t,\xb)-\bar{u}\|_{L^2(\Omega)} \le C e^{-\lambda t} \,,
\end{equation}
with some positive constants $C$ and $\lambda$, where
\begin{equation}
\bar{u} = \frac{1}{|\Omega|} \int_\Omega u(\xb) d\xb\,.
\end{equation}
\end{theorem}

Before we give the proof of Theorem 4, we establish a nonlocal-type Poincar\'e inequality as follows.
\begin{lemma}[Nonlocal Poincar\'e Inequality]
Supposed that $z(\xb)\in L^2(\Omega)$ and $\int_\Omega z(\xb)d\xb = 0$. Then there exists a constant $C>0$ such that
\[
\int_\Omega z(\xb)^2 d\xb \le C \int_\Omega\int_\Omega \rho(\xb,\yb)(z(\yb)-z(\xb))^2d\yb d\xb\,.
\]
\end{lemma}
\begin{proof}
We show that
\[
0 < m = \inf_{z(\xb)\in L^2(\Omega), \|z\|_{L^2}=1}\left( \int_\Omega\int_\Omega \rho(\xb,\yb)(z(\yb)-z(\xb))^2d\yb d\xb\right)\,.
\]
Clearly, $m\ge 0$. Suppose that $m=0$, then there exists a sequence $z_n(\xb)\in L^2(\Omega)$ and $\int_\Omega z_n(\xb)d\xb=0$ such that, for all $n$, $\|z_n\|_{L^2}=1$ and
\[
\lim_{n\to\infty}  \int_\Omega\int_\Omega \rho(\xb,\yb)(z_n(\yb)-z_n(\xb))^2d\yb d\xb=0\,.
\]
Then $\{z_n\}$ is precompact in $L^2(\Omega)$, which means that there exists a strong limit $z_\infty$ of $z_n$ in $L^2(\Omega)$. Then, on one hand, we have $\|z_\infty\|_{L^2}=1$ and $\int_\Omega z_\infty(\xb)d\xb=0$. On the other hand,
\[
\begin{aligned}
0 &= \lim_{n\to \infty} \int_\Omega\int_\Omega \rho(\xb,\yb)(z_n(\yb)-z_n(\xb))^2d\yb d\xb \\
&=\lim_{n\to \infty} -2\int_\Omega \mathcal{L} z_n(\xb) z_n(\xb) d\xb\\
&=-2\int_\Omega \mathcal{L} z_\infty(\xb) z_\infty(\xb) d\xb\\
&=  \int_\Omega\int_\Omega \rho(\xb,\yb)(z_\infty(\yb)-z_\infty(\xb))^2d\yb d\xb\,.
\end{aligned}
\]
The above integral is 0 if and only if $z_\infty$ is a constant function. Then by $\int_\Omega z_\infty(\xb)d\xb = 0$, we must have $z_\infty\equiv 0$, which contradicts to $\|z_\infty\|_{L^2}=1$. This completes the proof.
\end{proof}

Now we can start to prove Theorem 4.
\begin{proof}[Proof of Theorem 4]
Let $\bar{z}(t,\xb)=z(t,\xb)-\bar{u}$. Then we have
\[
\int_\Omega \bar{z}(t,\xb)d\xb=0\quad \text{for all } t\ge 0\,.
\]
Since $u\in L^2(\Omega)$, it is also easy to get that $\bar{z}(\cdot,\xb)\in L^2(\Omega)$. Moreover, $\bar{z}$ is the solution to
\[
\left\{
\begin{aligned}
&\bar{z}_t(t,\xb)-\int_\Omega \rho(\xb,\yb)(\bar{z}(t,\yb)-\bar{z}(t,\xb))d\yb=0\,,\\
& \bar{z}(0,\xb)=u(\xb)-\bar{u}\,.
\end{aligned}
\right.
\]
Then by the nonlocal Poincar\'e inequality, there exists a constant $\lambda>0$ such that
\[
\begin{aligned}
\frac{d}{dt} \int_\Omega \bar{z}(t,\xb)^2d\xb &= -\int_\Omega \rho(\xb,\yb)(\bar{z}(\yb)-\bar{z}(\xb))^2d\yb d\xb \\
& \le -2\lambda \int_\Omega \bar{z}(\xb)^2d\xb\,.
\end{aligned}
\]
Therefore, we can derive that
\[
\frac{d}{dt} \left( e^{2\lambda t}\int_\Omega \bar{z}(t,\xb)^2d\xb\right) \le 0\,.
\]
Let $C=\left(\int_\Omega \bar{z}(0,\xb)^2d\xb\right)^{1/2}$. Then we have
\[
\|z(t,\xb)-\bar{u}\|_{L^2(\Omega)} \le Ce^{-\lambda t}\,.
\]
\end{proof}

To sum up, all the properties derived from the nonlocal equation are also true for local diffusions. Moreover, since the nonlocal equation \eqref{nl-eq-con} is a continuum generalization of the proposed nonlocal network which can be regarded as the discretization of Eq.~\eqref{nl-eq-con}, we can view the formulation of the proposed nonlocal blocks as an analogue of local diffusive terms.












\end{document}


