\documentclass[acmtog,timestamp]{acmart}%,draft,


\setcopyright{acmcopyright}

\usepackage{amsmath}\usepackage{graphicx}\usepackage{array}\usepackage[titletoc,title]{appendix}\usepackage{mathtools}\usepackage{multirow}\makeatletter\newcommand\incircbin{%
  \mathpalette\@incircbin
}\newcommand\@incircbin[2]{%
  \mathbin%
  {%
    \ooalign{\hidewidth$#1#2$\hidewidth\crcr$#1\bigcirc$}%
  }%
}\newcommand{\oland}{\incircbin{\land}}\makeatother\DeclareMathOperator*{\argmin}{arg\,min}\newcommand\addcomment[1]{\textcolor{black}{#1}}\usepackage[normalem]{ulem}\newcommand\addnote[1]{\textcolor{black}{#1}}\usepackage{todonotes}%\newcommand\rev[1]{\textcolor{blue}{#1}}%\newcommand\new[1]{\textcolor{red}{#1}}%\newcommand\newa[1]{\textcolor{violet}{#1}}\newcommand\rev[1]{\textcolor{black}{#1}}\newcommand\new[1]{\textcolor{black}{#1}}\newcommand\newa[1]{\textcolor{black}{#1}}\DeclareMathOperator*{\cumsum}{\textrm{cumsum}}%\usepackage[]{algorithm2e}%\usepackage[]{algorithmic}\newcommand\revb[1]{\textcolor{black}{#1}}\newcommand\td[1]{\textcolor{red}{#1}}%\usepackage{float}\usepackage{stfloats}\begin{comment}


\newbox\jsavebox
\newcommand{\jsubfig}[2]{%
	\sbox\jsavebox{#1}%
	\parbox[t]{\wd\jsavebox}{\centering\usebox\jsavebox\\#2}%
	}
	
\newcommand{\jbox}[2]{
  \fbox{%
  	\begin{minipage}{#1}%
  		\hfill\vspace{#2}%
  	\end{minipage}%
  }}
\end{comment}\usepackage{csvsimple}\usepackage{xstring}%\usepackage{slashbox}%\usepackage{diagbox}%\usepackage{multirow,bigdelim}\newcommand{\ourmethod}{ALIGNet}%DIF PREAMBLE EXTENSION ADDED BY LATEXDIFF%DIF UNDERLINE PREAMBLE %DIF PREAMBLE\RequirePackage[normalem]{ulem}%DIF PREAMBLE\RequirePackage{color}\definecolor{RED}{rgb}{1,0,0}\definecolor{BLUE}{rgb}{0,0,1}%DIF PREAMBLE\providecommand{\DIFadd}[1]{{\protect\color{blue}\uwave{#1}}}%DIF PREAMBLE\providecommand{\DIFdel}[1]{{\protect\color{red}\sout{#1}}}%DIF PREAMBLE%DIF SAFE PREAMBLE %DIF PREAMBLE\providecommand{\DIFaddbegin}{}%DIF PREAMBLE\providecommand{\DIFaddend}{}%DIF PREAMBLE\providecommand{\DIFdelbegin}{}%DIF PREAMBLE\providecommand{\DIFdelend}{}%DIF PREAMBLE%DIF FLOATSAFE PREAMBLE %DIF PREAMBLE\providecommand{\DIFaddFL}[1]{\DIFadd{#1}}%DIF PREAMBLE\providecommand{\DIFdelFL}[1]{\DIFdel{#1}}%DIF PREAMBLE\providecommand{\DIFaddbeginFL}{}%DIF PREAMBLE\providecommand{\DIFaddendFL}{}%DIF PREAMBLE\providecommand{\DIFdelbeginFL}{}%DIF PREAMBLE\providecommand{\DIFdelendFL}{}%DIF PREAMBLE%DIF END PREAMBLE EXTENSION ADDED BY LATEXDIFF\usepackage[export]{adjustbox}%\usepackage{hyperref}%\hypersetup{%  colorlinks=true, linkcolor=blue%}%\usepackage{url}\usepackage{hyperref}

\begin{document}
\title{\rev{ALIGNet}: Partial-Shape Agnostic Alignment via Unsupervised Learning}


\author{Rana Hanocka}
\affiliation{\institution{Tel Aviv University}}

\author{Noa Fish}
\affiliation{\institution{Tel Aviv University}}

\author{Zhenhua Wang}\affiliation{\institution{Hebrew University}}

\author{Raja Giryes}
\affiliation{\institution{Tel Aviv University}}

\author{Shachar Fleishman}
\affiliation{\institution{Intel Corporation}}


\author{Daniel Cohen-Or}
\affiliation{\institution{Tel Aviv University}}


\begin{abstract}
The process of aligning a pair of shapes is a fundamental operation in computer graphics. Traditional approaches rely heavily on matching corresponding points or features to guide the alignment, a paradigm that falters when significant shape portions are missing. 
\rev{These techniques generally do not incorporate prior knowledge about expected shape characteristics, which can help compensate for any misleading cues left by inaccuracies exhibited in the input shapes.}
We present an approach based on a deep neural network, leveraging shape datasets 
\addcomment{to learn a \emph{shape-aware} prior for source-to-target alignment}%as a prior for learning shape to shape alignment 
that is robust to shape incompleteness.
In the absence of ground truth alignments for supervision, we train a network on the task of shape alignment using incomplete shapes generated from full shapes for self-supervision.
Our network, called \emph{\ourmethod{}}, is trained to warp complete source shapes to incomplete targets, as if the target shapes were complete, thus essentially rendering the alignment \emph{partial-shape agnostic}.
We aim for the network to develop specialized expertise over the common characteristics of the shapes in each dataset, thereby achieving a higher-level understanding of the expected shape space to which a local approach would be oblivious.
We constrain \emph{\ourmethod{}} through an anisotropic total variation identity regularization to promote piecewise smooth deformation fields, facilitating both partial-shape agnosticism and post-deformation applications. We demonstrate that \emph{\ourmethod{}} learns to align geometrically distinct shapes, and is able to infer plausible mappings even when the target shape is significantly incomplete. We show that our network learns the common expected characteristics of shape collections, without over-fitting or memorization, enabling it to produce plausible deformations on unseen data during test time.

\begin{comment}
Our network, called \emph{\ourmethod{}}, learns the space of warping transformations between shapes. Specifically, the network is trained to warp complete source shapes to incomplete targets, as if the target shapes were complete, thus essentially rendering the alignment \emph{partial-shape agnostic}.

Our network, called \emph{\ourmethod{}}, 
\end{comment}\begin{comment}
Ver1 submit to SIG:
The process of aligning a pair of shapes is a fundamental operation in computer graphics. Traditional approaches rely heavily on matching corresponding points or features to guide the alignment, a paradigm that falters when significant shape portions are missing. In this paper, we present a novel alignment technique that is robust to shape incompleteness. Lacking ground truth alignments for supervision, we train a neural network on the task of shape alignment using incomplete shapes generated from full shapes for self-supervision. Our network, called \emph{\ourmethod{}}, utilizes a high order spatial transformer to learn the space of warping transformations between shapes. Specifically, the network is trained to warp complete source shapes to incomplete targets, as if the target shapes were complete, thus essentially rendering the alignment \emph{partial-shape agnostic}. We aim for the network to form a specialized expertise over the common characteristics of the shapes in each dataset, supporting a higher-level understanding of the expected shape space that a local approach is oblivious to. We constrain \emph{\ourmethod{}} through an anisotropic total variation identity regularization to promote piecewise smooth deformation fields, facilitating both partial-shape agnosticism and post-deformation applications. We demonstrate that \emph{\ourmethod{}} learns to align geometrically distinct shapes, and is able to infer plausible mappings even when the target shape is significantly incomplete. We show our network learns the common expected characteristics of shape collections, without over-fitting or memorization, enabling it to produce plausible deformations on unseen data during test time.

\end{comment}\begin{comment}
The process of aligning a pair of shapes is a fundamental operation in computer graphics. Traditional \DIFdelbegin \DIFdel{alignment methods }\DIFdelend \DIFaddbegin \DIFadd{approaches }\DIFaddend rely heavily on matching corresponding points or features \DIFaddbegin \DIFadd{to guide the alignment}\DIFaddend , a paradigm that falters when significant shape portions are missing. In this paper, we present a novel alignment technique between two \DIFdelbegin \DIFdel{2D }\DIFdelend shapes that is robust to shape incompleteness. \DIFdelbegin \DIFdel{We }\DIFdelend \DIFaddbegin \DIFadd{Lacking ground truth alignments for supervision, we }\DIFaddend \DIFdelbegin \DIFdel{take an unsupervised learning approach, and}\DIFdelend train a neural network on the task of shape alignment using \DIFdelbegin \DIFdel{pairs of shapes }\DIFdelend \DIFaddbegin \DIFadd{incomplete shapes generated from full shapes }\DIFaddend for self-supervision. Our network, called \emph{\ourmethod{}}, \DIFdelbegin \DIFdel{learns }\DIFdelend \DIFaddbegin \DIFadd{utilizes a high order spatial transformer to learn }\DIFaddend the space of warping transformations between \DIFdelbegin \DIFdel{two shapes by performing a free-form deformation on a source shape such that it aligns well with a potentially geometrically distinct partial target shape. With this extensive training, we }\DIFdelend \DIFaddbegin \DIFadd{shapes. Specifically, the network is trained to warp complete source shapes to incomplete targets, as if the target shapes were complete, thus essentially rendering the alignment partial-shape agnostic. We }\DIFaddend aim for the network to form a specialized expertise over the common characteristics of the shapes in each dataset, supporting a higher-level understanding of the expected shape space that a local approach is oblivious to. \DIFdelbegin \DIFdel{Specifically, the network is trained to warp complete source shapes to incomplete targets generated from full shapes, as if the target shapes were complete, thus essentially rendering the alignment \emph{partial-shape agnostic}. }\DIFdelend We constrain \emph{\ourmethod{}} through an anisotropic total variation identity regularization to promote piecewise smooth deformation fields, facilitating both \DIFdelbegin \DIFdel{partial }\DIFdelend \DIFaddbegin \DIFadd{partial-shape }\DIFaddend agnosticism and post-deformation \DIFdelbegin \DIFdel{texture transfer}\DIFdelend \DIFaddbegin \DIFadd{applications}\DIFaddend . We demonstrate that \emph{\ourmethod{}} learns to align \DIFdelbegin \DIFdel{distinct shapes from the same class}\DIFdelend \DIFaddbegin \DIFadd{geometrically distinct shapes}\DIFaddend , and is able to infer plausible mappings even when the target shape is significantly incomplete. We show our network learns the \DIFdelbegin \DIFdel{common }\DIFdelend expected characteristics of shape collections, without over-fitting or memorization, enabling it to produce plausible deformations on unseen data during test time.
\end{comment}\begin{comment}
marked version:
The process of aligning a pair of shapes is a fundamental operation in computer graphics. Traditional \DIFdelbegin \DIFdel{alignment methods }\DIFdelend \DIFaddbegin \DIFadd{approaches }\DIFaddend rely heavily on matching corresponding points or features \DIFaddbegin \DIFadd{to guide the alignment}\DIFaddend , a paradigm that falters when significant shape portions are missing. In this paper, we present a novel alignment technique between two \DIFdelbegin \DIFdel{2D }\DIFdelend shapes that is robust to shape incompleteness. \DIFdelbegin \DIFdel{We }\DIFdelend \DIFaddbegin \DIFadd{Lacking ground truth alignments for supervision, we }\DIFaddend \DIFdelbegin \DIFdel{take an unsupervised learning approach, and}\DIFdelend train a neural network on the task of shape alignment using \DIFdelbegin \DIFdel{pairs of shapes }\DIFdelend \DIFaddbegin \DIFadd{incomplete shapes generated from full shapes }\DIFaddend for self-supervision. Our network, called \emph{\ourmethod{}}, \DIFdelbegin \DIFdel{learns }\DIFdelend \DIFaddbegin \DIFadd{utilizes a high order spatial transformer to learn }\DIFaddend the space of warping transformations between \DIFdelbegin \DIFdel{two shapes by performing a free-form deformation on a source shape such that it aligns well with a potentially geometrically distinct partial target shape. With this extensive training, we }\DIFdelend \DIFaddbegin \DIFadd{shapes. Specifically, the network is trained to warp complete source shapes to incomplete targets, as if the target shapes were complete, thus essentially rendering the alignment partial-shape agnostic. We }\DIFaddend aim for the network to form a specialized expertise over the common characteristics of the shapes in each dataset, supporting a higher-level understanding of the expected shape space that a local approach is oblivious to. \DIFdelbegin \DIFdel{Specifically, the network is trained to warp complete source shapes to incomplete targets generated from full shapes, as if the target shapes were complete, thus essentially rendering the alignment \emph{partial-shape agnostic}. }\DIFdelend We constrain \emph{\ourmethod{}} through an anisotropic total variation identity regularization to promote piecewise smooth deformation fields, facilitating both \DIFdelbegin \DIFdel{partial }\DIFdelend \DIFaddbegin \DIFadd{partial-shape }\DIFaddend agnosticism and post-deformation \DIFdelbegin \DIFdel{texture transfer}\DIFdelend \DIFaddbegin \DIFadd{applications}\DIFaddend . We demonstrate that \emph{\ourmethod{}} learns to align \DIFdelbegin \DIFdel{distinct shapes from the same class}\DIFdelend \DIFaddbegin \DIFadd{geometrically distinct shapes}\DIFaddend , and is able to infer plausible mappings even when the target shape is significantly incomplete. We show our network learns the \DIFdelbegin \DIFdel{common }\DIFdelend expected characteristics of shape collections, without over-fitting or memorization, enabling it to produce plausible deformations on unseen data during test time.
\end{comment}\begin{comment}
\addcomment{OPTION 2:
We present an unsupervised shape alignment technique which is robust to shape incompleteness. Traditional methods rely heavily on matching corresponding points or features to guide the alignment, a paradigm that falters when significant shape portions are missing. We introduce \emph{\ourmethod{}}, a neural network trained to align geometrically distinct shapes even when a target shape is significantly incomplete.  We utilize a high-order spatial transformer to learn the space of warping transformations between pairs of shapes. Absent known ground-truth alignments for supervision, we train our network to warp source shapes to potentially partial target shapes, evaluating shape alignment as a proxy for training the underlying alignment parameters. Specifically, we use incomplete shapes generated from full shapes for self-supervision, thereby rendering the alignment \emph{partial-shape agnostic}. We constrain our network through anisotropic total variation identity regularization to promote piecewise smooth deformation fields, facilitating both partial agnosticism and post-deformation applications. We demonstrate that \emph{\ourmethod{}} learns and develops expertise in identifying characteristics of shapes common to a particular category of objects, without overfitting or memorization, resulting in generalization on unseen data during test time.}
\end{comment}\begin{comment}
SUBMITTED TO TOG:
The process of aligning a pair of shapes is a fundamental operation in computer graphics. Traditional alignment methods rely heavily on matching corresponding points or features, a paradigm that falters when significant shape portions are missing. In this paper, we present a novel alignment technique between two 2D shapes that is robust to shape incompleteness. We take an unsupervised learning approach, and train a neural network on the task of shape alignment using pairs of shapes for self-supervision. Our network, called FFDnet, learns the space of warping transformations between two shapes by performing a free-form deformation on a source shape such that it aligns well with a potentially geometrically distinct partial target shape. With this extensive training, we aim for the network to form a specialized expertise over the common characteristics of the shapes in each dataset, supporting a higher-level understanding of the expected shape space that a local approach is oblivious to. Specifically, the network is trained to warp complete source shapes to incomplete targets generated from full shapes, as if the target shapes were complete, thus essentially rendering the alignment partial-shape agnostic. We constrain FFDnet through an anisotropic total variation identity regularization to promote piecewise smooth deformation fields, facilitating both partial agnosticism and post-deformation texture transfer. We demonstrate that FFDnet learns to align distinct shapes from the same class, and is able to infer plausible mappings even when the target shape is significantly incomplete. We show our network learns the common expected characteristics of shape collections, without over-fitting or memorization, enabling it to produce plausible deformations on unseen data during test time.
\end{comment}\begin{comment}
MOFIDIED:
The process of aligning a pair of shapes is a fundamental operation in computer graphics. Traditional approaches rely heavily on matching corresponding points or features to guide the alignment, a paradigm that falters when significant shape portions are missing. In this paper, we present a novel alignment technique between two shapes that is robust to shape incompleteness. Lacking ground truth alignments for supervision, we take an unsupervised learning approach and train a neural network on the task of shape alignment using incomplete shapes generated from full shapes for self-supervision. Our network, called \emph{\ourmethod{}}, utilizes a high order spatial transformer to learn the space of warping transformations between shapes. Specifically, the network is trained to warp complete source shapes to incomplete targets generated from full shapes, as if the target shapes were complete, thus essentially rendering the alignment partial-shape agnostic. We aim for the network to form a specialized expertise over the common characteristics of the shapes in each dataset, supporting a higher-level understanding of the expected shape space that a local approach is oblivious to. We constrain \emph{\ourmethod{}} through an anisotropic total variation identity regularization to promote piecewise smooth deformation fields, facilitating both partial-shape agnosticism and post-deformation applications. We demonstrate that \emph{\ourmethod{}} learns to align geometrically distinct shapes, and is able to infer plausible mappings even when the target shape is significantly incomplete. We show our network learns the expected characteristics of shape collections, without over-fitting or memorization, enabling it to produce plausible deformations on unseen data during test time.
\end{comment}\begin{comment}
Without prior knowledge about the expected characteristics of shape collections, it is challenging to preserve geometric features typical to the class of objects being warped   

Due to lack of prior knowledge about the shape in order to guide the warping in regions that are missing or how to preserve geometric features that are typical to the class of objects being warped.


Our network, called FFDnet, learns the space of warping transformations between two shapes by performing a free-form deformation on a source shape such that it aligns well with a potentially geometrically distinct partial target shape.


Our network, called \emph{\ourmethod{}}, learns the space of warping transformations between two shapes by \addcomment{employing a high order spatial transformer} on a source shape such that it aligns well with a potentially geometrically distinct partial target shape.





between pairs of potentially geometrically distinct shapes within the same shape category.


Specifically, the network is trained to warp complete source shapes to incomplete targets generated from full shapes, as if the target shapes were complete, thus essentially rendering the alignment \emph{partial-shape agnostic}.

Specifically, the network is trained to warp complete source shapes to potentially partial targets, as if the target shapes were complete, thus essentially rendering the alignment \emph{partial-shape agnostic}.


By leveraging 

Lacking prior knowledge about the shape fails to guide the alignment in regions that are missing or how to preserve geometric features that are typical to the class of objects being warped.

The failure is due to the inability of geometric optimization algorithms to utilize prior knowledge about the shape in order to guide the warping in regions that are missing or how to preserve geometric features that are typical to the class of objects being warped.


The process of aligning a pair of shapes is a fundamental operation in computer graphics. Traditional alignment methods rely heavily on matching corresponding points or features, matching points guide an optimization algorithm to compute a warp.This paradigm that fails when significant shape portions are missing. The failure is due to the inability of geometric optimization algorithms to utilize prior knowledge about the shape in order to guide the warping in regions that are missing or how to preserve geometric features that are typical to the class of objects being warped.
In this work, we propose a deep-networked based approach to overcome these limitations. Our network is... some details, self supervised... and demonstrate excellent results.


We utilize a high-order spatial transformer to learn the space of warping transformations between pairs of shapes,


Our network, called \ourmethod{}, utilizes a differential free-form deformation module (a type of spatial transformer) to learn the space of warping transformations between pairs of potentially geometrically distinct shapes within the same shape category. We aim for the network to form a specialized expertise over the common characteristics of the shapes in each genus, supporting a higher-level understanding of the expected shape space that a local approach is oblivious to.



forming a specialized expertise over the common characteristics of , supporting a higher-level understanding of the expected shape space that a local approach is oblivious to.


Our network, called \ourmethod{}, learns the space of warping transformations between two shapes by performing a free-form deformation on a source shape such that it aligns well with a potentially geometrically distinct partial target shape. 


Absent ground truth alignments for supervision, we investigate an unsupervised learning approach and train a neural network on the task of shape alignment using incomplete shapes generated from full shapes for self-supervision.

form a specialized expertise over the common characteristics of \addcomment{different shape categories}, supporting a higher-level understanding of the expected shape space that a local approach is oblivious to.

Our network, called \ourmethod{}, learns the space of warping transformations between two shapes by performing a free-form deformation on a source shape such that it aligns well with a potentially geometrically distinct partial target shape. 


Specifically, the network is trained to warp complete source shapes to incomplete targets generated from full shapes, as if the target shapes were complete, thus essentially rendering the alignment partial-shape agnostic.
\end{comment}
\end{abstract}


\citestyle{acmauthoryear}
\setcitestyle{square}

\maketitle



\section{Introduction}

Shape registration is a fundamental problem in computer graphics and computer vision, with diverse applications ranging from object recognition and scene understanding to texture or attribute transfer and synthesis. Classic approaches draw from a predefined family of transformations to obtain a warp that optimizes the registration between two shapes. While successful in straight-forward scenarios, the limitations of this paradigm are revealed as we seek to match increasingly distinct shapes differing in geometry and topology. Given such differences, restricting the pool of allowed transformations (\emph{e.g.,} affine transformations) is often insufficient, requiring higher-order deformations (\emph{e.g.,} free-form deformation). The problem becomes even more challenging when the shape is partial, for example due to missing data or occlusions.

Typically, the desired transformation between two shapes is obtained by solving a set of constraints defined by sparsely matching corresponding points or features on the shapes in question. Point matching necessitates a modicum of geometric resemblance between point neighborhoods, which is not always at hand under moderate to large differences. To alleviate this difficulty, features or descriptors are computed and matched instead. However, designing descriptors to be sensitive to interesting aspects of the shape yet robust to less significant variations, as well as to a wide range of shape types, is a daunting, and at times even intractable, task. 
As a further complication, when the shape is incomplete,
the process of feature matching becomes even more ambiguous and prone to errors. 
-data /home/rana/data/silhouettes/rendered/vase -retrain /home/rana/data/silhouettes/rendered/vase/checkpoint/cagenet_2_12ctrls_silhouette,augmentAffine=t,batchSize=200,cage_reg=1e-05,delCage=t,learn_beta=t,mpgPart=t,nEpochs=100/MonSep1119:22:422017/model_100.t7 -batchSize 30 -cage_reg 1e-5 -delCage -evalOnly -learn_beta -mpgPart -src_name ./../test_cache/texture_tests/pumpkin2b.h5 -saveHR -mpgSRng 1.1..1.8
\end{comment}\begin{figure}[t!]
\newcommand{\sfig}{3}
\setlength\tabcolsep{2pt}
\begin{tabular}{ c c c }

\includegraphics[width=\sfig cm]{figures/teaser2/sample006_source.png} &
\includegraphics[width=\sfig cm]{figures/teaser2/sample006_target.png} &
\includegraphics[width=\sfig cm]{figures/teaser2/sample006_targetPred.png} \\

source &
target &
aligned \\

\end{tabular}    
\caption{\rev{\ourmethod{} aligns a \emph{pumpkin} to a partial \emph{vase} (missing part visualized in pink), resulting in a \emph{vase}-like \emph{pumpkin} (aligned).}}
\label{fig:teaser2}
\end{figure}

Inspired by the recent irrefutable success of deep learning methods applied to various computer vision problems, we propose a data-driven learning approach to address the problem of \addcomment{partial} shape alignment. Instead of providing a local solution, we opt to leverage the ever-growing availability of large collections of shapes to learn a data-aware global warping model. Many of these shape collections are consistently pre-oriented (or can be oriented \emph{e.g.,} by employing hierarchical alignment~\cite{shapenet2015}), and contain a rich variety of examples that collectively form a comprehensive guide capturing the characteristic attributes of the set, such as common geometry, topology, and even semantic high-level features such as part existence.
We arrange this data into random pairs of source and target shapes, and train a convolutional neural network (CNN) to learn the mapping between input source and target shapes.
We employ grid-based free-form deformation (FFD\new{~\cite{sederberg1986free}}), a new type of high order spatial transformer \addcomment{(STN)~\cite{spatialtransformer}}, which is simple yet expressive enough to support a wide range of alignments.
\rev{Unlike STN, we deliberately learn to align a source shape to an incomplete target, as though it were complete (\emph{e.g.,} Figure~\ref{fig:teaser2}).}%}%We employ grid-based free-form deformation (FFD), a new type of high order spatial transformer \addcomment{(STN)~\cite{spatialtransformer}}, which is simple yet expressive enough to support a wide range of alignments.
As shown in Figure~\ref{fig:teaser}, we are able to warp a model's texture to different partial target shapes, even when the target is missing more than 50\% of the original shape (all shapes are from the test set and therefore unseen during training). Furthermore, we are able to faithfully predict the scale (even in ambiguous scenarios such as target B) and correctly reconstruct symmetries of missing parts (target A).


Our approach is completely unsupervised, in that we do not supply any ground-truth warps that the network is expected to reproduce. Instead, the network is trained with a shape alignment loss, comparing the overlap between the warped source and the expected (complete) target, acting as a proxy for learning the desired underlying FFD parameters. As the network is trained, it essentially familiarizes itself with the common characteristics of the shapes in each dataset, allowing it to form a higher-level understanding of the expected shape space 
to which a local approach would be oblivious.
As such, our network is able to deal with incomplete, partial shapes, inferring plausible mappings in the missing regions by computing a \emph{class-aware} alignment. By training to warp complete source shapes to incomplete targets, as if the target shapes were complete, %\addnote{
our network learns to be \emph{partial-shape agnostic}: unaffected by missing shape parts.

Due to the unsupervised nature of our training, in its simplest form, the model is given free reign in terms of the warp fields it is allowed to output, as long as the end result is deemed appropriate. 
Specifically, there are many vastly different FFDs that, when applied on \rev{\emph{binary shapes}}, yield the same shape alignment cost.
Such a behavior is undesirable when the warp field is needed for further processing; thus, to ensure the quality and dependability of our warps, we introduce a key addition to regulate the space of possible transformations.
This addition encourages piecewise smooth warp fields by penalizing the total variation of the deviation of the warp field from the identity.
By tuning the weighted contribution of this regularization component to the overall score, we are better able to control the degree of freedom given to the network for warp computation. We show that this step benefits not only post-deformation processes, but also partial agnosticism.

\begin{comment}
Training the network to specialize on a given family of shapes for robustness to incompleteness, essentially limits the scope of the system. For any new class of shapes, the network is required to be trained on the appropriate data we wish to support. We demonstrate, however, that for a given set of shapes, our network generalizes well by learning the common expected characteristics of the shapes, without over-fitting or memorization, enabling it to produce plausible deformations on unseen data during test time. See the two bottom rows of Figure~\ref{fig:teaser} for examples of a successful application of our system on shape instances from an entirely different class than the training data. 
\end{comment}\begin{comment}
We henceforth summarize our contributions:
\begin{itemize}
\item We introduce a data-driven self-supervised learning approach for shape alignment, based on Free Form Deformation (FFD).
\item The suggested network is agnostic to missing parts in the shape and generalizes to shapes outside the training set.
\item We introduce a key regularization operation that enforces axial monotonicity of the warp field in order to constrain the FFD and promote smoother deformations.
\item We demonstrate the applicability of our technique for texture and segmentation transfer.
\end{itemize}
\end{comment}\begin{comment}
(caused, for instance, by occlusions or missing parts)
\addnote{We argue that these limitations are due to lack of prior knowledge about the expected shape, which could guide the alignment in regions that are missing and preserve geometric features typical to the class of objects being warped.}
\end{comment}
\section{Related Works}%\subsection{Full shapes}
Common shape registration methods estimate a transformation from a pre-determined class of transformations, by evaluating the overall shape-to-shape alignment. The classic Iterative Closest Point (ICP) method~\cite{ICP} uses nearest neighbor correspondences to refine the transformation, which minimizes the mismatch between the source and target points. Follow-up works propose numerous variants of ICP involving modifying the constraints defined to compute the transformation~\cite{ICPVariants_Levoy2001}. Other methods assume a global probabilistic approach~\cite{CPD,Jian2011,Tsin2004}, or more recently incorporate local structure to further improve results~\cite{ma2014robust,ma2016non}. To handle partial shapes, these approaches must explicitly incorporate outlier rejection and intelligently select a subset of appropriate points, a challenging task in and of itself.

To handle partial shapes, approaches such as RANSAC~\cite{RANSAC}, sample a small number of points to form candidate transformation models, and determine the best one by voting. This approach works well when the two shapes share similar geometries~\cite{amo_fpcs_sig_08}, and the transformation model can be expressed with a rather small number of variables, e.g., rigid or affine transformation. However, their search space becomes prohibitively large and even intractable if their geometries significantly differ. Later, we show (Figure~\ref{fig:comparisons_qual}) a restrictive alignment between shapes using affine transformations. The shapes that we successfully align in our work require higher order deformations, for example FFDs with 128 degrees of freedom.

The use of local descriptors compensates for significant geometry differences~\cite{Belongie2002,Zheng2006,Mori2003,Thayananthan2003,ling2007shape}. \rev{Works which use functional maps for correspondence rely on good local descriptors to drive the matching process~\cite{litany2017fully}}. These descriptors are typically hand-crafted and often tailored to a specific subset of shapes. When the transformation model is a high-order deformation, it is challenging to find good correspondences between the local descriptors, which are often ambiguous. 
Recently, many works have taken a deep learning approach to directly compute correspondence or learn an invariant feature descriptor, e.g.,~\cite{choy2016universal,simo2015discriminative, tian2017l2}. WarpNet~\cite{kanazawa2016warpnet} uses exemplar warps for training a network to compute visual correspondences. Zbontar et al.~\shortcite{zbontar2016stereo} train a CNN to accurately match correspondences across stereo image pairs (later improved by~\cite{shaked2016improved}). 
\revb{Fischer et. al.~\shortcite{FischerDIHHGSCB15} proposed using supervised CNNs to solve the optical flow estimation problem.} 
In 3D, Zeng et al.~\shortcite{zeng20163dmatch} used 3D-CNNs to learn a noise invariant feature representation. 

The Spatial Transformer Network (STN)~\cite{spatialtransformer} showed that incorporating a transformation module into the network enables the task at hand to be transformation invariant (further expanded in~\cite{li2017dense}), demonstrating inspiring results on digit classification. \ourmethod{} can be considered of as a new class of high-order spatial transformers, designed for the task of partial shape non-rigid alignment. In our presented work, \ourmethod{} learns an FFD, however it can similarly learn other classes of transformations, such as affine, projective or TPS~\cite{TPS}, as used in STN. Note that unlike any of the aforementioned works, we deliberately learn to align a source shape to an incomplete target shape, as though the target were complete.


\section{Overview}\begin{figure*}[ht]
\includegraphics[width=16cm]{figures/cagenet/alignet.png}
\caption{Illustration of \ourmethod{} architecture. During training, \ourmethod{} learns to compute FFD grids without labeled ground-truth FFDs via the proposed unsupervised training scheme.
Given input source and target shapes, a random mask is applied to the target shape to form its partial version. A feature extraction CNN and a fully connected (FC) plus absolute value layer are then employed to output the differential of the warp field. The integrator layer, implemented via cumulative sum, results in the warp field. To compute the shape loss $\ell_s$, the warp field is applied on the source shape through the FFD warping layer, and the regularization loss $\ell_r$ is computed on the differential warp field. At test time, after the network weights have been learned, only the sub-portion of \ourmethod{} that computes the FFD grid deformation is required (bounded inside the gray rectangle).}
\label{fig:FFD_arch}
\end{figure*}\begin{comment}
\caption{Illustration of FFDnet architecture. During training time, FFDnet learns to compute FFD grids without labeled ground-truth FFDs via the proposed unsupervised training scheme (outside gray region). (a) Input source and target shapes, (b) select random mask (c) apply to target shape. (d) Feature extraction CNN and (e) fully connected output layer plus (f) absolute layer which outputs the differential of the warp field. Then (g) integrates via cumulative sum layer resulting in the warp field. To compute the shape loss $\ell_S$, the warp field is applied on the source shape through (h) FFD warping layer, and the regularization loss $\ell_r$ is computed on the differential warp field. During test time, after the network weights have been learned, only the sub-portion of FFDnet that computes the FFD grid deformation is required (bounded inside the gray square).}

\end{comment}\begin{comment}
paragraphs are as follows:
1. A paragraph that describes the "Vanilla architecture" and FFD deformation

2. self-supervision learning, the loss function, and training data, and define the setting, what do we learn, like how we define the occlusions and what in general is the scope of the network

3. The regularizations and any additions on the Vanilla scheme
\end{comment}\ourmethod{} is a neural network that aligns a source shape to a target shape via FFD. The FFD controllers are defined on a uniform grid, whose resolution trades off between deformation expressiveness and warping smoothness. Defining the FFD on a uniform grid is a judicious technique for providing an expressive, yet smooth, deformation, with a relatively small number of parameters. The power of \ourmethod{} lies is its ability to learn to align a source shape to an incomplete target shape as though the target were complete.

Training a network when ground-truth information is unavailable is a major challenge.
Our approach employs unsupervised learning, \emph{i.e.,} the true FFD that best aligns the source shape to the target is not utilized during training. The network scheme is illustrated in Figure~\ref{fig:FFD_arch}. \ourmethod{} is trained by drawing two instances from a pool of training data of objects from the same class, deeming one instance as the source shape and the other as the target shape. To mimic missing data, we remove a random part of the target shape, and feed the partial target as input to the network, holding out the full shape for loss computation. The input shapes are then fed forward through a series of neural network layers yielding an FFD, which when applied on the source shape yields a deformed source, called the estimated target. The training loss measures the difference between the estimated target and the complete target. Consequently, \ourmethod{} is \emph{partial shape agnostic}, that is, it learns to estimate the deformation that aligns the source shape to the partial target as if it were the complete target.

Without any restrictions on the FFD, \ourmethod{} can conjure up deformations which do not preserve internal structure. 
To this end, we introduce a total variation penalty that is key to 
smooth deformations.
Rather than directly computing the \emph{actual displacement} values, we compute \emph{relative displacements}, where each displacement is relative to its preceding displacement, such that the cumulative summed result (in each axis) yields the actual grid displacement field.
Since these relative displacements are essentially displacement gradients, we utilize them to employ anisotropic total variation (TV) on the deviation from the identity warp field, which encourages uniformly spaced piecewise smooth deformation fields.
To further encourage smoothness, we maintain axial monotonicity by enforcing positive relative grid displacements which ensure consecutively increasing values in each axis of the displacement grid.
We present and evaluate \ourmethod{} on 2D shapes, and demonstrate its extension to 3D in section~\ref{sec:threed}.
\section{Method}

The input to our system is made up of pairs of source and target shapes, where a target shape may be partial at various locations and to varying extents. The baseline system consists of a neural network component $f$ that computes an FFD in the form of a grid-based warp field $W_c$.
The input source shape $S$ and the potentially partial target $T^p$ are stacked as a two-channel image and passed to $f$ to obtain the low resolution FFD warp field: $f(S,T^p) = W_c$. The architecture of $f$ is given by a series of max pooling, convolutions and rectified linear activation units (repeated four times), followed by two fully-connected layers. Similar to~\cite{spatialtransformer}, we train the network from scratch by setting the convolutional layer weights randomly, and initializing the last FC to yield the identity displacement field (weights set to zero and biases to the identity displacement field). This is followed by the differentiable FFD sampling layer, which warps the source to the target by upsampling the low resolution FFD.

The network is trained in an unsupervised manner, such that at no point during training is it shown the ground-truth warp field that correctly aligns the source to the target shape.
Instead, the pairs of shapes given as input during training are in fact triplets, where the target is passed twice - once in its complete form and once with missing parts. 
This scheme facilitates training by bestowing a self-supervised shape similarity loss function $(\ell_s)$: comparing the warped source to the full target (see Figure~\ref{fig:FFD_arch}), aimed at teaching the network to become oblivious to possible missing data. 

In addition to the shape similarity loss function, we add a regularization loss $(\ell_r)$ which encourages the network to produce a warp field that is varying uniformly vis-\`a-vis the identity warp, limiting the likelihood of fold-overs. Our loss function thus amounts to: $loss = \ell_s + \lambda \ell_r$, where $\lambda$ is a user defined parameter that trades off between shape alignment and smoothness of the warp field.

\begin{figure}[h]
\jsubfig{\includegraphics[height=3cm]{figures/FFD/source.png}}{}%
\; \jsubfig{\includegraphics[height=3cm]{figures/FFD/warped.png}}{}%
\\
\jsubfig{Source}{}%
\hspace*{2cm}
\jsubfig{Target}{}%
\caption{FFD illustration. The uniform 8x8 grid in the target domain shown in blue, applied in a backwards manner: each location in the target grid corresponds to a look-up location in the source.}
\label{fig:FFD_visualization}
\end{figure}
\end{comment}\begin{figure}[h]
\newcommand{\sfig}{2.7}
\setlength\tabcolsep{2pt}
\begin{tabular}{ c c }

\includegraphics[height=\sfig cm]{figures/FFD/source.png} &
\includegraphics[height=\sfig cm]{figures/FFD/warped.png} \\

source &
warped \\
\end{tabular}   
\caption{
\addcomment{2D FFD grid deformation. The uniform 8x8 grid in the warped domain (right) shown in blue, applied in a backwards manner: each location in the warped domain grid corresponds to a look-up location in the source (left).}}
\label{fig:FFD_visualization}
\end{figure}\subsection{FFD Representation}
We constrain the space of possible warp fields by incorporating regularization that encourages piecewise smooth deformation fields and simultaneously enforces axial monotonicity (illustrated in Figure~\ref{fig:FFD_regularization}).
\includegraphics[height=5cm]{figures/cagenet/alignet_regularization.png}
\caption{Regularization component commissioned for training \ourmethod{} to compute plausible FFD grids. The network weights predict a differential grid, passed through an absolute value operator to enforce axial monotonicity. Followed by $\ell_1$ regularization on $\Delta W - \Delta I$ (equivalent to anisotropic total variation regularization on $W - I$). Lastly, the differential $\cumsum$ layer (Integrator) restores the absolute displacement grid.}
\label{fig:FFD_regularization}
\end{figure}%Followed by anisotropic total variation regularization on $\Delta W - \Delta I$, encouraging piecewise-smooth deformation fields.%%
We facilitate these warp field constraints by representing the FFD as a \emph{differential} displacement grid, where each value is relative to the preceding value in the grid. Instead of directly computing the absolute FFD values after the FC layer, we compute the differential FFD values, which when cumulatively summed result in an absolute FFD grid. The differential displacement grids encourage shift-invariance, since the displacement at each grid cell is relative to its neighborhood. 

To illustrate differential displacement fields, consider a 1D absolute sequence $a$ of length n, and the corresponding differential sequence $\Delta a$ as
\begin{subequations}
\begin{align}
a  & = \Big\{ a_k \Big\}_{k=1}^n  = ( a_1, a_2, a_3, \cdots, a_n ) \label{eq:oned_seq_abs}\\
\begin{split}
\Delta a & = \Big\{a_k - a_{k-1} \Big\}_{k=1}^n = ( \Delta a_1, \Delta a_2, \Delta a_3, \cdots, \Delta a_n )
\\
& = (a_1 - a_0, a_2 - a_1, a_3 - a_2,  \cdots, a_n - a_{n-1} ),
\end{split}
\label{eq:oned_seq_rel}
\end{align}
\end{subequations}
where $a_0$ is a scalar offset (here $a_0 = 0$) that can be learned during training. Given the differential 1D $\Delta a$, the absolute sequence can be exactly recovered using the cumulative sum operator
\begin{equation}
a = \cumsum( \Delta a ,a_0) = \Big\{ a_0 + \sum_{i=1}^k \Delta a_i \Big\}_{k=1}^n.
\label{eq:cumsum}
\end{equation}
The proposed $\cumsum$ layer is differentiable and can be added into a network for training. It is apparent in the 1D example that the derivative is given by
\begin{equation}
\partial \Delta a=\Big\{\sum_{i=k}^n \partial a_i  \Big\}_{k=1}^n.
\label{eq:cumsum_deriv}
\end{equation}%

The deformation field in \ourmethod{} is a 2D grid with vertical and horizontal displacement components, where the $\cumsum$ operates along each dimension, respectively. The learned \ourmethod{} weights map the input shapes to the differential displacement field $\Delta W_c$, subsequently passing through the absolute and $\cumsum$-layer, resulting in the final displacement field $W_c = \cumsum(\Delta W_c, W_0)$ (where $W_0$ is a learned scalar).

Observe that for any strictly positive differential 1D sequence ($\Delta a_k \geq 0$ for all $k$), the recovered absolute 1D sequence $a$ is monotonic
\begin{equation}
a   = \Big\{ a_k \Big\}_{k=1}^n \quad \text{\textit{s.t.}  } \; a_k \leq a_{k+1}
\label{eq:cumsum_mono}
\end{equation}%%a \triangleq \cumsum(\Delta a, a_0) \textrm{ , where } \Delta a_k \geq 0 \textrm{ for all } k.
In \ourmethod{}, we enforce the differential sequence at the end of the network to be positive. In the 2D case, enforcing strictly positive relative displacement values $\Delta W_c \geq 0$, we ensure axial monotonicity in the displacement fields. Axial monotonicity implies that each axis increases monotonically independently of the opposing axis.

The displacement map used to express the FFD is a low resolution 2D displacement grid, defining the inverse warp field from target to source (see Figure~\ref{fig:FFD_visualization}). To obtain the full resolution target image, pixels in the target image are copied from their corresponding source image locations as indicated by a bilinear interpolation upsampling of the low resolution warp field. This is accomplished by a 2-step bilinear sampling operation. First, use the bilinear sampler $\oland$ to upsample the lower resolution grid $W_c$ to the full size of the source image ($mxn$) by $W = W_c \oland \mathcal{G}^{mxn}$,
where $\mathcal{G}^{mxn}$ is a regular grid of coordinates for every pixel in the full resolution $mxn$. 
The second sampling applies the upsampled warp field on the source shape $\hat{T} = S \oland W$,
resulting in an estimate of the target shape. For brevity, we represent the two-step sampling as
\begin{equation}
\hat{T} = S \oland W_c \leftarrow S \oland  (W_c \oland \mathcal{G}^{mxn}).
\label{eq:sampler}
\end{equation}\subsection{Loss Functions}\label{sec:loss}
Estimating FFDs without direct supervision is ill-posed, since the large degree of freedom (DoF) in the FFD results in wildly different deformations - all with very similar cost. Specifically, \ourmethod{} operates solely on binary silhouettes, since knowing the true corresponding RGB textures between source and target shapes requires a form of ground-truth annotation. Binary silhouettes are problematic since any FFD that deforms source foreground pixels to target foreground pixels, even if parts are completely rearranged, results in the same shape similarity loss. Absent regularization, \ourmethod{} is entitled to falsely conjecture about the possible deformation field.

Our overall loss function is composed of two components. The first is the shape alignment loss, encouraging a faithful deformation from source to target. The second, which measures anisotropic total variation with regards to deviation from the identity warp, serves as a strong regularization component that helps produce smoother warp fields. 

\subsubsection{Shape Alignment Loss}
The self-supervised training loss evaluates the difference between the complete target $T$ and the source that has been aligned by the FFD (\emph{i.e.,} estimated target $\hat{T}$) to train \ourmethod{}. For every example $n$ ($S^n$, $T^n$) in the training set $\mathcal{N}$, we train the network by minimizing the loss given by
f = \argmin_{f} 
\sum_{n \in \mathcal{N}}
|| S^{n} \oland f(S^{n},{T^{p}}^n) - T^{n} || 
= \sum_{n \in \mathcal{N}}
|| S^{n} \oland W_c - T^{n} ||,
\label{eq:FFDnet_loss}
\end{equation}
where $||\cdot||$ can be any error measure. We use the $\ell_2$-norm, which is given by
 \ell_s(T,\hat{T}) = \sum_{i \in \forall (\hat{T},T)} \frac{1}{2}(T_i-\hat{T_i})^2,
\label{eq:huber}
\end{equation}
for every pixel $i$ in the target and estimated target.

The proposed approach enables training without ground-truth FFDs while simultaneously learning to be partial-shape agnostic. At test time, given a source shape $S$ and a target shape with different parts missing $T^{p_1}, T^{p_2}, \cdots T^{p_n} \in T$ (from test set), \ourmethod{} demonstrates partiality-agnostic behavior by computing very similar FFDs for different partialities of the same underlying target (see Figure~\ref{fig:sweeping_partialT}). We attribute this generalization to the unsupervised nature of the training process.

We facilitate minimal distortion deformation fields by imposing an additional penalty in the form of TV regularization. Observe that the differential sequence as defined in Equation~\ref{eq:oned_seq_rel}, is actually the discrete gradient of the absolute sequence in Equation~\ref{eq:oned_seq_abs}. 
Similarly for the $2$D counterpart, the differential grid $\Delta W$ is made up of two channels, $\Delta W_x$,$\Delta W_y$, which are the horizontal and vertical derivatives of $W$ respectively. Recall that the anisotropic total variation regularization for a 2D matrix is given by the $\ell_1$-norm of the sum of the horizontal and vertical gradients
\ell_{TV}(W) = ||\Delta W_x ||_1 + ||\Delta W_y ||_1.
\label{eq:TV_reg}
\end{equation}%
TV regularization encourages deformations that are piecewise smooth, favoring smoothness but simultaneously allowing for large discontinuities due to the $\ell_1$-norm. Piecewise smoothness is the essence of a favorable deformation field: large discontinuities at the shape boundaries with a smooth shape interior. 

Yet, the displacement field $W$ that would lead to the lowest cost in the standard TV regularization in Equation~\ref{eq:TV_reg} would be simply a zero valued differential displacement field, which is an undesirable result. Instead, we encourage a non-zero-valued uniform spacing by subtracting the differential identity warp field from the differential warp, favoring a uniformly spaced grid over the size of the image.

For a 1D analogy, consider an identity sequence $I$ of length $n$ normalized to the interval $[-1, 1]$, which is uniformly spaced by a scalar $\delta_k = \frac{2}{n-1} $. Then we can write $I$ and the corresponding differential $\Delta I$ as
\begin{align}
\begin{split}
\label{eq:id_seq}
I & = \Big\{ -1 + (k - 1) \cdot \delta_k  \Big\}_{k=1}^n \\ 
& = (-1, -1 +\delta_k  , -1 +2 \delta_k , \cdots, 1)
\end{split}
\\
\label{eq:id_seq_rel}
\Delta I & = \Big\{ \delta_k  \Big\}_{k=1}^n,
\end{align}
\end{subequations}%
where the differential identity sequence $\Delta I$ contains a single unique value: the uniform spacing constant $\delta_k$. In $2$D, it follows that the proposed anisotropic TV identity regularization becomes 
\ell_{r} = \ell_{TV}(W - I) = ||\Delta W_x - \Delta I_x||_1 + ||\Delta W_y  - \Delta I_y||_1.
\label{eq:TV_reg_id}
\end{equation}%
The overall contribution of the regularization term is controlled by a weighting factor $\lambda$.


\section{Results and Evaluation}

In this section we examine and test \ourmethod{} through various qualitative and quantitative evaluations (code and data are available at \href{https://github.com/ranahanocka/ALIGNet}{\textbf{https://github.com/ranahanocka/ALIGNet}}). All experiments are performed on the reserved \emph{test set data}, meaning that these shapes were never trained on. All networks are trained on strictly binary silhouettes, but we overlay a checkerboard texture on the source shape to clearly visualize the smoothness of the estimated mappings and display the texture transfer abilities facilitated by our system. Note that the missing components of the target are illustrated in light pink (for visualization purposes only), but the actual network input is binary. For all results and classes, we use the same parameters and architecture described in Table~\ref{table:params}, unless stated otherwise (\emph{e.g.,} in the ablation study).

\begin{center}
 \begin{tabular}{|c || c|} 
 \hline
 \multicolumn{2}{|c|}{Architecture} \\
 \hline
 conv1 &  $5 \times 5 \times 20$  \\ 
 conv2 &  $5 \times 5 \times 20$  \\ 
 conv3 &  $2 \times 2 \times 20$  \\ 
 conv4 &  $4 \times 4 \times 20$  \\ 
 fc1 &  $20$  \\ 
 fc2 &  $2 \cdot m \cdot n$  \\ 
 \hline
\end{tabular}
\quad
 \begin{tabular}{|c || c|} 
 \hline
 \multicolumn{2}{|c|}{Parameters} \\
 \hline
 n & 8 \\ 
 m & 8 \\ 
 $\lambda$ & $1e-5$ \\ 
solver & ADAM \\ 
LR & $1e-3$ \\ 
 \hline
\end{tabular}
\end{center}
\caption{\ourmethod{} architecture and parameters used for all results (except where otherwise specified), \addcomment{where $m$ and $n$ represent the resolution of the grid.}}
\label{table:params}
\end{table}\subsection{Training Data}\label{sec:un_data}
In our experiments, we train \ourmethod{} on a class of shapes where pairs of source and target instances are randomly drawn from the pre-defined training set. Specifically, we have experimented with sets containing renders of lower and upper case letters for 330 fonts (resulting in 56 classes), as well as 2D projections of 3D objects from ShapeNet~\shortcite{shapenet2015} and COSEG~\cite{wang2012active}. 

\begin{comment}
Figure~\ref{fig:instace_examples} features a small subset of silhouette instances that serve as input to \ourmethod{}.

\begin{figure}[h]
\begin{tabular}{ c c }
\includegraphics[width=3.5 cm]{figures/sample_data/H_samples25.png} &
\includegraphics[width=3.5 cm]{figures/sample_data/airplane_samples25.png} \\
uppercase H & airplane \\
\end{tabular}
\caption{A small random subset of rendered silhouettes: displaying instances of shape data from two different classes.}
\label{fig:instace_examples}
\end{figure}
\end{comment}\paragraph{Train and Test Sets}
For each class dataset, we reserve 30 models for testing and use the rest for training. For example, a class with a set of 330 shapes yields $\sim$90k distinct training pairs and 870 test pairs. We augment the number of training set examples by generating an arbitrarily large amount of partial data, and by applying small vertical and horizontal scaling on-the-fly during training. We generate rectangular partial data masks by setting a random foreground pixel as the center, with width and height sampled from a uniform distribution. 

During testing, we evaluate performance on rectangular masks and verify generalization on non-rectangular partial data masks created from the MPEG-7~\cite{latecki2000shape} dataset. %(see Figure~\ref{fig:complex_part} and throughout).\begin{figure}
\newcommand{\conv}{3}
\setlength\tabcolsep{5pt} % default value: 6pt
\begin{tabular}{cc}
\hspace*{4mm} Train &
\hspace*{4mm} Test \\
\includegraphics[width=\conv cm]{figures/conv_plots/train_vase.png} &
\includegraphics[width=\conv cm]{figures/conv_plots/test_vase.png} \\

\hspace*{5mm}Iterations &
\hspace*{5mm}Iterations \\

\end{tabular}
\caption{\revb{Train (left) and test (right) error convergence plots for the \emph{vase} class.}}
\label{fig:conv_plot}
\end{figure}\subsection{Ablation Study}
We analyze the qualitative and quantitative effects of grid regularization by training \ourmethod{} in three different settings: without any regularization (none), total variation regularization (TV) and total variation regularization with axial monotonicity (TV \& M). Regularization controls the trade-off between: fidelity, in our case the silhouette alignment accuracy, and the prior, which is the smoothness of the warp field. We quantitatively measure accuracy via IOU between the source and true underlying target, and measure smoothness by the difference between the warp field gradient and the identity field gradient (TV loss in Equation~\ref{eq:TV_reg_id}). 

The fidelity term in \ourmethod{} (Equation~\ref{eq:FFDnet_loss}) without additional regularization is a greedy criteria, aspiring to improve the alignment accuracy (synonymous to IOU) at any cost. Without any regularization, advanced training iterations yield increasingly unsmooth warp fields. This continued training results in a deceitful improvement in alignment accuracy, since the resulting deformation is inconsistent with plausible FFDs (visuals in Figure~\ref{fig:visual_reg_ablation}). Importantly, training with TV regularization preserves smooth fields for increasing training iterations, stipulating faithful improvements in alignment accuracy (plots in Figure~\ref{fig:itr_reg_plot}). Naturally, incorporating an additional regularization term leads to slightly inferior IOU accuracy, but gains significantly smoother, and therefore more conceivable, warp fields.

Recall Equation~\ref{eq:TV_reg} which defined anisotropic TV, isotropic TV is given by}\begin{equation}
\revb{\ell_{TV}(W) = ||\sqrt{\Delta W_x^2 + \Delta W_y^2} ||_1,}
\end{equation}\revb{where the square and square-root operations are applied elementwise.}\begin{figure}
\newcommand{\rfig}{1.6}
\setlength\tabcolsep{2pt}
\begin{tabular}{ c c c c }
\includegraphics[width=\rfig cm]{figures/ablation/tviso/sample190_source.png} &
\includegraphics[width=\rfig cm]{figures/ablation/tviso/sample190_target.png} &
\includegraphics[width=\rfig cm]{figures/ablation/tviso/sample190_targetPred-l1.png} &
\includegraphics[width=\rfig cm]{figures/ablation/tviso/sample190_targetPred-l2.png} \\
source &
target & 
isotropic &
anisotropic \\
\end{tabular}
\caption{\revb{\ourmethod{} is modular, with the ability to employ alternative forms of regularization depending on the application. Applying isotropic vs. anisotropic TV regularization can lead to slight differences in the outcome.}}
\label{fig:isotv}
\end{figure}\begin{comment}
 Starting with a 2$\times$2 grid with 8 corresponding degrees of freedom, the predicted deformations are noticeably inadequate. The  4$\times$4 grid is insufficient for the disparate geometries between the airplanes. To enable high order deformations, it may appear worthwhile to increase the grid resolution as much as possible; however, the resulting exponential increase in the DoF, necessitates more training data, deeper networks, and longer training time. To keep training overhead low while simultaneously enabling high order deformations, we opt for an 8$\times$8 grid (128 DoF).
\end{comment}\begin{comment}

guitar

python benchmark/runEvalt7file.py -data guitar -itr 100 -batchSize 50 -retrain cagenet_2_12ctrls_silhouette,augmentAffine=t,batchSize=200,cage_reg=1e-05,delCage=t,learn_beta=t,nEpochs=100/SatSep214:05:062017 cagenet_2_12ctrls_silhouette,augmentAffine=t,batchSize=200,cage_reg=1e-05,delCage=t,learn_beta=t,nEpochs=100,range_fct=none/SunSep317:46:152017 cagenet_2_12ctrls_silhouette,augmentAffine=t,batchSize=200,nEpochs=100/ThuAug3120:45:432017

87
91

vase



tv
-data /home/rana/dan3/silhouettes/rendered/vase -retrain /home/rana/dan3/silhouettes/rendered/vase/checkpoint/cagenet_2_12ctrls_silhouette,augmentAffine=t,batchSize=200,cage_reg=1e-05,delCage=t,learn_beta=t,nEpochs=25,range_fct=none/SunSep313:35:442017/model_25.t7 -batchSize 50 -cage_reg 1e-5 -delCage -evalOnly -learn_beta -range_fct none

tv - res:
/home/rana/dan3/silhouettes/rendered/vase/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=50,cage_reg=1e-05,delCage=t,evalOnly=t,learn_beta=t,range_fct=none/TueSep517:01:312017

tv + m 
-data /home/rana/dan3/silhouettes/rendered/vase -retrain /home/rana/dan3/silhouettes/rendered/vase/checkpoint/cagenet_2_12ctrls_silhouette,augmentAffine=t,batchSize=200,cage_reg=1e-05,delCage=t,learn_beta=t/SunAug2719:59:252017/model_25.t7 -batchSize 50 -cage_reg 1e-5 -delCage -evalOnly -learn_beta

res:
/home/rana/dan3/silhouettes/rendered/vase/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=50,cage_reg=1e-05,delCage=t,evalOnly=t,learn_beta=t/TueSep517:11:372017/qual

/home/rana/dan3/silhouettes/rendered/vase/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=50,evalOnly=t,fullShapeflag=t/MonSep1121:04:482017
/home/rana/data/silhouettes/rendered/vase/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=50,evalOnly=t/MonSep1121:09:362017


/home/rana/dan3/silhouettes/rendered/vase/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=50,cage_reg=1e-05,delCage=t,evalOnly=t,learn_beta=t,range_fct=none/MonSep1120:57:352017
/home/rana/dan3/silhouettes/rendered/vase/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=50,cage_reg=1e-05,delCage=t,evalOnly=t,fullShapeflag=t,learn_beta=t,range_fct=none/MonSep1121:00:532017

/home/rana/dan3/silhouettes/rendered/vase/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=50,cage_reg=1e-05,delCage=t,evalOnly=t,learn_beta=t/TueSep517:11:372017/qual
/home/rana/dan3/silhouettes/rendered/vase/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=50,cage_reg=1e-05,delCage=t,evalOnly=t,fullShapeflag=t,learn_beta=t/MonSep1120:53:532017/qual

45/205/




-data /home/rana/dan3/silhouettes/rendered/vessel -retrain /home/rana/dan3/silhouettes/rendered/vessel/checkpoint/cagenet_2_12ctrls_silhouette,augmentAffine=t,batchSize=200,nEpochs=25/ThuAug3121:47:372017/model_25.t7 -batchSize 30 -evalOnly


-data /home/rana/dan3/silhouettes/rendered/vessel -retrain /home/rana/dan3/silhouettes/rendered/vessel/checkpoint/cagenet_2_12ctrls_silhouette,augmentAffine=t,batchSize=200,cage_reg=1e-05,delCage=t,learn_beta=t,nEpochs=25,range_fct=none/SunSep314:15:212017/model_25.t7 -batchSize 30 -evalOnly -cage_reg 1e-5 -delCage -learn_beta -range_fct none

-data /home/rana/dan3/silhouettes/rendered/vessel -retrain /home/rana/dan3/silhouettes/rendered/vessel/checkpoint/cagenet_2_12ctrls_silhouette,augmentAffine=t,batchSize=200,cage_reg=1e-05,delCage=t,learn_beta=t,nEpochs=25/FriSep105:19:092017/model_25.t7 -batchSize 30 -evalOnly -cage_reg 1e-5 -delCage -learn_beta

\end{comment}\begin{figure}[h]
\newcommand{\ablfig}{1.6}
\newcommand{\ablfigsp}{0.2}
\setlength\tabcolsep{1pt} % default value: 6pt
\centering
\begin{tabular}{c c c c c}

\includegraphics[height=\ablfig cm]{figures/ablation/vase/sample530_source.png} &
\includegraphics[height=\ablfig cm]{figures/ablation/vase/sample530_target.png} &
\includegraphics[height=\ablfig cm]{figures/ablation/vase/no_reg/sample530_targetPred.png} &
\includegraphics[height=\ablfig cm]{figures/ablation/vase/tv/sample530_targetPred.png} &
\includegraphics[height=\ablfig cm]{figures/ablation/vase/tv_mono/sample530_targetPred.png} \\

\includegraphics[height=\ablfig cm]{figures/ablation/vase/sample530_source.png} &
\includegraphics[height=\ablfig cm]{figures/ablation/vase/full/sample530_target.png}  &
\includegraphics[height=\ablfig cm]{figures/ablation/vase/full/no_reg/sample530_targetPred.png} &
\includegraphics[height=\ablfig cm]{figures/ablation/vase/full/tv/sample530_targetPred.png} &
\includegraphics[height=\ablfig cm]{figures/ablation/vase/full/tv_mono/sample530_targetPred.png} \\[\ablfigsp cm]
\hline
\vspace{\ablfigsp cm}

\includegraphics[height=\ablfig cm]{figures/ablation/guitar/sample087_source.png} &
\includegraphics[height=\ablfig cm]{figures/ablation/guitar/sample087_target.png}  &
\includegraphics[height=\ablfig cm]{figures/ablation/guitar/no_reg/sample087_targetPred.png} &
\includegraphics[height=\ablfig cm]{figures/ablation/guitar/tv/sample087_targetPred.png} &
\includegraphics[height=\ablfig cm]{figures/ablation/guitar/tv_mono/sample087_targetPred.png} \\[\ablfigsp cm]
\hline
\vspace{\ablfigsp cm}


\includegraphics[height=\ablfig cm]{figures/ablation/vessel2/sample466_source.png} &
\includegraphics[height=\ablfig cm]{figures/ablation/vessel2/sample466_target.png} &
\includegraphics[height=\ablfig cm]{figures/ablation/vessel2/no_reg/sample466_targetPred.png} &
\includegraphics[height=\ablfig cm]{figures/ablation/vessel2/tv/sample466_targetPred.png} &
\includegraphics[height=\ablfig cm]{figures/ablation/vessel2/tv_mono/sample466_targetPred.png} \\


\includegraphics[height=\ablfig cm]{figures/ablation/vessel2/sample466_source.png} &
\includegraphics[height=\ablfig cm]{figures/ablation/vessel2/full/sample466_target.png} &
\includegraphics[height=\ablfig cm]{figures/ablation/vessel2/full/no_reg/sample466_targetPred.png} &
\includegraphics[height=\ablfig cm]{figures/ablation/vessel2/full/tv/sample466_targetPred.png} &
\includegraphics[height=\ablfig cm]{figures/ablation/vessel2/full/tv_mono/sample466_targetPred.png} \\
 
source &
target &
none &
TV &
TV \& M \\
\end{tabular}
\caption{Ablation study visual examples for increasing levels of FFD regularization: no regularization, TV regularization and TV regularization with axial monotonicity. Without regularization, \ourmethod{} is not partial agnostic: the deformation is disparate for the full and partial version of the same target.  }
\label{fig:visual_reg_ablation}
\end{figure}\begin{comment}

\includegraphics[height=\ablfig cm]{figures/ablation/vessel/sample466_source.png} &
\includegraphics[height=\ablfig cm]{figures/ablation/vessel/sample466_target.png} &
\includegraphics[height=\ablfig cm]{figures/ablation/vessel/no_reg/sample466_targetPred.png} &
\includegraphics[height=\ablfig cm]{figures/ablation/vessel/tv/sample466_targetPred.png} &
\includegraphics[height=\ablfig cm]{figures/ablation/vessel/tv_mono/sample466_targetPred.png} \\


\includegraphics[height=\ablfig cm]{figures/ablation/vessel/sample466_source.png} &
\includegraphics[height=\ablfig cm]{figures/ablation/vessel/full/sample466_target.png} &
\includegraphics[height=\ablfig cm]{figures/ablation/vessel/full/no_reg/sample466_targetPred.png} &
\includegraphics[height=\ablfig cm]{figures/ablation/vessel/full/tv/sample466_targetPred.png} &
\includegraphics[height=\ablfig cm]{figures/ablation/vessel/full/tv_mono/sample466_targetPred.png} \\

\end{comment}\begin{comment}
python benchmark/runBatches.py -data bottle -itr 1 5 10 15 20 25 30 35 40 45 50 -epochNumber 25 -retrain cagenet_2_12ctrls_silhouette,augmentAffine=t,batchSize=200,cage_reg=1e-05,delCage=t,learn_beta=t,nEpochs=50/MonSep408:33:452017 cagenet_2_12ctrls_silhouette,augmentAffine=t,batchSize=200,nEpochs=50/MonSep400:59:212017 cagenet_2_12ctrls_silhouette,augmentAffine=t,batchSize=200,cage_reg=1e-05,delCage=t,learn_beta=t,nEpochs=50,range_fct=none/SunSep317:31:382017

python benchmark/runBatches.py -data airplane -itr 1 5 10 15 20 25 30 35 40 45 50 -epochNumber 25 -retrain cagenet_2_12ctrls_silhouette,augmentAffine=t,batchSize=200,cage_reg=1e-05,delCage=t,learn_beta=t,nEpochs=50,range_fct=none/SunSep318:23:092017 cagenet_2_12ctrls_silhouette,augmentAffine=t,batchSize=200,nEpochs=50/MonSep402:04:122017 cagenet_2_12ctrls_silhouette,augmentAffine=t,batchSize=200,cage_reg=1e-05,delCage=t,learn_beta=t,nEpochs=50/MonSep410:04:092017

python benchmark/runBatches.py -data vessel -itr 25  -itr 1 5 10 15 20 25 -epochNumber 25 -retrain cagenet_2_12ctrls_silhouette,augmentAffine=t,batchSize=200,cage_reg=1e-05,delCage=t,learn_beta=t,nEpochs=25/FriSep105:19:092017 cagenet_2_12ctrls_silhouette,augmentAffine=t,batchSize=200,cage_reg=1e-05,delCage=t,learn_beta=t,nEpochs=25,range_fct=none/SunSep314:15:212017 cagenet_2_12ctrls_silhouette,augmentAffine=t,batchSize=200,nEpochs=25/ThuAug3121:47:372017

\end{comment}\begin{figure}

\newcommand{\sfig}{3.1}
\setlength\tabcolsep{2pt}
\begin{tabular}{ c c }
\includegraphics[height=\sfig cm]{figures/ablation/acc.png} &
\includegraphics[height=\sfig cm]{figures/ablation/sm.png} \\
\end{tabular} 
\caption{Quantitative ablation study for no regularization (none: red), total variation regularization (tv: blue) and total variation regularization plus axial monotonicity (tvm: green). Left: increasing gains in accuracy do not necessarily correspond to plausible warp fields (visuals in Figure~\ref{fig:visual_reg_ablation}). Right: observe that without regularization the unsmoothness error metric (Equation~\ref{eq:TV_reg_id}) increases as training advances, however the smoothness remains constant with regularization present.}
\label{fig:itr_reg_plot}
\end{figure}\begin{comment}
\centering
	\newcommand{\mysize}{3.1}
    
    \jsubfig{\includegraphics[height=\mysize cm]{{figures/ablation/acc.png}}}{}%
	\hfill \jsubfig{\includegraphics[height=\mysize cm]{figures/ablation/sm.png}}{}
   
   
    
    
    
\end{comment}%Right: observe that without regularization the unsmoothness error metric (Equation~\ref{eq:TV_reg_id}) increases as training advances, while shape alignment accuracy deceitfully increases: left. When TV regularization is present, the change in unsmoothness error is nominal, while the increasing gains in accuracy remain steadfast.\begin{comment}


--airplane---
2x2
/home/rana/data/silhouettes/rendered/airplane/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=50,cage_reg=2.5e-06,csz=2,delCage=t,evalOnly=t,fullShapeflag=t,learn_beta=t/ThuSep715:12:512017/qual

4x4

/home/rana/data/silhouettes/rendered/airplane/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=50,cage_reg=5e-06,csz=4,delCage=t,evalOnly=t,fullShapeflag=t,learn_beta=t/ThuSep715:17:022017

6x6
/home/rana/data/silhouettes/rendered/airplane/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=50,cage_reg=7.5e-06,csz=6,delCage=t,evalOnly=t,fullShapeflag=t,learn_beta=t/ThuSep715:19:562017

7x7
/home/rana/data/silhouettes/rendered/airplane/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=50,cage_reg=8.75e-06,csz=7,delCage=t,evalOnly=t,fullShapeflag=t,learn_beta=t/ThuSep715:22:582017


cagenet_2_12ctrls_silhouette,batchSize=50,cage_reg=1e-05,delCage=t,evalOnly=t,fullShapeflag=t,learn_beta=t/ThuSep715:26:002017

23x23
silhouettes/rendered/airplane/checkpoint/alignet_2d_16,batchSize=200,cage_reg=1e-07,csz=23,delCage=t,learn_beta=t/WedJan1723:40:522018



273 / 279
54

--vase---

2x2 cage
------------
/home/rana/data/silhouettes/rendered/vase/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=50,cage_reg=2.5e-06,csz=2,delCage=t,evalOnly=t,learn_beta=t/TueSep521:06:222017/qual

full:
/home/rana/data/silhouettes/rendered/vase/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=50,cage_reg=2.5e-06,csz=2,delCage=t,evalOnly=t,fullShapeflag=t,learn_beta=t/TueSep522:16:252017

4x4
------------
/home/rana/data/silhouettes/rendered/vase/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=50,cage_reg=5e-06,csz=4,delCage=t,evalOnly=t,learn_beta=t/TueSep521:21:192017

/home/rana/data/silhouettes/rendered/vase/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=50,cage_reg=5e-06,csz=4,delCage=t,evalOnly=t,fullShapeflag=t,learn_beta=t/TueSep522:19:192017

6x6
----
/home/rana/data/silhouettes/rendered/vase/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=50,cage_reg=7.5e-06,csz=6,delCage=t,evalOnly=t,learn_beta=t/TueSep521:30:092017

full:
/home/rana/data/silhouettes/rendered/vase/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=50,cage_reg=7.5e-06,csz=6,delCage=t,evalOnly=t,fullShapeflag=t,learn_beta=t/TueSep522:22:142017

7x7 cage
----------
/home/rana/data/silhouettes/rendered/vase/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=50,cage_reg=8.75e-06,csz=7,delCage=t,evalOnly=t,learn_beta=t/TueSep521:15:332017/qual

full:
/home/rana/data/silhouettes/rendered/vase/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=50,cage_reg=8.75e-06,csz=7,delCage=t,evalOnly=t,fullShapeflag=t,learn_beta=t/TueSep522:25:092017

8x8 cage
-----------
/home/rana/data/silhouettes/rendered/vase/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=50,cage_reg=1e-05,delCage=t,evalOnly=t,learn_beta=t/TueSep517:11:372017

full:
/home/rana/data/silhouettes/rendered/vase/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=50,cage_reg=1e-05,delCage=t,evalOnly=t,fullShapeflag=t,learn_beta=t/TueSep522:28:03201


321...
354
408
466 / 470
528
533
640????? / 644
841
843
847
860!
867


\end{comment}\begin{figure*}
\newcommand{\gridresfig}{2.1}

\begin{tabular}{c c c c c c c}

\includegraphics[height=\gridresfig cm]{figures/grid_res/airplane/sample273_source.png} &
\includegraphics[height=\gridresfig cm]{figures/grid_res/airplane/2x2/sample273_targetPred.png} &
\includegraphics[height=\gridresfig cm]{figures/grid_res/airplane/4x4/sample273_targetPred.png} &
\includegraphics[height=\gridresfig cm]{figures/grid_res/airplane/6x6/sample273_targetPred.png} &
\includegraphics[height=\gridresfig cm]{figures/grid_res/airplane/8x8/sample273_targetPred.png} &
\includegraphics[height=\gridresfig cm]{figures/grid_res/airplane/23x23/sample273_targetPred.png} &
\includegraphics[height=\gridresfig cm]{figures/grid_res/airplane/sample273_target.png} \\


source &
$2\times2$ &
$4\times4$ &
$6\times6$ &
$8\times8$ &
\new{$23\times23$} &
target \\

\end{tabular}
\caption{Grid resolution expressiveness. We experiment with increasing grid resolutions to explore the change in warping flexibility. Results were obtained using \ourmethod{} under the exact same conditions, apart from grid size. \new{Observe that a smaller grid resolution facilitates a smooth but not expressive enough deformation, while a higher resolution yields  a deformation that is more expressive \newa{but contains redundant deformations.}}}
\label{fig:grid_res}

\end{figure*}\begin{comment}
\hfill \jsubfig{\includegraphics[height=\gridresfig cm]{figures/grid_res/airplane/sample273_source.png}} %{}%
\hfill \jsubfig{\includegraphics[height=\gridresfig cm]{figures/grid_res/airplane/2x2/sample273_targetPred.png}} %{}%
\hfill \jsubfig{\includegraphics[height=\gridresfig cm]{figures/grid_res/airplane/4x4/sample273_targetPred.png}} %{}%
\hfill \jsubfig{\includegraphics[height=\gridresfig cm]{figures/grid_res/airplane/6x6/sample273_targetPred.png}} %{}%
\hfill \jsubfig{\includegraphics[height=\gridresfig cm]{figures/grid_res/airplane/7x7/sample273_targetPred.png}} %{}%
\hfill \jsubfig{\includegraphics[height=\gridresfig cm]{figures/grid_res/airplane/8x8/sample273_targetPred.png}} %{}%
\hfill \jsubfig{\includegraphics[height=\gridresfig cm]{figures/grid_res/airplane/sample273_target.png}}{}%

 \hspace*{0.3cm} \hfill \jsubfig{\includegraphics[height=\gridresfig cm]{figures/grid_res/vase/sample860_source.png}}{source}%
\hfill \jsubfig{\includegraphics[height=\gridresfig cm]{figures/grid_res/vase/2x2/sample860_targetPred.png}}{$2\times2$}%
\hfill \jsubfig{\includegraphics[height=\gridresfig cm]{figures/grid_res/vase/4x4/sample860_targetPred.png}}{$4\times4$}%
\hfill \jsubfig{\includegraphics[height=\gridresfig cm]{figures/grid_res/vase/6x6/sample860_targetPred.png}}{$6\times6$}%
\hfill \jsubfig{\includegraphics[height=\gridresfig cm]{figures/grid_res/vase/7x7/sample860_targetPred.png}}{$7\times7$}%
\hfill \jsubfig{\includegraphics[height=\gridresfig cm]{figures/grid_res/vase/8x8/sample860_targetPred.png}}{$8\times8$}%
\hfill \jsubfig{\includegraphics[height=\gridresfig cm]{figures/grid_res/vase/sample860_target.png}}{target}%
\end{comment}\subsection{\ourmethod{} Generalization}%\addcomment{Does this section need to be re-worded?}
In this section we demonstrate the generalization capabilities of \ourmethod{}. We show test results on novel classes with similar characteristics, demonstrating \ourmethod{} does not over-fit or memorize the training data. We deform a novel class source to a target from the class trained on in Figure~\ref{fig:novel_source_class}. For example, consider the top section of Figure~\ref{fig:novel_source_class}, where \ourmethod{} trained on vases is used to align a pear and a pumpkin source shapes (novel yet geometrically similar classes) to vases from the test set, resulting in \emph{pear-esque} and \emph{pumpkin-esque} vases. 
We demonstrate further generalization abilities in Figure~\ref{fig:novel_novel_class}, where we utilize a network trained on a certain class to perform alignments from source to target such that \emph{both} belong to a novel class. 
Figure~\ref{fig:sweeping_partialT} features similarly predicted \ourmethod{} alignments for the same source and underlying target, with differing missing element locations. This missing part location indifference simultaneously demonstrates missing part generalization and partial agnosticism of \ourmethod{}. Similarly, in Figure~\ref{fig:stress_test}, we present a \emph{stress test} on the same source and underlying target by computing alignments for an increasing amount of missing data in the target. Observe that the estimated mappings are initially predicted consistently, and even when significant portions of the data are missing, the estimated mappings remain plausible.

\begin{figure*}[h]
\newcommand{\nnfigc}{1.7}
\setlength\tabcolsep{1pt} % default value: 6pt
\begin{tabular}{ c c c c c c c c c c}
 & 
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/pears_vases/sample003_target.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/pears_vases/sample006_target.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/pears_vases/sample008_target.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/pears_vases/sample019_target.png} &
 & 
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/pumpkin_vases/sample002_target.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/pumpkin_vases/sample004_target.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/pumpkin_vases/sample011_target.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/pumpkin_vases/sample012_target.png} \\
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/pears_vases/sample001_source.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/pears_vases/sample003_targetPred.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/pears_vases/sample006_targetPred.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/pears_vases/sample008_targetPred.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/pears_vases/sample019_targetPred.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/pumpkin_vases/sample001_source.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/pumpkin_vases/sample002_targetPred.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/pumpkin_vases/sample004_targetPred.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/pumpkin_vases/sample011_targetPred.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/pumpkin_vases/sample012_targetPred.png} \\
\hline
 & 
\includegraphics[width=\nnfigc cm]{figures/novel_class/violins_guitars/sample003_target.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class/violins_guitars/sample004_target.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class/violins_guitars/sample027_target.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class/violins_guitars/sample030_target.png} &
 & 
\includegraphics[width=\nnfigc cm]{figures/novel_class/broom_guitar/sample008_target.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class/broom_guitar/sample022_target.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class/broom_guitar/sample023_target.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class/broom_guitar/sample027_target.png} \\
\includegraphics[width=\nnfigc cm]{figures/novel_class/violins_guitars/sample001_source.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class/violins_guitars/sample003_targetPred.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class/violins_guitars/sample004_targetPred.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class/violins_guitars/sample027_targetPred.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class/violins_guitars/sample030_targetPred.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class/broom_guitar/sample008_source.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class/broom_guitar/sample008_targetPred.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class/broom_guitar/sample022_targetPred.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class/broom_guitar/sample023_targetPred.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class/broom_guitar/sample027_targetPred.png} \\
\hline
 & 
\includegraphics[width=\nnfigc cm]{figures/novel_class/kites_airplanes/sample005_target.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class/kites_airplanes/sample012_target.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class/kites_airplanes/sample018_target.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class/kites_airplanes/sample002_target.png} &
 & 
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/bat_airplanes/sample012_target.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/bat_airplanes/sample022_target.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/bat_airplanes/sample027_target.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/bat_airplanes/sample029_target.png} \\
\includegraphics[width=\nnfigc cm]{figures/novel_class/kites_airplanes/sample001_source.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class/kites_airplanes/sample005_targetPred.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class/kites_airplanes/sample012_targetPred.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class/kites_airplanes/sample018_targetPred.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class/kites_airplanes/sample002_targetPred.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/bat_airplanes/sample001_source.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/bat_airplanes/sample012_targetPred.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/bat_airplanes/sample022_targetPred.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/bat_airplanes/sample027_targetPred.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/bat_airplanes/sample029_targetPred.png} \\
\end{tabular}
\caption{
Qualitative results of applying \ourmethod{} to untrained classes. We utilize \ourmethod{} trained on three different classes to deform source shapes from novel classes (untrained) that share geometric commonalities with one of the trained classes. In these examples, we deform a \emph{pear} and a \emph{pumpkin} into \emph{vases} (top), a \emph{violin} and a \emph{broom} into \emph{guitars} (middle) and a \emph{kite} and a \emph{bat} into \emph{airplanes} (bottom).}
\label{fig:novel_source_class}
\end{figure*}\begin{comment}
    
    
    
\hline
 & 
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/rocket_airplanes/sample002_target.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/rocket_airplanes/sample026_target.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/rocket_airplanes/sample027_target.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/rocket_airplanes/sample015_target.png} &
 & 
\includegraphics[width=\nnfigc cm]{figures/novel_class/kites_airplanes/sample005_target.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class/kites_airplanes/sample012_target.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class/kites_airplanes/sample018_target.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class/kites_airplanes/sample002_target.png} \\
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/rocket_airplanes/sample001_source.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/rocket_airplanes/sample002_targetPred.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/rocket_airplanes/sample026_targetPred.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/rocket_airplanes/sample027_targetPred.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class_2/rocket_airplanes/sample015_targetPred.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class/kites_airplanes/sample001_source.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class/kites_airplanes/sample005_targetPred.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class/kites_airplanes/sample012_targetPred.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class/kites_airplanes/sample018_targetPred.png} &
\includegraphics[width=\nnfigc cm]{figures/novel_class/kites_airplanes/sample002_targetPred.png} \\
    
    
    
    \hline
 & 
\includegraphics[width=\nnfigc cm]{} &
\includegraphics[width=\nnfigc cm]{} &
\includegraphics[width=\nnfigc cm]{} &
\includegraphics[width=\nnfigc cm]{} &
 & 
\includegraphics[width=\nnfigc cm]{} &
\includegraphics[width=\nnfigc cm]{} &
\includegraphics[width=\nnfigc cm]{} &
\includegraphics[width=\nnfigc cm]{} \\
\includegraphics[width=\nnfigc cm]{} &
\includegraphics[width=\nnfigc cm]{} &
\includegraphics[width=\nnfigc cm]{} &
\includegraphics[width=\nnfigc cm]{} &
\includegraphics[width=\nnfigc cm]{} &
\includegraphics[width=\nnfigc cm]{} &
\includegraphics[width=\nnfigc cm]{} &
\includegraphics[width=\nnfigc cm]{} &
\includegraphics[width=\nnfigc cm]{} &
\includegraphics[width=\nnfigc cm]{} \\

\end{comment}\begin{comment}

-data /home/rana/data/silhouettes/rendered/airplane -retrain /home/rana/dan3/silhouettes/rendered/airplane/checkpoint/cagenet_2_12ctrls_silhouette,augmentAffine=t,batchSize=200,cage_reg=1e-05,delCage=t,learn_beta=t,mpgPart=t,nEpochs=100/MonSep1119:22:222017/model_100.t7 -batchSize 30 -cage_reg 1e-5 -delCage -evalOnly -learn_beta -mpgPart -src_name ./../test_cache/texture_tests/bird9.h5 -tgt_name ./../test_cache/texture_tests/bird6.h5Airport City

\end{comment}\begin{figure}[h]
\newcommand{\nnfig}{2}
\setlength\tabcolsep{1pt}
\begin{tabular}{c c c}


\includegraphics[width=\nnfig cm]{figures/novel_novel_class/bird1bird3/sample001_source.png} &
\includegraphics[width=\nnfig cm]{figures/novel_novel_class/bird1bird3/sample001_target.png} &
\includegraphics[width=\nnfig cm]{figures/novel_novel_class/bird1bird3/sample001_targetPred.png} \\

\hline

\includegraphics[width=\nnfig cm]{figures/novel_novel_class/pear6pear7/sample002_source.png} &
\includegraphics[width=\nnfig cm]{figures/novel_novel_class/pear6pear7/sample002_target.png} &
\includegraphics[width=\nnfig cm]{figures/novel_novel_class/pear6pear7/sample002_targetPred.png} \\

\includegraphics[width=\nnfig cm]{figures/novel_novel_class/pumpkin1pumpkin3/sample001_source.png} &
\includegraphics[width=\nnfig cm]{figures/novel_novel_class/pumpkin1pumpkin3/sample001_target.png} &
\includegraphics[width=\nnfig cm]{figures/novel_novel_class/pumpkin1pumpkin3/sample001_targetPred.png} \\

novel source &
novel target &
warped \\

\end{tabular}



    
\caption{
Qualitative results of applying \ourmethod{} to untrained classes. 
Both source and target shapes belong to novel classes that were not trained on, yet they share geometric commonalities with the trained class.
The top row features a \emph{bird}-to-\emph{bird} deformation, performed using \ourmethod{} trained on \emph{airplanes}, while in the bottom two rows, \emph{pear}-to-\emph{pear} and \emph{pumpkin}-to-\emph{pumpkin} deformations are carried out using a \emph{vase}-trained network.}
\label{fig:novel_novel_class}
\end{figure}\begin{comment}
    \jsubfig{\includegraphics[width=\nnfig cm]{{figures/novel_novel_class/bat1bat2/sample002_source.png}}}{}%
    \hfill \jsubfig{\includegraphics[width=\nnfig cm]{figures/novel_novel_class/bat1bat2/sample002_target.png}}{}%
    \hfill \jsubfig{\includegraphics[width=\nnfig cm]{figures/novel_novel_class/bat1bat2/sample002_targetPred.png}}{}%
    
    
    \jsubfig{\includegraphics[width=\nnfig cm]{figures/novel_novel_class/bird1bird3/sample001_source.png}}{}%
    \hfill \jsubfig{\includegraphics[width=\nnfig cm]{figures/novel_novel_class/bird1bird3/sample001_target.png}}{}%
    \hfill \jsubfig{\includegraphics[width=\nnfig cm]{figures/novel_novel_class/bird1bird3/sample001_targetPred.png}}{}%
    \\
	\jsubfig{\includegraphics[width=8cm]{{figures/novel_class/line.png}}}{}% 
	\\
    \jsubfig{\includegraphics[width=\nnfig cm]{figures/novel_novel_class/pear6pear7/sample002_source.png}}{}%
    \hfill \jsubfig{\includegraphics[width=\nnfig cm]{figures/novel_novel_class/pear6pear7/sample002_target.png}}{}%
    \hfill \jsubfig{\includegraphics[width=\nnfig cm]{figures/novel_novel_class/pear6pear7/sample002_targetPred.png}}{}%
    
    \jsubfig{\includegraphics[width=\nnfig cm]{figures/novel_novel_class/pumpkin1pumpkin3/sample001_source.png}}{}%
    \hfill \jsubfig{\includegraphics[width=\nnfig cm]{figures/novel_novel_class/pumpkin1pumpkin3/sample001_target.png}}{}%
    \hfill \jsubfig{\includegraphics[width=\nnfig cm]{figures/novel_novel_class/pumpkin1pumpkin3/sample001_targetPred.png}}{}%
\end{comment}\begin{comment}

airplane
th main.lua -data /home/rana/data/silhouettes/rendered/airplane -retrain /home/rana/data/silhouettes/rendered/airplane/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=200,cage_reg=1e-05,delCage=t,learn_beta=t/ThuJul2701:53:192017/model_49.t7 -batchSize 50 -cage_reg 1e-5 -delCage -evalOnly -learn_beta -src_tgt_id 1.2 

vase
th main.lua -data /home/rana/dan3/silhouettes/rendered/vase -retrain /home/rana/dan3/silhouettes/rendered/vase/checkpoint/cagenet_2_12ctrls_silhouette,augmentAffine=t,batchSize=200,cage_reg=1e-05,delCage=t,learn_beta=t/SunAug2719:59:252017/model_99.t7 -batchSize 50 -cage_reg 1e-5 -delCage -evalOnly -learn_beta -src_tgt_id 3.2

captial H
th main.lua -data /home/rana/dan3/silhouettes/rendered/fonts_1 -retrain /home/rana/dan3/silhouettes/rendered/fonts_1/checkpoint/cagenet_2_12ctrls_silhouette,LR=0.0005,batchSize=200,cage_reg=1e-05,delCage=t,learn_beta=t,viewNumber=34/MonJul3117:14:062017/model_84.t7 -batchSize 50 -cage_reg 1e-5 -delCage -evalOnly -learn_beta -viewNumber 34 -src_tgt_id 2.20

\end{comment}\begin{figure}
\newcommand{\sfig}{1.6}
\setlength\tabcolsep{1pt}
\begin{tabular}{ c c c c c}


 &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/airplane/sample011_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/airplane/sample001_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/airplane/sample003_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/airplane/sample008_target.png} \\

\includegraphics[width=\sfig cm]{figures/sweeping_partial/airplane/sample001_source.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/airplane/sample011_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/airplane/sample001_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/airplane/sample003_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/airplane/sample008_targetPred.png} \\


 &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/vase/sample016_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/vase/sample001_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/vase/sample004_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/vase/sample009_target.png} \\

\includegraphics[width=\sfig cm]{figures/sweeping_partial/vase/sample001_source.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/vase/sample016_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/vase/sample001_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/vase/sample004_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/vase/sample009_targetPred.png} \\


 &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/capital_H_by/sample026_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/capital_H_by/sample002_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/capital_H_by/sample012_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/capital_H_by/sample025_target.png} \\

\includegraphics[width=\sfig cm]{figures/sweeping_partial/capital_H_by/sample001_source.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/capital_H_by/sample026_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/capital_H_by/sample002_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/capital_H_by/sample012_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/capital_H_by/sample025_targetPred.png} \\



\end{tabular}
\caption{Qualitative demonstration of network generalization and partial-agnostic behavior. Displayed are examples from the test set for three different classes (top: \emph{airplane}, middle: \emph{vase}, bottom: \emph{uppercase H}). The warp computed by \ourmethod{} on a particular source and different partialities of the same target results in a visually consistent deformed shape. }
\label{fig:sweeping_partialT}
\end{figure}\begin{comment}
 &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/airplane/sample011_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/airplane/sample001_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/airplane/sample002_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/airplane/sample003_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/airplane/sample004_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/airplane/sample005_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/airplane/sample006_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/airplane/sample007_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/airplane/sample008_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/airplane/sample010_target.png} \\

\includegraphics[width=\sfig cm]{figures/sweeping_partial/airplane/sample001_source.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/airplane/sample011_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/airplane/sample001_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/airplane/sample002_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/airplane/sample003_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/airplane/sample004_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/airplane/sample005_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/airplane/sample006_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/airplane/sample007_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/airplane/sample008_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/airplane/sample010_targetPred.png} \\


 &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/vase/sample016_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/vase/sample001_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/vase/sample004_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/vase/sample005_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/vase/sample006_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/vase/sample009_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/vase/sample010_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/vase/sample011_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/vase/sample013_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/vase/sample014_target.png} \\

\includegraphics[width=\sfig cm]{figures/sweeping_partial/vase/sample001_source.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/vase/sample016_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/vase/sample001_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/vase/sample004_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/vase/sample005_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/vase/sample006_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/vase/sample009_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/vase/sample010_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/vase/sample011_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/vase/sample013_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/vase/sample014_targetPred.png} \\


 &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/capital_H_by/sample026_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/capital_H_by/sample002_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/capital_H_by/sample005_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/capital_H_by/sample007_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/capital_H_by/sample010_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/capital_H_by/sample012_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/capital_H_by/sample014_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/capital_H_by/sample015_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/capital_H_by/sample021_target.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/capital_H_by/sample025_target.png} \\

\includegraphics[width=\sfig cm]{figures/sweeping_partial/capital_H_by/sample001_source.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/capital_H_by/sample026_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/capital_H_by/sample002_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/capital_H_by/sample005_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/capital_H_by/sample007_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/capital_H_by/sample010_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/capital_H_by/sample012_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/capital_H_by/sample014_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/capital_H_by/sample015_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/capital_H_by/sample021_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/sweeping_partial/capital_H_by/sample025_targetPred.png} \\
\end{comment}\begin{comment}


    \jsubfig{}{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/capital_H_by/sample026_target.png}} %{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/capital_H_by/sample002_target.png}} %{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/capital_H_by/sample005_target.png}} %{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/capital_H_by/sample007_target.png}} %{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/capital_H_by/sample010_target.png}} %{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/capital_H_by/sample012_target.png}} %{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/capital_H_by/sample014_target.png}} %{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/capital_H_by/sample015_target.png}} %{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/capital_H_by/sample021_target.png}} %{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/capital_H_by/sample025_target.png}}{}%
    
    \jsubfig{\includegraphics[height=1.6cm]{{figures/sweeping_partial/capital_H_by/sample001_source.png}}}{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/capital_H_by/sample026_targetPred.png}}%{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/capital_H_by/sample002_targetPred.png}} %{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/capital_H_by/sample005_targetPred.png}} %{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/capital_H_by/sample007_targetPred.png}} %{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/capital_H_by/sample010_targetPred.png}} %{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/capital_H_by/sample012_targetPred.png}} %{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/capital_H_by/sample014_targetPred.png}} %{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/capital_H_by/sample015_targetPred.png}} %{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/capital_H_by/sample021_targetPred.png}} %{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/capital_H_by/sample025_targetPred.png}}{}%

    \jsubfig{}{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/airplane/sample011_target.png}} %{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/airplane/sample001_target.png}} %{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/airplane/sample002_target.png}} %{(c) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/airplane/sample003_target.png}} %{(d) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/airplane/sample004_target.png}} %{(d) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/airplane/sample005_target.png}} %{(d) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/airplane/sample006_target.png}} %{(d) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/airplane/sample007_target.png}} %{(d) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/airplane/sample008_target.png}} %{(d) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/airplane/sample010_target.png}}{ }%
    
    \jsubfig{\includegraphics[height=1.6cm]{{figures/sweeping_partial/airplane/sample001_source.png}}}{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/airplane/sample011_targetPred.png}} %{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/airplane/sample001_targetPred.png}} %{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/airplane/sample002_targetPred.png}} %{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/airplane/sample003_targetPred.png}} %{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/airplane/sample004_targetPred.png}} %{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/airplane/sample005_targetPred.png}} %{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/airplane/sample006_targetPred.png}} %{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/airplane/sample007_targetPred.png}} %{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/airplane/sample008_targetPred.png}} %{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/airplane/sample010_targetPred.png}}{ }%
\\
\jsubfig{\includegraphics[width=16cm]{{figures/novel_class/line.png}}}{}% 
\\
    \jsubfig{}{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/vase/sample016_target.png}} %{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/vase/sample001_target.png}} %{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/vase/sample004_target.png}} %{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/vase/sample005_target.png}} %{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/vase/sample006_target.png}} %{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/vase/sample009_target.png}} %{ }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/vase/sample010_target.png}} %{ }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/vase/sample011_target.png}} %{ }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/vase/sample013_target.png}} %{ }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/vase/sample014_target.png}}{ }%
    
    \jsubfig{\includegraphics[height=1.6cm]{{figures/sweeping_partial/vase/sample001_source.png}}}{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/vase/sample016_targetPred.png}} %{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/vase/sample001_targetPred.png}} %{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/vase/sample004_targetPred.png}} %{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/vase/sample005_targetPred.png}} %{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/vase/sample006_targetPred.png}} %{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/vase/sample009_targetPred.png}} %{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/vase/sample010_targetPred.png}} %{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/vase/sample011_targetPred.png}} %{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/vase/sample013_targetPred.png}} %{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/sweeping_partial/vase/sample014_targetPred.png}}{ }%
\\
\jsubfig{\includegraphics[width=16cm]{{figures/novel_class/line.png}}}{}% 
\\


\end{comment}\begin{comment}

-- airplane ---
th main.lua -data /home/rana/data/silhouettes/rendered/airplane -retrain /home/rana/data/silhouettes/rendered/airplane/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=200,cage_reg=1e-05,delCage=t,learn_beta=t/ThuJul2701:53:192017/model_49.t7 -batchSize 50 -evalOnly -learn_beta -nstress_test 12 -src_tgt_id 12.18 -cage_reg 1e-05 -delCage

-- vase ---
th main.lua -data /home/rana/data/silhouettes/rendered/vase -retrain /home/rana/dan3/silhouettes/rendered/vase/checkpoint/cagenet_2_12ctrls_silhouette,augmentAffine=t,batchSize=200,cage_reg=1e-05,delCage=t,learn_beta=t/SunAug2719:59:252017/model_99.t7 -batchSize 50 -evalOnly -learn_beta -nstress_test 12 -src_tgt_id 6.7 -cage_reg 1e-05 -delCage

th main.lua -data /home/rana/data/silhouettes/rendered/vase -retrain /home/rana/dan3/silhouettes/rendered/vase/checkpoint/cagenet_2_12ctrls_silhouette,augmentAffine=t,batchSize=200,cage_reg=1e-05,delCage=t,learn_beta=t/SunAug2719:59:252017/model_99.t7 -batchSize 50 -evalOnly -learn_beta -nstress_test 12 -src_tgt_id 2.13 -cage_reg 1e-05 -delCage

\end{comment}\begin{figure}
\newcommand{\sfig}{1.6}
\setlength\tabcolsep{1pt}
\begin{tabular}{ c c c c c}



 &
\includegraphics[width=\sfig cm]{figures/stress_test/airplane/sample003_target.png} &
\includegraphics[width=\sfig cm]{figures/stress_test/airplane/sample005_target.png} &
\includegraphics[width=\sfig cm]{figures/stress_test/airplane/sample006_target.png} &
\includegraphics[width=\sfig cm]{figures/stress_test/airplane/sample007_target.png} \\

\includegraphics[width=\sfig cm]{figures/stress_test/airplane/sample001_source.png} &
\includegraphics[width=\sfig cm]{figures/stress_test/airplane/sample003_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/stress_test/airplane/sample005_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/stress_test/airplane/sample006_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/stress_test/airplane/sample007_targetPred.png} \\

 &
\includegraphics[width=\sfig cm]{figures/stress_test/vase212/sample002_target.png} &
\includegraphics[width=\sfig cm]{figures/stress_test/vase212/sample005_target.png} &
\includegraphics[width=\sfig cm]{figures/stress_test/vase212/sample006_target.png} &
\includegraphics[width=\sfig cm]{figures/stress_test/vase212/sample009_target.png} \\

\includegraphics[width=\sfig cm]{figures/stress_test/vase212/sample001_source.png} &
\includegraphics[width=\sfig cm]{figures/stress_test/vase212/sample002_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/stress_test/vase212/sample005_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/stress_test/vase212/sample006_targetPred.png} &
\includegraphics[width=\sfig cm]{figures/stress_test/vase212/sample009_targetPred.png} \\

\end{tabular}
\caption{Qualitative stress test for FFD computation under increasing percentage of missing shape data.}
\label{fig:stress_test}
\end{figure}\begin{comment}

  \jsubfig{}{} \hspace*{1.6cm} %
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/airplane/sample013_target.png}} \;%{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/airplane/sample001_target.png}} \;%{(b) }%
	\hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/airplane/sample002_target.png}} \;%{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/airplane/sample003_target.png}} \;%{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/airplane/sample004_target.png}} \;%{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/airplane/sample005_target.png}} \;%{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/airplane/sample006_target.png}} \;%{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/airplane/sample007_target.png}} \;%{(b) }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/airplane/sample008_target.png}}{} \;%
    
    \jsubfig{\includegraphics[height=1.6cm]{{figures/stress_test/airplane/sample001_source.png}}}{} \;
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/airplane/sample013_targetPred.png}} \; %{ }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/airplane/sample001_targetPred.png}} \; %{ }%
	\hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/airplane/sample002_targetPred.png}} \; %{ }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/airplane/sample003_targetPred.png}} \; %{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/airplane/sample004_targetPred.png}} \; %{ }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/airplane/sample005_targetPred.png}} \; %{ }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/airplane/sample006_targetPred.png}} \; %{ }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/airplane/sample007_targetPred.png}} \; %{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/airplane/sample008_targetPred.png}}{} \;%
    
    \jsubfig{}{} \hspace*{1.6cm} %
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase212/sample013_target.png}} \;%{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase212/sample002_target.png}} \;%{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase212/sample004_target.png}} \;%{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase212/sample005_target.png}} \;%{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase212/sample006_target.png}} \;%{ }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase212/sample007_target.png}} \;%{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase212/sample008_target.png}} \;%{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase212/sample009_target.png}} \;{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase212/sample010_target.png}} \;{}%
    
   \jsubfig{\includegraphics[height=1.6cm]{{figures/stress_test/vase212/sample001_source.png}}}{} \;
   \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase212/sample013_targetPred.png}} \; %{}%
   \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase212/sample002_targetPred.png}} \; %{}%
   \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase212/sample004_targetPred.png}} \; %{}%
   \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase212/sample005_targetPred.png}} \; %{}%
   \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase212/sample006_targetPred.png}} \; %{}%
   \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase212/sample007_targetPred.png}} \; %{}%
   \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase212/sample008_targetPred.png}} \; %{}%
   \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase212/sample009_targetPred.png}} \; %{}%
   \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase212/sample010_targetPred.png}} \; %{}%
\end{comment}\begin{comment}
    \jsubfig{}{} \hspace*{1.6cm} %
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase/sample013_target.png}} \;%{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase/sample002_target.png}} \;%{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase/sample003_target.png}} \;%{ }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase/sample004_target.png}} \;%{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase/sample005_target.png}} \;%{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase/sample006_target.png}} \;%{ }%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase/sample007_target.png}} \;%{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase/sample009_target.png}} \;%{}%
    \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase/sample010_target.png}} \;{}%
    
   \jsubfig{\includegraphics[height=1.6cm]{{figures/stress_test/vase/sample001_source.png}}}{} \;
   \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase/sample013_targetPred.png}} \; %{}%
   \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase/sample002_targetPred.png}} \; %{}%
   \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase/sample003_targetPred.png}} \; %{}%
   \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase/sample004_targetPred.png}} \; %{}%
   \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase/sample005_targetPred.png}} \; %{}%
   \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase/sample006_targetPred.png}} \; %{}%
   \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase/sample007_targetPred.png}} \; %{}%
   \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase/sample009_targetPred.png}} \; %{}%
   \hfill \jsubfig{\includegraphics[height=1.6cm]{figures/stress_test/vase/sample010_targetPred.png}} \; %{}%
   
   \\
\jsubfig{\includegraphics[width=16cm]{{figures/novel_class/line.png}}}{}% 
\\
\end{comment}%\input{figures/complex_part/figure.tex}\subsection{\rev{Applications in 3D}}\label{sec:threed}\rev{
Throughout the paper we have shown alignment results obtained by applying \ourmethod{} on shapes within the 2D domain, and demonstrated the integrity of the estimated warps and their readiness to support alternative post-process 2D applications.
In this section, we shift our focus to the 3D domain, and present additional applications following a simple extension of \ourmethod{} to 3D. We train on volumetric data (of resolution 32x32x32) and replace the convolution, pooling, integrator and FFD layer modules with their 3D counterparts. We learn to regress an FFD grid of resolution 7x7x7 (1029 DoF), which is applied directly on the source mesh vertices. \new{Note that neither the grid nor voxelization resolution affect the quality of the output mesh, since the deformation is applied on the input high-resolution mesh.} See Figure~\ref{fig:threeD_grid} for a visualization of our 3D grid.
}\begin{figure}
\newcommand{\tfig}{4}
\setlength\tabcolsep{2pt}
\begin{tabular}{c c}


\adjincludegraphics[width=\tfig cm,trim={{.2\width} {.1\height} {.2\width} {.1\height}},clip]{figures/threeD/grid/0196_0097_fwd_orig_1_5.png} &
\adjincludegraphics[width=\tfig cm,trim={{.2\width} {.1\height} {.2\width} {.1\height}},clip]{figures/threeD/grid/0196_0097_fwd_warped_1_5.png} \\



\end{tabular}    
\caption{
\rev{3D deformation grid. The source shape is placed within the uniform 7x7x7 grid (left). A deformation on the grid induces the shape to warp as shown on the right.}}
\label{fig:threeD_grid}
\end{figure}\begin{comment}
\multirow{1}{*}[6em]{\rotatebox[origin=c]{90}{source}} &
\includegraphics[width=\tfig cm]{figures/limitations/temp.png} &
\includegraphics[width=\tfig cm]{figures/limitations/temp.png} \\
\multirow{1}{*}[6em]{\rotatebox[origin=c]{90}{deformed}} &
\includegraphics[width=\tfig cm]{figures/limitations/temp.png} &
\includegraphics[width=\tfig cm]{figures/limitations/temp.png} \\
 &
angle 1 &
angle 2\\
\end{comment}\rev{
Volumetric CNNs typically operate on voxelized shape representations where not only is the surface of the shape marked as occupied, but also the inner volume. 
Generating a voxelized representation of surface-based shapes represented, \emph{e.g.}, as meshes, is straight-forward.
The same, however, cannot be said for shapes represented, \emph{e.g.}, as point clouds, which sparsely sample the surface of an object.
To this end, we train \ourmethod{} on (hollow) volumetric surfaces automatically generated from 3D meshes.}\rev{
During training, we draw pairs of source and target shapes from the pool of voxelized surfaces, and remove random portions from the target surface. As in 2D, we train \ourmethod{} by comparing the overlap between the warped source surface and the expected (complete) target surface. While the 3D grid is estimated for the voxelized surface, we can simply apply it on the original mesh.
Below we show applications of 3D warping on point-cloud registration and segmentation transfer.}\paragraph{\textbf{\rev{Point Cloud Registration.}}}\rev{
In many applications ranging from VR and AR to scene understanding, it is desirable to align a mesh (containing semantic information) to a partial point cloud obtained by a scanning device.}\rev{
Figure~\ref{fig:threeD} features several examples of alignments performed between 3D meshes and point clouds using our method \revb{(quantitative results in Table~\ref{tab:quant3d})}.
\rev{Recall that our 3D network processes pairs of voxelized surfaces as input, such that the source is complete, and the target is generated from a complete surface by removing parts at random. At test time, however, we are given a target shape represented as a point cloud, requiring conversion into our network format.  
To this end, we simply discretize the continuous coordinates of the point cloud and form a sparse surface occupancy grid. 
Despite not having trained on such sparsely sampled surfaces, \ourmethod{} generalizes well and is able to produce plausible deformations.}}%\revb{See Table~\ref{tab:quant3d} for quantitative evaluation }\begin{figure}
\newcommand{\sfig}{2.7}
\newcommand{\vfig}{2}
\setlength\tabcolsep{0pt}
\begin{tabular}{ c c c}

\begin{comment}


\adjincludegraphics[width=\vfig cm,trim={{.28\width} {.15\height} {.28\width} {.15\height}},clip]{figures/threeD/vase_res/green/0025.png} &
\adjincludegraphics[width=\vfig cm,trim={{.28\width} {.15\height} {.28\width} {.15\height}},clip]{figures/threeD/vase_res/green/0025_0236_pc.png} &
\adjincludegraphics[width=\vfig cm,trim={{.28\width} {.15\height} {.28\width} {.15\height}},clip]{figures/threeD/vase_res/green/0025_0236_fwd.png} \\
\end{comment}

\adjincludegraphics[width=\vfig cm,trim={{.28\width} {.15\height} {.28\width} {.15\height}},clip]{figures/threeD/vase_res/green/0097.png} &
\adjincludegraphics[width=\vfig cm,trim={{.28\width} {.15\height} {.28\width} {.15\height}},clip]{figures/threeD/vase_res/green/0097_0268_pc.png} &
\adjincludegraphics[width=\vfig cm,trim={{.28\width} {.15\height} {.28\width} {.15\height}},clip]{figures/threeD/vase_res/green/0097_0268_fwd.png} \\

\adjincludegraphics[width=\vfig cm,trim={{.28\width} {.15\height} {.28\width} {.15\height}},clip]{figures/threeD/vase_res/green/0180.png} &
\adjincludegraphics[width=\vfig cm,trim={{.28\width} {.15\height} {.28\width} {.15\height}},clip]{figures/threeD/vase_res/green/0180_0280_pc.png} &
\adjincludegraphics[width=\vfig cm,trim={{.28\width} {.15\height} {.28\width} {.15\height}},clip]{figures/threeD/vase_res/green/0180_0280_fwd.png} \\

\adjincludegraphics[width=\sfig cm,trim={{.33\width} {.33\height} {.25\width} {.3\height}},clip]{figures/threeD/airplane/113.png} &
\adjincludegraphics[width=\sfig cm,trim={{.33\width} {.33\height} {.25\width} {.3\height}},clip]{figures/threeD/airplane/113_3238_pc.png} &
\adjincludegraphics[width=\sfig cm,trim={{.33\width} {.33\height} {.25\width} {.3\height}},clip]{figures/threeD/airplane/113_3238_fwd.png} \\

\adjincludegraphics[width=\sfig cm,trim={{.33\width} {.33\height} {.25\width} {.3\height}},clip]{figures/threeD/airplane/1878.png} &
\adjincludegraphics[width=\sfig cm,trim={{.33\width} {.33\height} {.25\width} {.3\height}},clip]{figures/threeD/airplane/1878_2586_pc.png} &
\adjincludegraphics[width=\sfig cm,trim={{.33\width} {.33\height} {.25\width} {.3\height}},clip]{figures/threeD/airplane/1878_2586_fwd.png} \\

\adjincludegraphics[width=\sfig cm,trim={{.33\width} {.33\height} {.25\width} {.3\height}},clip]{figures/threeD/airplane/2427.png} &
\adjincludegraphics[width=\sfig cm,trim={{.33\width} {.33\height} {.25\width} {.3\height}},clip]{figures/threeD/airplane/2427_113_pc.png} &
\adjincludegraphics[width=\sfig cm,trim={{.33\width} {.33\height} {.25\width} {.3\height}},clip]{figures/threeD/airplane/2427_113_fwd.png} \\

\adjincludegraphics[width=\vfig cm,trim={{.28\width} {.14\height} {.28\width} {.09\height}},clip]{figures/threeD/chair/2339.png} &
\adjincludegraphics[width=\vfig cm,trim={{.28\width} {.14\height} {.28\width} {.09\height}},clip]{figures/threeD/chair/2339_3648_pc.png} &
\adjincludegraphics[width=\vfig cm,trim={{.28\width} {.14\height} {.28\width} {.09\height}},clip]{figures/threeD/chair/2339_3648_fwd.png} \\

source &
partial target &
aligned \\

\end{tabular}    
\caption{
\rev{Aligning a high quality 3D mesh to a point cloud. \ourmethod{} estimates the 7x7x7 grid deformation between a 3D source mesh (represented as a complete voxelized surface) and a point cloud (sparse voxelized surface), which we apply on the vertices of the source mesh to obtain the aligned version (last column). 
\ourmethod{}, trained on complete surfaces with segments removed at random, generalizes to sparse surfaces at test time.}}
\label{fig:threeD}
\end{figure}\begin{comment}

/home/rana/dan3_local/localdata/silhouettes/rendered/vase_3D/checkpoint/ffdnet_3d_b,LR=0.0001,cage_reg=1e-06,csz=7,delCage=t,evalOnly=t,hdf5Loader=dataset_rendered_3d,learn_beta=t,pWinSz1=3,pWinSz2=6,pc_tgtsurface=pc/pc_3000,saveObj=t,surface=t,swap=t,winsize_file=winSz_3_6/SatJan620:47:472018/warped_objs/matlab

500:
/home/rana/dan3_local/localdata/silhouettes/rendered/vase_3D/checkpoint/ffdnet_3d_b,LR=0.0001,cage_reg=1e-06,csz=7,delCage=t,evalOnly=t,hdf5Loader=dataset_rendered_3d,learn_beta=t,pWinSz1=3,pWinSz2=6,pc_tgtsurface=pc/pc_500,saveObj=t,surface=t,swap=t,winsize_file=winSz_3_6/SatJan623:38:512018/warped_objs

/home/rana/dan5/localdata/silhouettes/rendered/airplane_3D/checkpoint/ffdnet_3d_b,LR=0.0001,cage_reg=1e-06,csz=7,delCage=t,evalOnly=t,hdf5Loader=dataset_rendered_3d,learn_beta=t,pWinSz1=3,pWinSz2=6,pc_tgtsurface=pc/pc_500,saveObj=t,surface=t,swap=t,winsize_file=winSz_3_6/MonJan2216:01:222018/warped_objs/matlab


\end{comment}\begin{table}
\begin{center}
\begin{tabular}{c||c || c} 
\revb{samples} & \revb{500} & \revb{3000} \\
\hline
\revb{IOU} & \revb{53} & \revb{57} \\ [0.2ex]
\end{tabular}
\end{center}
\caption{
\revb{Quantitative results for 3D point cloud registration on the \emph{airplane} class. We compute the average IOU between deformed source and full target given target in point cloud form for both 500 and 3000 point samples. Note that the IOU is computed on the full voxelized representation of the shapes generated from their mesh forms, where the applied warp field is computed using the point cloud representation of the target.}}
\label{tab:quant3d}
\end{table}%Given pairs of source and target shapes we compute the \td{? measure} between the deformed source and full target. \paragraph{\textbf{\rev{Segmentation Transfer.}}}\rev{Semantically segmenting a shape into its constituent parts is an important first step for a wide range of applications in shape analysis and synthesis. A popular approach to solve this task is the transfer of an existing high-quality segmentation from one shape to another. Naturally, some form of correspondence between the two shapes must be given or computed in order to support the transference.}\rev{To further demonstrate the integrity of \ourmethod{} warps, we show results on segmentation transfer in 3D. Using the same trained model that was used in the previous application (Figure~\ref{fig:threeD}), we estimate the deformation between a segmented (source) shape and an unsegmented (target) shape. Given the underlying correspondence determined by the alignment, we transfer the segmentation labels from the source shape faces to their corresponding faces on the target shape, and perform a simple graph cut optimization to obtain a complete and smooth result (see Figure~\ref{fig:threeD_seg}).}\begin{figure}[h!]
\newcommand{\tfig}{2.2}
\newcommand{\vfig}{1.7}
\setlength\tabcolsep{10pt}
\begin{tabular}{c c c}


\adjincludegraphics[width=\vfig cm,trim={{.3\width} {.18\height} {.3\width} {.15\height}},clip]{figures/threeD/seg/vase/0036.png} &
\adjincludegraphics[width=\vfig cm,trim={{.3\width} {.18\height} {.3\width} {.15\height}},clip]{figures/threeD/seg/vase/0036_0275_id.png} &
\adjincludegraphics[width=\vfig cm,trim={{.3\width} {.18\height} {.3\width} {.15\height}},clip]{figures/threeD/seg/vase/0036_0275.png} \\







\adjincludegraphics[width=\tfig cm,trim={{.3\width} {.3\height} {.25\width} {.3\height}},clip]{figures/threeD/seg/airplane/006.png} &
\adjincludegraphics[width=\tfig cm,trim={{.3\width} {.3\height} {.25\width} {.3\height}},clip]{figures/threeD/seg/airplane/006_1949_id.png} &
\adjincludegraphics[width=\tfig cm,trim={{.3\width} {.3\height} {.25\width} {.3\height}},clip]{figures/threeD/seg/airplane/006_1949.png} \\

\adjincludegraphics[width=\tfig cm,trim={{.3\width} {.3\height} {.25\width} {.3\height}},clip]{figures/threeD/seg/airplane/604.png} &
\adjincludegraphics[width=\tfig cm,trim={{.3\width} {.3\height} {.25\width} {.3\height}},clip]{figures/threeD/seg/airplane/604_602_id.png} &
\adjincludegraphics[width=\tfig cm,trim={{.3\width} {.3\height} {.25\width} {.3\height}},clip]{figures/threeD/seg/airplane/604_602.png} \\

\adjincludegraphics[width=\tfig cm,trim={{.3\width} {.3\height} {.25\width} {.3\height}},clip]{figures/threeD/seg/airplane/002.png} &
\adjincludegraphics[width=\tfig cm,trim={{.3\width} {.3\height} {.25\width} {.3\height}},clip]{figures/threeD/seg/airplane/002_3280_id.png} &
\adjincludegraphics[width=\tfig cm,trim={{.3\width} {.3\height} {.25\width} {.3\height}},clip]{figures/threeD/seg/airplane/002_3280.png} \\



\adjincludegraphics[width=\vfig cm,trim={{.3\width} {.18\height} {.3\width} {.13\height}},clip]{figures/threeD/seg/chair/0054.png} &
\adjincludegraphics[width=\vfig cm,trim={{.3\width} {.18\height} {.3\width} {.12\height}},clip]{figures/threeD/seg/chair/0054_0296_id.png} &
\adjincludegraphics[width=\vfig cm,trim={{.3\width} {.18\height} {.3\width} {.12\height}},clip]{figures/threeD/seg/chair/0054_0296.png} \\

\adjincludegraphics[width=\vfig cm,trim={{.2\width} {.1\height} {.2\width} {.1\height}},clip]{figures/threeD/seg/chair/0069.png} &
\adjincludegraphics[width=\vfig cm,trim={{.2\width} {.1\height} {.2\width} {.1\height}},clip]{figures/threeD/seg/chair/0069_0385_id.png} &
\adjincludegraphics[width=\vfig cm,trim={{.2\width} {.1\height} {.2\width} {.1\height}},clip]{figures/threeD/seg/chair/0069_0385.png} \\

segmented &
naive transfer &
\ourmethod{} transfer \\

\end{tabular}    
\caption{\rev{\ourmethod{} results on 3D segmentation transfer (test set). The labels in the source shape (left) transferred to the target shapes using the correspondence generated with \ourmethod{} (right). The estimated alignment is used in combination with a simple graph cut optimization as a post process. In the middle column, we compare to a baseline transfer, where the identity warp is used to guide the correspondence.}} 
\label{fig:threeD_seg}
\end{figure}%. Using only graph cut optimization without alignment is shown in the center column (naive).\begin{comment}


\paragraph{\textbf{Texture Transfer.}}
\addcomment{Many sophisticated 3D models can contain hours of hand-crafted and tuned parameters for rendering (\addnote{AM I RIGHT? I DONT KNOW... texture coordinates, uv maps, lighting, etc.}). We show, again, that \ourmethod{} is able to preserve these intracies in Figure~\ref{fig:threeD_txt}.}

\begin{figure}
\newcommand{\tfig}{3}
\setlength\tabcolsep{1pt}
\begin{tabular}{c c c}


\includegraphics[width=\tfig cm]{figures/threeD/texture/orig.png} &
\includegraphics[width=\tfig cm]{figures/limitations/temp.png} &
\includegraphics[width=\tfig cm]{figures/threeD/texture/warped_0036c.png} \\

source &
target &
deformed \\

\end{tabular}    
\caption{\addcomment{Using \ourmethod{} to deform a 3D model containing hand-crafted render parameters.}}
\label{fig:threeD_txt}
\end{figure}
\end{comment}\subsection{Robustness}\paragraph{\textbf{Symmetry.}}
Bilateral symmetry is characteristic of many man-made as well as natural shapes. Accordingly, most of our datasets are composed either entirely, or almost entirely, of bilaterally symmetric shapes. We have shown that our system is able to recover such symmetries even when a target is missing portions in a pattern that impairs the original symmetry of the shape. However, symmetry is a strong cue that often assists in tasks of reconstruction, and it is possible that \ourmethod{} is more easily trained on a dataset that is inherently symmetric.

We are therefore particularly interested in examining the performance of \ourmethod{} on datasets that are asymmetric in nature. Examples for alignments performed on rendered asymmetric letters ("C" and "L") are presented in Figure~\ref{fig:asym}. We observe the satisfactory recovery of the structure of the shapes, suggesting that our approach does not require symmetry as a leading prior to learn to align.

\begin{comment}

-data /home/rana/data/silhouettes/rendered/fonts_1 -retrain /home/rana/data/silhouettes/rendered/fonts_1/checkpoint/cagenet_2_12ctrls_silhouette,augmentAffine=t,batchSize=200,cage_reg=1e-05,delCage=t,learn_beta=t,mpgPart=t,nEpochs=100,viewNumber=29/WedSep1313:36:162017/model_100.t7 -evalOnly -batchSize 30 -cage_reg 1e-05 -delCage -learn_beta -mpgPart -viewNumber 29 -mpgSRng 1.3..2

/home/rana/data/silhouettes/rendered/fonts_1/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=30,cage_reg=1e-05,delCage=t,evalOnly=t,learn_beta=t,mpgPart=t,mpgSRng=1.3..2,viewNumber=29/MonSep1817:13:042017/qual/subplots


-data /home/rana/data/silhouettes/rendered/fonts_1 -retrain /home/rana/local_data/silhouettes/rendered/fonts_1/checkpoint/cagenet_2_12ctrls_silhouette,augmentAffine=t,batchSize=200,cage_reg=1e-05,delCage=t,learn_beta=t,nEpochs=100,viewNumber=38/MonSep1819:00:012017/model_21.t7 -evalOnly -batchSize 100 -cage_reg 1e-05 -delCage -learn_beta -viewNumber 38


\end{comment}\begin{figure}[h]
\newcommand{\asym}{1.7}
\setlength\tabcolsep{1pt}
\begin{tabular}{c c c c}


 &
\includegraphics[width=\asym cm]{figures/asym/C/sample154_target.png} &
\includegraphics[width=\asym cm]{figures/asym/C/sample156_target.png} &
\includegraphics[width=\asym cm]{figures/asym/C/sample172_target.png} \\

\includegraphics[width=\asym cm]{figures/asym/C/sample154_source.png} &
\includegraphics[width=\asym cm]{figures/asym/C/sample154_targetPred.png} &
\includegraphics[width=\asym cm]{figures/asym/C/sample156_targetPred.png} &
\includegraphics[width=\asym cm]{figures/asym/C/sample172_targetPred.png} \\



 &
\includegraphics[width=\asym cm]{figures/asym/L/sample088_target.png} &
\includegraphics[width=\asym cm]{figures/asym/L/sample103_target.png} &
\includegraphics[width=\asym cm]{figures/asym/L/sample112_target.png} \\

\includegraphics[width=\asym cm]{figures/asym/L/sample088_source.png} &
\includegraphics[width=\asym cm]{figures/asym/L/sample088_targetPred.png} &
\includegraphics[width=\asym cm]{figures/asym/L/sample103_targetPred.png} &
\includegraphics[width=\asym cm]{figures/asym/L/sample112_targetPred.png} \\

\end{tabular}
    

    

    
    
\caption{Examples of alignment of asymmetrical shapes.}
\label{fig:asym}
\end{figure}\paragraph{\textbf{Orientation.}}\addcomment{Our baseline experimental setting assumes that input datasets are consistently pre-oriented, an assumption that holds for many readily available shape datasets. Where this assumption does not hold, consistent orientation methods can be employed (e.g. hierarchical alignment~\cite{shapenet2015}). Even so, we investigate the ability of our system to handle orientation errors.}%While this assumption holds for many readily available shape datasets, we investigate the abilities of our system to handle orientation errors.\begin{comment}
We augment our dataset by introducing rotations uniformly sampled from the range 
$\lbrack -15^{\circ}, 15^{\circ}\rbrack$, and train \ourmethod{} as previously described. Figure~\ref{fig:rots} contains qualitative results of alignments obtained both with the original pre-oriented airplane dataset (third column), as well as with added orientation perturbations (fourth column). The results demonstrate the potential of the system with regard to orientation robustness.
\begin{figure}
\newcommand{\rotfig}{1.9}
\setlength\tabcolsep{3pt}
\begin{tabular}{c c c c}


\includegraphics[width=\rotfig cm]{figures/rot/airplanes_r/sample141_source.png} &
\includegraphics[width=\rotfig cm]{figures/rot/airplanes_r/sample141_target.png} &
\includegraphics[width=\rotfig cm]{figures/rot/airplanes_str/sample141_targetPred.png} &
\includegraphics[width=\rotfig cm]{figures/rot/airplanes_r/sample141_targetPred.png} \\



source  &
target  &
baseline  &
augmented  \\


\end{tabular}
\caption{Robustness to orientation deviations. 
We align a source shape to a target with a deviated orientation. Results in the third column are obtained with \ourmethod{} trained on the original airplane dataset and show inability to recover target orientation. Conversely, results in the fourth column are obtained with \ourmethod{} trained on the same set with added augmentation of orientation perturbations of up to $\pm 15^{\circ}$, and demonstrate correct orientation recovery and the potential to handle heterogeneously oriented datasets or imperfect orientation pre-processing.}
\label{fig:rots}
\end{figure}\begin{comment}





\includegraphics[width=\rotfig cm]{figures/rot/airplanes_str_str/sample331_source.png}  & 
\includegraphics[width=\rotfig cm]{figures/rot/airplanes_str_str/sample331_target.png} &
\includegraphics[width=\rotfig cm]{figures/rot/airplanes_str_str/sample331_targetPred.png} \\

\includegraphics[width=\rotfig cm]{figures/rot/airplanes_r/sample331_source.png}  & 
\includegraphics[width=\rotfig cm]{figures/rot/airplanes_r/sample331_target.png} &
\includegraphics[width=\rotfig cm]{figures/rot/airplanes_r/sample331_targetPred.png} \\



343 // 341





th main.lua -data /home/rana/data/silhouettes/rendered/airplane -retrain /home/rana/data/silhouettes/rendered/airplane/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=200,cage_reg=1e-05,delCage=t,learn_beta=t/ThuJul2701:53:192017/model_49.t7 -cage_reg 1e-5 -delCage -evalOnly -learn_beta

/home/rana/data/silhouettes/rendered/airplane/checkpoint/cagenet_2_12ctrls_silhouette,cage_reg=1e-05,delCage=t,evalOnly=t,learn_beta=t/TueSep1917:11:452017/qual

th main.lua -data /home/rana/data/silhouettes/rendered/airplane -retrain /home/rana/data/silhouettes/rendered/airplane/checkpoint/cagenet_2_12ctrls_silhouette,RRng=-0.25..0.25,augmentAffine=t,batchSize=200,cage_reg=1e-05,delCage=t,learn_beta=t,nEpochs=150/SunSep1718:50:012017/model_100.t7 -cage_reg 1e-5 -delCage -evalOnly -learn_beta -RRng -0.2..-0.2 -mpgRRng -0.2..-0.2

/home/rana/data/silhouettes/rendered/airplane/checkpoint/cagenet_2_12ctrls_silhouette,RRng=-0.2..-0.2,cage_reg=1e-05,delCage=t,evalOnly=t,learn_beta=t,mpgRRng=-0.2..-0.2/TueSep1917:48:432017/qual


\includegraphics[width=\nnfig cm]{figures/novel_novel_class/bat1bat2/sample002_source.png} 


th main.lua -data /home/rana/data/silhouettes/rendered/airplane -retrain /home/rana/data/silhouettes/rendered/airplane/checkpoint/cagenet_2_12ctrls_silhouette,RRng=-0.25..0.25,augmentAffine=t,batchSize=200,cage_reg=1e-05,delCage=t,learn_beta=t,nEpochs=150/SunSep1718:50:012017/model_100.t7 -evalOnly -batchSize 30 -cage_reg 1e-05 -delCage -learn_beta

/home/rana/data/silhouettes/rendered/airplane/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=30,cage_reg=1e-05,delCage=t,evalOnly=t,learn_beta=t/TueSep1909:22:102017/qual



/home/rana/data/silhouettes/rendered/airplane/checkpoint/cagenet_2_12ctrls_silhouette,RRng=-0.25..-0.25,batchSize=30,cage_reg=1e-05,delCage=t,evalOnly=t,learn_beta=t/TueSep1909:51:012017/qual

/home/rana/data/silhouettes/rendered/airplane/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=30,cage_reg=1e-05,delCage=t,evalOnly=t,learn_beta=t/TueSep1909:22:102017/qual

\end{comment}
\end{comment}\revb{We augment our dataset by introducing rotations uniformly sampled from the range 
$\lbrack -30^{\circ}, 30^{\circ}\rbrack$, and train \ourmethod{} as previously described. Figure~\ref{fig:grot}
shows the qualitative alignment results of our augmented network trained on shapes with arbitrary rotations.}%/home/rana/dan3/silhouettes/rendered/airplane/checkpoint/alignet_2d_globalR,RRng=-0.5..0.5,augmentAffine=t,batchSize=200,cage_reg=1e-05,delCage=t,evalOnly=t,global_r=t,pWinSz1=20,pWinSz2=20,src_tgt_id=28.24,sweepR=t,winsize_file=sz2020/SatApr2118:56:072018%/home/rana/dan3/silhouettes/rendered/airplane/checkpoint/alignet_2d_globalR,RRng=-0.5..0.5,augmentAffine=t,batchSize=200,cage_reg=1e-05,delCage=t,evalOnly=t,global_r=t,pWinSz1=20,pWinSz2=20,src_tgt_id=13.19,sweepR=t,winsize_file=sz2020/SatApr2119:36:032018/qual\begin{figure}[h]
\newcommand{\rotfig}{1.5}
\setlength\tabcolsep{3pt}
\begin{tabular}{c c c c c}

\includegraphics[width=\rotfig cm]{figures/grot/airplane2/sample001_source.png} &
\includegraphics[width=\rotfig cm]{figures/grot/airplane2/sample001_target.png} &
\includegraphics[width=\rotfig cm]{figures/grot/airplane2/sample004_target.png} &
\includegraphics[width=\rotfig cm]{figures/grot/airplane2/sample015_target.png} &
\includegraphics[width=\rotfig cm]{figures/grot/airplane2/sample018_target.png} \\

 &
\includegraphics[width=\rotfig cm]{figures/grot/airplane2/sample001_targetPred.png} &
\includegraphics[width=\rotfig cm]{figures/grot/airplane2/sample004_targetPred.png} &
\includegraphics[width=\rotfig cm]{figures/grot/airplane2/sample015_targetPred.png} &
\includegraphics[width=\rotfig cm]{figures/grot/airplane2/sample018_targetPred.png} \\

\includegraphics[width=\rotfig cm]{figures/grot/airplane3/sample001_source.png} &
\includegraphics[width=\rotfig cm]{figures/grot/airplane3/sample001_target.png} &
\includegraphics[width=\rotfig cm]{figures/grot/airplane3/sample004_target.png} &
\includegraphics[width=\rotfig cm]{figures/grot/airplane3/sample015_target.png} &
\includegraphics[width=\rotfig cm]{figures/grot/airplane3/sample018_target.png} \\

 &
\includegraphics[width=\rotfig cm]{figures/grot/airplane3/sample001_targetPred.png} &
\includegraphics[width=\rotfig cm]{figures/grot/airplane3/sample004_targetPred.png} &
\includegraphics[width=\rotfig cm]{figures/grot/airplane3/sample015_targetPred.png} &
\includegraphics[width=\rotfig cm]{figures/grot/airplane3/sample018_targetPred.png} \\

source &
-29$^{\circ}$ &
-19$^{\circ}$ &
19$^{\circ}$ &
29$^{\circ}$ \\

\end{tabular}
\caption{\revb{Robustness to orientation deviations by adding data augmentation: training on offsets within the range $[-30^{\circ},30^{\circ}]$. Test results shown for orientation offsets of -29$^{\circ}$, -19$^{\circ}$, 19$^{\circ}$ and 29$^{\circ}$ for the same source and target pairs.}}
\label{fig:grot}
\end{figure}\paragraph{\textbf{\rev{Multi Class.}}}\rev{A core strength of \ourmethod{} lies in the higher level understanding of a particular shape category, enabling it to preserve geometric features that are typical to the class of objects at hand. However, we investigate the robustness of \ourmethod{} to handle multiple classes simultaneously. By sampling three different classes during training, using the same architecture, we observe that \ourmethod{} can jointly handle several classes simultaneously (see Figure~\ref{fig:multiclass}).}\rev{To further assess the scalability of \ourmethod{}, we train a single network on an increasing number of classes. We begin with our baseline of single-class training, and add a new class at each step, up to a total of six classes. 
In Figure~\ref{fig:multiclass_robust} we deform a source shape to two different targets from the airplane class, and track the results as the number of trained classes increases. We note the apparent degradation exhibited in the top row, versus the very slight degradation in the bottom row. These results align with our expectations for the general robustness and scalability of the network, accompanied by a little degradation that is a natural outcome of the strain placed on the network as we load more and more classes. To mitigate the effects of degradation, one can consider deepening and widening the network to allow for a larger capacity of shape priors.}\begin{figure}[h!]
\newcommand{\tfig}{1.9}
\setlength\tabcolsep{1pt}
\begin{tabular}{c c c}



\includegraphics[width=\tfig cm]{figures/multiclass/airplane/sample040_source.png} &
\includegraphics[width=\tfig cm]{figures/multiclass/airplane/sample044_target.png} &
\includegraphics[width=\tfig cm]{figures/multiclass/airplane/sample044_targetPred.png} \\




\includegraphics[width=\tfig cm]{figures/multiclass/vase/sample858_source.png} &
\includegraphics[width=\tfig cm]{figures/multiclass/vase/sample858_target.png} &
\includegraphics[width=\tfig cm]{figures/multiclass/vase/sample858_targetPred.png} \\


\includegraphics[width=\tfig cm]{figures/multiclass/vessel/sample615_source.png} &
\includegraphics[width=\tfig cm]{figures/multiclass/vessel/sample615_target.png} &
\includegraphics[width=\tfig cm]{figures/multiclass/vessel/sample615_targetPred.png} \\


\end{tabular}    
\caption{\rev{Results of a single \ourmethod{} network trained on three classes.}}
\label{fig:multiclass}
\end{figure}\begin{comment}

vessel:
/home/rana/dan3_local/localdata/silhouettes/rendered/vessel/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=200,cage_reg=1e-05,evalOnly=t,learn_beta=t,mpgPart=t,mpgSRng=1.1..1.8/FriJan1219:07:512018/qual

vase:
/home/rana/dan3_local/localdata/silhouettes/rendered/vase/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=200,cage_reg=1e-05,evalOnly=t,learn_beta=t,mpgPart=t/FriJan1217:51:062018/qual

airplane:
/home/rana/dan3_local/localdata/silhouettes/rendered/airplane/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=200,cage_reg=1e-05,evalOnly=t,learn_beta=t,mpgPart=t,mpgSRng=1.1..1.8/FriJan1219:17:392018/qual

\end{comment}\begin{figure*}[h]
\newcommand{\tfig}{1.9}
\setlength\tabcolsep{1pt}
\begin{tabular}{c c c c c c c c}



\includegraphics[width=\tfig cm]{figures/multiclass/robust/sample727_source.png} &
\includegraphics[width=\tfig cm]{figures/multiclass/robust/sample727_target.png} &
\includegraphics[width=\tfig cm]{figures/multiclass/robust/class1/sample727_targetPred.png} &
\includegraphics[width=\tfig cm]{figures/multiclass/robust/class2/sample727_targetPred.png} &
\includegraphics[width=\tfig cm]{figures/multiclass/robust/class3/sample727_targetPred.png} &
\includegraphics[width=\tfig cm]{figures/multiclass/robust/class4/sample727_targetPred.png} &
\includegraphics[width=\tfig cm]{figures/multiclass/robust/class5/sample727_targetPred.png} &
\includegraphics[width=\tfig cm]{figures/multiclass/robust/class6/sample727_targetPred.png} \\

\includegraphics[width=\tfig cm]{figures/multiclass/robust/sample730_source.png} &
\includegraphics[width=\tfig cm]{figures/multiclass/robust/sample730_target.png} &
\includegraphics[width=\tfig cm]{figures/multiclass/robust/class1/sample730_targetPred.png} &
\includegraphics[width=\tfig cm]{figures/multiclass/robust/class2/sample730_targetPred.png} &
\includegraphics[width=\tfig cm]{figures/multiclass/robust/class3/sample730_targetPred.png} &
\includegraphics[width=\tfig cm]{figures/multiclass/robust/class4/sample730_targetPred.png} &
\includegraphics[width=\tfig cm]{figures/multiclass/robust/class5/sample730_targetPred.png} &
\includegraphics[width=\tfig cm]{figures/multiclass/robust/class6/sample730_targetPred.png} \\

source &
target &
1 class &
2 classes &
3 classes &
4 classes &
5 classes &
6 classes \\


\end{tabular}    
\caption{\rev{Robustness of a single \ourmethod{} network to handle a large number of classes. Given the same network architecture, we incrementally increase the number of trained classes from one class to six (\emph{airplane, vase, vessel, guitar, uppercase L} and \emph{bottle}). Observe that while there is little to no degradation for some examples (bottom row), there is a more pronounced performance degradation in others (top row).}}
\label{fig:multiclass_robust}
\end{figure*}\begin{comment}
\subsection{\newa{Global Orientation}}
\newa{
In this work, we assume that shapes are globally oriented, and the network learns to non-rigidly align them via FFD deformation. Generally, global alignment can be achieved using STN~\cite{spatialtransformer} as a pre-processing step. Training a network to estimate orientation angles from a single image was shown in~\cite{Su_2015_ICCV}. Similarly,~\cite{richardson2017learning} train a supervised network to estimate the global position of faces.
}
\newa{
Conversely, here we show that the global alignment rotation and the deformation can be learned simultaneously. 
By learning an additional rotation parameter $R$, jointly with the displacement field, the requirement for oriented (\emph{i.e.,} supervised) training data is relaxed.
We use the estimated rotation parameter to generate a warp field $W_R$ which is applied to the FFD transformation (in a differential manner). In this case, Equation~\ref{eq:sampler} is  augmented to:
\begin{equation}
\hat{T} = S \oland (W_c \oland W_R).
\end{equation}
In Figure~\ref{fig:grot} we show the alignment results of our augmented network which was trained on shapes with arbitrary rotations uniformly sampled between $[-30, 30]$ \revb{(quantitative results in Table~\ref{tab:grot})}. Note that unlike the common supervised training of orientation networks, here our training is completely unsupervised (\emph{i.e.,} no ground-truth orientation angles used during training).
}
\begin{table}
\begin{center}
\begin{tabular}{c || c || c|} 
orientation estimation & 2D & 3D \\
\hline
without & \td{x} & \td{x} \\ [0.2ex] 
\hline
with & \td{x} & \td{x} \\ [0.2ex] 
\end{tabular}
\end{center}
\caption{\revb{Quantitative global orientation results on rotated partial \emph{airplanes} test set. Training on rotated shapes \emph{without} (top) and \emph{with} (bottom) adding global orientation estimation. Rotational offsets are uniformly sampled in the range of $[-30^{\circ},30^{\circ}]$ (as shown in Figure~\ref{fig:grot}). Displayed: test accuracy results of \ourmethod{} with and without incorporation of global orientation estimation (bottom and top, respectively).}}
\label{tab:grot}
\end{table}%Incorporating orientation estimation handles rotated shapes better in 2D and 3D.
\end{comment}\subsection{Comparisons}
In this section we compare the results of applying \ourmethod{}, versus existing approaches, on the $870$%In this section we compare \ourmethod{} to existing approaches on the $870$ 
reserved test set pairs for each class (as described in Section~\ref{sec:un_data}). The quantitative results are summarized in Table~\ref{table:eval_iou}, with visual examples shown in Figure~\ref{fig:comparisons_qual}.

Since \addcomment{existing} approaches usually operate on point-sets, we compute the deformations on the contour of the shapes. For comparable results, we apply the contour deformation on the entire image. The high order deformation computed using CPD~\cite{CPD} contains two regularization parameters: first to control deformation field smoothness, and second to control the distortion allowed on the source shape. Setting the CPD deformation field smoothness weight too high restricts the expressiveness allowed in the warp field (similar phenomenon as in \ourmethod{}). However, the parameter which controls the source shape distortion is trickier; a large weight restricts the source shape from conforming to the partial components while also restricting the source shape to conform to the target. We also show results of the shape context descriptor (SC), matching and alignment of~\cite{Belongie2002}. In addition to the global and local structure preserving point set registration (PRGLS) of~\cite{ma2016non}.

To demonstrate the restrictiveness of lower order deformations, we present two different affine transformation results. The first, a RANSAC-based approach, samples three points on the source and target to compute an affine transformation, and selects the best transformation based on source and target voting. \addcomment{Since the aforementioned} approaches do not estimate alignment parameters cognizant of the attributes specific to a particular class, estimating simple parameters such as scale becomes difficult (see last row of Figure~\ref{fig:comparisons_qual}).
However, the limiting DoF in STN is often over-restrictive, which prevents the scale from being accurately recovered (see rows 2 \& 3 Figure~\ref{fig:comparisons_qual}).}\begin{comment}
vessel: 389
\end{comment}\begin{figure}[h]
\newcommand{\compfig}{1}
\setlength\tabcolsep{0.1pt} % default value: 6pt
\begin{tabular}{ c c c c c c c c }
\includegraphics[width=\compfig cm]{figures/comparison/airplane/FFDnet/sample373_source.png} &
\includegraphics[width=\compfig cm]{figures/comparison/airplane/FFDnet/sample373_target.png} &
\includegraphics[width=\compfig cm]{figures/comparison/airplane/CPD/sample373_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/airplane/STN/sample373_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/airplane/RANSAC/sample373_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/airplane/SM/sample373_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/airplane/PRGLS/sample373_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/airplane/FFDnet/sample373_targetPred.png} \\

\includegraphics[width=\compfig cm]{figures/comparison/airplane/FFDnet/sample009_source.png} &
\includegraphics[width=\compfig cm]{figures/comparison/airplane/FFDnet/sample009_target.png} &
\includegraphics[width=\compfig cm]{figures/comparison/airplane/CPD/sample009_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/airplane/STN/sample009_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/airplane/RANSAC/sample009_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/airplane/SM/sample009_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/airplane/PRGLS/sample009_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/airplane/FFDnet/sample009_targetPred.png} \\

\includegraphics[width=\compfig cm]{figures/comparison/airplane/FFDnet/sample505_source.png} &
\includegraphics[width=\compfig cm]{figures/comparison/airplane/FFDnet/sample505_target.png} &
\includegraphics[width=\compfig cm]{figures/comparison/airplane/CPD/sample505_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/airplane/STN/sample505_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/airplane/RANSAC/sample505_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/airplane/SM/sample505_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/airplane/PRGLS/sample505_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/airplane/FFDnet/sample505_targetPred.png} \\

\hline

\includegraphics[width=\compfig cm]{figures/comparison/vessel/FFDnet/sample076_source.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vessel/FFDnet/sample076_target.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vessel/CPD/sample076_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vessel/STN/sample076_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vessel/RANSAC/sample076_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vessel/SM/sample076_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vessel/PRGLS/sample076_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vessel/FFDnet/sample076_targetPred.png} \\

\includegraphics[width=\compfig cm]{figures/comparison/vessel/FFDnet/sample642_source.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vessel/FFDnet/sample642_target.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vessel/CPD/sample642_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vessel/STN/sample642_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vessel/RANSAC/sample642_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vessel/SM/sample642_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vessel/PRGLS/sample642_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vessel/FFDnet/sample642_targetPred.png} \\


\hline

\includegraphics[width=\compfig cm]{figures/comparison/vase/FFDnet/sample344_source.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vase/FFDnet/sample344_target.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vase/CPD/sample344_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vase/STN/sample344_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vase/RANSAC/sample344_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vase/SM/sample344_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vase/PRGLS/sample344_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vase/FFDnet/sample344_targetPred.png} \\

\includegraphics[width=\compfig cm]{figures/comparison/vase/FFDnet/sample539_source.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vase/FFDnet/sample539_target.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vase/CPD/sample539_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vase/STN/sample539_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vase/RANSAC/sample539_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vase/SM/sample539_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vase/PRGLS/sample539_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vase/FFDnet/sample539_targetPred.png} \\

\includegraphics[width=\compfig cm]{figures/comparison/vase/FFDnet/sample058_source.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vase/FFDnet/sample058_target.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vase/CPD/sample058_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vase/STN/sample058_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vase/RANSAC/sample058_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vase/SM/sample058_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vase/PRGLS/sample058_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/comparison/vase/FFDnet/sample058_targetPred.png} \\


source &
target &
CPD &
\rev{STN} &
RANSAC &
SC &
PRGLS &
\ourmethod{} \\
\end{tabular}    
\caption{Visual examples of test set results for: CPD~\cite{CPD}, STN~\cite{spatialtransformer} (in \ourmethod{} framework), affine RANSAC, SC~\cite{Belongie2002}, PRGLS~\cite{ma2016non} and our method. The same set of parameters is used across all examples for \ourmethod{} and existing approaches. Observe that \ourmethod{} is able to faithfully estimate the scale of missing components and infer plausible mappings on the missing pieces.}
\label{fig:comparisons_qual}
\end{figure}\begin{comment}
    
    \jsubfig{\includegraphics[width=\compfig cm]{{figures/comparison/airplane/FFDnet/sample373_source.png}}}{}%
    \hfill \jsubfig{\includegraphics[width=\compfig cm]{figures/comparison/airplane/FFDnet/sample373_target.png}}{}%
    \hfill \jsubfig{\includegraphics[width=\compfig cm]{figures/comparison/airplane/CPD/sample373_targetPred.png}}{}%
    \hfill \jsubfig{\includegraphics[width=\compfig cm]{figures/comparison/airplane/CPD_affine/sample373_targetPred.png}}{}%
    \hfill \jsubfig{\includegraphics[width=\compfig cm]{figures/comparison/airplane/RANSAC/sample373_targetPred.png}}{}%
    \hfill \jsubfig{\includegraphics[width=\compfig cm]{figures/comparison/airplane/SM/sample373_targetPred.png}}{}%
    \hfill \jsubfig{\includegraphics[width=\compfig cm]{figures/comparison/airplane/PRGLS/sample373_targetPred.png}}{}%
    \hfill \jsubfig{\includegraphics[width=\compfig cm]{figures/comparison/airplane/FFDnet/sample373_targetPred.png}}{}%
    
    \jsubfig{\includegraphics[width=\compfig cm]{{figures/comparison/vessel/FFDnet/sample076_source.png}}}{}%
    \hfill \jsubfig{\includegraphics[width=\compfig cm]{figures/comparison/vessel/FFDnet/sample076_target.png}}{}%
    \hfill \jsubfig{\includegraphics[width=\compfig cm]{figures/comparison/vessel/CPD/sample076_targetPred.png}}{}% 1
    \hfill \jsubfig{\includegraphics[width=\compfig cm]{figures/comparison/vessel/CPD_affine/sample076_targetPred.png}}{}% 2
    \hfill \jsubfig{\includegraphics[width=\compfig cm]{figures/comparison/vessel/RANSAC/sample076_targetPred.png}}{}% 3
    \hfill \jsubfig{\includegraphics[width=\compfig cm]{figures/comparison/vessel/SM/sample076_targetPred.png}}{}%  4
    \hfill \jsubfig{\includegraphics[width=\compfig cm]{figures/comparison/vessel/PRGLS/sample076_targetPred.png}}{}%
    \hfill \jsubfig{\includegraphics[width=\compfig cm]{figures/comparison/vessel/FFDnet/sample076_targetPred.png}}{}%
    
    \jsubfig{\includegraphics[width=\compfig cm]{{figures/comparison/vase/FFDnet/sample344_source.png}}}{}%
    \hfill \jsubfig{\includegraphics[width=\compfig cm]{figures/comparison/vase/FFDnet/sample344_target.png}}{}%
    \hfill \jsubfig{\includegraphics[width=\compfig cm]{figures/comparison/vase/CPD/sample344_targetPred.png}}{}%
    \hfill \jsubfig{\includegraphics[width=\compfig cm]{figures/comparison/vase/CPD_affine/sample344_targetPred.png}}{}%
    \hfill \jsubfig{\includegraphics[width=\compfig cm]{figures/comparison/vase/RANSAC/sample344_targetPred.png}}{}%
    \hfill \jsubfig{\includegraphics[width=\compfig cm]{figures/comparison/vase/SM/sample344_targetPred.png}}{}%
    \hfill \jsubfig{\includegraphics[width=\compfig cm]{figures/comparison/vase/PRGLS/sample344_targetPred.png}}{}%
    \hfill \jsubfig{\includegraphics[width=\compfig cm]{figures/comparison/vase/FFDnet/sample344_targetPred.png}}{}%
   
    
    
\jsubfig{{\hspace*{\compfig cm}}}{source}  %
\hfill \jsubfig{{\hspace*{\compfig cm}}}{target}  %
\hfill \jsubfig{{\hspace*{\compfig cm}}}{CPD}%
\hfill \jsubfig{{\hspace*{\compfig cm}}}{affine CPD}  %
\hfill \jsubfig{{\hspace*{\compfig cm}}}{RANSAC}  %
\hfill \jsubfig{{\hspace*{\compfig cm}}}{SC}  %
\hfill \jsubfig{{\hspace*{\compfig cm}}}{PRGLS}  %
\hfill \jsubfig{{\hspace*{\compfig cm}}}{FFDnet}  %

\end{comment}\begin{table}
\center
\csvautotabular{figures/comparison/vase/eval.csv}
\csvautotabular{figures/comparison/airplane/eval.csv}
\csvautotabular{figures/comparison/vessel/eval.csv}
\caption{Quantitative results of average IOU on the entire test set of the \emph{vase}, \emph{airplane} and \emph{vessel} classes. All approaches use a fixed set of parameters for all classes. We compute \ourmethod{} results using TV \& M regularization.}
\label{table:eval_iou}
\end{table}\paragraph{\textbf{\rev{Full Shapes.}}}\rev{While a core asset of \ourmethod{} is its ability to handle partial shapes, we are interested in the performance of our system in a simpler setting of complete shapes, and extend our evaluation accordingly. The quantitative results are summarized in Table~\ref{table:eval_iou_full}, with visual examples shown in Figure~\ref{fig:comparisons_qual_full}.}\rev{\ourmethod{} uses the learned prior to compute full-shape warps that maintain shape semantics post-alignment (see  Figure~\ref{fig:comparisons_qual_full}). Observe that non-data-driven approaches often struggle to preserve shape features, for instance the symmetry of vase handles in rows (a) and (b), or the airplane tail in (e). Lastly, many of the non-data-driven approaches use a regularization term to encourage smooth contour-to-contour deformations which may result in a less accurate alignment (see row (d) and quantitatively in Table~\ref{table:eval_iou_full}).}% Furthermore, the geometrically distinct source and target from row (c) is most plausible using \ourmethod{}.\begin{figure}[h]
\newcommand{\compfig}{1.2}
\setlength\tabcolsep{0.1pt} % default value: 6pt
\begin{tabular}{ c c c c c c c c c }

a &
\includegraphics[width=\compfig cm]{figures/fullshape/vase/alignet/sample298_source.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/vase/alignet/sample298_target.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/vase/CPD/sample298_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/vase/STN/sample298_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/vase/SM/sample298_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/vase/PRGLS/sample298_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/vase/alignet/sample298_targetPred.png} \\

b &
\includegraphics[width=\compfig cm]{figures/fullshape/vase/alignet/sample476_source.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/vase/alignet/sample476_target.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/vase/CPD/sample476_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/vase/STN/sample476_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/vase/SM/sample476_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/vase/PRGLS/sample476_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/vase/alignet/sample476_targetPred.png} \\

c &
\includegraphics[width=\compfig cm]{figures/fullshape/vase/alignet/sample553_source.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/vase/alignet/sample553_target.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/vase/CPD/sample553_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/vase/STN/sample553_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/vase/SM/sample553_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/vase/PRGLS/sample553_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/vase/alignet/sample553_targetPred.png} \\


d &
\includegraphics[width=\compfig cm]{figures/fullshape/airplane/alignet/sample127_source.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/airplane/alignet/sample127_target.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/airplane/CPD/sample127_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/airplane/STN/sample127_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/airplane/SM/sample127_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/airplane/PRGLS/sample127_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/airplane/alignet/sample127_targetPred.png} \\

e &
\includegraphics[width=\compfig cm]{figures/fullshape/airplane/alignet/sample264_source.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/airplane/alignet/sample264_target.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/airplane/CPD/sample264_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/airplane/STN/sample264_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/airplane/SM/sample264_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/airplane/PRGLS/sample264_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/airplane/alignet/sample264_targetPred.png} \\

f &
\includegraphics[width=\compfig cm]{figures/fullshape/airplane/alignet/sample342_source.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/airplane/alignet/sample342_target.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/airplane/CPD/sample342_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/airplane/STN/sample342_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/airplane/SM/sample342_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/airplane/PRGLS/sample342_targetPred.png} &
\includegraphics[width=\compfig cm]{figures/fullshape/airplane/alignet/sample342_targetPred.png} \\

 &
source &
target &
CPD &
STN &
SC &
PRGLS &
\ourmethod{} \\
\end{tabular}    
\caption{\rev{Visual examples of complete shapes (no missing shape data). Test set results for: CPD~\shortcite{CPD}, STN~\shortcite{spatialtransformer} (in \ourmethod{} framework), SC~\shortcite{Belongie2002}, PRGLS~\shortcite{ma2016non} and our method. The same set of parameters is used across all examples for \ourmethod{} and existing approaches. }
\label{fig:comparisons_qual_full}}
\end{figure}\begin{comment}
/home/rana/dan3/silhouettes/rendered/vase/checkpoint/cagenet_2_12ctrls_silhouette,batchSize=50,cage_reg=1e-05,delCage=t,evalOnly=t,fullShapeflag=t,learn_beta=t/SunSep1012:09:472017/qual





/home/rana/dan4/localdata/silhouettes/rendered/airplane/checkpoint/stn_alignet,batchSize=200,cage_reg=1e-05,delCage=t,evalOnly=t,fullShapeflag=t,learn_beta=t/SunJan1419:20:372018/qual
\end{comment}\begin{table}
\center
\csvautotabular{figures/fullshape/vase/eval_full.csv}
\csvautotabular{figures/fullshape/airplane/eval_full.csv}
\csvautotabular{figures/fullshape/vessel/eval_full.csv}
\caption{\rev{Quantitative results on complete (not partial) shapes. Average IOU on the entire test set of the \emph{vase}, \emph{airplane} and \emph{vessel} classes. All approaches use a fixed set of parameters for all classes. We compute \ourmethod{} results using TV \& M regularization.}}
\label{table:eval_iou_full}
\end{table}%To show the restrictiveness of lower order deformations, we show two affine transformation results: affine CPD~\shortcite{CPD} and affine RANSAC. The RANSAC approach simply samples 3 points on the source and target to compute an affine transformation, and selects the best transformation based on source and target voting. Naturally, affine deformations are not expressive enough to capture the fine-grained alignment required in such a task. Despite their lower order nature, they are still unable to accurately estimate scale.% %\addcomment{I THINK WE NEED TO SAY FEW WORDS ON THE OTHER TECHNIQUES... -> will do.}%The missing components of the target lure existing approaches to undesirable local minima.%\addcomment{todo:timing comparisons}%\addcomment{todo:partial agnostic}%\subsection{Additional Results}%\input{figures/novel_class_2/figure.tex}%\input{figures/HoF/figure.tex}\subsection{Limitations}%\ourmethod{} is trained in a class-specific manner, yielding a system that is an expert on the class it has been trained on, such that unseen examples are aligned successfully despite possible missing parts. This data-driven approach, however, naturally carries a limitation in the form of low inter-class generalization; a system that was trained on one class of shapes is hard-pressed to perform well on another (see top of Figure~\ref{fig:limitations}). We have shown, however, that a trained system is transferable between geometrically similar classes, \emph{e.g.}, \ourmethod{} trained on airplanes can be used to align birds to airplanes (Figure~\ref{fig:novel_source_class}), and birds to birds (Figure~\ref{fig:novel_novel_class}).

We note that a grid-based deformation limits the space of transformations that the shape can undergo for alignment.
A finer resolution increases the degrees of freedom and becomes more susceptible to implausible deformations, which we have shown can be mitigated with an appropriate regularization mechanism. 
\revb{However, as can be seen in Figure~\ref{fig:grid_res}, when the degree of deformation is too large compared to the network size, even our regularization term will not suffice.} 
While the proposed approach produces complex deformations even with regularization, we recognize that such a step in itself results in a confinement of the deformation expressiveness.
\rev{The proposed approach cannot handle large differences in topologies.}
Furthermore, our grid-based deformation would not be an appropriate method of transformation for shapes with articulations.
\revb{We have shown how to incorporate an unsupervised global orientation estimation into \ourmethod{} in Figure~\ref{fig:grot}. However, these results suggest that a more comprehensive solution may be necessary to achieve higher orientation and shape alignment quality.}%We have shown that augmenting the training data with rotations results in generalization on orientation offsets (Figure~\ref{fig:rots}). However, these results suggest that a more comprehensive solution may be necessary to achieve higher orientation and shape alignment quality.%A finer resolution extends freedom of expression, but more freedom also leaves more room for error, which we have shown can be mitigated with an appropriate regularization mechanism.% A finer resolution extends freedom of expression, but more freedom also leaves more room for error. We have shown that negative tendencies can be mitigated with an appropriate regularization mechanism, but recognize that such a step in itself results in a confinement of the freedom of expression.%the proposed approach can produce %\input{figures/limitations/topology.tex}%\input{figures/limitations/class_sp_fig.tex}
\section{Conclusion}

Aligning one shape to another with a high order deformation is a fundamentally ill-posed problem, but when missing parts are thrown into the mix, the ambiguity only deepens. Classic alignment techniques are often at a loss under such circumstances, as the missing information can be highly misleading.
To alleviate this, we take advantage of large collections of shapes, and train a system to align pairs of source and potentially partial target shapes. The system learns the space of plausible deformations associated with the dataset, and effectively becomes agnostic to missing parts, without memorization or over-fitting.

Our technique introduces a key contribution in the form of a regularization mechanism that promotes smoothness. We have shown (Figure~\ref{fig:visual_reg_ablation}) that allowing the system to run wild with its free form deformation computations is not only detrimental to structure preservation, which is crucial for post processing (\emph{e.g.}, texture \& segmentation transfer and preserving 3D geometry), but also hinders partial agnosticism. Our regularization relies on a differential grid deformation that naturally supports a total variation penalty encouraging piecewise smooth warps.

The benefits associated with data-driven paradigms are central to the performance of our system. The information contained in the training data assists the network to learn a shape prior that helps preserve the characteristic features of the input set of shapes. Our experimental results show that important attributes  such as part symmetry and scale are well recovered by our system, suggesting that the network has learned the expected shape and form of the underlying class. 
We have shown (Figures~\ref{fig:teaser2},~\ref{fig:teaser},~\ref{fig:novel_source_class}, and~\ref{fig:novel_novel_class})
that our trained system generalizes well to unseen examples from the learned classes or from untrained but similar ones, and that the incorporation of prior knowledge benefits not only a complex setting where the target shape may be substantially partial, but also a simpler setting where the target shape is complete (Figure~\ref{fig:comparisons_qual_full}).


Looking forward, we suggest several directions for further research. 
\new{While our current setting uses a simple loss function, it would be interesting to use more complex loss functions such as the perceptual loss~\cite{johnson2016perceptual} or the earth-mover distance. Another possibility is to incorporate the \textit{curriculum-learning} technique: first start with "easy" training examples (similar topologies and no missing parts), and as the network adapts to handle them, gradually include increasingly harder examples.
Furthermore, extending the network scope to handle both partial source and target shapes \newa{is a challenging problem that may lead to a more generic solution}.}%In our current setting, the shapes in each dataset are mostly consistently placed. We have seen that small deviations from the canonical placement are not contradictory to a successful alignment, but extending our system to operate under larger rotational offsets will lead to a more general solution. \rev{Our warping grid is set to $8 \times 8$ for 2D shapes, and $7 \times 7 \times 7$ for 3D shapes. We deliberately opt for a milder degree of deformation, which inherently fosters smooth deformations. 
We note that to achieve a tighter alignment, it is possible to apply an ICP-based approach as a post-process.}%Another interesting direction would be to investigate dealing with occlusions or superfluous foreground pixels. 
Finally, the incorporation of part awareness may potentially support a more semantic alignment, where advanced operations such as part addition and removal could help to maintain the structural integrity of the target shape.

\section{Acknowledgements}
We thank the anonymous reviewers for their helpful comments. This research was supported by the Israel Science Foundation as part of the ISF-NSFC joint program grant number (2217/15, 2472/17), and partially supported by ISF grant 2366/16. This research was partially supported by ERC-StG grant no. 757497 (SPADE).






\bibliographystyle{ACM-Reference-Format}
\bibliography{correspondence}
\end{document}
