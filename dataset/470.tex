\documentclass{article}
\date{}
\pdfoutput=1

\newcommand{\etal}{\textit{et al}.}
\newcommand{\ie}{\textit{i}.\textit{e}.}
\newcommand{\eg}{\textit{e}.\textit{g}.}
\newcommand{\etc}{\textit{etc}}

\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amssymb,amsmath,amsthm}
\usepackage{subcaption}
\usepackage{fullpage}
\usepackage{floatrow}
\usepackage{graphicx}

\title{Perceptual Adversarial Networks for \\ Image-to-Image Transformation}

\author{Chaoyue Wang\textsuperscript{$\dagger$}, Chang Xu\textsuperscript{$\ddagger$}, Chaohui Wang\textsuperscript{$\star$}, Dacheng Tao\textsuperscript{$\ddagger$}\\
\textsuperscript{$\dagger$}Centre for Artificial Intelligence, School of Software, \\
University of Technology Sydney, Australia\\
\textsuperscript{$\ddagger$}UBTech Sydney AI Institute, School of IT, FEIT, \\
The University of Sydney, Australia
\\
\textsuperscript{$\star$}Universit\'{e} Paris-Est, LIGM (UMR 8049), CNRS, ENPC,\\
 ESIEE Paris, UPEM, Marne-la-Vall\'{e}e, France\\
chaoyue.wang@student.uts.edu.au, c.xu@sydney.edu.au\\
 chaohui.wang@u-pem.fr, dacheng.tao@sydney.edu.au
}

\begin{document}
\maketitle

\begin{abstract}
In this paper, we propose a principled Perceptual Adversarial Networks (PAN) for image-to-image transformation tasks. Unlike existing application-specific algorithms, PAN provides a generic framework of learning mapping relationship between paired images (Fig.~\ref{Demo}), such as mapping a rainy image to its de-rained counterpart, object edges to its photo, semantic labels to a scenes image, \etc. The proposed PAN consists of two feed-forward convolutional neural networks (CNNs), the image transformation network $T$ and the discriminative network $D$. Through combining the generative adversarial loss and the proposed perceptual adversarial loss, these two networks can be trained alternately to solve image-to-image transformation tasks. Among them, the hidden layers and output of the discriminative network $D$ are upgraded to continually and automatically discover the discrepancy between the transformed image and the corresponding ground-truth. Simultaneously, the image transformation network $T$ is trained to minimize the discrepancy explored by the discriminative network $D$. Through the adversarial training process, the image transformation network $T$ will continually narrow the gap between transformed images and ground-truth images. Experiments evaluated on several image-to-image transformation tasks (\eg, image de-raining, image inpainting, \etc.) show that the proposed PAN outperforms many related state-of-the-art methods. 
\end{abstract}

\section{Introduction}
\label{sec:introduction}
Image-to-image transformation tasks aim to transform an input image into the desired ouput image, and they exist in a series of applications about image processing, computer graphics, and computer vision. For example, generating high-quality images from corresponding degraded (e.g. simplified, corrupted or low-resolution) images, and transforming a color input image into its semantic or geometric representations. More examples include, but not limited to, image de-noising~\cite{elad2006image}, image in-painting~\cite{bertalmio2000image}, image super-resolution~\cite{nasrollahi2014super}, image colorization~\cite{luan2007natural}, image segmentation~\cite{khan2014survey}, \etc.

In recent years, the convolutional neural networks (CNNs) are trained in a supervised manner for various image-to-image transformation tasks~\cite{fu2016clearing,  pathak2016context, dong2016image, zhang2016colorful}. They encode input image into hidden representation, which is then decoded to the output image. By penalizing the discrepancy between the output image and ground-truth image, optimal CNNs can be trained to discover the mapping from the input image to the transformed image of interest. These CNNs are developed with distinct motivation and differ in the loss function design. 

One of the most straightforward approaches is to pixel-wisely evaluate output images~\cite{dong2016image, cheng2015deep, shelhamer2016fully}, \eg, L2 (or L1) norm to calculate the distance between the output and ground-truth images in the pixel space. Though it can generate reasonable images in many image-to-image transformation tasks, there used to be some unignorable defects associated with their outputs, such as blurred results (lack of high-frequency information), artifacts (lack of perceptual information).

Besides per-pixel losses, some advanced loss functions have been introduced to measure the discrepancy between the output and ground-truth images from other perspectives.  There is a large body of works based on (conditional) generative adversarial networks (GANs, cGANs)~\cite{goodfellow2014generative, mirza2014conditional}. GANs (or cGANs) perform an adversarial process alternating between identifying and faking, and the generative adversarial losses are formulated to evaluate the discrepancy between the generated distribution and the real-world distribution. Experimental results show that generative adversarial losses are beneficial for generating more `realistic' images. 
There are also some works~\cite{pathak2016context, isola2016image} to integrate per-pixel losses and generative adversarial losses as a new joint loss function to train image-to-image transformation models, resulting in sharper results.

\begin{figure*}[!t]
\centering
\includegraphics[width=6.5in]{demo.png}
\caption{Image-to-image transformation tasks. Many tasks in image processing, computer graphics, and computer vision can be regarded as image-to-image transformation tasks, where a model is designed to transform an input image into the required output image. We proposed a principled Perceptual Adversarial Networks (PAN) to solve the image-to-image transformation between paired images. For each pair of the images we demonstrated, the left one is the input image, and the right one is the transformed result of the proposed PAN.}
\label{Demo}
\end{figure*}

In parallel, some recent works~\cite{johnson2016perceptual, dosovitskiy2016generating, bruna2015super} proposed the perceptual losses as a novel kind of measurements for evaluating the discrepancy between the output and ground-truth images. Usually, a well-trained image classification network (\eg, VGG-16~\cite{simonyan2014very}) is employed to extract high-level features (\eg, content or texture) of both output images and ground-truth images. Through penalizing the discrepancy between extracted high-level features, these models are trained to transform the input image into the output which has same high-level features with the corresponding ground-truth. Besides image style transfer, the perceptual losses were used in some image-to-image transformation tasks for suppressing artifacts occurred in the output images~\cite{johnson2016perceptual,zhang2017image}. 

Aforementioned different losses evaluate the discrepancy between the output image and corresponding ground-truth from different perspectives, and therefore, make them have their own advantages. Yet, single loss (\eg, per-pixel, perceptual or generative adversarial loss) is hard to discover all discrepancy between the output and ground-truth images, which usually lead to the production of some defects in their output images. In order to enhance the performance, some works attempted to integrate losses, \ie, evaluate the discrepancy from multiple perspectives. Among them, through integrating all of the per-pixel, perceptual and generative adversarial losses, Zhang~\etal~\cite{zhang2017image} and Ledig~\etal~\cite{ledig2016photo} achieved the state-of-art performance on single image de-raining and single image super-resolution, respectively. 
Based on aforementioned observations, there are raised two questions: Have the existing methods penalized all possible discrepancy between the output image and corresponding ground-truth? If not, can we find a novel loss which can evaluate the discrepancy from more perspectives than existing methods, and further enhance the performance of image-to-image transformation tasks? 

In this paper, we proposed the perceptual adversarial networks (PAN) for image-to-image transformation tasks. The PAN was inspired from the GANs framework and consisted of an image transformation network $T$ and a discriminative network 
$D$. Both the generative adversarial loss and the principled perceptual adversarial loss are employed to train the PAN. 
Firstly, as same as GANs, we introduced the generative adversarial loss into our framework and utilized it as the statistical measurement to evaluate the distribution of output images. Then, we used the representations on the hidden layers of the discriminative network $D$ as dynamic perceptual measurements (\ie, perceptual adversarial loss) to penalize the discrepancy between the output and ground-truth images. Specifically, utilizing the representations on hidden layers of the network $D$, the network $T$ is trained to generate the output image that has the same high-level features with the corresponding ground-truth. At the same time, if the discrepancy measured on the current high-dimensional space is small enough, the hidden layers of $D$ will be updated to find new high-dimensional spaces which stress the discrepancy that still exists between the output and ground-truth images. Different from the aforementioned perceptual losses which calculate the representations on hidden layers of well-trained CNNs, our proposed the perceptual adversarial loss performs an adversarial training process between the image transformation network and the discriminative network, and has the capability to continually and automatically discover the discrepancy that has not been minimized. Therefore, the perceptual adversarial loss provides a strategy to penalize the discrepancy between the output and ground-truth images from as many perspectives as possible.

 In summary, our paper makes the following contributions: 

\begin{itemize}
\item We proposed a principled perceptual adversarial loss which utilizes the hidden layers of the discriminative network to evaluate the discrepancy between the output and ground-truth images through an adversarial training process.

\item Through combining the perceptual adversarial loss and the generative adversarial loss, we presented the PAN as a framework for solving image-to-image transformation tasks.

\item We evaluated the performance of the PAN on several image-to-image transformation tasks (Fig.~\ref{Demo}). Experiments demonstrate the proposed PAN has the great capability of solving the image-to-image transformation tasks.
\end{itemize}

The rest of the paper is organized as follows: after a brief
summary of previous related works in section~\ref{sec:background}, we illustrate
the proposed PAN together with its training losses
in section~\ref{sec:methods}. Then we exhibit the experimental validation
of the whole method in section~\ref{sec:experiments}. Finally, we conclude
this work with some future directions in section~\ref{sec:conclusion}.

\section{Related Work}
\label{sec:background}

In this section, we first introduce some representative image-to-image transformation works using the feed-forward CNNs. And then, some related works based on the GANs and perceptual losses are briefly summarized.

\subsection{Image-to-image transformation with feed-forward CNNs}

Deep learning methods have received lots of attention in recent years. We witnessed that a variety of feed-forward CNNs have been proposed for image-to-image transformation tasks. These feed-forward CNNs can be easily trained using the back-propagation algorithm~\cite{rumelhart1988learning}, and the transformed images can be generated by simply forward passing the input image through the well-trained CNNs in the test stage.

Individual per-pixel loss or per-pixel loss accompanied with other losses are employed in a number of image-to-image transformations. Image super-resolution tasks estimate a high-resolution image from its low-resolution counterpart~\cite{dong2016image, johnson2016perceptual, ledig2016photo}. Image de-raining (or de-snowing) methods attempt to remove the rain (or snow) strikes in the pictures, which caused by the uncontrollable weather conditions~\cite{fu2016clearing, eigen2013restoring, yang2016joint, zhang2017image}. Given a damaged image, image inpainting aims to re-fill the missing part of the input image~\cite{pathak2016context, li2017generative}. Image semantic segmentation methods produce dense scene labels based on a single input image~\cite{farabet2013learning, noh2015learning, long2015fully, eigen2015predicting}. Given an input image of an object, some feed-forward CNNs were trained to synthesize the image of the same object under a new viewpoint~\cite{yang2015weakly,tatarchenko2016multi}. More, image-to-image transformation tasks based on feed-forward CNNs, include, but not limited to, image colorization~\cite{cheng2015deep}, depth estimations~\cite{eigen2014depth,eigen2015predicting}, \etc.

\subsection{GANs-based works}
Generative adversarial networks (GANs)~\cite{goodfellow2014generative} provide an important approach for learning a generative model which generates samples from the real-world data distribution. GANs consists of a generative network and a discriminative network. Through playing a minimax game between these two networks, GANs is trained to generate more and more `realistic' samples. Since the great performance on learning real-world distributions, there is emergence of a large number of GANs-based works. Some of these GANs-based works are committed to training a better generative model, such as the InfoGAN~\cite{chen2016infogan}, the WGAN~\cite{arjovsky2017wasserstein} and the Energy-based GAN~\cite{zhao2016energy}. There are also some works employing the GANs into their models to improve the performance of some classical tasks. For example, the PGN~\cite{lotter2015unsupervised} for video prediction, the SRGAN~\cite{ledig2016photo} for super-resolution, the ID-CGAN for image de-raining~\cite{zhang2017image},the iGAN~\cite{zhu2016generative} for interactive application, the IAN~\cite{brock2016neural} for photo modification, and the Context-Encoder for image in-painting~\cite{pathak2016context}. 
Most recently, Isola~\etal~\cite{isola2016image} proposed the pix2pix-cGANs to perfrom several image-to-image transformation tasks (named image-to-image translation in their work), such as translating the semantic labels into the street scene, object edges into pictures, aerial photos into maps, \etc.

\subsection{Perceptual loss}
Recent years, some theoretical analysis and experimental results suggested that the high-level features extracted from a well-trained image classification network have the capability to capture some perceptual information from the real-world images~\cite{gatys2015neural,johnson2016perceptual}. Specifically, the hidden representations directly extracted by the well-trained image classifier can be regarded as semantic content of the input image, and the image style distribution can be captured by the Gram matrix of the hidden representations. Therefore, high-level features extracted from hidden layers of a well-trained CNNs are introduced to optimize image generation models. Dosovitskiy and Brox~\cite{dosovitskiy2016generating} calculated the Euclidean distances in high-level feature space of deep neural networks as the deep perceptual similarity metrics to improve their image generation performance. Johnson et al.~\cite{johnson2016perceptual}, Bruna
et al.~\cite{bruna2015super} and Ledig et al.~\cite{ledig2016photo} use the features extracted from a
well-trained VGG network to improve the performance of the single image super-resolution task. In addition, there are works applying high-level features in image style-transfer~\cite{gatys2015neural, johnson2016perceptual}, image de-raining~\cite{zhang2017image} and image view synthesis~\cite{park2017transformation} tasks.

\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{framework.png}%
\caption{PAN framework. The PAN consists of an image transformation network $T$ and a discriminative network $D$. The image transformation network $T$ is trained to synthesize the transformed images given the input images. It is composed of a stack of Convolution-BatchNorm-LeakyReLU encoding layers and Deconvolution-BatchNorm-ReLU decoding layers, and the skip-connections are used between mirrored layers. The discriminative network $D$ is also a CNNs that consists of Convolution-BatchNorm-LeakyReLU layers. Hidden layers of the network $D$ are utilized to evaluate the perceptual adversarial loss, and the output of the network $D$ are used to distinguish the transformed images and the real-world images.}
\label{framework}
\end{figure*}

\section{Methods}
\label{sec:methods}
In this section, we introduce the proposed Perceptual Adversarial Networks (PAN) for image-to-image transformation tasks. Firstly, we explain the generative and the perceptual adversarial losses, respectively. Then, we give the whole framework of the proposed PAN. Finally, we illustrate the details of the training procedure and network architectures. 

\subsection{Generative adversarial loss}
We begin with the generative adversarial loss in the original GANs. A generative network $G$ is trained to map samples from noise distribution $p_z$ to real-world data distribution $p_\text{data}$ through playing a minimax game with a discriminative network $D$. In the training procedure, the discriminative network $D$ aims to distinguish the real samples $x \sim p_\text{data}$ and the generated samples $G(z) \sim p_g$. In contrary, the generative network $G$ tries to confuse the discriminative network $D$ by generating more and more `realistic' samples. This minimax game can be formulated as
\begin{equation}
\min_G \max_D \mathbb{E}_{x \in \mathcal{X}_\text{data}}[\log D(x)] +\mathbb{E}_{z \in \mathcal{Z}} [\log (1-D(G(z)))]
\end{equation} 

Nowadays, GANs-based models have shown strong capability in learning generative models, especially for image generation~\cite{arjovsky2017wasserstein, lotter2015unsupervised, chen2016infogan}. We therefore adapt the GANs learning strategy to solve image-to-image transformation tasks as well. As shown in Fig.~\ref{framework}, the image transformation network $T$ is used to generate transformed image $T(x)$ given the input image $x \in \mathcal{X}_\text{input}$. Meanwhile, each input image $x$ has a corresponding ground-truth image $y$. 
We suppose that all target images $y \in \mathcal{Y}_\text{ground-truth}$ obey the distribution $p_\text{real}$, and the transformed images $T(x)$ is encouraged to have the same distribution with that of targets images $y$, i.e. $T(x) \sim p_\text{real}$. Based the generative adversarials learning strategy, a discriminative network $D$ is additionally introduced, and the generative adversarial loss can be written as
\begin{equation}
\min_T \max_D \mathbb{E}_{y \in \mathcal{Y}}[\log D(y)] +\mathbb{E}_{x \in \mathcal{X}_\text{input}} [\log (1-D(T(x)))]
\end{equation} 
The generative adversarial loss acts as a statistical measurement to penalize the discrepancy between the distributions of the transformed images and the ground-truth images. 

\subsection{Perceptual adversarial loss}
Different from the original GANs that randomly generate samples from the data distribution $p_\text{data}$, our goal is inferring the transformed images according to the input images. This is thus a further step of GANs to explore the mapping from the input image to its ground truth. 

As mentioned in Sections~\ref{sec:introduction} and~\ref{sec:background}, per-pixel losses and perceptual losses are two widely used losses in existing works for generating images towards the ground truth. The per-pixel losses penalize the discrepancy occurred in the pixel space, but often produce blurry results~\cite{pathak2016context, zhang2016colorful}. The perceptual losses explore the discrepancy between high-dimensional representations of images extracted from a well-trained CNNs, \eg, the VGG net trained on the ImageNet dataset~\cite{simonyan2014very}. Although hidden layers of well-trained CNNs have been experimentally validated to map the image from pixel space to high-level feature spaces, how to extract the effective features for image-to-image transformation tasks through the hidden layers is still an open problem. 
In image-to-image transformation tasks, it is reasonable to assume that the output images are consistent with the ground-truth images under any measurements (\ie, any high-dimensional spaces). Here we employ the trainable hidden layers of the discriminative network $D$ to measure the perceptual adversarial loss between transformed images and ground-truth images. Formally, given a paired data $(x,y) \in (\mathcal{X}_\text{input}, \mathcal{Y}_\text{ground-truth})$, a poistive margin $m$, the perceptual adversarial loss can be written as
\begin{equation}
L_T(x,y) = \sum_{i=1}^N \lambda_i P_i(T(x),y)
\end{equation}
\begin{equation}
\begin{aligned} 
L_P(x,y) = [m - \sum_{i=1}^N \lambda_i P_i(T(x),y)]^+
\end{aligned} 
\end{equation} 
where $[\cdot]^+=\max(0,\cdot)$. The function $P_i(,)$ measures the discrepancy between the high-level features that extracted by the $i$-th hidden layer of the discriminative network $D$. $\{\lambda_i\}_{i=1}^N$ are hyper-parameters balancing the influence of $N$ different hidden layers. In our experiments, the L1 norm is employed to calculate the discrepancy of the high-dimensional representations on the hidden layers, \ie, 
\begin{equation}
P_i(T(x),y) = ||H_i(y) - H_i(T(x)) ||
\end{equation} 
where $H_i()$ is the image representation on the $i$-th hidden layer of the discriminative network $D$.

By minimizing the perceptual adversarial loss function $L_T$ with respect to parameters of $T$, we will encourage the network $T$ to generate image $T(x)$ that has similar high-level features with its ground-truth $y$. If the sum of discrepancy between the current learned representations of transformed image $T(x)$ and ground-truth image $y$ is less than the positive margin $m$, the loss function $L_D$ will upgrade the discriminative network $D$ for new high-dimensional spaces, which discover the discrepancy that still exist between the transformed images and corresponding ground-truth. Therefore, based on the perceptual adversarial loss, the discrepancy between the transformed and ground-truth images can be constantly explored and exploited.

\subsection{The perceptual adversarial networks}

Based on the aforementioned generative adversarial loss and perceptual adversarial loss, we develop the PAN framework, which consists of an image transformation network $T$ and a discriminative network $D$. These two networks play a minimax game by optimizing different loss functions. Given a pair of data $(x,y)\in(\mathcal{X}_\text{input},\mathcal{Y}_\text{ground-truth})$, the loss function of image transformation network $J_T$ and the loss function of discriminative network $J_D$ are formally defined as
\begin{equation}
J_T =  \log(1-D(T(x))) + \sum_{i} \lambda_i P_i(T(x),y)
\end{equation} 
\begin{equation}
\begin{aligned}
J_D = &  -\log(D(y)) - \log(1-D(T(x))) + [m - \sum_{i} \lambda_i P_i(T(x),y)]^+
\end{aligned}
\end{equation}
minimizing $J_T$ with respect to the parameters of $T$ is consistent with maximizing the second and the third terms of $J_D$. When $P(T(x),y) \geq m$, the third term of $J_D$ will have zero gradients, because of the positive margin $m$.

In general, given paired images $(x, y) \in (\mathcal{X}_\text{input}, \mathcal{Y}_\text{ground-truth})$, the discriminative network $D$ aims to distinguish transformed images from ground-truth images from both the statistical (the first and second terms of $J_D$) and dynamic perceptual (the third term of $J_D$) aspects. On the other hand, the image transformation network $T$ is trained to generate increasingly better images by reducing the discrepancy between the output and ground-truth images. 

\subsection{Network architectures}

Fig.~\ref{framework} illustrates the framework of the proposed PAN, which is composed of two CNNs, \ie, the image transformation network $T$ and the discriminative network $D$. 

\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{The architecture of the image transformation network.}
\label{table:netG}
\centering
\begin{tabular}{l}
\hline
\\\\\\\\\\\\\\\\\\\\\\\\{\bf Image transformation network} $T$\\
\hline{\bf Input}: Image \\
\hline[layer 1]    \\\\\Conv. (3, 3, \64), stride=2; \emph{LReLU};     \\\\\\\
\hline[layer 2]    \\\\\ Conv. (3, 3, 128), stride=2; Batchnorm; \\
\hline[layer 3]    \\\ \\ \emph{LReLU}; Conv. (3, 3, 256), stride=2; Batchnorm; \\
\hline[layer 4]    \\\\\  \emph{LReLU}; Conv. (3, 3, 512), stride=2; Batchnorm;\\
\hline[layer 5]    \\\\\  \emph{LReLU}; Conv. (3, 3, 512), stride=2; Batchnorm;\\
\hline[layer 6]    \\\\\ \emph{LReLU}; Conv. (3, 3, 512), stride=2; Batchnorm; \emph{LReLU};     \\\\\
\hline[layer 7]    \\\\\ DeConv. (4, 4, 512), stride=2; Batchnorm; \    \\\ \\
\hline
\\\\\\\\\\\\\\\\Concatenate Layer(Layer 7, Layer 5); \emph{ReLU}; \\\\ \\
\hline[layer 8]    \\\\\ DeConv. (4, 4, 256), stride=2; Batchnorm; \\
\hline
\\\\\\\\\\\\\\\\Concatenate Layer(Layer 8, Layer 4); \emph{ReLU}; \\\\ \\
\hline[layer 9]    \\\\\ DeConv. (4, 4, 128), stride=2; Batchnorm;  \\\ \\
\hline
\\\\\\\\\\\\\\\\Concatenate Layer(Layer 9, Layer 3); \emph{ReLU}; \\\\ \\
\hline[layer 10]    \\\\ DeConv. (4, 4, \64), stride=2; Batchnorm; \\
\hline
\\\\\\\\\\\\\\\\Concatenate Layer(Layer 10, Layer 2); \emph{ReLU}; \\\\ \\
\hline[layer 11]    \\\\ DeConv. (4, 4, \64), stride=2; Batchnorm; \emph{ReLU}; \\
\hline[layer 12]  \\\\ DeConv. (4, 4, 3), stride=2; \emph{Tanh};     \\\\\\\
\hline{\bf Output}: Transformed image\\
\hline
\end{tabular}
\end{table}

\subsubsection{Image transformation network $T$}
The image transformation network $T$ is proposed for generating the transformed image given the input image. Following the network architectures in~\cite{radford2015unsupervised,isola2016image}, the network $T$ firstly encodes the input image into high-dimensional representation using a stack of Convolution-BatchNorm-LeakyReLU layers, and then, the output image can be decoded by the remaining Deconvolution-BatchNorm-ReLU layers. Note that the output layer of the network $T$ does not use batchnorm and replaces the ReLU with Tanh activation. Moreover, the skip-connections are used to connect mirrored layers in the encoder and decoder stacks. More details of the transformation network $T$ are listed in the Table~\ref{table:netG}. For all experiments in this paper, if we do not give an additional explanation, the same architecture of the network $T$ is used.

\subsubsection{Discriminative network $D$}
In the proposed PAN framework, the discriminative network $D$ is introduced to compute the discrepancy between the transformed images and the ground-truth images. Specifically, given an input image, the discriminative network $D$ extracts high-level features using a series of Convolution-BatchNorm-LeakyReLU layers. The 1\textsuperscript{st}, 4\textsuperscript{th}, 6\textsuperscript{th}, and 8\textsuperscript{th} layers are utilized to measure the perceptual adversarial loss for pairs of transformed image and its corresponding ground-truth. Finally, the last convolution layer is flattened and then fed into a single sigmoid output. The output of the discriminative network $D$ estimates the probability that the input image comes from the real-world dataset rather than from the image transformation network $T$. 
The same discriminative network $D$ for all tasks demonstrated in this paper, and details of the network $D$ are shown in Table~\ref{table:netD}.

\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{The architecture of the discriminative network.}
\label{table:netD}
\centering
\begin{tabular}{l}
\hline
\\\\\\\\\\\ \\\\\\\\\\\\{\bf Discriminative network} $D$\\
\hline{\bf Input}: Image  \\
\hline[layer 1]    \\\Conv. (3, 3, 64), stride=1; \emph{LReLU}; \\
\hline
\\\\\\\\\\\\\\(Perceptual adversarial loss: $P_1$)\\
\hline[layer 2]    \\\ Conv. (3, 3, 128), stride=2; Batchnorm; \emph{LReLU};     \\\\\ \\
\hline[layer 3]    \\\ Conv. (3, 3, 128), stride=1; Batchnorm; \emph{LReLU};     \\\\\ \\
\hline[layer 4]    \\\Conv. (3, 3, 256), stride=2; Batchnorm; \emph{LReLU};\\
\hline
\\\\\\\\\\\\\\(Perceptual adversarial loss: $P_2$)\\
\hline[layer 5]    \\\ Conv. (3, 3, 256), stride=1; Batchnorm; \emph{LReLU};     \\\\\ \\
\hline[layer 6]    \\\Conv. (3, 3, 512), stride=2; Batchnorm; \emph{LReLU};\\
\hline
 \\\\\\\\\\\\\\(Perceptual adversarial loss: $P_3$)\\
\hline[layer 7]    \\\ Conv. (3, 3, 512), stride=1; Batchnorm; \emph{LReLU};     \\\\\\
\hline[layer 8]    \\\ Conv. (3, 3, 512), stride=2; Batchnorm; \emph{LReLU};\\
\hline
\\\\\\\\\\\\\\ (Perceptual adversarial loss: $P_4$)\\
\hline[layer 9]     \\\ Conv. (3, 3, 8), stride=2; \emph{LReLU};     \\\\\ \\
\hline[layer 10]    \\ Fully connected (1); \emph{Sigmoid};    \\\\\ \\
\hline{\bf Output}: Real or Fake (Probability)\\
\hline
\end{tabular}
\end{table}

\section{Experiments}
\label{sec:experiments}

In this section, we evaluate the performance of the proposed PAN on several image-to-image transformation tasks, which are popular in fields of image processing (\eg, image de-raining), computer vision (\eg, semantic segmentation) and computer graphics (\eg, image generation).
\subsection{Experimental setting up}
\label{sec:setup}
 For fair comparisons, we adopted the same settings with existing works, and reported experimental results using several evaluation metrics. These tasks and data settings include: 

\begin{itemize}
\item \emph{Single image de-raining}, on the dataset provided by ID-CGAN~\cite{zhang2017image}. 
\item \emph{Image Inpainting}, on a subset of ILSVRC'12 (same as context-encoder~\cite{pathak2016context}).
\item \emph{Semantic labels$\leftrightarrow$images}, on the Cityscapes dataset~\cite{cordts2016cityscapes} (same as pix2pix~\cite{isola2016image}).
\item \emph{Edges$\to$images}, on the dataset created by pix2pix~\cite{isola2016image}. The original data is from~\cite{zhu2016generative} and~\cite{yu2014fine}, and the HED edge detector~\cite{xie2015holistically} was used to extract edges.
\item \emph{aerial$\to$map}, on the dataset from pix2pix~\cite{isola2016image}.
\end{itemize}

Furthermore, all experiments were trained on Nvidia Titan-X GPUs using Theano~\cite{bergstra2010theano}. According to the generative and perceptual adversarial losses, we alternately updated the image transformation network $T$ and the discriminative network $D$. Specifically, Adam solver~\cite{kingma2014adam} with a learning rate of 0.0002 and a first momentum of 0.5 was used in network training. After one updating of the discriminative network $D$, the image transformation $T$ will be updated three times. Hyper-parameters $\lambda_1=5$, $\lambda_2=1.5$, $\lambda_3=1.5$, $\lambda_4=5$, and batch size of 4 were used for all tasks. Since the dataset sizes for different tasks are changed largely, the training epochs of different tasks were set differently. Overall, the number of training iterations was around 100k. 

\subsection{Evaluation metrics}
To illustrate the performance of image-to-image transformation tasks, we used qualitative and quantitative metrics to evaluate the performance of the transformed images. For the qualitative experiments, we directly showed the input and transformed images. Meanwhile, we used quantitative measures to evaluate the performance over the test sets, such as Peak Signal to Noise Ratio (PSNR), Structural Similarity Index (SSIM)~\cite{wang2004image}, Universal Quality Index (UQI)~\cite{995823} and Visual Information Fidelity (VIF)~\cite{sheikh2006image}. 

\subsection{Analysis of the loss functions}

\begin{table}[!t]
\renewcommand{\arraystretch}{1.5}
\caption{De-raining}
\label{table:deraining}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
  & PSNR(dB) & SSIM & UQI & VIF \\
\hline
\hline
 L2 & 22.77 & \\0.7959 \\ & \\0.6261 \\&  \\0.3570 \\ \\
\hline
 L2+CGAN & 22.19 & 0.8083 & 0.6278 & 0.3640 \\
\hline
 ID-CGAN & 22.91 & 0.8198 & 0.6473 & 0.3885\\
\hline
 PAN & {\bf 23.35} & {\bf 0.8303} & {\bf 0.6644} & {\bf 0.4050}\\
\hline
\end{tabular}
\end{table}

\begin{figure}
\captionsetup[subfigure]{labelformat=empty}
\centering
 \begin{subfigure}[b]{0.19\textwidth}
    \includegraphics[width=\textwidth]{snow_input.png}%
    \label{fig_first_case}
 \end{subfigure}
 \begin{subfigure}[b]{0.19\textwidth}
    \includegraphics[width=\textwidth]{snow_pixel.png}%
    \label{fig_second_case}
 \end{subfigure}
 \begin{subfigure}[b]{0.19\textwidth}
    \includegraphics[width=\textwidth]{snow_cgan.png}%
    \label{fig_first_case}
 \end{subfigure}
 \begin{subfigure}[b]{0.19\textwidth}
    \includegraphics[width=\textwidth]{snow_idcgan.png}%
    \label{fig_first_case}
 \end{subfigure}
 \begin{subfigure}[b]{0.19\textwidth}
    \includegraphics[width=\textwidth]{snow_ours.png}%
    \label{fig_first_case}
 \end{subfigure}
 \begin{subfigure}[b]{0.19\textwidth}
    \includegraphics[width=\textwidth]{snow_input_1.png}
    \caption{Input}
    \label{fig_first_case}
 \end{subfigure}
 \begin{subfigure}[b]{0.19\textwidth}
    \includegraphics[width=\textwidth]{snow_pixel_1.png}
    \caption{L2}
    \label{fig_second_case}
 \end{subfigure}
 \begin{subfigure}[b]{0.19\textwidth}
    \includegraphics[width=\textwidth]{snow_cgan_1.png}
    \caption{L2+cGAN}
    \label{fig_first_case}
 \end{subfigure}
 \begin{subfigure}[b]{0.19\textwidth}
    \includegraphics[width=\textwidth]{snow_idcgan_1.png}
    \caption{ID-CGAN}
    \label{fig_first_case}
 \end{subfigure}
 \begin{subfigure}[b]{0.19\textwidth}
    \includegraphics[width=\textwidth]{snow_ours_1.png}
    \caption{PAN}
    \label{fig_first_case}
 \end{subfigure}
\caption{Comparison of snow-streak removal using different loss functions. Given the same input image (leftmost), each column shows results trained under different loss. The loss function of ID-CGAN~\cite{zhang2017image} combined the per-pixel loss (using L2 norm), cGANs loss and perceptual loss, \ie, \emph{L2+cGAN+perceptual}. For better visual comparison, zoomed versions of the specific regions-of-interest are demonstrated below the test images.}
\label{derain_real}
\end{figure}


\begin{figure*}[!t]
\captionsetup[subfigure]{labelformat=empty}
\centering
 \begin{subfigure}[b]{0.16\textwidth}
    \includegraphics[width=\textwidth]{491_input.png}%
    \label{fig_first_case}
 \end{subfigure}
 \begin{subfigure}[b]{0.16\textwidth}
    \includegraphics[width=\textwidth]{491_1.png}%
    \label{fig_first_case}
 \end{subfigure}
 \begin{subfigure}[b]{0.16\textwidth}
    \includegraphics[width=\textwidth]{491_2.png}%
    \label{fig_first_case}
 \end{subfigure}
 \begin{subfigure}[b]{0.16\textwidth}
    \includegraphics[width=\textwidth]{491_3.png}%
    \label{fig_first_case}
 \end{subfigure}
 \begin{subfigure}[b]{0.16\textwidth}
    \includegraphics[width=\textwidth]{491_4.png}%
    \label{fig_first_case}
 \end{subfigure}
 \begin{subfigure}[b]{0.16\textwidth}
    \includegraphics[width=\textwidth]{491_gt.png}%
    \label{fig_first_case}
 \end{subfigure}
 \begin{subfigure}[b]{0.16\textwidth}
    \includegraphics[width=\textwidth]{491_input_1.png}
    \caption{Input}
    \label{fig_first_case}
 \end{subfigure}
 \begin{subfigure}[b]{0.16\textwidth}
    \includegraphics[width=\textwidth]{491_1_1.png}
    \caption{$P_1$}
    \label{fig_first_case}
 \end{subfigure}
 \begin{subfigure}[b]{0.16\textwidth}
    \includegraphics[width=\textwidth]{491_2_1.png}
    \caption{$P_2$}
    \label{fig_first_case}
 \end{subfigure}
 \begin{subfigure}[b]{0.16\textwidth}
    \includegraphics[width=\textwidth]{491_3_1.png}
    \caption{$P_3$}
    \label{fig_first_case}
 \end{subfigure}
 \begin{subfigure}[b]{0.16\textwidth}
    \includegraphics[width=\textwidth]{491_4_1.png}
    \caption{$P_4$}
    \label{fig_first_case}
 \end{subfigure}
 \begin{subfigure}[b]{0.16\textwidth}
    \includegraphics[width=\textwidth]{491_gt_1.png}
    \caption{Ground-Truth}
    \label{fig_first_case}
 \end{subfigure}
\caption{Transforming the semantic labels to cityscapes images use the perceptual adversarial loss. Within the perceptual adversarial loss, a different hidden layer is utilized for each experiment. For better visual comparison, zoomed versions of the specific regions-of-interest are demonstrated below the test images. For higher layers, the transformed images look sharper, but less color information is preserved.}
\label{fig:analysis}
\end{figure*}


As discussed in Section~\ref{sec:introduction}, the design of loss function will largely influence the performance of image-to-image transformation. Firstly, the per-pixel loss (using L2 norm) is widely used in various image-to-image transformation works~\cite{tatarchenko2016multi,dong2014learning,kingma2013auto}. Then, the joint loss integrating the per-pixel loss and the conditional generative adversarial loss are proposed to synthesize more `realistic' transformed images~\cite{pathak2016context,isola2016image}. Most recently, through introducing the perceptual loss, \ie, penalizing the discrepancy between high-level features that extracted by well-trained CNNs, the performance of some image-to-image transformation tasks are further enhanced~\cite{ledig2016photo,johnson2016perceptual,zhang2017image}. Different from these existing methods, the proposed PAN loss integrates the generative adversarial loss and the proposed perceptual adversarial loss to train image-to-image transformation networks. Here, we compare the performance between the proposed PAN loss with those deployed in existing methods. 
For fair comparison, we adopted the same image transformation network and data settings from ID-CGAN~\cite{zhang2017image}, and used aforementioned loss functions to perform the image de-raining (de-snowing) task. 
The quantitative results over the synthetic test set were shown in Table~\ref{table:deraining}, while the qualitative results test on the real-world images were shown in Fig.~\ref{derain_real}. 
From both quantitative and qualitative comparisons, we find that only using the per-pixel loss (L2 norm) achieved the worst result, due to the transformed image still leaves many snow-streaks (Fig.~\ref{derain_real}). Through introducing the cGANs loss, the de-snowing performance was indeed improved, but artifacts can be observed (Fig.~\ref{derain_real}) and the PSNR performance dropped (Table~\ref{table:deraining}). Combining the per-pixel,  cGAN and perceptual loss\footnote{Using the per-trained VGG-16 net~\cite{simonyan2014very}.} together, \ie, using the loss function of ID-CGAN~\cite{zhang2017image}, the quality of transformed images have been further refined on both observations and quantitative measurements. However, from Fig.~\ref{derain_real}, we observe that the transformed images have some color distortion compared to the input images. Finally, the proposed PAN loss not only removed most streaks without color distortion, but also achieved the best performance on quantitative measurements. 

Furthermore, in the proposed PAN, we selected four hidden layers of the discriminative network $D$ to calculate the perceptual adversarial loss. We next proceed to analyze the property of these hidden layers.  Specifically, we trained four configurations of the PAN to perform the task of transforming the semantic labels to the cityscapes images. For each configuration, we set one hyper-parameter $\lambda_i$ as one, and the others $\lambda_{-i}$ as zero, \ie,  
we used only one hidden layer to evaluate the perceptual adversarial loss in each configuration. As shown in Fig.~\ref{fig:analysis}, the lower layers (\eg, $P_1$, $P_2$) pay more
attention to the patch-to-patch transformation and the color transformation, but the transformed images are blurry and lack of fine details. On the other hand, higher layers (\eg, $P_1$, $P_2$) capture more high-frequency information, but lose the color information. Therefore, by integrating different properties of these hidden layers, the proposed PAN can be expected to achieve better performance, and the final results of this task are shown in Fig.~\ref{cityscapes2} and Table~\ref{table:pix2pix}.

\begin{figure}[!t]
\captionsetup[subfigure]{labelformat=empty}
\centering
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{ILSVRC2012_val_00001555_correpted.png}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{ILSVRC2012_val_00001555_context.png}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{ILSVRC2012_val_00001555_ours.png}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{ILSVRC2012_val_00001555_real.png}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{ILSVRC2012_val_00000585_correpted.png}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{ILSVRC2012_val_00000585_context.png}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{ILSVRC2012_val_00000585_ours.png}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{ILSVRC2012_val_00000585_real.png}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{ILSVRC2012_val_00001453_correpted.png}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{ILSVRC2012_val_00001453_context.png}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{ILSVRC2012_val_00001453_ours.png}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{ILSVRC2012_val_00001453_real.png}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{ILSVRC2012_val_00003666_correpted.png}
    \caption{Input}    
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{ILSVRC2012_val_00003666_context.png}
    \caption{CE}
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{ILSVRC2012_val_00003666_ours.png}
    \caption{PAN}
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{ILSVRC2012_val_00003666_real.png}
    \caption{Ground-truth}
    \label{fig_first_case}
\end{subfigure}
\caption{Comparsion of image in-painting results using the Context-Encoder with the proposed PAN. Given the central region missed input image (leftmost), the in-painted images and the ground-truth are list on its rightside.}
\label{fig_sim}
\end{figure}

\begin{table}[!t]
\renewcommand{\arraystretch}{1.5}
\caption{In-painting}
\label{table:inpainting}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
  & PSNR(dB) & SSIM  & UQI & VIF\\
\hline
\hline
 Context-Encoder &  21.74  & \\ 0.8242 \\ & \\0.7828 \\ & \\0.5818 \\ \\
\hline
 PAN & {\bf 21.85} & {\bf 0.8307} & {\bf 0.7956} & {\bf 0.6104} \\
\hline
\end{tabular}
\end{table}

\subsection{Comparing with existing works}

\begin{figure*}[!t]
\captionsetup[subfigure]{labelformat=empty}
\centering
 \begin{subfigure}[b]{0.27\textwidth}
    \includegraphics[width=\textwidth]{27_input.png}%
    \label{fig_first_case}
 \end{subfigure}
 \begin{subfigure}[b]{0.27\textwidth}
    \includegraphics[width=\textwidth]{27_idcgan.png}%
    \label{fig_first_case}
 \end{subfigure}
 \begin{subfigure}[b]{0.27\textwidth}
    \includegraphics[width=\textwidth]{27_ours.png}%
    \label{fig_first_case}
 \end{subfigure}
 \begin{subfigure}[b]{0.27\textwidth}
    \includegraphics[width=\textwidth]{27_input_2.png}%
    \label{fig_first_case}
 \end{subfigure}
 \begin{subfigure}[b]{0.27\textwidth}
    \includegraphics[width=\textwidth]{27_idcgan_2.png}%
    \label{fig_first_case}
 \end{subfigure}
 \begin{subfigure}[b]{0.27\textwidth}
    \includegraphics[width=\textwidth]{27_ours_2.png}%
    \label{fig_first_case}
 \end{subfigure}
 \begin{subfigure}[b]{0.27\textwidth}
    \includegraphics[width=\textwidth]{75_input.png}%
    \label{fig_first_case}
 \end{subfigure}
 \begin{subfigure}[b]{0.27\textwidth}
    \includegraphics[width=\textwidth]{75_idcgan.png}%
    \label{fig_first_case}
 \end{subfigure}
 \begin{subfigure}[b]{0.27\textwidth}
    \includegraphics[width=\textwidth]{75_ours.png}%
    \label{fig_first_case}
 \end{subfigure}
 \begin{subfigure}[b]{0.27\textwidth}
    \includegraphics[width=\textwidth]{75_input_1.png}
    \caption{Input}
    \label{fig_first_case}
 \end{subfigure}
 \begin{subfigure}[b]{0.27\textwidth}
    \includegraphics[width=\textwidth]{75_idcgan_1.png}
    \caption{ID-CGAN}
    \label{fig_first_case}
 \end{subfigure}
 \begin{subfigure}[b]{0.27\textwidth}
    \includegraphics[width=\textwidth]{75_ours_1.png}
    \caption{PAN}
    \label{fig_first_case}
 \end{subfigure}
\caption{Comparsion of rain-streak removal using the ID-CGAN with the proposed PAN on real-world rainy images. For better visual comparison, zoomed versions of the specific regions-of-interest are demonstrated below the test images.}
\label{derain_idcgan}
\end{figure*}

\begin{figure*}[!t]
\captionsetup[subfigure]{labelformat=empty}
\centering
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{12_input.png}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{12_pix2pix.png}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{12_ours.png}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{12.png}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{19_input.png}
    \caption{Input}
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{19_pix2pix.png}
    \caption{pix2pix-cGAN}
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{19_ours.png}
    \caption{PAN}
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{19.png}
    \caption{Ground-truth}
    \label{fig_first_case}
\end{subfigure}
\caption{Comparison of transforming the semantic labels to cityscapes images using the pix2pix-cGAN with the proposed PAN. Given the semantic labels (leftmost), the transformed cityscapes images and the ground-truth are listed on the rightside.}
\label{cityscapes2}
\end{figure*}


In this subsection, we compared the performance of the proposed PAN with some recent state-of-the-art algorithms for image-to-image transformation tasks.

\begin{figure*}[!t]
\captionsetup[subfigure]{labelformat=empty}
\centering
 \begin{subfigure}[b]{0.16\textwidth}
    \includegraphics[width=\textwidth]{18_AB_input.png}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.16\textwidth}
    \includegraphics[width=\textwidth]{18_AB.jpg}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.16\textwidth}
    \includegraphics[width=\textwidth]{18_AB.png}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.16\textwidth}
    \includegraphics[width=\textwidth]{113_AB_input.png}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.16\textwidth}
    \includegraphics[width=\textwidth]{113_AB.jpg}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.16\textwidth}
    \includegraphics[width=\textwidth]{113_AB.png}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.16\textwidth}
    \includegraphics[width=\textwidth]{48_AB_input.png}
    \caption{Input}
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.16\textwidth}
    \includegraphics[width=\textwidth]{48_AB.jpg}
    \caption{pix2pix-cGAN}
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.16\textwidth}
    \includegraphics[width=\textwidth]{48_AB.png}
    \caption{PAN}
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.16\textwidth}
    \includegraphics[width=\textwidth]{7_AB_input.png}
    \caption{Input}
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.16\textwidth}
    \includegraphics[width=\textwidth]{7_AB.jpg}
    \caption{pix2pix-cGAN}
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.16\textwidth}
    \includegraphics[width=\textwidth]{7_AB.png}
    \caption{PAN}
    \label{fig_first_case}
\end{subfigure}
\caption{Comparison of transforming the object edges to correspoding images using the pix2pix-cGAN with the proposed PAN. Given the edges (leftmost), the generated images of shoes and handbags are list on the rightside.}
\label{edges}
\end{figure*}

\subsubsection{Context-encoder}

Context-Encoder (CE)~\cite{pathak2016context} trained a CNNs for the single image inpainting task, which generates the corrupted contents of an image based on its surroundings. Given corrupted images as input, the image inpainting aims to generate the inpainted images, and thus it can be formulated as an image-to-image transformation task. The per-pixel loss (using L2 norm) and the generative adversarial loss were combinded in the Context-Encoder to explore the relationship between the input surroundings and its central missed region.

To compare with the Context-Encoder, we performed our proposed PAN on the same image inpainting task, which attempted to inpaint the central region missed images over one of the most challenge datasets - the ImageNet dataset. As illustrated in Section~\ref{sec:setup}, 100k images were randomly selected from the ILSVRC'12 dataset to train both Context-Encoder and PAN, and 50k images from the ILSVRC'12 validation set were used for test purpose. Moreover, since the image inpainting model are asked to generate the missing region of the input image but not the whole image, we directly employ the architecture of the image transformation network from~\cite{pathak2016context}.

In Fig.~\ref{fig_sim}, we reported some inpainted results in the test set. For each input image, the missing part is mixed by the foreground objects and backgrounds. The goal is to predict and repaint the missing part through understanding the uncorrupted surroundings. From the inpainted results, we find the proposed PAN performed better on understanding the surroundings and inpainting the missing part with semantic contents. However, the context-encoder tended to use the nearest region (usually the background) to inpaint the missing part. In addition, although both the context-encoder and our PAN did not give the perfect results, PAN attempted to synthesize more details in the missing parts. Last but not the least, in Table~\ref{table:inpainting}, we reported the quantitative results calculated over all 50k test images, which also demonstrated that the proposed PAN achieves better performance on understanding the context of the input images and synthesizing the corresponding missing parts.

\subsubsection{ID-CGAN}

Single image de-raining task aims to remove rain streaks in a given rainy image. Considering the unpredictable weather conditions, the single image de-raining (de-snowing) is another challenge image-to-image transformation task. Most recently, the Image De-raining Conditional Generative Adversarial Networks (ID-CGAN) was proposed to tackle the image de-raining problem. Through combining the per-pixel (L2 norm), conditional generative adversarial, and perceptual losses (VGG-16), ID-CGAN achieved the state-of-the-art performance on single image de-raining. 

We attempted to solve image de-raining by the proposed PAN using the same setting with that of ID-CGAN. Since there is a lack of large-scale datasets consisting of paired rainy and de-rained images, we resort to synthesize the training set~\cite{zhang2017image} of 700 images. Zhang~\etal~\cite{zhang2017image} provided 100 synthetic images and 50 real-world rainy images for test. Since the ground-truth is available for synthetic test images, we calculated and reported the quantitative results in Table~\ref{table:deraining}. Moreover, we test both ID-CGAN and PAN on real-world rainy images, and the results were shown in Fig.~\ref{derain_idcgan}. For better visual comparison, we zoomed up the specific regions-of-interest below the test images. 

From Fig.~\ref{derain_idcgan}, we found both ID-CGAN and PAN achieved great performance on single image de-raining. However, by observing the zoomed region, the PAN removed more rain-strikes with less color distortion. Additionally, as shown in Table~\ref{table:deraining}, for synthetic test images, the de-rained results of PAN are much more similar with the corresponding ground-truth than that of ID-CGAN. Dealing with the uncontrollable weather condition, why the proposed PAN can achieve better results? One possible reason is that ID-CGAN utilized the well-trained CNNs classifier to extract the high-level features of the output and ground-truth images, and penalize the discrepancy between them (\ie, the perceptual loss). The high-level features extracted by the well-trained classifier usually focus on the content information, and may hard to capture other image information, such as color information. Yet, the proposed PAN used the adversarial perceptual loss which attempts to penalize the discrepancy from as many perspectives as possible. The different training strategy of PAN may help the model to learn a better mapping from the input to output images, and resulting in better performance. 

\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{Comparison with pix2pix-CGAN}
\label{table:pix2pix}
\centering
\begin{tabular}{|c|c|c|c|c|}

\multicolumn{5}{c}{Senmatic labels $\to$ Cityscapes imags}  \\
\hline
 & PSNR(dB) & SSIM & UQI & VIF \\
\hline
\hline
  pix2pix-cGAN & 15.74  & \\0.4275 \\& \0.07315 \& \0.05208 \\\
\hline
 PAN & {\bf 16.06} & {\bf 0.4820} & {\bf 0.1116} & {\bf 0.06581} \\

\hline
\multicolumn{5}{c}{Edges $\to$  Shoes} \\
\hline
 & PSNR(dB) & SSIM & UQI & VIF \\
\hline
\hline
 ID-cGAN & {\bf 20.07} & 0.7504 & 0.2724 & 0.2268 \\
\hline
 PAN & 19.51 & {\bf 0.7816} & {\bf 0.3442} & {\bf 0.2393}\\

\hline
\multicolumn{5}{c}{Edges $\to$  Handbags} \\
\hline
 & PSNR(dB) & SSIM & UQI & VIF \\
\hline
\hline
 ID-cGAN & {\bf 16.50} & 0.6307 & 0.3978 & 0.1723\\
\hline
 PAN & 15.90 & {\bf 0.6570} & {\bf 0.4042} & {\bf 0.1841} \\
\hline
\multicolumn{5}{c}{Cityscapes images $\to$  Semantic labels} \\
\hline
 & PSNR(dB) & SSIM & UQI & VIF \\
\hline
\hline
 ID-cGAN & 19.46 & 0.7270 & 0.1555 & 0.1180\\
\hline
 PAN & {\bf 20.67} & {\bf 0.7725} & {\bf 0.1732} & {\bf 0.1638}\\
\hline
\multicolumn{5}{c}{Aerial photos $\to$  Maps} \\
\hline
 & PSNR(dB) & SSIM & UQI & VIF \\
\hline
\hline
 ID-cGAN & 26.10 & 0.6465 & 0.09125 & 0.02913\\
\hline
 PAN & {\bf 28.32} & {\bf 0.7520} & {\bf 0.3372} & {\bf 0.1617} \\
\hline
\end{tabular}
\end{table}


\begin{figure*}[!t]
\captionsetup[subfigure]{labelformat=empty}
\centering
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{38_input.png}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{38.jpg}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{38.png}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{38_gt.png}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{10_input.png}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{10.jpg}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{10.png}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{10_gt.png}%
    \label{fig_first_case}
\end{subfigure}
\begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{8_input.png}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{8.jpg}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{8.png}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{8_gt.png}%
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{37_input.png}
    \caption{Input}
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{37.jpg}
    \caption{pix2pix-cGAN}
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{37.png}
    \caption{PAN}
    \label{fig_first_case}
\end{subfigure}
 \begin{subfigure}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{37_gt.png}
    \caption{Ground-truth}
    \label{fig_first_case}
\end{subfigure}
\caption{Comparison of some other tasks using the pix2pix-cGAN with the proposed PAN. In the first row, semantic labels are generated based on the real-world cityscpaes images. And, the second row reports the generated maps given the aerial photos as input.}
\label{maps}
\end{figure*}

\subsubsection{Pix2pix-cGAN}

Isola~\etal~\cite{isola2016image} utilized cGANs as a general-purpose solution to image-to-image translation (transformation) tasks. In their work, the per-pixel loss (using L1 norm) and Patch-cGANs loss are employed to solve a serials of image-to-image transformation tasks, such as translating the object edges to its photos, semantic labels to scene images, gray images to color images, \etc. The image-to-image transformation tasks performed by pix2pix-cGAN can also be solved by the proposed PAN. Here, we implemented some of them and compared with pix2pix-cGAN. 

Firstly, we attempted to translate the semantic labels to cityscapes images. 
Unlike the image segmentation problems, this inverse translation is an ill-posed problem and image transformation network has to learn prior knowledge from the training data. As shown in Fig.~\ref{cityscapes2}, given semantic labels as input images, we listed the transformed cityscapes images of pix2pix-cGAN, PAN and the corresponding ground-truth on the rightside. From the comparison, we found the proposed PAN captured more details with less deformation, which led the synthetic images are looked more realistic. Moreover, the quantitative comparison in Table~\ref{table:pix2pix} also indicated that the PAN can achieve much better performance.

Generating real-world objects from corresponding edges is also one kind of image-to-image transformation task. Based on the dataset provided by~\cite{isola2016image}, we trained the PAN to translate edges to object photos, and compared its performance with that of pix2pix-cGAN. Given edges as input, Fig.~\ref{edges} presented shoes and handbags synthesized by pix2pix-cGAN and PAN. At the same time, the quantitative results over the test set were shown in the Table~\ref{table:pix2pix}. Observing the generated object photos, we think that both pix2pix-cGAN and PAN achieved promising performance, yet it's hard to tell which one is better. Quantitative results are also very close, PAN performed slightly inferior to pix2pix-cGAN on the PSNR measurement, yet superior on other quantitative measurements.

In addition, we compared PAN with pix2pix-cGAN on tasks of generating semantic labels from cityscapes photos, and generating maps from the aerial photos. Some example images generated using PAN and pix2pix-cGAN and their corresponding quantitative results were shown in Fig.~\ref{maps} and Table~\ref{table:pix2pix}, respectively. To perform these two tasks, the image-to-image transformation models are asked to capture the semantic information from the input image, and synthesize the corresponding transformed images. Since pix2pix-cGAN employed the per-pixel and generative adversarial losses to training their model, it may hard to capture perceptual infromation from the input image, which causes that their results are poor, especially on transforming the arial photos to maps. However, we can observe that the proposed PAN can still achieved promising performance on these tasks. These experiments showed that the proposed PAN can also effectively extract the perceptual information from the input image.

\section{Conclusion}
\label{sec:conclusion}
In this paper, we proposed the perceptual adversarial networks (PAN) for image-to-image transformation tasks. As a generic framework of learning mapping relationship between paired images, the PAN combines the generative adversrial loss and the proposed perceptual adversarial loss as a novel training loss function. According to this loss function, a discriminative network $D$ are trained to continually and automatically explore the discrepancy between the transformed images and the corresponding ground-truth images. Simultaneously, an image transformation network $T$ is trained to narrow the discrepancy explored by the discriminative network $D$. Through the adversarial training process, these two networks are updated alternately. Finally, experimental results on several image-to-image transformation tasks demonstrated that the proposed PAN framework is effective and promising for practical image-to-image transformation applications. 

\bibliographystyle{abbrv}
\bibliography{main}

\end{document}




