\documentclass{article} % For LaTeX2e
\usepackage{iclr2019_conference,times}
\usepackage[colorlinks,
            linkcolor=black,
            anchorcolor=black,
            citecolor=black,
            urlcolor=black
            ]{hyperref}


\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{soul}
\usepackage{wrapfig}
\usepackage{color}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{tikz}
    \usetikzlibrary{fit,positioning,calc}
    \usetikzlibrary{patterns,arrows}
\usepackage{upgreek}
\usepackage{mathrsfs}
\usepackage{amsfonts,amssymb,bm}
\usepackage{graphicx}
    \graphicspath{{./}{figures/}}
\usepackage{multicol}
\usepackage{times}
\usepackage{mdwlist}
\usepackage{amssymb}
\usepackage[linesnumbered]{algorithm2e}
\usepackage{subfigure}
\usepackage{comment}
\usepackage{hyperref}
\usepackage{float}
\usepackage{array}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{url}
\usepackage{soul}
\usepackage{adjustbox}
\definecolor{Xue_color}{rgb}{0,0,1}
\definecolor{Shaokai}{rgb}{0,0.5,0.5}
\definecolor{Tianyun_color}{rgb}{0.5,0.5,0}
\usepackage{tablefootnote}
\usepackage{adjustbox}
\usepackage{tablefootnote}
\usepackage[para,online,flushleft]{threeparttable}


\title{Progressive Weight Pruning of Deep Neural Networks using ADMM}




\author{\bf{Shaokai Ye$^{1*}$, Tianyun Zhang$^{1*}$,  Kaiqi Zhang$^{1*}$, Jiayu Li$^1$, Kaidi Xu$^2$, Yunfei Yang, Fuxun Yu$^3$,}\\ \bf{Jian Tang$^1$, Makan Fardad$^1$, Sijia Liu$^4$, Xiang Chen$^3$, Xue Lin$^2$ \& Yanzhi Wang$^2$}\\ 
1. Syracuse University, USA \\
\texttt{\{sye106,tzhan120,kzhang17,jli221,jtang02,makan\}@syr.edu} \\
2. Northeastern University, USA  \\ 
3. George Mason University, USA\\
4. MIT-IBM Watson AI Lab, IBM Research\\
$^*$Equal Contribution\\
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}


\maketitle
\begin{abstract}
Deep neural networks (DNNs), although achieving human-level performance in many domains, have very large model size that hinders their broader applications on edge computing devices.
Extensive research work has been conducted on DNN model compression or pruning.
However, most of the previous work has taken heuristic approaches.
This work proposes a progressive weight pruning approach based on ADMM (Alternating Direction Method of Multipliers), a powerful technique to deal with non-convex optimization problems with potentially combinatorial constraints.
Motivated by dynamic programming, the proposed method reaches extremely high pruning rate by using partial prunings with moderate pruning rates.
Therefore, it resolves the accuracy degradation and long convergence time problems when pursuing extremely high pruning ratios.
It achieves up to 34$\times$ pruning rate for ImageNet data set and 167$\times$ pruning rate for MNIST data set, significantly higher than those reached by existing work in the literature.
Under the same number of epochs, the proposed method also achieves faster convergence and higher compression rates. The codes and pruned DNN models are avilable in the link: \url{bit.ly/2zxdlss	}.
\end{abstract}











\section{Introduction}

Deep neural networks (DNNs) have achieved human-level performance in many application domains such as image classification \citep{krizhevsky2012imagenet}, object recognition \citep{lecun1998,he2016deep}, natural language processing \citep{hinton2012deep,dahl2012context}, etc.
At the same time, the networks are growing deeper and bigger for higher classification/recognition performance (i.e., accuracy) \citep{simonyan2015very}.
However, the very large DNN model size increases the computation time of the inference phase.
To make matters worse, the large model size hinders DNN' deployments on edge computing, which provides the ubiquitous application scenarios of DNNs besides cloud computing applications.

As a result, extensive research efforts have been devoted to \emph{DNN model compression}, in which DNN \emph{weight pruning} is a representative technique.
\citet{han2015learning} is the first work to present the DNN weight pruning method, which prunes the weights with small magnitudes and retrains the network model, heuristically and iteratively. 
After that, more sophisticated heuristics have been proposed for DNN weight pruning, e.g., incorporating both weight pruning and growing \citep{guo2016dynamic}, $L_1$ regularization \citep{wen2016learning}, and genetic algorithms \citep{dai2017nest}.
Other improvement directions of weight pruning include trading-off between accuracy and compression rate, e.g., \emph{energy-aware pruning} \citep{yang2017designing}, incorporating regularity, e.g., \emph{channel pruning} \citep{he2017channel}, and \emph{structured sparsity learning} \citep{wen2016learning}.

 

While the weight pruning technique explores the redundancy in the number of weights of a network model, there are other sources of redundancy in a DNN model. For example, the weight quantization \citep{leng2017extremely,park2017weighted,zhou2017incremental,lin2016fixed,wu2016quantized,rastegari2016xnor,hubara2016binarized,courbariaux2015binaryconnect} and clustering \citep{zhu2017trained,han2015deep} techniques explore the redundancy in the number of bits for weight representation.
The activation pruning technique \citep{jung2018joint,sharify2018loom} leverages the redundancy in the intermediate results. %\textcolor{Xue_color}{Any other type of techniques leveraging other sources of redundancy?}
While our work focuses on weight pruning as the major DNN model compression technique, it is orthogonal to the other model compression techniques and might be integrated under a single ADMM-based framework for achieving more compact network models.

The majority of prior work on DNN weight pruning take heuristic approaches to reduce the number of weights as much as possible, while preserving the expressive power of the DNN model.
Then one may ask, how can we push for the utmost sparsity of the DNN model without hurting accuracy? and what is the maximum compression rate we can achieve by weight pruning?
Towards this end, \citet{zhang2018systematic} took a tentative step by proposing an optimization-based approach that leverages ADMM (Alternating Direction Method of Multipliers), a powerful technique to deal with non-convex optimization problems with potentially combinatorial constraints. This direct ADMM-based weight pruning technique can be perceived as a smart DNN regularization where the regularization target is dynamically changed in each ADMM iteration. As a result it achieves higher compression (pruning) rate than heuristic methods.

Inspired by \citet{zhang2018systematic}, in this paper we propose a progressive weight pruning approach that incorporates both an ADMM-based algorithm and masked retraining, and takes a progressive means targeting at extremely high compression (pruning) rates with negligible accuracy loss. The contributions of this work are summarized as follows:
\begin{itemize}
\item We make a key observation that when pursuing extremely high compression rates (say 150$\times$ for LeNet-5 or 30$\times$ for AlexNet), the direct ADMM-based weight pruning approach \citep{zhang2018systematic} cannot produce exactly sparse models upon convergence, in that many weights to be pruned are close to zero but not exactly zero. Certain accuracy degradation will result from this phenomenon if we simply set these weights to zero.
\item We propose and implement the progressive weight pruning paradigm that reaches an extremely high compression rate through multiple partial prunings with progressive pruning rates. This progressive approach, motivated by dynamic programming, helps to mitigate the long convergence time by direct ADMM pruning.
\item Extensive experiments are performed by comparing with many state-of-the-art \textcolor{black}{weight pruning} approaches and the highest compression rates in the literature are achieved by our progressive weight pruning framework, while the loss of accuracy is kept negligible. Our method achieves up to 34$\times$ pruning rate for the ImageNet data set and 167$\times$ pruning rate for the MNIST data set, with virtually no accuracy loss. Under the same number of epochs, the proposed method achieves notably faster convergence and higher compression rates than prior iterative pruning and direct ADMM pruning methods.
\end{itemize}

We provide codes (both Caffe and TensorFlow versions) and pruned DNN models (both for the ImageNet and MNIST data sets) in the link: \url{bit.ly/2zxdlss	}.


 


\section{The Progressive Weight Pruning Framework of DNNs}

This section introduces the proposed progressive weight pruning framework using ADMM. 
Section \ref{sec:overall} describes the overall framework. 
Section \ref{sec:regularization} discusses the ADMM-based algorithm for DNN weight pruning \citep{zhang2018systematic}, which we will improve and incorporate into the progressive weight pruning framework.
Section \ref{sec:retrain} proposes a direct improvement of masked retraining to restore accuracy.
Section \ref{sec:proposed} provides the motivations and details of the proposed progressive weight pruning framework.


\subsection{The Overall Framework}\label{sec:overall}

\begin{figure}[h]
\begin{center}
\includegraphics[scale = 0.4]{cropped-horizontal-pipeline.pdf}
\end{center}
\caption{The overall progressive weight pruning framework including masked ADMM-based algorithm, thresholding mask updating, and masked retraining steps.}\label{fig:framework}
\end{figure}

The overall framework of progressive weight pruning is shown in Figure \ref{fig:framework}. 
It applies the ADMM-based pruning algorithm on a pre-trained (uncompressed) network model.
Then it defines thresholding masks, with which the weights smaller than thresholds are forced to be zero.
To restore accuracy, the masked retraining step is applied, that only updates nonzero weights specified by the thresholding masks.
The ADMM-based algorithm, thresholding mask updating, and masked retaining steps are performed for several rounds, and each round is considered as a partial pruning, progressively pushing for the utmost of the DNN model pruning. 
Note that in our progressive weight pruning framework, we change the ADMM-based algorithm into a ``masked'' version that reuses the partially pruned model by masking the gradients of the pruned weights, thereby preventing them from recovering to nonzero weights and thus accelerating convergence.











\subsection{ADMM-based Pruning Algorithm}\label{sec:regularization}

Our ADMM-based pruning algorithm takes a pre-trained network as the input and outputs a pruned network model satisfying some sparsity constraints.
Consider an $N$-layer DNN, where the collection of weights in the $i$-th (convolutional or fully-connected) layer is denoted by ${\bf{W}}_{i}$ and the collection of biases in the $i$-th layer is denoted by ${\bf{b}}_{i}$. The loss function associated with the DNN is denoted by $f \big( \{{\bf{W}}_{i}\}_{i=1}^N, \{{\bf{b}}_{i} \}_{i=1}^N \big)$. 

The DNN weight pruning problem can be formulated as:
\begin{equation}
\begin{aligned}
& \underset{ \{{\bf{W}}_{i}\},\{{\bf{b}}_{i} \}}{\text{minimize}}
& & f \big( \{{\bf{W}}_{i} \}, \{{\bf{b}}_{i} \} \big),
\\ & \text{subject to}
& & {\bf{W}}_{i}\in {\bf{S}}_{i}, \; i = 1, \ldots, N,
\end{aligned}
\end{equation}
where ${\bf{S}}_{i}= \{{\bf{W}}_i\mid \mathrm{card}({\bf{W}}_i)\le l_{i}  \}, i=1,\dots,N$ and $l_{i}$ is the desired number of weights in the $i$-th layer of the DNN. It is clear that ${\bf{S}}_{1},\dots,{\bf{S}}_{N}$ are nonconvex sets, and it is in general difficult to solve optimization problems with nonconvex constraints. 

The problem can be equivalently rewritten in a format without constraints, namely
\begin{equation}
 \underset{ \{{\bf{W}}_{i}\},\{{\bf{b}}_{i} \}}{\text{minimize}}
\ \ \ f \big( \{{\bf{W}}_{i} \}, \{{\bf{b}}_{i} \} \big)+\sum_{i=1}^{N} g_{i}({\bf{W}}_{i}), 
\end{equation}
where $g_{i}(\cdot)$ is the indicator function of ${\bf{S}}_{i}$, i.e.,
\begin{eqnarray}g_{i}({\bf{W}}_{i})=
\begin{cases}
 0 & \text { if } \mathrm{card}({\bf{W}}_{i})\le l_{i}, \\ 
 +\infty & \text { otherwise. }
\end{cases}
\end{eqnarray}

The ADMM technique \citep{boyd2011} can be applied to solve the weight pruning problem by formulating it as:
\begin{equation*}
\begin{aligned}
& \underset{ \{{\bf{W}}_{i}\},\{{\bf{b}}_{i} \}}{\text{minimize}}
& & f \big( \{{\bf{W}}_{i} \}, \{{\bf{b}}_{i} \} \big)+\sum_{i=1}^{N} g_{i}({\bf{Z}}_{i}),
\\ & \text{subject to}
& & {\bf{W}}_{i}={\bf{Z}}_{i}, \; i = 1, \ldots, N.
\end{aligned}
\end{equation*}

Through the augmented Lagrangian, the ADMM technique decomposes the weight pruning problem into two subproblems, and solving them iteratively until convergence. The first subproblem is:
\begin{equation}
\label{4}
 \underset{ \{{\bf{W}}_{i}\},\{{\bf{b}}_{i} \}}{\text{minimize}}
\ \ \ f \big( \{{\bf{W}}_{i} \}, \{{\bf{b}}_{i} \} \big)+\sum_{i=1}^{N} \frac{\rho_{i}}{2}  \| {\bf{W}}_{i}-{\bf{Z}}_{i}^{k}+{\bf{U}}_{i}^{k} \|_{F}^{2}. \\
\end{equation}
This subproblem is equivalent to the original DNN training plus an $L_2$ regularization term, and can be effectively solved using stochastic gradient descent with the same complexity as the original DNN training.
Note that we cannot prove global optimality of the solution to subproblem (\ref{4}), just as we cannot prove optimality of the solution to the original DNN training problem.

On the other hand, the second subproblem is:
\begin{equation*}
 \underset{ \{{\bf{Z}}_{i} \}}{\text{minimize}}
\ \ \ \sum_{i=1}^{N} g_{i}({\bf{Z}}_{i})+\sum_{i=1}^{N} \frac{\rho_{i}}{2} \| {\bf{W}}_{i}^{k+1}-{\bf{Z}}_{i}+{\bf{U}}_{i}^{k} \|_{F}^{2}. \\
\end{equation*}
Since $g_{i}(\cdot)$ is the indicator function of the set ${\bf{S}}_{i}$, the globally optimal solution to this subproblem can be explicitly derived as in \citet{boyd2011}:
\begin{equation}
\label{5}
  {\bf{Z}}_{i}^{k+1} = {{\bf{\Pi}}_{{\bf{S}}_{i}}}({\bf{W}}_{i}^{k+1}+{\bf{U}}_{i}^{k}),
\end{equation}
where ${{\bf{\Pi}}_{{\bf{S}}_{i}}(\cdot)}$ denotes the Euclidean projection onto the set ${\bf{S}}_{i}$. Note that ${\bf{S}}_{i}$ is a nonconvex set, and computing the projection onto a nonconvex set is a difficult problem in general. However, the special structure of ${\bf{S}}_{i}= \{{\bf{W}}_i\mid \mathrm{card}({\bf{W}}_i)\le l_{i} \}$ allows us to express this Euclidean projection analytically. Namely, the optimal solution (\ref{5}) is to keep the $l_{i}$ largest elements of ${\bf{W}}_{i}^{k+1}+{\bf{U}}_{i}^{k}$ and set the rest to zero \citep{boyd2011}.  

Finally, we update the dual variable ${\bf{U}}_{i}$ as ${\bf{U}}_{i}^{k+1}={\bf{U}}_{i}^{k}+{\bf{W}}_{i}^{k+1}-{\bf{Z}}_{i}^{k+1}$. This concludes one iteration of the ADMM. 

In the context of deep learning, the ADMM-based algorithm for DNN weight pruning can be understood as a smart DNN regularization technique (see Eqn. (\ref{4})), in which the regularization target (in the $L_2$ regularization term) is dynamically updated in each ADMM iteration. This is one reason that the ADMM-based algorithm for weight pruning achieves higher performance than heuristic methods and other regularization techniques \citep{wen2016learning}, and the Projected Gradient Descent technique \citep{zhang2018learning}.

\subsection{Masked Retraining Step}\label{sec:retrain}

Applying the ADMM-based pruning algorithm alone has limitations for high compression rates. 
At convergence, the pruned DNN model will not be exactly sparse, in that many weights to be pruned will be close to zero instead of being exactly equal to zero. This is due to the non-convexity of Subproblem 1 in the ADMM-based algorithm. Certain accuracy degradation will result from this phenomenon if we simply set those weights to zero. This accuracy degradation will be non-negligible for high compression rates.

Instead of waiting for the full convergence of the ADMM-based algorithm, a masked retraining step is proposed, that (i) terminates the ADMM iterations early, (ii) keeps the $l_i$ largest weights (in terms of magnitude) and sets the other weights to zero, and (iii) performs retraining on the nonzero weights (with zero weights masked) using the training data set.
More specifically, masks are applied to gradients of zero weights, preventing them from updating.
Essentially, the ADMM-based algorithm sets a good starting point, and then the masked retraining step encourages the remaining nonzero weights to learn to recover classification accuracies.

Integrating masked retraining after the ADMM-based algorithm, a good compression rate can be achieved with reasonable training time. For example, we can achieve 21$\times$ model pruning rate without accuracy loss for AlexNet using a total of 417 epochs, much faster than the iterative weight pruning method of \citet{han2015deep}, which achieves 9$\times$ pruning rate in a total of 960 epochs. When translating into training time, our time of training is 72 hours using single NVIDIA 1080Ti GPU, whereas the reported training time in \citet{han2015deep} is 173 hours.
 


\subsection{Progressive Weight Pruning}\label{sec:proposed}

Although the ADMM-based pruning algorithm in Section \ref{sec:regularization} and the masked retraining step in Section \ref{sec:retrain} together can achieve the state-of-the-art model compression (pruning) rates for many network models, we find limitations to this  approach at extremely high pruning rates, for example at 150$\times$ pruning rate for LeNet-5 or 30$\times$ pruning rate for AlexNet. 

Specifically, with a very high weight pruning rate, it takes a relatively long time for the ADMM-based algorithm to choose which weights to prune. 
For example, it is difficult for the ADMM-based algorithm to converge for 30$\times$ pruning rate on AlexNet but easy for 21$\times$ pruning rate.

To overcome this difficulty, we propose the progressive weight pruning method. 
This technique is motivated by dynamic programming, achieving a high weight pruning rate by using partial pruning models with moderate pruning rates. 
We use Figure \ref{fig:progressive} as an example to show the process used to achieve 30$\times$ weight pruning rate in AlexNet without accuracy loss. 
In Figure \ref{fig:progressive} (a), we start from three partial pruning models, with 15$\times$, 18$\times$, and 21$\times$ pruning rates, which can be directly derived from the uncompressed DNN model via the ADMM-based algorithm with masked retraining. 
To achieve 24$\times$ weight pruning rate, we start from these three models and check which gives the highest accuracy (suppose it is the 15$\times$ one). Because we start from partial pruning models, the convergence rate is fast. 
We then replace 15$\times$ partial pruning model by 24$\times$ model to derive the 27$\times$ model, see Figure \ref{fig:progressive} (b).
In this way we always maintain three partial results and limit the total searching time.
Suppose this time the 18$\times$ pruning model results in the highest accuracy and then we replace it with the 27$\times$ one.
Finally, in Figure \ref{fig:progressive} (c), we find 24$\times$ model gives highest accuracy to reach 30$\times$ pruning rate.



\begin{figure}[h]
\begin{center}
\includegraphics[scale = 0.25]{cropped-dynamic-feeling.pdf}
\end{center}
\caption{Illustration of the progressive weight pruning algorithm.}
\label{fig:progressive}
\end{figure}

Note that during progressive weight pruning, to leverage the partial pruning models, we use ``masked'' training when we reuse the partial pruning models in the ADMM-based algorithm.
Specifically, it masks the gradients of the already pruned weights to prevent them from recovering to nonzero values.
In this way, the algorithm is encouraged to focus on pruning nonzero weights.




\begin{figure}[h]
\begin{center}
\includegraphics[scale = 0.38]{cropped-retrain-loss.pdf}
\end{center}
\caption{The value of loss function associated with AlexNet versus retraining steps for
(a) ADMM-based algorithm plus masked retraining and (b) proposed progressive pruning algorithm. }\label{fig:comparison}
\end{figure} 






Figure \ref{fig:comparison} demonstrates the value of the loss function associated with AlexNet versus retraining steps for (a) the ADMM-based algorithm with masked retraining and (b) the proposed progressive pruning. Both methods target at 30$\times$ pruning rate. The ADMM-based algorithm with masked retraining performs one-round pruning to 30$\times$, while the proposed progressive pruning performs multiple partial prunings (15$\times$ to 24$\times$ to 30$\times$). We apply the same total number of iterations of both methods for fair comparison. The total number of epochs will be 730 for both cases, which is still lower than 960 epochs in \citep{han2015deep}. We can observe in Figure \ref{fig:comparison} that by using multiple partial prunings we can achieve faster convergence with lower loss. 

























































\section{Experimental Results and Discussions}

\begin{table*}[h]
\centering
\caption{Comparisons of weight pruning results on AlexNet for ImageNet data set.}\label{table:AlexNet}
\begin{tabular}{p{7.5cm}p{2cm}p{1.5cm}p{0.8cm}}
\hline
\hline
Method & Top-5 Acc. & No. Para. & Rate \\ 
\hline
\shortstack[l]{Uncompressed} & 80.27\% & 61.0M & 1$\times$ \\ \hline
\shortstack[l]{Network Pruning \citep{han2015learning}} & 80.3\% & 6.7M & 9$\times$ \\ \hline
\shortstack[l]{Optimal Brain Surgeon \citep{dong2017learning}} & 80.0\% & 6.7M & 9.1$\times$ \\ \hline
\shortstack[l]{Low Rank and Sparse Decomposition \citep{yu2017compressing}} & 80.3\% & 6.1M & 10$\times$ \\ \hline
\shortstack[l]{Fine-Grained Pruning \citep{mao2017exploring}} & 80.4\% & 5.1M & 11.9$\times$ \\ \hline
\shortstack[l]{NeST \citep{dai2017nest}} & 80.2\% & 3.9M & 15.7$\times$ \\ \hline
\shortstack[l]{Dynamic Surgery \citep{guo2016dynamic}} & 80.0\% & 3.4M & 17.7$\times$ \\ \hline
\shortstack[l]{ADMM Pruning \citep{zhang2018systematic}} & 80.2\% & 2.9M & 21$\times$ \\ \hline{\bf{Progressive Weight Pruning}} (BVLC Model) & 80.2\% & 2.02M & 30$\times$ \\ \hline{\bf{Progressive Weight Pruning}} (BVLC Model) & 80.0\% & 1.97M & 31$\times$ \\ \hline{\bf{Progressive Weight Pruning}} (CaffeNet Model) & 80.2\% & 2.02M & 30$\times$ \\ \hline{\bf{Progressive Weight Pruning}} (CaffeNet Model) & 80.0\% & 1.97M & 31$\times$ \\
\hline
\hline
\end{tabular}
\end{table*}


\begin{table*}[h]
\centering
\caption{Top-5 accuracy of direct ADMM pruning \citep{zhang2018systematic} and progressive pruning at different pruning rates on AlexNet for ImageNet data set.}\label{table:AlexNet_2}
\begin{tabular}{p{2cm}p{4cm}p{4cm}}
\hline
\hline
Pruning Rate & Direct ADMM Pruning & Progressive Weight Pruning\\ \hline
18$\times$  & 80.3\% &  80.9\%\\ \hline
21$\times$  & 80.2\% &  80.8\%\\ \hline
30$\times$  & 76.7\% &  80.2\%\\ \hline \hline
\end{tabular}
\end{table*}



\begin{table*}[h]
\centering
\caption{Comparisons of weight pruning results on VGG-16 for ImageNet data set.}\label{table:VGG-16}
\begin{tabular}{p{7.5cm}p{2cm}p{1.5cm}p{0.8cm}}
\hline
\hline
Method & Top-5 Acc. & No. Para. & Rate \\ 
\hline
Uncompressed & 88.7\% & 138M & 1$\times$ \\ \hline
\shortstack[l]{Network Pruning \citep{han2015learning}}  & 89.1\% & 10.6M & 13$\times$ \\ \hline
\shortstack[l]{Optimal Brain Surgeon \citep{dong2017learning}}  & 89.0\% & 10.3M & 13.3$\times$ \\ \hline
\shortstack[l]{Low Rank and Sparse Decomposition \citep{yu2017compressing}}  & 89.1\% & 9.2M & 15$\times$ \\ \hline
\shortstack[l]{ADMM Pruning \citep{zhang2018systematic}}  & 88.7\% & 7.26M & 19.5$\times$ \\ \hline
\bf{Progressive Weight Pruning}  & 88.7\% & 4.6M & 30$\times$ \\ \hline
\bf{Progressive Weight Pruning}  & 88.2\% & 4.1M & 34$\times$ \\ \hline \hline
\end{tabular}
\end{table*}






\begin{table}[h]
\centering
\caption{Comparisons of weight pruning results on ResNet-50 for ImageNet data set.}\label{table:ResNet-50}
\begin{adjustbox}{max width=1\textwidth }
\begin{threeparttable}
\begin{tabular}{p{6cm}p{2cm}p{1.5cm}p{0.8cm}}
\hline
\hline
Method & Top-5 Acc. & No. Para. & Rate \\ 
\hline
Uncompressed & 92.4\% & 25.6M & 1$\times$ \\ \hline
\shortstack[l]{Fine-grained Pruning \citep{mao2017exploring}}  & 92.3\% & 9.8M & 2.6$\times$ \\ 
\hline
\bf{Progressive Weight Pruning}  & 92.0\% & 2.8M & 9.16$\times$ \\ \hline
\bf{Progressive Weight Pruning}  & 91.5\% & 1.47M & 17.43$\times$ \\ \hline
\hline 
\hline
\end{tabular}
\begin{tablenotes}

\end{tablenotes}
\end{threeparttable}
\end{adjustbox}
\end{table}


\begin{table*}[h]
\centering
\caption{Comparisons of weight pruning results on LeNet-5 for MNIST data set.}\label{table:LeNet-5}
\begin{tabular}{p{6.5cm}p{1.5cm}p{1.5cm}p{0.8cm}}
\hline
\hline
Method & Accuracy & No. Para. & Rate \\ 
\hline
Uncompressed & 99.2\% & 431K & 1$\times$ \\ \hline
\shortstack[l]{Network Pruning \citep{han2015learning}}  & 99.2\% & 36K & 12.5$\times$ \\ \hline
\shortstack[l]{ADMM Pruning \citep{zhang2018systematic}}  & 99.2\% & 6.05K & 71.2$\times$ \\ \hline
\shortstack[l]{Optimal Brain Surgeon \citep{dong2017learning}}  & 98.3\% & 3.88K & 111$\times$ \\ \hline
\bf{Progressive Weight Pruning}  & 99.0\% & 2.58K & 167$\times$ \\ \hline \hline
\end{tabular}
\end{table*}




\begin{table*}[h]
\centering
\caption{Comparisons of weight pruning with quantization results on LeNet-5 for MNIST data set.}\label{table:LeNetwQuantization}
\begin{tabular}{p{2.4cm}p{0.7cm}p{0.8cm}p{0.7cm}p{1.7cm}p{2.4cm}p{2.4cm}}
\hline
\hline
Method &  Acc. Loss  & No. Para. & Conv \quad No. bits & FC No. bits & Total data size /Compress rate & Total size w. index /Compress rate \\ 
\hline
Uncompressed & 0.0\% &  430.5K   & 32 & 32 & 1.7MB  & 1.7MB \\
\hline
Iterative pruning \citep{han2015deep} & 0.1\% & 35.8K  & 8 & 5 & 24.2KB / 70.2$\times$ & 52.1KB / 33$\times$\\
\hline
Learning to share \citep{ullrich2017soft} & 0.2\% & --  & -- & -- & -- & 10.4KB / 162$\times$\\
\hline
\bf{Our Method} & 0.2\% & 2.57K  & 3 & 2 (3 for output layer) & 0.89KB / \bf{1,910$\times$} &  2.73KB / 623$\times$\\
\hline
\hline
\end{tabular}
\end{table*}





\subsection{Experimental Setups}
We evaluate the proposed ADMM-based progressive weight pruning framework on the ImageNet ILSVRC-2012 data set \citep{deng2009imagenet} and MNIST data set \citep{lecun1998}. 
We also use DNN weight pruning results from many previous works for comparison.
For ImageNet data set, we test on a variety of DNN models including AlexNet (both BAIR/BVLC model and CaffeNet model), VGG-16, and ResNet-50 models. We test on LeNet-5 model for MNIST data set. The accuracies of the uncompressed DNN models are reported in the tables for reference.

We implement our codes in Caffe \citep{jia2014caffe}.
Experiments are tested on 12 Nvidia GTX 1080Ti GPUs and 12 Tesla P100 GPUs.
As the key parameters in ADMM-based weight pruning, we set the ADMM penalty parameter $\rho$ to $1.5\times 10^{-3}$ for the masked ADMM-based algorithm.
When targeting at a high weight pruning rate, we change it to $3.0\times 10^{-3}$ for higher performance.
To eliminate the already pruned weights in partial pruning results from the masked ADMM-based algorithm, $\rho_i$ is forced to be zero if no more pruning is performed for a specific layer $i$.
We use an initial learning rate of $1.0\times 10^{-3}$ for the masked ADMM-based algorithm and an initial learning rate of $1.0\times 10^{-2}$ for masked retraining.

We provide the codes (both Caffe and TensorFlow versions) and all pruned DNN models (both for ImageNet and MNIST data sets) in the link: \url{bit.ly/2zxdlss	}.




\subsection{Comparison Results and Discussions}

Table \ref{table:AlexNet} presents the weight pruning comparison results on the AlexNet model between our proposed method and prior works. Our weight pruning results clearly outperform the prior work, in that we can achieve 31$\times$ weight reduction rate without loss of accuracy. 
Our progressive weight pruning also outperforms the direct ADMM weight pruning in \citet{zhang2018systematic} that achieves 21$\times$ compression rate. Also the CaffeNet model results in slightly higher accuracy compared with the BVLC AlexNet model. 
Table \ref{table:AlexNet_2} presents more comparison results with the direct ADMM pruning. It can be observed that (i) with the same compression rate, our progressive weight pruning outperforms the direct pruning in accuracy; (ii) the direct ADMM weight pruning suffers from significant accuracy drop with high compression rate (say 30$\times$ for AlexNet); and (iii) for a good compression rate (18$\times$ and 21$\times$), our progressive weight pruning technique can even achieve higher accuracy compared with the original, uncompressed DNN model.

Table \ref{table:VGG-16}, Table \ref{table:ResNet-50}, and Table \ref{table:LeNet-5} present the comparison results on the VGG-16, ResNet-50, and LeNet-5 (for MNIST) models, respectively. These weight pruning results we achieved clearly outperform the prior work, consistently achieving the highest sparsities in the benchmark DNN models. On the VGG-16 model, we achieve 30$\times$ weight pruning with comparable accuracy with prior works, while the highest pruning rate in prior work is 19.5$\times$. We also achieve 34$\times$ weight pruning with minor accuracy loss. For ResNet-50 model, we have tested 17.43$\times$ weight pruning rate and confirmed minor accuracy loss. In fact, there is limited prior work on ResNet weight pruning for ImageNet data set, due to (i) the difficulty in weight pruning since ResNet mainly consists of convolutional layers, and (ii) the slow training speed of ResNet. Our method, on the other hand, achieves a relatively high training speed, thereby allowing for the weight pruning testing on different large-scale DNN models.

For LeNet-5 model compression, we achieve 167$\times$ weight reduction with almost no accuracy loss, which is much higher than prior work under the same accuracy. The prior work Optimal Brain Surgeon \citep{dong2017learning} also achieves a high pruning rate of 111$\times$, but suffers from accuracy drop of around 1\% (already non-negligible for MNIST data set).

For other types of DNN models, we have tested the proposed method on the facial recognition application on two representative DNN models \citep{cvpr2016_gazecapture,JostineHo2016}. We demonstrate over 10$\times$ weight pruning rate with 0.2\% and 0.4\% accuracy loss, respectively, compared with the original DNN models.

In summary, the experimental results demonstrate that our framework applies to a broad set of representative DNN models and consistently outperforms the prior work. It also applies to the DNN models that consist of mainly convolutional layers, which are different with weight pruning using prior methods. These promising results will significantly contribute to the energy-efficient implementation of DNNs in mobile and embedded systems, and on various hardware platforms.

Finally, some recent work have focused on the simultaneous weight pruning and weight quantization, as both will contribute to the model storage compression of DNNs. Weight pruning and quantization can be unified under the ADMM framework, and we demonstrate the comparison results in Table \ref{table:LeNetwQuantization} using the LeNet-5 model as illustrative example. As can be observed in the table, we can simultaneously achieve 167$\times$ weight reduction and use 2-bit for fully-connected layer weight quantization and 3-bit for convolutional layer weight quantization. The overall accuracy is 99.0\%. When we focus on the weight data storage, the compression rate is unprecendented 1,910$\times$ compared with the original DNN model with floating point representation. When indices (required in weight pruning) are accounted for, the overall compression rate is 623$\times$, which is still much higher than the prior work. It is interesting to observe that the amount of storage for indices is even higher than that for actual weight data.















\section{Related Work on DNN Weight Pruning/Model Compression}

The pioneering work by \citet{han2015learning} shows that DNN weights could be effectively pruned while maintaining the same accuracy after iterative retraining, which gives 9$\times$ pruning in AlexNet and 13$\times$ pruning in VGG-16. 
However, higher compression rates could hardly be obtained as the method remains highly heuristic and time-consuming. Extensions of this initial work apply algorithm-level improvements. 
For example, \citet{guo2016dynamic} adopts a method that performs both pruning and growing of DNN weights, achieving 17.7$\times$ pruning rate in AlexNet.
\citet{dai2017nest} applies the evolutionary algorithm that prunes and grows weights in a random manner, achieving 15.7$\times$ pruning rate in AlexNet. The Optimal Brain Surgeon technique has been proposed \citet{dong2017learning}, achieving minor improvement in AlexNet/VGGNet but a good pruning ratio of 111$\times$ with less than 1\% accuracy degradation in MNIST. The $L_1$ regularization method \citep{wen2016learning} achieves 6$\times$ weight pruning in the convolutional layers of CaffeNet. \citet{mao2017exploring} uses different versions of DNN weight pruning methods, from the fine-grained pruning to channel-wise regular pruning methods.
Recently, the direct ADMM weight pruning algorithm has been developed \citep{zhang2018systematic}, which is a systematic weight pruning framework and achieves state-of-the-art performance in multiple DNN models.

The above weight pruning methods result in irregularity in weight storage, in that indices are need to locate the next weight in sparse matrix representations. To mitigate the associated overheads, many recent work have proposed to incorporate regularity and structure in the weight pruning framework. Representative work include the channel pruning methods \citep{he2017channel,mao2017exploring}, and row/column weight pruning method \citep{wen2016learning}. The latter has been extended in a systematic way in \citet{zhang2018adam}. These work can partially mitigate the overheads in GPU, embedded systems, and hardware implementations and result in higher acceleration in these platforms, but typically cannot result in higher pruning ratio than unrestricted pruning. We will investigate the application of progressive weight pruning to the regular/structured pruning as future work.



\section{Conclusion}
This work proposes a progressive weight pruning approach based on ADMM, a powerful technique to deal with non-convex optimization problems with potentially combinatorial constraints.
Motivated by dynamic programming, the proposed method reaches extremely high pruning rates by using partial prunings, with moderate pruning rates in each partial pruning step.
Therefore, it resolves the accuracy degradation and long convergence time problems when pursuing extremely high pruning ratios.
It achieves up to 34$\times$ pruning rate for the ImageNet data set and 167$\times$ pruning rate for the MNIST data set, significantly higher than those reached by work in the existing literature.
Under the same number of epochs, the proposed method also achieves faster convergence and higher compression rates.



























\subsubsection*{Acknowledgments}

Financial support from the National Science Foundation under
awards CAREER CMMI-1750531 and ECCS-1609916 is gratefully acknowledged.


\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Boyd et~al.(2011)Boyd, Parikh, Chu, Peleato, and Eckstein]{boyd2011}
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein.
\newblock Distributed optimization and statistical learning via the alternating
  direction method of multipliers.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  3\penalty0 (1):\penalty0 1--122, 2011.

\bibitem[Courbariaux et~al.(2015)Courbariaux, Bengio, and
  David]{courbariaux2015binaryconnect}
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David.
\newblock Binaryconnect: Training deep neural networks with binary weights
  during propagations.
\newblock In \emph{Advances in neural information processing systems}, pp.\  3123--3131, 2015.

\bibitem[Dahl et~al.(2012)Dahl, Yu, Deng, and Acero]{dahl2012context}
George~E Dahl, Dong Yu, Li~Deng, and Alex Acero.
\newblock Context-dependent pre-trained deep neural networks for
  large-vocabulary speech recognition.
\newblock \emph{IEEE Transactions on audio, speech, and language processing},
  20\penalty0 (1):\penalty0 30--42, 2012.

\bibitem[Dai et~al.(2017)Dai, Yin, and Jha]{dai2017nest}
Xiaoliang Dai, Hongxu Yin, and Niraj~K Jha.
\newblock Nest: a neural network synthesis tool based on a grow-and-prune
  paradigm.
\newblock \emph{arXiv preprint arXiv:1711.02017}, 2017.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\ 248--255, 2009.

\bibitem[Dong et~al.(2017)Dong, Chen, and Pan]{dong2017learning}
Xin Dong, Shangyu Chen, and Sinno Pan.
\newblock Learning to prune deep neural networks via layer-wise optimal brain
  surgeon.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\  4857--4867, 2017.

\bibitem[Guo et~al.(2016)Guo, Yao, and Chen]{guo2016dynamic}
Yiwen Guo, Anbang Yao, and Yurong Chen.
\newblock Dynamic network surgery for efficient dnns.
\newblock In \emph{Advances In Neural Information Processing Systems}, pp.\  1379--1387, 2016.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{han2015learning}
Song Han, Jeff Pool, John Tran, and William Dally.
\newblock Learning both weights and connections for efficient neural network.
\newblock In \emph{Advances in neural information processing systems}, pp.\  1135--1143, 2015.

\bibitem[Han et~al.(2016)Han, Mao, and Dally]{han2015deep}
Song Han, Huizi Mao, and William~J Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2016.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\ 770--778, 2016.

\bibitem[He et~al.(2017)He, Zhang, and Sun]{he2017channel}
Yihui He, Xiangyu Zhang, and Jian Sun.
\newblock Channel pruning for accelerating very deep neural networks.
\newblock In \emph{Computer Vision (ICCV), 2017 IEEE International Conference
  on}, pp.\ 1398--1406. IEEE, 2017.

\bibitem[Hinton et~al.(2012)Hinton, Deng, Yu, Dahl, Mohamed, Jaitly, Senior,
  Vanhoucke, Nguyen, Sainath, and Kingsbury]{hinton2012deep}
Geoffrey Hinton, Li~Deng, Dong Yu, George~E Dahl, Abdel-rahman Mohamed, Navdeep
  Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara~N Sainath, and
  Brian Kingsbury.
\newblock Deep neural networks for acoustic modeling in speech recognition: The
  shared views of four research groups.
\newblock \emph{IEEE Signal Processing Magazine}, 29\penalty0 (6):\penalty0
  82--97, 2012.

\bibitem[Ho(2016)]{JostineHo2016}
Jostine Ho.
\newblock mememoji.
\newblock \url{https://github.com/JostineHo/mememoji}, 2016.

\bibitem[Hubara et~al.(2016)Hubara, Courbariaux, Soudry, El-Yaniv, and
  Bengio]{hubara2016binarized}
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua
  Bengio.
\newblock Binarized neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\  4107--4115, 2016.

\bibitem[Jia et~al.(2014)Jia, Shelhamer, Donahue, Karayev, Long, Girshick,
  Guadarrama, and Darrell]{jia2014caffe}
Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross
  Girshick, Sergio Guadarrama, and Trevor Darrell.
\newblock Caffe: Convolutional architecture for fast feature embedding.
\newblock In \emph{Proceedings of the 22nd ACM international conference on
  Multimedia}, pp.\ 675--678. ACM, 2014.

\bibitem[Jung et~al.(2018)Jung, Son, Lee, Son, Kwak, Han, and
  Choi]{jung2018joint}
Sangil Jung, Changyong Son, Seohyung Lee, Jinwoo Son, Youngjun Kwak, Jae-Joon
  Han, and Changkyu Choi.
\newblock Joint training of low-precision neural network with quantization
  interval parameters.
\newblock \emph{arXiv preprint arXiv:1808.05779}, 2018.

\bibitem[Krafka et~al.(2016)Krafka, Khosla, Kellnhofer, Kannan, Bhandarkar,
  Matusik, and Torralba]{cvpr2016_gazecapture}
Kyle Krafka, Aditya Khosla, Petr Kellnhofer, Harini Kannan, Suchendra
  Bhandarkar, Wojciech Matusik, and Antonio Torralba.
\newblock Eye tracking for everyone.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2016.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\  1097--1105, 2012.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{lecun1998}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Leng et~al.(2017)Leng, Li, Zhu, and Jin]{leng2017extremely}
Cong Leng, Hao Li, Shenghuo Zhu, and Rong Jin.
\newblock Extremely low bit neural network: Squeeze the last bit out with admm.
\newblock \emph{arXiv preprint arXiv:1707.09870}, 2017.

\bibitem[Lin et~al.(2016)Lin, Talathi, and Annapureddy]{lin2016fixed}
Darryl Lin, Sachin Talathi, and Sreekanth Annapureddy.
\newblock Fixed point quantization of deep convolutional networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\  2849--2858, 2016.

\bibitem[Mao et~al.(2017)Mao, Han, Pool, Li, Liu, Wang, and
  Dally]{mao2017exploring}
Huizi Mao, Song Han, Jeff Pool, Wenshuo Li, Xingyu Liu, Yu~Wang, and William~J
  Dally.
\newblock Exploring the regularity of sparse structure in convolutional neural
  networks.
\newblock \emph{arXiv preprint arXiv:1705.08922}, 2017.

\bibitem[Park et~al.(2017)Park, Ahn, and Yoo]{park2017weighted}
Eunhyeok Park, Junwhan Ahn, and Sungjoo Yoo.
\newblock Weighted-entropy-based quantization for deep neural networks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\ 7197--7205, 2017.

\bibitem[Rastegari et~al.(2016)Rastegari, Ordonez, Redmon, and
  Farhadi]{rastegari2016xnor}
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi.
\newblock Xnor-net: Imagenet classification using binary convolutional neural
  networks.
\newblock In \emph{European Conference on Computer Vision}, pp.\ 525--542.
  Springer, 2016.

\bibitem[Sharify et~al.(2018)Sharify, Lascorz, Siu, Judd, and
  Moshovos]{sharify2018loom}
Sayeh Sharify, Alberto~Delmas Lascorz, Kevin Siu, Patrick Judd, and Andreas
  Moshovos.
\newblock Loom: Exploiting weight and activation precisions to accelerate
  convolutional neural networks.
\newblock In \emph{Proceedings of the 55th Annual Design Automation
  Conference}, pp.\~20. ACM, 2018.

\bibitem[Simonyan \& Zisserman(2015)Simonyan and Zisserman]{simonyan2015very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2015.

\bibitem[Ullrich et~al.(2017)Ullrich, Meeds, and Welling]{ullrich2017soft}
Karen Ullrich, Edward Meeds, and Max Welling.
\newblock Soft weight-sharing for neural network compression.
\newblock \emph{arXiv preprint arXiv:1702.04008}, 2017.

\bibitem[Wen et~al.(2016)Wen, Wu, Wang, Chen, and Li]{wen2016learning}
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
\newblock Learning structured sparsity in deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\  2074--2082, 2016.

\bibitem[Wu et~al.(2016)Wu, Leng, Wang, Hu, and Cheng]{wu2016quantized}
Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, and Jian Cheng.
\newblock Quantized convolutional neural networks for mobile devices.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\ 4820--4828, 2016.

\bibitem[Yang et~al.(2017)Yang, Chen, and Sze]{yang2017designing}
Tien-Ju Yang, Yu-Hsin Chen, and Vivienne Sze.
\newblock Designing energy-efficient convolutional neural networks using
  energy-aware pruning.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\ 6071--6079, 2017.

\bibitem[Yu et~al.(2017)Yu, Liu, Wang, and Tao]{yu2017compressing}
Xiyu Yu, Tongliang Liu, Xinchao Wang, and Dacheng Tao.
\newblock On compressing deep models by low rank and sparse decomposition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\ 7370--7379, 2017.

\bibitem[Zhang et~al.(2018{\natexlab{a}})Zhang, Wang, Figueiredo, and
  Balzano]{zhang2018learning}
Dejiao Zhang, Haozhu Wang, Mario Figueiredo, and Laura Balzano.
\newblock Learning to share: Simultaneous parameter tying and sparsification in
  deep learning.
\newblock 2018{\natexlab{a}}.

\bibitem[Zhang et~al.(2018{\natexlab{b}})Zhang, Ye, Zhang, Tang, Wen, Fardad,
  and Wang]{zhang2018systematic}
Tianyun Zhang, Shaokai Ye, Kaiqi Zhang, Jian Tang, Wujie Wen, Makan Fardad, and
  Yanzhi Wang.
\newblock A systematic dnn weight pruning framework using alternating direction
  method of multipliers.
\newblock \emph{arXiv preprint arXiv:1804.03294}, 2018{\natexlab{b}}.

\bibitem[Zhang et~al.(2018{\natexlab{c}})Zhang, Zhang, Ye, Li, Tang, Wen, Lin,
  Fardad, and Wang]{zhang2018adam}
Tianyun Zhang, Kaiqi Zhang, Shaokai Ye, Jiayu Li, Jian Tang, Wujie Wen, Xue
  Lin, Makan Fardad, and Yanzhi Wang.
\newblock Adam-admm: A unified, systematic framework of structured weight
  pruning for dnns.
\newblock \emph{arXiv preprint arXiv:1807.11091}, 2018{\natexlab{c}}.

\bibitem[Zhou et~al.(2017)Zhou, Yao, Guo, Xu, and Chen]{zhou2017incremental}
Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen.
\newblock Incremental network quantization: Towards lossless cnns with
  low-precision weights.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Zhu et~al.(2017)Zhu, Han, Mao, and Dally]{zhu2017trained}
Chenzhuo Zhu, Song Han, Huizi Mao, and William~J Dally.
\newblock Trained ternary quantization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\end{thebibliography}

\end{document}


