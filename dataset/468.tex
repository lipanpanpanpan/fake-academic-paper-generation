
\documentclass{sig-arxiv}
\usepackage{lipsum}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{color}
\usepackage{pgfplots}
\usepackage{microtype}
\usepackage{tikz}
\usepackage{array}
\usepackage[tableposition=top]{caption}
\usepackage{float}

\usetikzlibrary{fit,shapes.geometric}
\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}
\newcounter{nodemarkers}
\newcommand\circletext[1]{%
    \tikz[overlay,remember picture] 
        \node (marker-\arabic{nodemarkers}-a) at (0,1.5ex) {};%
    #1%
    \tikz[overlay,remember picture]
        \node (marker-\arabic{nodemarkers}-b) at (0,0){};%
    \tikz[overlay,remember picture,inner sep=2pt]
        \node[draw,ellipse,fit=(marker-\arabic{nodemarkers}-a.center) (marker-\arabic{nodemarkers}-b.center)] {};%
    \stepcounter{nodemarkers}%
}


\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\usepackage{graphicx}
\definecolor{bblue}{HTML}{4F81BD}
\definecolor{rred}{HTML}{C0504D}
\definecolor{ggreen}{HTML}{9BBB59}
\definecolor{ppurple}{HTML}{9F4C7C}
\usepackage{adjustbox}
\begin{document}




\clubpenalty=10000 
\widowpenalty = 10000


\title{Bags of Local Convolutional Features for Scalable Instance Search}




\numberofauthors{2}
\author{
\alignauthor Eva Mohedano\thanks{denotes equal contribution.}, Kevin McGuinness and Noel E. O'Connor \\
\affaddr{Insight Center for Data Analytics} \\
\affaddr{Dublin City University} \\
\affaddr{Dublin, Ireland} \\
\email{eva.mohedano@insight-centre.org}
\alignauthor Amaia Salvador\samethanks, Ferran Marqu\'es, \\and  Xavier Gir\'o-i-Nieto\\
\affaddr{Image Processing Group} \\
\affaddr{Universitat Politecnica de Catalunya} \\
\affaddr{Barcelona, Catalonia/Spain} \\
\email{\{amaia.salvador, xavier.giro\}@upc.edu}
} 


\maketitle
\begin{abstract}

This work proposes a simple instance retrieval pipeline based on encoding the convolutional features of CNN using the bag of words aggregation scheme (BoW). Assigning each local array of activations in a convolutional layer to a visual word produces an \textit{assignment map}, a compact representation that relates regions of an image with a visual word. We use the assignment map for fast spatial reranking, obtaining object localizations that are used for query expansion. We demonstrate the suitability of the BoW representation based on local CNN features for instance retrieval, achieving competitive performance on the Oxford and Paris buildings benchmarks. We show that our proposed system for CNN feature aggregation with BoW outperforms state-of-the-art techniques using sum pooling at a subset of the challenging TRECVid INS benchmark.
\end{abstract}




\terms{Algorithms}


\keywords{Instance Retrieval, Convolutional Neural Networks, Bag of Words}

\section{Introduction}\label{1_intro}% Visual search%Visual image retrieval is concerned with organizing and structuring datasets of images based on their visual content. %The proliferation of ubiquitous cameras in the last decade has motivated researchers in the field to push the limits of visual search systems with scalable yet effective solutions. % CNNs
Convolutional neural networks (CNNs) have been demonstrated to produce global representations that effectively capture the semantics in images. Such CNN-based representations ~\cite{babenko2015,neuralcodes,cnnofftheshelf,razavian2015,tolias2015,xie2015image} have improved upon the state-of-the-art for instance retrieval.

\begin{figure}
  \includegraphics[width=\columnwidth]{figures/rankings_short.png}
  \caption{Examples of the top generated rankings for queries of three different datasets: Oxford (top), Paris (middle), TRECVid INS 2013 (bottom).}
  \label{fig:rankings}
\end{figure}% Limitations for Instance searchDespite CNN-based descriptors performing remarkably well in retrieval benchmarks like Oxford and Paris Buildings, state-of-the-art solutions for more challenging datasets such as TRECVid Instance Search (INS) have not yet adopted pipelines that depend solely on CNN features. Current INS systems~\cite{nguyen2015query,zhang2015topological,zhou2014practical, zhu2012large} are still based on aggregating local hand-crafted features using bag of words (BoW) encoding~\cite{bow} to produce very high-dimensional sparse image representations. Such high-dimensional sparse representations are more likely to be linearly separable, while having relatively few non-zero elements makes them efficient both in terms of storage and computation. 

Many successful image retrieval engines combine an initial highly-scalable ranking mechanism on the full image database with a more computational demanding yet higher-precision reranking mechanism (e.g. geometric verification) applied to the top retrieved items.


Inspired by advances in CNN-based descriptors for image retrieval, yet still focusing on instance search, we revisit the BoW encoding scheme using local features from convolutional layers of a CNN. 
This work presents three contributions: 

\begin{itemize}
\setlength{\itemsep}{0pt}
\item We propose a sparse visual descriptor based on a Bag of Local Convolutional Features (BLCF), which allows fast image retrieval by means of an inverted index. 

\item We introduce the assignment map as a new compact representation of the image, which maps pixels in the image to their corresponding visual words, allowing the fast composition of a BoW descriptor for any region of the image. 

\item We take advantage of the scalability properties of the assignment map to perform a local analysis of multiple image regions for reranking, followed by a query expansion stage using the obtained object localizations. 

\end{itemize}%
Using this approach, we present a retrieval system that achieves state-of-the-art performance in several instance retrieval benchmarks.
Figure~\ref{fig:rankings} illustrates some of the rankings produced by our system on several datasets.

\section{Related Work}\label{2_soa}

Many works in the literature have proposed CNN-based representations for image retrieval. Early works \cite{neuralcodes,cnnofftheshelf} focused on replacing traditional hand-crafted descriptors with features from fully connected layers of a CNN pre-trained for image classification. A second generation of works reported significant gains in performance when switching from fully connected to either sum \cite{babenko2015,kalantidis2015} or max \cite{razavian2015,tolias2015} pooled convolutional features. Our work shares similarities with all the former in that we use convolutional features extracted from a pre-trained CNN. Unlike these approaches, however, we propose using a sparse, high-dimensional encoding that better represents local image features.

Several works have tried to exploit local information in images by passing multiple image sub patches through a CNN to obtain local features from either fully connected~\cite{cnnofftheshelf, deepindex} or convolutional~\cite{gong2014multi} layers, which are in turn aggregated using techniques like average pooling~\cite{cnnofftheshelf}, BoW~\cite{deepindex}, or VLAD~\cite{gong2014multi}. Although many of these methods perform well in retrieval benchmarks, they require CNN feature extraction from many image patches, which slows down indexing and feature extraction at retrieval time. Instead, other works \cite{ng2015, netvlad} treat the activations of the different neuron arrays across all feature maps in a convolutional layer as local features. This way, a single forward pass of the entire image through the CNN is enough to obtain the activations of its local patches, which are then encoded using VLAD. Our approach is similar to the ones in~\cite{ng2015,netvlad} in that we also treat the features in a convolutional layer as local features. We, however, use BoW encoding instead of VLAD to take advantage of sparse representations for fast retrieval in large-scale databases.

  \includegraphics[width=\textwidth]{figures/bow.pdf}
  \caption{The Bag of Local Convolutional Features pipeline (BLCF).}
  \label{fig:bow}
  \vspace{-10.5pt}
\end{figure*}% Spatial search with CNNs
Some of the approaches cited above propose systems that are based or partially based on spatial search over multiple regions of the image. Razavian et al.~\cite{razavian2015} achieve a remarkable increase in performance by applying a spatial search strategy over an arbitrary grid of windows at different scales. Although they report high performance in several retrieval benchmarks, their proposed approach is very computationally costly and does not scale well to larger datasets and real-time search scenarios. Tolias et al.~\cite{tolias2015} introduce a local analysis of multiple image patches, which is only applied to the top elements of an initial ranking. They propose an efficient workaround for sub patch feature pooling based on integral images, which allows them to quickly evaluate many image windows. Their approach improves their baseline ranking and also provides approximate object localizations. In this direction, our work proposes using the assignment map to quickly build the BoW representation of any image patch, which allows us to apply a spatial search for reranking. Unlike~\cite{tolias2015}, we use the object localizations obtained with spatial search to mask out the activations of the background and perform query expansion using the detected object location. 

\section{Bag of Words Framework}\label{3_bow}% CNN architecture / Conv activations as local features

The proposed pipeline for feature extraction uses the activations at different locations of a convolutional layer in a pre-trained CNN as local features. Similar to~\cite{tolias2015,kalantidis2015}, we discard the softmax and fully connected layers of the original network while keeping the original image aspect ratio. Each convolutional layer in the network has $D$ different $N \times M$ feature maps, which can be viewed as $N \times M$ local descriptors of dimension $D$.

We propose to use bag of words encode the local convolutional features of an image into a single vector. Although more elaborate aggregation strategies have been shown to outperform BoW-based approaches for some tasks in the literature~\cite{vlad,fishervectors}, Bag of words encodings produce sparse high-dimensional codes that can be stored in inverted indices, which are beneficial for fast retrieval. Moreover, BoW-based representations are more compact, faster to compute and easier to interpret.

Bag of words models require constructing a visual codebook to map vectors to their nearest centroid. We use $k$-means on local CNN features to fit this codebook. Each local CNN feature in the convolutional layer is then assigned its closest visual word in the learned codebook. This procedure generates the \textit{assignment map}, i.e. a 2D array of size $N \times M$ that relates each local CNN feature with a visual word. The assignment map is therefore a compact representation of the image which relates each pixel of the original image with its visual word with a precision of $ \left(\frac{W}{N}, \frac{H}{M}\right)$ pixels, where $W$ and $H$ are the width and height of the original image. This property allows us to quickly generate the BoW vectors of not only the full image, but also its parts. We describe the use of this property in our work in Section~\ref{5_retrieval}.

Figure~\ref{fig:bow} shows the pipeline of the proposed approach, which encodes the image into a sparse high dimensional descriptor to be used for instance retrieval.


\section{Image Retrieval}\label{5_retrieval}

This section describes the image retrieval pipeline, which consists of an initial ranking stage, followed by a spatial reranking, and query expansion.

\textbf{(a) Initial search: }  The initial ranking is computed using the cosine similarity between the BoW vector of the query image and the BoW vectors of the full images in the database. We use a sparse matrix based inverted index and GPU-based sparse matrix multiplications to allow fast retrieval. The image list is then sorted based on the cosine similarity of its elements to the query. We use two types of image search based on the query information that is used:


\setlength{\itemsep}{0pt}
\item Global search (GS): The BoW vector of the query is built with the visual words of all the local CNN features in the convolutional layer extracted for the query image.

\item Local search (LS): The BoW vector of the query contains only the visual words of the local CNN features that fall inside the query bounding box.
\end{itemize}\vspace{-2.25pt}% \begin{figure}% \centering% \includegraphics[width=\columnwidth]% {figures/LQS.png}% \caption{Selection of Local CNN features to construct the BoW query descriptor for the Local Search (LS).}% \label{fig_lqs}% \end{figure}\textbf{(b) Local reranking (R): } After the initial search, the top $T$ images in the ranking are reranked based on a localization score. We choose windows of all possible combinations of width $w \in \{W, \frac{W}{2}, \frac{W}{4}\}$ and height $h \in \{H, \frac{H}{2}, \frac{H}{4}\}$, where $W$ and $H$ are the width and height of the assignment map. We use a sliding window strategy directly on the assignment map with 50\% of overlap in both directions. 


We additionally perform a simple filtering strategy to discard those windows whose aspect ratio is too different to the aspect ratio of the query. Let the aspect ratio of the query bounding box be $AR_q = \frac{W_q}{H_q}$ and $AR_w = \frac{W_w}{H_w}$ be the aspect ratio of the window. The score for window $w$ is defined as
$score_{w} = \frac{min(AR_w,AR_q)}{max(AR_w,AR_q)}$.
All windows with a score lower than a threshold $th$ are discarded. 

For each of the remaining windows, we construct the BoW vector representation and compare it with the query representation using cosine similarity. The window with the highest cosine similarity is picked as the object localization and its score is kept for the image. We also enhance the BoW window representation with spatial pyramid matching~\cite{spm} with $L = 2$ resolution levels (i.e. the full window and its 4 sub regions). We construct the BoW representation of all sub regions at the 2 levels, and weight their contribution to the similarity score with inverse proportion to the resolution level of the region. The cosine similarity of a sub region $r$ to the corresponding query sub region is therefore weighted by $ w_{r} = \frac{1}{2^{(L-l_{r})}} $, where $l_{r}$ is the resolution level of the region $r$. With this procedure, the top $T$ elements of the ranking are sorted based on the cosine similarity of their regions to the query's, and also provides the region with the highest score as a rough localization of the object.


\setlength{\itemsep}{0pt}
\item Global query expansion (GQE): The BoW vectors of the $N$ images at the top of the ranking are averaged together with the BoW of the query to form the new representation for the query.

\item Local query expansion (LQE): Locations obtained in the local reranking step are used to mask out the background and build the BoW descriptor of only the region of interest of the $N$-top images in the ranking. These BoW vectors are averaged together with the BoW of the query bounding box to perform a second search.

\end{itemize}
\section{Experiments}\label{5_experiments}% \subsection{Datasets}% We use the following datasets to evaluate the performance of our approach:% \textbf{Oxford Buildings \cite{philbin2007object}} contains 5,063 still images, including 55 query images of 11 different buildings in Oxford. A bounding box surrounding the target object is provided for query images. An additional 100,000 distractor images are also available for the dataset. We refer to the original and extended versions of the dataset as Oxford 5k and Oxford 105k, respectively.% \textbf{Paris Buildings \cite{paris6k}} contains 6,412 still images collected from Flickr including query images of 12 different Paris landmarks with associated bounding box annotations. A set of 100,000 images is added to the original dataset (Paris 6k) to form its extended version (Paris 106k).% \textbf{TRECVid Instance Search 2013 \cite{trecvid}} contains 244 video files (464 hours in total), each containing a week's worth of BBC EastEnders programs. Each video is divided in different shots of short duration (between 5 seconds and 2 minutes). We perform uniform keyframe extraction at 1/4 fps. The dataset also includes 30 queries and provides 4 still images for each of them (including a binary mask of the object location). In our experiments, we use a subset of this dataset that contains only those keyframes that are positively annotated for at least one of the queries. The dataset, which we will refer to as the TRECVid INS subset, is composed of 23,614 keyframes.% Figure~\ref{fig:datasets} includes three examples of query objects from the three datasets.% \begin{figure}% \centering% \includegraphics[width=\columnwidth]% {figures/datasets.png}% \caption{Query examples from the three different datasets. Top: Paris buildings (1-3) and Oxford buildings (4-6); bottom: TRECVid INS 2013.}% \label{fig:datasets}% \end{figure}%  The top row includes 3 examples of both Paris and Oxford buildings datasets. From left to right: The Eiffel Tower, The Notre Dame Cathedral and The Sacre Coeur Basilica in Paris, The All Souls College, The Ashmolean Museum and The Magdalen College in Oxford. The bottom row includes three examples of the visual queries in the TRECVid INS dataset: a no smoking logo, an Audi logo, and a wooden bench.% AMAIA: Can we use TRECVid images in the paper? I've seen the following in the TRECVid Website:% When using BBB EastEnders video, remember the following section of the data permission agreement:%    2.2 Small video excerpts of the Programming or small numbers of%    derived still images may be displayed to third-parties or published %    in a scientific or technical context, solely for the purpose of%    describing the research and development of the Project. No other%    public display of the Programming shall be made by the Licensee%    without the prior written permission of the BBC. Any use of the%    Programming within the context of this Clause 2.2 shall include %    the following acknowledgement: "Programme material copyrighted by BBC"\subsection{Preliminary experiments}
We experiment with three different instance retrieval benchmarks to evaluate the performance of our approach: Oxford Buildings \cite{philbin2007object}, Paris Buildings \cite{paris6k} and a subset of TRECVid Instance Search 2013 (INS 23k) \cite{trecvid} containing only those keyframes that are relevant to at least one of the queries. For the Paris and Oxford datasets, we also test the performance when adding 100,000 distractor images collected from Flickr to the original datasets (resulting in the Oxford 105k and Paris 106k datasets, respectively). 

Feature extraction was performed using \emph{Caffe}~\cite{caffe} and the VGG16 pre-trained network~\cite{vgg}. We extracted features from the last three convolutional layers (conv5\_1, conv5\_2 and conv5\_3) and compared their performance on the Oxford 5k dataset. We experimented on different image input sizes: 1/3 and 2/3 of the original image. Following several other authors~\cite{babenko2015,kalantidis2015}, we $l_2$-normalize all local features, followed by PCA, whitening, and a second round of $l_2$-normalization. The PCA models were fit on the same dataset as the test data in all cases. Unless stated otherwise, all experiments used a visual codebook of 25,000 centroids fit using the ($L^2$-PCA-$L^2$ transformed) local CNN features of all images in the same dataset.

We apply bilinear interpolation on the convolutional layers to obtain higher resolution maps as a workaround of using bigger input size image. We find this strategy to be beneficial for all layers tested, achieving the best results when using conv5\_1. Inspired by the boost in performance of the Gaussian center prior in SPoC features~\cite{babenko2015}, we apply a weighting scheme on the visual words to provide more importance to those belonging to the center of the image. 

We apply the local reranking (R) stage on the top-100 images in the initial ranking, using the sliding window approach described in Section~\ref{5_retrieval}. 
The presented aspect ratio filtering is applied with a threshold $th = 0.4$, which was chosen based on a visual inspection of results on a subset of Oxford 5k. 
Query expansion is later applied considering the top-10 images of the resulting ranking. 
Table~\ref{q_exp} contains the results for the different stages in the pipeline.

\begin{table}
\centering
\small
\caption{mAP on Oxford 5k and Paris 6k for the different stages in the pipeline introduced in Section~\ref{5_retrieval}.}
\label{q_exp}
\begin{tabular}{@{}ccccccc@{}}
\toprule
                           &  &  Baseline    & +R    & +GQE  & \multicolumn{1}{l}{\begin{tabular}[c]{@{}l@{}}+R\\ +GQE\end{tabular}} & \multicolumn{1}{l}{\begin{tabular}[c]{@{}l@{}}+R\\ +LQE\end{tabular}} \\ \midrule
\multirow{2}{*}{Oxford} & GS    & 0.653 & 0.701 & 0.702 & 0.771                                                                 & \textbf{0.782}                                                        \\
                           & LS    & 0.738 & 0.734 & 0.773 & 0.769                                                                 & \textbf{0.788}                                                        \\
\multirow{2}{*}{Paris}  & GS    & 0.699 & 0.719 & 0.774 & 0.801                                                                 & \textbf{0.835}                                                        \\
                           & LS    & 0.820 & 0.815 & 0.814 & 0.807                                                                 & \textbf{0.848}                                                        \\ \midrule
\end{tabular}
\vspace{-10.5pt}
\end{table}The results indicate that the local reranking is significantly beneficial only when applied to a ranking obtained from a search using the global BoW descriptor of the query image (GS). This is consistent with the work by Tolias et al.~\cite{tolias2015}, who also apply a spatial reranking followed by query expansion to a ranking obtained with a search using descriptors of full images. They achieve a mAP of 0.66 in Oxford 5k, which is increased to 0.77 after spatial reranking and query expansion, while we reach similar results (e.g. from 0.652 to 0.769). However, our results indicate that a ranking originating from a local search (LS) does not benefit from local reranking. Since the BoW representation allows us to effectively perform a local search (LS) in a database of full indexed images, we find the local reranking stage applied to LS to be redundant in terms of the achieved quality of the ranking. However, the local reranking stage does provide with a rough localization of the object in the images of the ranking, as depicted in Figure~\ref{fig:rankings}. We use this information to perform query expansion based on local features (LQE). 

Results indicate that query expansion stages greatly improve performance in Oxford 5k. We do not observe significant gains after reranking and QE in the Paris 6k dataset, although we achieve our best result with LS + R + LQE.


We compare our approach with other CNN-based representations that make use of features from convolutional layers on the Oxford and Paris datasets. Table~\ref{tab_soa} includes the best result for each approach in the literature. Our performance using global search is comparable to that of Ng et al.~\cite{ng2015}, which is the one that most resembles our approach. However, they achieve this result using VLAD features, which are more expensive to compute and, being a dense high-dimensional representation, do not scale as well to larger datasets. Similarly, Razavian et al.~\cite{razavian2015} achieve the highest performance of all approaches in both the Oxford and Paris benchmarks by applying a spatial search at different scales for all images in the database. Such approach is prohibitively costly when dealing with larger datasets, especially for real-time search scenarios. Our BoW-based representation is highly sparse, allowing for fast retrieval using inverted indices, and achieves consistently high mAP in all tested datasets. 


We also compare our local reranking and query expansion results with similar approaches in the state-of-the-art. The authors of R-MAC~\cite{tolias2015} apply a spatial search for reranking, followed by a query expansion stage, while the authors of CroW~\cite{kalantidis2015} only apply query expansion after the initial search. Our proposed approach also achieves competitive results in this section, achieving the best result for Oxford 5k.

\begin{table}[t]
\centering
\caption{Comparison to state-of-the-art CNN representations (mAP). Results in the lower section consider reranking and/or query expansion.}
\label{tab_soa}
\begin{tabular}{lcccc}
\hline
                      & \multicolumn{2}{c}{Oxford} & \multicolumn{2}{c}{Paris} \\ 
                      & 5k           & 105k        & 6k          & 106k        \\ \hline
Ng \emph{et al.} \cite{ng2015}                    & 0.649        & -           & 0.694       & -           \\ 
Razavian \emph{et al.} \cite{razavian2015}              &\textbf{0.844}& -           & \textbf{0.853}       & -           \\ 
SPoC \cite{babenko2015}                & 0.657        & 0.642       & -           & -           \\ 
R-MAC \cite{tolias2015}                & 0.668        & 0.616       & 0.830       & \textbf{0.757}       \\ 
CroW \cite{kalantidis2015}                 & 0.682        & \textbf{0.632}       & 0.796       & 0.710       \\ 
uCroW \cite{kalantidis2015}                 & 0.666        & 0.629       & 0.767      & 0.695       \\ 
GS            & 0.652        & 0.510      & 0.698       & 0.421       \\ 
LS            & 0.739        & 0.593      & 0.820       & 0.648       \\ 
\hline
CroW + GQE \cite{kalantidis2015}            & 0.722        & 0.678       & 0.855       & 0.797       \\ 

R-MAC + R + GQE \cite{tolias2015}      & 0.770        & \textbf{0.726}       & \textbf{0.877}       & \textbf{0.817}       \\ 
LS + GQE      &0.773& 0.602           & 0.814       & 0.632           \\ 
LS + R + LQE & \textbf{0.788} & 0.651           & 0.848       & 0.641           \\ 
\hline
\end{tabular}
\vspace{-10.5pt}
\end{table}\subsection{Experiments on TRECVid INS}

In this section, we compare the Bag of Local Convolutional Features (BLCF) with the sum pooled convolutional features proposed in several works in the literature. We use our own implementation of the \emph{uCroW} descriptor from \cite{kalantidis2015} and compare it with BLCF for the TRECVid INS subset. For the sake of comparison, we test our implementation of sum pooling using both our chosen CNN layer and input size (conv5\_1 and 1/3 image size), and the ones reported in \cite{kalantidis2015} (pool5 and full image resolution). For the BoW representation, we train the codebook using 3M local CNN features chosen randomly from the INS subset. In this case, we do not apply center prior to the feature to avoid down weighting local features from image areas where the objects might appear. Table \ref{trecvid_table} compares sum pooling with BoW in Oxford, Paris, and TRECVid subset datasets. As stated in earlier sections, sum pooling and BoW have similar performance in Oxford and Paris datasets. For the TRECVid INS subset, however, Bag of Words significantly outperforms sum pooling, which demonstrates its suitability for challenging instance search datasets, in which queries are not centered and have variable size and appearance.  We also observe a different behavior when using the provided query object locations (LS) to search, which was highly beneficial in Oxford and Paris datasets, but does not provide any gain in TRECVid INS. We hypothesize that the fact that the size of the instances is much smaller in TRECVid than in Paris and Oxford datasets causes this drop in performance. Global search (GS) achieves better results on TRECVid INS, which suggests that query instances are in many cases correctly retrieved due to their context. 

\centering
\caption{mAP of sum pooling and BoW aggregation techniques in Oxford, Paris and TRECVid INS subset.}
\label{trecvid_table}


\begin{tabular}{@{}ccccc@{}}
\toprule
                                                                                 &  & Oxford 5k & Paris 6k & INS 23k \\ \midrule
\multirow{2}{*}{BoW}                                                             & GS & 0.650     & 0.698    & \textbf{0.323}   \\
                                                                                 & LS & \textbf{0.739}     & \textbf{0.819}    & 0.295   \\ \midrule
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Sum pooling\\ (as ours)\end{tabular}} & GS & 0.606     & 0.712    & 0.156   \\
                                                                                 & LS & 0.583     & 0.742    & 0.097   \\ \midrule
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Sum pooling\\ (as in \cite{kalantidis2015})\end{tabular}}  & GS & 0.672     & 0.774    & 0.139   \\
                                                                                 & LS & 0.683     & 0.763    & 0.120   \\  \midrule
\end{tabular}
\end{table}
\section{Conclusion}\label{6_conclusions}

We proposed an aggregation strategy based on Bag of Words to encode features from convolutional neural networks into a sparse representations for instance search. We achieved competitive performance with respect to other CNN-based representations in Oxford and Paris benchmarks, while being more scalable in terms of index size, cost of indexing, and search time. We also compared our BoW encoding scheme with sum pooling of CNN features in the far more challenging TRECVid instance search task, and demonstrated that our method consistently and significantly performs better. Our method does, however, appear to be more sensitive to large numbers of distractor images than methods based on sum and max pooling. We speculate that this may be because the distractor images are drawn from a different distribution to the original dataset, and may therefore require a larger codebook to better represent the diversity in the visual words. 

\section*{Acknowledgements}
This publication has emanated from research conducted with the financial support of Science Foundation Ireland (SFI) under grant number SFI/12/RC/2289, project BigGraph TEC2013-43935-R, funded by the Spanish Ministerio de Econom\'ia y Competitividad and the European Regional Development Fund (ERDF), as well as the donation of a GeForce GTX Titan X from NVIDIA Corporation.


{
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}




