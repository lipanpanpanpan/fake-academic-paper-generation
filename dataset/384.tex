\documentclass{article} % For LaTeX2e
\usepackage{iclr2019_conference,times}


\usepackage{hyperref}
\usepackage{url}


\usepackage{comment}
\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{varwidth}
\usepackage{algpseudocode}
\usepackage{wrapfig}
\usepackage{graphicx}


\usepackage{tabularx, booktabs}
\newcolumntype{I}{!{\vrule width 3pt}}
\newlength\savedwidth
\newcommand\whline{\noalign{\global\savedwidth\arrayrulewidth
                            \global\arrayrulewidth 1pt}%
                   \hline
                   \noalign{\global\arrayrulewidth\savedwidth}}
\newlength\savewidth
\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
                            \global\arrayrulewidth 1.5pt}%
                   \hline
                   \noalign{\global\arrayrulewidth\savewidth}}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\renewcommand{\algorithmicforall}{\textbf{for each}}
\algtext*{EndWhile}
\algtext*{EndIf}
\algtext*{EndFor}
\algtext*{EndProcedure}

\title{Interpretable Convolutional Filter Pruning}


\author{
	Zhuwei Qin$^1$, Fuxun Yu$^2$, Chenchen Liu$^3$, Liang Zhao$^4$, Xiang Chen$^5$ \\
	$^{1,2,5}$Department of Electrical Computer Engineering, George Mason University, Fairfax, VA 22030 \\
	$^{4}$Department of Information Science and Technology, George Mason University, Fairfax, VA 22030 \\
	$^{3}$Department of Electrical Computer Engineering, Clarkson University, Potsdam, NY 13699 \\
	\texttt{zqin@gmu.edu$^1$, fyu2@gmu.edu$^2$, chliu@clarkson.edu$^3$, lzhao9@gmu.edu$^4$}\\
	\texttt{xchen26@gmu.edu$^5$}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}
\maketitle

\vspace{-3mm}
\begin{abstract}
The sophisticated structure of Convolutional Neural Network (CNN) allows for outstanding performance, but at the cost of intensive computation.
	As significant redundancies inevitably present in such a structure, many works have been proposed to prune the convolutional filters for computation cost reduction.
	Although extremely effective, most works are based only on quantitative characteristics of the convolutional filters, and highly overlook the qualitative interpretation of individual filter's specific functionality.
	In this work, we interpreted the functionality and redundancy of the convolutional filters from different perspectives, and proposed a functionality-oriented filter pruning method.
	With extensive experiment results, we proved the convolutional filters' qualitative significance regardless of magnitude, demonstrated significant neural network redundancy due to repetitive filter functions, and analyzed the filter functionality defection under inappropriate retraining process.
	Such an interpretable pruning approach not only offers outstanding computation cost optimization over previous filter pruning methods, but also interprets filter pruning process.



\end{abstract}
\section{Introduction}
The great success of Convolutional Neural Network (CNN) is benefited from its advanced algorithm and architecture, which utilize inter-connected multi-layer network structure to hierarchically abstract the data feature for recognition tasks (\cite{Kriz:2012:NIPS}).
	However, this complex mechanism offers CNN outstanding performance at the price of intensive computation cost.
	Therefore, many CNN optimization works have been proposed to alleviate this computation cost (\cite{han2015learning}, \cite{jaderberg2014speeding}, \textit{etc}).

While those network redundancy leveraging works are mostly based on quantitative evaluation, the qualitative analysis based on network interpretation is highly overlooked.
	With the neural network interpretation, CNN can be well analyzed regarding its hierarchical structure and individual component's functionality, and therefore no longer a ``black-box''.
	However, most neural network interpretation works is merely considered as a post-analysis approach, and very few works have utilized it to analysis and guide the optimization directly (\cite{Yosinski:2015:ICML:AM}).

Considering the absence of the network interpretation analysis and optimization, in this work we utilized different CNN interpretation techniques to analysis the convolutional filter functionality and optimize the filter pruning method.
In this work, we have the following major contributions:

\vspace{-1mm}\begin{itemize}
	\item We utilized CNN visualization techniques and backward-propagation gradients analysis to interpret the convolutional filter functionality, and demonstrate significant network redundancy due to repetitive filter functions rather than insignificant magnitude;
	\item We designed a hierarchical filter functionality-oriented pruning method to explore the interpretable neural network optimization;
	\item We observed the filter functionality transission with model tuning, and reveals the inappropriate pruning and retraining methods may significantly defect the filter functionality and cause unnecessary network reconstruction effort.
\end{itemize}\vspace{-1mm}

Experiment results with various CNN models and image dataset show that, the proposed filter interpretation approach can effectively analysis the convolutional filters' the functionality and similarity.
	The proposed functionality-oriented pruning methods also achieves outstanding performance compared to traditional filter pruning methods with better training efficiency and interpretability

\section{Related work}\label{sec:prelim}\textbf{Convolutional Filter Pruning}
~~It is well known that the major computation cost in a CNN comes from the convolutional layers.
	Therefore, most CNN optimization works fall into convolutional filter pruning or compact CNN training with convolutional filter constraints:
	\cite{he2017channel} also applied Lasso-regression in the filter pruning for batched processing.
The filter significance is also evaluated with the filter feature maps:
	\cite{lLuo2017Thinet} evaluate filter's significance by testing its impact on feature map reconstruction errors after being pruned.

Most of these works are based on quantitative analysis on the filter significance.
	However, such an approach is already questioned by some recent works: \cite{huang2018learning} proved that certain redundancy also exists in filters with large ``significance'';
	while \cite{ye2018rethinking} correspondingly demonstrated that filters with small values also significantly affect the neural network performance.
Therefore, rather than merely evaluating the convolutional filters in a quantitative approach, we analysis the filter functionality in a qualitative manner to guide the neural network optimization.

\textbf{Convolutional Filter Visualization}
~~As the convolutional filters are designed to capture certain input features, the semantics of the captured feature can effectively indicate the functionality of each filter.
	However, as the convolutional filter are embodied by matrices, the  functionality is hard to directly interpret, which significantly hinders the qualitative neural network optimization.

Recently, many CNN visualization techniques have been proposed to qualitatively analysis the CNN in perspectives of network structure and component functionality,
	such as Activation Maximization (AM) (\cite{Yosinski:2015:ICML:AM}), Network Inversion (\cite{mahendran2015:Network-Inversion}), Deconvolutional Neural Networks (\cite{Zeiler:2014:ECCV:DeconvolNet}), and Network Dissection based Visualization (\cite{Zhou:2016:CVPR:Network-Dissection}).
	Among those methods, AM offers the most efficient and effective interpretation for the convolutional filter functionality, which is defined by each filter's maximum activation pattern.
In this work, we utilize the AM as our major visualization tool for convolutional filter analysis.

\section{Convolutional Filter Functionality Interpretation \\ \hspace{5.5mm} and Filter Redundancy Analysis}\label{sec:visual}\begin{figure}
	\begin{minipage}[c]{0.5\linewidth}
		\includegraphics[width=2.5in]{./1_pattern.pdf}
		\caption{The illustration of visualized patterns of the convolutional filters.}
		\label{fig:vis}
	\end{minipage}
	\hfill
	\begin{minipage}[c]{0.5\linewidth}
	\vspace{-2mm}
		\includegraphics[width=2.6in]{./2_correlation.pdf}
		\vspace{1mm}
		\caption{Filter similarity matrices.}
		\label{fig:sim}
	\end{minipage}
\end{figure}

Different from previous quantitative filter significance analysis, the convolutional filter functionality is qualitatively interpreted in this paper.
	Based on the functionality interpretation, the neural network redundancy is further analyzed regarding the filter functionality.
	For preliminary demonstration, we adopt a VGG-16 model trained on CIFAR-10 dataset for intuitive result demonstration.

\subsection{Interpretable Convolutional Filter Functionality Analysis}% Different from previous works that mainly quantitatively analyze the convolutional filters, we utilize AM and backward-propagation gradients for qualitative functionality analysis from two perspectives:% in this work, we utilize the
To qualitatively interpret the convolutional filter functionality, both AM visualization and backward-propagation gradients analysis are utilized.
AM visualization interprets a filter's feature extraction preference from the neural network inputs, while the backward-propagation gradients evaluate the filters' contribution to classification outputs.

In AM visualization, the feature extraction preference of a filter $F_i^l$ from layer $L_l$ is represented by a synthesized input image $X$ that causes the highest activation of $F_i^l$ (\textit{i.e.} the convolutional feature map value).
	Mathematically, the synthesis process of such an input can be formulated as:
\begin{equation}
	V(F^l_i)=\argmax_{X} {A^l_i(X),
	\hspace{0.6cm} X \leftarrow X} + \eta \cdot \frac{\partial A^l_i(X)}{ \partial X},
	\label{eq:am}
\end{equation}
where $A^l_i(X)$ is the activation of filter $F_i^l$ from an input image $X$, $\eta$ is the gradient ascent step size.
	With $X$ initialized as an input image of random noises, each pixel of this input is iteratively changed along the $\partial A^l_i(X)$/$\partial X$ increment direction to achieve the highest activation.
	Eventually, $X$ demonstrates a specific visualized pattern $V(F^l_i)$, which contains the filter's most sensitive input features with certain semantics, and represents the filter's functional preference for feature extraction.

Fig.~\ref{fig:vis} demonstrates several filters' visualized patterns from different layers.
	By interpreting the content of these patterns, it is clear to see that the filters in lower layers are more sensitive to fundamental colors and shapes.
	While with the layer depth increasing, the patterns tend to show specific objects and eventually recognizable class targets.

In the backward-propagation gradients analysis,
	the gradients indicate the impact of each convolutional filter to individual classification targets:
\begin{equation}
	\gamma_{i,y_{j}} = \frac{1}{N}\sum_{n=1}^{N} \left \| \frac{\partial P(y_{j})}{\partial A_{i}^{l}(x_{n})} \right \|,
	\label{eq:contri}
\end{equation}
where $P(y_{j})$ is the output probability of a sample image $x_n$ to the class $y_{j}$, and $A_{i}^{l}(x_{n})$ is the activation of filter $F_i^l$ for each test image $x_n$.
	$\partial P(y_{j}) / \partial A_{i}(x_{n})$ is the backward-propagation gradient of class $y_j$ respective to the filter's activation, which indicates the dedicated contribution of filter $F_i^l$ to class $y_j$.
	With preliminary experiments, we also find that some filters' visualized patterns also constrain obvious class objects, which indicates large impact to certain classes $y_j$.
	Therefore, the connection between filters and dedicated class outputs can be also determined.

Associating these two analysis methods, namely the CNN visualization and backward-propagation gradients analysis, the filters' functionalities can be well interpreted in a qualitative manner.

\subsection{Function Similarity based Filter Redundancy Analysis}
Based on the above analysis, the convolutional filter functionality can be well interpreted regarding the input feature extraction preference and output contribution.
	These qualitative interpretations also guide certain neural network redundancy analysis, where filters with similar functionalities may indicate unnecessary structural repetition.
	Such repetition can be well identified among the filters with similar visualized patterns, as shown in the Fig.~\ref{fig:vis}:
	as denoted by the red blocks, significant functionality similarities present among the convolutional filters in each layer.

To quantitatively determinate possible filter functionality repetition, a similarity degree between filter $F_k^{(c,l)}$ and $F_i^{(c,l)}$ is formulated regarding both AM visualized patterns and the back-propagation gradients:
	S_{D}[V(F_{i}^{(c, l)}), V(F_{k}^{(c, l)})]=\|V(F_i^{(c,l)}) - V(F_k^{(c,l)}) \|^2 \cdot \|\gamma_{i} - \gamma_{k} ) \|^2,
	\label{eq:sd}
\end{equation}
where $\|V(F_i^{(c,l)}) - V(F_k^{(c,l)}) \|^2$ calculates the Euclidean distance between two visualized patterns, which indicate feature extraction preference similarity
	\footnote{Although image similarity analysis tools like SSIM are more specialized, they cannot be applied into low layer filters with extremely small resolutions. Therefore, the Euclidean distance is adopted for its generality.}.
	And $\|\gamma_{i} - \gamma_{k} ) \|^2$ calculates the similarity between the two filter's contribution to specific output classes.
	To equally evaluate these two criterion, both vectores are normalized, and the summation is utilized as the filter similarity degree.

According to the qualitative analysis of Eq.\ref{eq:sd}, a set of matrices filter similarity degrees are constructed as shown in Fig.\ref{fig:sim}.
	The matrix axes represent the filter index in each layer (\textit{e.g.} 512x512 for filter Conv5\_2), and each cross-point indicates the similarity degree $S_{D}$ between two filters.
	The magnitudes of the similarity degrees are denoted by a color range, where the minimum is denoted as red and the maximum is denoted as white.
	From Fig.\ref{fig:sim}, it can be observed that the filter similarity widely present in different layers.
	Meanwhile, significant filter similarity exit in the shallowest and deepest layers of the neural network (\textit{e.g.} Conv1\_1 and Conv5\_2).
	Those high similarity degrees indicate considerable functionality repetition among the convolutional filters, which might be leveraged for effective neural network optimization.
In later sections, the filter functionality evaluation approach is widely applied for network redundancy analysis and filter pruning guidance.
\section{Hierarchical Filter Functionality-Oriented Pruning}\label{sec:pruning}\begin{figure}[t]
	\centering
	\includegraphics[width=5.5in]{./pruning_visua.pdf}
	\caption*{\hspace{1mm}(a) Proposed functionality-oriented filter pruning. \hspace{10mm} (b) $\ell_1$ ranking based filter pruning.}
	\caption{Case study of the hierarchical filter functionality-oriented  and filter $\ell_1$ ranking based filter pruning on the Conv5\_1 of VGG-16.
		The convolutional filters are shown by their visualized patterns, and aligned according to the visualization based filter clustering.}
	\label{fig:pruning}
\end{figure}

Based on the convolutional filter functionality and redundancy analysis, we further propose a novel hierarchical filter pruning method oriented by the qualitative functionality interpretation.
The proposed functionality-oriented filter pruning method is designed in a hierarchical manner to address the functionality redundancy in different neural network component levels.

The method has the following major stages:
	(1) Filter Clustering -- Based on the aforementioned filter similarity analysis with AM visualization and the backpropagation gradients, the filters with similar functionalities in each layer are clustered into multiple groups.
	The filters in the same cluster are considered to have repetitive functionalities;
	(2) Filter Level Pruning -- Inside each cluster, the filters' relative contribution to the output classes are qualitative evaluated by the backpropagation gradients analysis.
	The contribution index of each filter can be considered as the filter significance regarding its functionality;
	(3) Cluster Level Pruning -- Inside each layer, the relative cluster pruning rate is based on the cluster volume size to guide the filter pruning in each cluster;
	(4) Layer Level Pruning --  Guided by the layer sensitive to pruning, the relative pruning ratio for each layer is calculated to guide the pruning in each layer and therefore each cluster.

Fig.~\ref{fig:pruning}(a) illustrate an intuitive example for the filter clustering in the layer Conv5\_1, where each row represents one cluster in the layer, and the horizontal axis indicates the filter's contribution index.
	Given a pruning rate for the model, the relative pruning rate for each layer and cluster can be determined, and the filters with least classification contribution will be pruned first (marked by green crosses).
Meanwhile, we also demonstrate $\ell_1$ ranking based filter pruning in the Fig.~\ref{fig:pruning}(b).
	Considering $\ell_1$ ranking's quantitative filter significance evaluation, the pruned filters (marked by red crosses) have no interpretable selection patterns and no functionality correlations.
	Moreover, comparing to $\ell_1$ ranking based filter pruning, the proposed functionality-oriented pruning requires extremely small effort for model retraining, which will be further explained in later sections.

In the next section, the details for each processing stage will be presented.
	\caption{Visualization based k-means Filter Clustering}
	\begin{algorithmic}[1]
	\Require CNN, Number of layers L, and Number of filters in each layer $I_{l}$
	\Ensure  L Clusters
				\\ \textbf{for each}{$Layer~L_{l}$} \textbf{do} $List_{l}$=[];
					\\ \hspace{4.5mm} \textbf{for each}{$Filter~F_{i}^{l}$} \textbf{do} Generate the pattern: $V(F_i^l)$=$X_{i}^{l}$; $List_{l}$.append($V(F_i^l)$);
		\\ \textbf{for} c in range (1, $I_{l}/2$) \textbf{do} Grid search all possible cluster numbers: $C_{l}$=kmeans.cluster($List_{l}$, c);
			\\ \hspace{4.5mm} \textbf{for} $C_{l}^{c}$ in $C_{l}$ \textbf{do} Merge the single filter clusters: $C_{locked}^{l}$ = [];
				\\ \hspace{9mm} \textbf{if}{Len($C_{c}^{l}$)== 1} \textbf{then} $C_{locked}^{l}$.append($C_{c}^{l}$); k=k-1.
		\State Select the maximal c as the optimal cluster number: c=Max(c)
		\Return $C_{c}^{l}$
	\end{algorithmic}
	\label{alg:cluster}
\end{algorithm}\textbf{Filter Clustering}
~~$S_D$ derived from Eq.~3 demonstrates the functionality similarity of any two convolutional filter.
	While to evaluate all the filters' similarity in a comprehensive neural network model, we applies \textit{k}-means algorithm in each layer to cluster the filters.

As described in Alg.~\ref{alg:cluster}, we first utilize AM to obtain the visualized pattern of each convolutional filter.
	Then, we apply the \textit{k}-means algorithm to $S_D$ values of all the convolutional filters in each individual layer.
	To determine proper cluster amount in each layer (i.e. \textit{k}), a grid search is conducted from one to half of the total filter number.
	It is worth noting that there are inevitably certain amount of filters with extremely minimal similarity with others, which are also group together and won't be considered for pruning due to their possible instinct functionalities.

Each of the convolutional filter clusters can be considered as an interpretable functionality node.
	However, they can be also considered as neural network redundancy units that extract repetitive features from the inputs, and possibly defect the neural network computation efficiency.
~~Ideally, the filters with the exactly same functionality can be substituted to each other, and each cluster can reserve only one filter and prune out the rest.
	However, due to the complex mechanism of neural networks, the filters with similar functionalities may have distinct contribution to the classification results.
	Therefore a relative filter contribution evaluation inside the cluster is necessary to determine the pruning significance for each filter.

To quantitatively identify the filters' contribution variation, the backward-propagation gradients is utilized to rank the filters in each cluster.
	Given a layer $l$ with $I_{l}$ filters, the neural network output could be formulated by any layer $l$'s output feature maps in the following format:
	\begin{equation}
		Z(F, A^{l}) = F^L (... ~\alpha(F^{l+1} * A^{l}+b^{l+1}) ...) + b^L,
		\label{eq:1}
	\end{equation}
	where $F$ is the filter weights and bias in every layer.
	$A^{l}$ is the output feature maps (activation maps) of layer $l$.
	Due to the high complexity of $F$ and multiple layers' combination, an approximate function $Z$ is needed.
	Here we use $Z$'s first-order Taylor expansion for approximating:
	\begin{equation}
		Z(A^l+\Delta) = Z(A^l) + \frac{\partial Z(A^l)}{\partial A^l} \cdot \Delta.
		\label{eq:1}
	\end{equation}
	In our pruning method, when filter $i$ in the $l-1$ layer is pruned, the filter's output feature map is corresponding set to zero, i.e. changing the $i^th$ dimension of $A^l$ to zero.
	Therefore, the influence on $Z$ can be qualitatively evaluated as:
	\begin{equation}
		\begin{aligned}
			\frac{\partial Z(A_i^l)}{\partial A_i^l} \cdot \Delta, ~ where ~ \Delta = A_i^l \rightarrow 0.
			\label{eq:1}
		\end{aligned}
	\end{equation}

	Before pruning, each filter $F^{(c,l)}_i$ in the cluster $C_c^l$ of layer $l$ is firstly ranked by the contribution index, $I(F^{(c,l)}_i)$, which is calculated by examining the backward-propagation gradients.
	Specifically,
	\begin{equation}
		\medmuskip=-2mu
		I(F^{(c,l)}_i) = \frac{1}{N}\sum_{n=1}^{N} \left \| \frac{\partial Z(F, A^{l})}{\partial A_{i}^{l}(x_{n})} \right \|,
		\label{eq:contri}
		\vspace{-0.5mm}
	\end{equation}
	where the $Z(F, A^{l})$ is the neural network output loss of a sample image $n$, and the $A_i^l(x_{n})$ is the feature map of filter $i$ for each test image $n$.

	As shown in Fig.~\ref{fig:pruning}, the filters in each cluster are ranked with contribution index.
	When a specific filter pruning amount is assigned to the cluster, filters with small contribution will be firstly pruned.

\textbf{Cluster Level Pruning}
~~As filter clusters are supposed to bear certain inner redundancy, they can perform filter pruning independently.
	Hence a parallel pruning operation can be well executed in each cluster as shown in Fig.~\ref{fig:pruning}.
However, the significant variance of filter distribution also presents in each cluster.
	In each layer, a few clusters may have significantly larger volumes, while there are also a certain amount of filters that can't be well clustered.
	Therefore, a dedicated cluster level scheme is necessary to balance the pruning rates between clusters.
	Considering the cluster volume, we define an adaptive pruning rate of $R_{clt}^{(c,l)}= length(C_c^l)$.
	The $length(C_c^l)$ calculates the cluster volume size, since larger cluster volume demonstrates more filter redundancy.
	And a larger $R_{clt}^{(c,l)}$ value will lead significantly more aggressive cluster level pruning.
	Therefore, we can see different filter pruning amounts between clusters in the Fig.~\ref{fig:pruning}.

\textbf{Layer Level Pruning}% ~~In many state-of-the-art convolutional filter pruning methods, the layer level pruning is iteratively tested with retraining operation layer by layer due to unpredictable neural network accuracy defection.
	As the functionalities of each layer are relatively independent, all the layers can be pruned in a parallel manner with minor accuracy defection.
	In our pruning method, we derive a hyper-parameter $Pr$ based on each layers' accuracy sensitive to the pruning.
	Specifically, we prune filters in each layer independently and evaluate the pruned model's accuracy on the test set without retraining.
	$Pr$ is the neural network accuracy drop by pruning certain amount filters in each layer.
	Since each layer has different accuracy sensitivity to pruning, we empirically determine the same accuracy drop $Pr$ for all layers.
	By setting the hyper-parameter $Pr$, we can find a certain amount of filters $r$ to prune for each layer, corresponding amount of filters can be well assigned to each cluster as $r\cdot R_{clt}^{(c,l)}$.
	And in each cluster, the filters to be pruned can be well identified by the contribution index of $I(F_k^{(c,l)})$.

	Leveraging the convolutional filters' qualitative functionality analysis, our proposed method is expected to leverage the neural network interpretability for more accurate redundant filter allocation, faster pruning speed, and optimal computation efficiency.
\paragraph{Cluster Level Pruning}
As each cluster is supposed to bear certain inner redundancy, it can perform filter pruning independently.
	Hence a parallel pruning operation can be well executed in each cluster as shown in Fig.~\ref{fig:pruning}, which could significantly improve the network pruning speed.

However, significant variance of filter distribution also present in each cluster.
	In each layer, a few clusters may have significant larger repetition volumes, while there are also certain amount of clusters that can't be well clustered.
	Therefore, dedicated cluster level scheme is necessary to balance the pruning rates between clusters.
	Considering the cluster volume and filter contribution distribution, we define an adaptive pruning rate of $R_{clt}^{(c,l)}$:
\begin{equation}
	R_{clt}^{(c,l)}=\alpha\cdot length(C_c^l) \times \beta\cdot \frac{1}{\frac{1}{C}\sum_{n=1}^{C} I(F^{(c,l)}_i)},
	\label{eq:cluster}
\end{equation}

where $\frac{1}{C}\sum_{n=1}^{C} I(F^{(c,l)}_i)$ calculates the average contribution of filters in cluster $C^l_c$, and $length(C_c^l)$ calculates the cluster volume size.

In Eq.~\ref{eq:cluster}, large average filter contribution indicates more important filter function, while larger cluster volume demonstrates more filter redundancy.
	Taking these two factors into consideration, we set empirical factors (\textit{i.e.} $\alpha$ and $\beta$) for the clusters in each layers.
	And a larger $R_{clt}^{(c,l)}$ value will lead significantly more aggressive cluster level pruning.
	Therefore, we can see distinct pruning rates between clusters in Fig.~\ref{fig:pruning}.
\end{comment}
\section{Experiments}

In this section, we will evaluate the functionality-oriented filter pruning method and testify the interpretable filter functionality that the method derived from.

\subsection{Experiment Setup}

The visualization analysis and filter pruning method are implemented in Caffe environment (\cite{jia2014caffe}).
The evaluation is performed on our designed ConvNet and VGG-16 on CIFAR-10, VGG-16 and ResNet-32 on ImageNet.

In evaluations on CIFAR-10, a data argumentation procedure is processed firstly through horizontal flip and random crop, and a 4 pixel padded training data set is generated.
	The whole training data is utilized in calculating the contribution index in Eq.\ref{eq:contri} for more accurate estimation.
	Our designed ConvNet is designed based on the classic AlexNet model, in which the convolutional filter has the size of 3x3 and the number as the same as the original model
	\footnote{The detailed neural network structure of the ConvNet and the VGG-16 are demonstrated in the Appendix.}.

In evaluations on ImageNet, 10 classes images out of 1000 classes, namely ImageNet-10 is utilized in this work.
	And each class contains 1300 training images and 50 validation images.
	The pre-trained VGG-16 and ResNet-32 is fine-tuned on ImageNet-10 with a batch size of 128 and a learning rate of $1e^{-3}$, achieving 98.6\% and 97.1\% testing accuracy respectively.
As the same, the train data set is fully utilized to calculate the contribution index.

The ResNets-32 for ImageNet have three stages of residual blocks for feature maps with sizes of 56x56, 28x28, and 14x14.
	The three stages contain 3, 4, and 2 residual blocks respectively, and each residual block has three convolutional layers.
	The first convolutional layer of ResNets-32 is numbered as 1 and the layer number is defined to increase from shallower to deeper layers.
	Note that the projection layer located in the junction of two adjacent stages is neither numbered nor pruned as was done in other previous works.
	In each residual block, only the first two convolutional layers are pruned to keep the input and output feature maps to be identical.
	Hence, only filters from layer 2, 3, 5,... ,9 in stage 1, layer 11, 12,... 18 in stage 2, and layer 23, 24, 26, 27 in stage 3 are pruned.

In the experiment, the well trained neural network models are pruned by both proposed methods and $\ell_1$ are compared from the perspectives of layer-wise pruning, model-wise pruning, filter training behavior, as well as the overall performance.
The retraining process is executed on 20 epochs (i.e. 1/8 original training epochs) with a constant learning rate of 0.001.

  \centering
  \includegraphics[width=5.5in]{./layer_sen.pdf}
  \vspace{-6mm}
  \caption{Sensitivity of each individual layer to pruning methods.}
  \label{fig:layer}
  \vspace{-2mm}
\end{figure}

We demonstrate the effectiveness of our proposed method by comparing the accuracy defection sensitivity of individual layers to the proposed pruning method and $\ell_1$ (\cite{Li:2016:pruning}) based pruning.

The first row of Fig.~\ref{fig:layer} shows the layer-wise network accuracy degradation under different pruning ratio according to our method, while the the accuracy degradation caused by the $\ell_1$ based method is demonstrated in the second row.

The following phenomenon is observed:
(1) The proposed pruning has a slower accuracy degradation rate compared with the $\ell_1$ ranking based pruning method in majority of layers, especially in the scenario of VGG-16 on the CIFAR-10;
(2) In the proposed pruning, the accuracy degradation is slight and degradation rate is small before more than 50\% repetitive convolutional filters are pruned;
(3) The proposed method can slow down the accuracy degradation of some representative layers of ResNet-32 with the increasing of pruning rate.
The reason is the ResNet contains much less filters compared to other neural network model.
In addition, the deeper layers of ResNet are more sensitive to pruning than the shallower layers, which is different with the other network models.


As such, our proposed method demonstrates significant pruning stability and robustness in the layer-wise pruning.
It also means the functionality redundant filters can be more accurately identified and pruned through the proposed interpretive approach.

\subsection{Model-wise Pruning Analysis}\begin{figure}[b]
  \centering
  \includegraphics[width=5.5in]{./model_sen.pdf}
  \caption{Model-wise pruning analysis.}
  \label{fig:model}
  \vspace{-2mm}
\end{figure}

In this section, we further analyze the sensitivity to pruning of different neural network models.
The performance of the proposed method is compared with both the $\ell_1$ and the activation based methods.
In the activation based filter pruning method, filters with small output activation are removed~(\cite{polyak2015channel}).

Fig.~\ref{fig:model} shows the model-wise pruning results of different network models under different pruning rate in the three pruning methods that are named as \emph{Cluster}, \emph{L1}, \emph{Activation} respectively.
Here, all convolutional layers in a network model are pruned simultaneously.
The results depict that
(1) The proposed method outperforms the $\ell_1$ and activation based methods on all models, especially when the neural networks pruning rate is between 20\% and 40\%;
(2) The proposed method has comparable performance with the $\ell_1$ based method when the pruning rate is smaller than 10\%.

The above results demonstrate that our proposed method can be applied on full model compression with less accuracy degradation.
The overall performance is discussed in the next section.
\caption{CIFAR-10 Pruning Configuration}
\centering
\begin{tabular}{lllllll}
\toprule
         &    Pruning    & FLOPs    & FLOPs          & prune    & retrain        \\
network  &    Rate       & (x$10^{8}$)  & reduction      & accuracy & accuracy       \\ \midrule
ConvNet  &           0   &  8.53      &                &          &  90.05\%*        \\
prune A  &        40.2\% &  5.34      &  \textbf{37\%}     & 87.88\%  &  \textbf{90.04\%}\\
A-L1     &        40.2\% &  5.34      &  37\%          & 83.29\%  &  89.42\%         \\
prune B  &        64.5\% &  2.1       &  \textbf{63\%}     & 65.61\%  &  \textbf{88.57\%}\\
B-L1    &        64.5\% &  2.1       &  63\%          & 53.72\%  &  87.88\%         \\ \bottomrule
VGG-16   &           0   &  3.31      &                &          &  90.2\%*         \\
prune A &        42.9\% &  1.85        &  \textbf{41\%}     &  88.1\%  &  \textbf{90.3\%} \\
A-L1     &        42.9\% &  1.85      &   41\%         &  82.4\%  &  89.82  \%       \\
prune A  &        66.6\% &  1.03        &  \textbf{68\%}     &  72.1\%  &  \textbf{89.9\%} \\
B-L1    &        66.6\% &  1.03      &   68\%         &  60.2\%  &  89.73   \%      \\ \bottomrule
\end{tabular}
\label{tab:CIFAR-10}
\caption*{\hspace{-65mm}\footnotesize{*Baseline accuracy}}
\vspace{-6mm}
\end{table}\begin{table}[b]
\caption{ImageNet-10 Pruning Configuration}
\centering
\begin{tabular}{lllllll}
\toprule
         &      Pruning  & FLOPs      & FLOPs          & prune    & retrain        \\
network  &      Rate     & (x$10^{10}$) & reduction      & accuracy & accuracy       \\ \midrule
VGG-16   &           0   &  1.54      &                &          &  98.6\%*         \\
prune A  &         28.3\%&  0.84      &  \textbf{45\%}     & 51.9\%   &  \textbf{96.8\%} \\
A-L1     &         28.3\%&  0.84      &  45\%          & 40.7\%   &  95.7\%          \\
prune B  &         40.9\%&  0.57      &  \textbf{63\%}     & 25.1\%   &  \textbf{94.3\%} \\
B-L1     &         40.9\%&  0.57      &  63\%          & 21.2\%   &  93.2\%          \\ \bottomrule
ResNet-32&           0   &  2.31      &                &          &  97.12\%*        \\
prune A  &         21.5\%&  1.73      &  \textbf{25\%}     &  94.75\% &  \textbf{97.50\%}\\
A-L1     &         21.5\%&  1.73      &   25\%         &  92.87\% &  96.25\%         \\
prune B  &         30.2\%&  1.55      &  \textbf{33\%}     &  91.25\% &  \textbf{96.25\%}\\
B-L1     &         30.2\%&  1.55      &   33\%         &  90.37\% &  94.75   \%      \\ \bottomrule
\end{tabular}
\label{tab:imageNet-10}
\caption*{\hspace{-65mm}\footnotesize{*Baseline accuracy}}
\vspace{-6mm}
\end{table}

In this section, the overall pruning performance of our proposed method and the $\ell_1$ based method is evaluated and compared, which are depicted as \emph{prune} and \emph{L1} respectively in Table~\ref{tab:CIFAR-10} and Table~\ref{tab:imageNet-10}.
In the evaluation, a retraining procedure is performed to further recover the accuracy.
Table~\ref{tab:CIFAR-10} shows the performance in FLOPs reduction, accuracy after pruning, and accuracy after retraining of ConvNet and VGG on the CIFAR-10 dataset, and Table~\ref{tab:imageNet-10} shows the results of  VGG and ResNet-32 on the ImageNet-10 dataset.
In the evaluations,  two pruning configuration is utilized.
In \emph{prune A} and \emph{A-L1}, a small amount of filters are pruned and the original accuracy can be completely recovered by retraining.
\emph{prune B} and \emph{B-L1} indicates the scenarios that filters are pruned aggressively to maximize the FLOPs reduction with optimal accuracy.
The pruning rate is the percentage of the pruned over the total filters in a neural network model, and the comparison is executed on the same pruning rate.


Table~\ref{tab:CIFAR-10} and Table~\ref{tab:imageNet-10} demonstrates that our proposed method introduce less accuracy degradation than the previous $\ell_1$ based method.
In addition, the accuracy caused by the proposed pruning can be easier recovered through retraining with better accuracy.
For example, in the evaluations of VGG on the CIFAR-10 dataset, the proposed method can achieve 41\% FLOPs reduction with retraining accuracy even higher than the original value, however, the accuracy loss caused in the $\ell_1$ based pruning can not be well recovered.
As is shown in Table~\ref{tab:imageNet-10}, large accuracy loss occurs in the VGG on the ImageNet-10 dataset in the two pruning methods.
And the proposed method induces less accuracy degradation and higher retraining accuracy compared with the $\ell_1$ based pruning.
Table~\ref{tab:imageNet-10} also shows that it is hard to recover the accuracy drop through retraining in the pruning of the ResNet on the ImageNet-10 dataset, and hence acceptable pruning rate in this scenario is relatively small.

In most state-of-the-art works for neural network compression, the retraining is a necessary operation to compensate the inaccurately pruned filters and enhance the network performance.
In this section, we evaluate the retraining operation quantitatively and qualitatively.

In Fig.~\ref{fig:Retrain}, we first quantitatively explore the retraining process in terms of the model accuracy recovery.
  As shown in Fig.~\ref{fig:Retrain}, the soiled lines represent the pruned model accuracy recovery based on our proposed method whereas the dot lines represent the $\ell_1$ based method.
  We can observed that:
  (1) The models pruned by our method always demonstrate more quickly accuracy recovery.
  (2) To recover to the same accuracy, our method requires less retraining iterations.

Meanwhile, we also qualitatively explore the retraining process by the AM. Here we mainly focus on the functionality transition during network retraining process.
  As shown in Fig.~\ref{fig:Retrain_pattern}, we randomly select one filter that has not been pruned by both our method and $\ell_1$ based method and its original function analysis pattern is showed in the first column.
  Then we use the same visualization methods to visualize the pattern during different retraining iterations, e.g. every $100$ iterations.
  The AM patterns show that, regardless the retraining iterations, the remained filter function of our proposed pruning method remains unchanged.
  However, the $\ell_1$ based method changes the original filter functionality during retraining process. This means that the retraining process of the $\ell_1$ based method rebuilds the filter functionality, which therefore needs more retraining iterations to restore the model accuracy. The reasons behind that are $\ell_1$ based method may partially destruct the original neural network's functionality composition, which then need more iterations to reconstruct and balance the functionality composition\footnote{More selected filters are demonstrated in the Appendix.}.

  That's also the reason why our pruning method causes significantly less accuracy drop and requires less retraining efforts since our pruning method induces less influence to the neural network functionality composition.
  Therefore, with more interpretable and accurate redundant filter functionality identification and pruning in our method, the costly retraining process will become less necessary, as our retraining analysis results demonstrate.

\begin{figure}[t]
  \centering
  \includegraphics[width=5.5in]{./Retrain.pdf}
  \vspace{-6mm}
  \caption*{\hspace{2mm}(a) CIFAR-10\hspace{50mm} (b) ImageNet-10}
  \caption{Pruned model accuracy recovery by retraining.}
  \label{fig:Retrain}
  \vspace{-2mm}
\end{figure}\begin{comment}
\begin{table}[t]
\caption{VGG on Cifar10 Pruning Configuration}
\centering
\begin{tabular}{llllllll}
\toprule
                 &        &         & base   & prune A & A-$\ell_1$ & prune B & B-$\ell_1$  \\
layer            & output & cluster & filter & filter  & filter     & filter  & filter    \\ \midrule
Conv1\_1         & 32x32  & 15      & 64     & 19      & 19         & 36      & 36      \\
Conv2\_1         & 16x16  & 20      & 128    & 8       & 8          & 38      & 38      \\
Conv3\_1         &  8x8   & 26      & 256    & 21      & 21         & 64      & 64      \\
Conv4\_1         &  4x4   & 27      & 512    & 146     & 146        & 329     & 329     \\
Conv5\_1         &  2x2   & 46      & 512    & 398     & 398      & 483     & 483     \\ \hline
p                &        &         &        & 0.5     & -        & 1       & -     \\
FLOPs (x$10^{8}$)&  3.13  &         &  -     & 1.85    & 1.85       & 1.03    & 1.03    \\
FLOPs reduction  &        &         &  -     & \textbf{41\%}    & 41\%      & \textbf{68\%}        & 68\%     \\
prune accuracy   &        &         &  -     & 88.1\%  & 82.4\%     & 72.1\%  & 60.2\%      \\
retrain accuracy &        &         & 90.2\%*& \textbf{90.3\%}  & 89.82\%     & \textbf{89.92\%}   & 89.73\%  \\ \bottomrule
\end{tabular}
\caption*{\hspace{-105mm}\footnotesize{*Baseline accuracy}}
\label{tab:VGG}
\end{table}


\begin{table}[t]
\caption{VGG on Cifar10 Pruning Configuration}
\centering
\begin{tabular}{llllllll}
\toprule
                 &        &         & base   & prune A & A-$\ell_1$ & prune B & B-$\ell_1$  \\
layer            & output & cluster & filter & filter  & filter     & filter  & filter    \\ \midrule
Conv1\_1         & 32x32  & 15      & 64     & 19      & 19         & 36      & 36      \\
Conv1\_2         & 32x32  & 17      & 64     & 3       & 3        & 19      & 19      \\
Conv2\_1         & 16x16  & 20      & 128    & 8       & 8          & 38      & 38      \\
Conv2\_2         & 16x16  & 31      & 128    & 17      & 17         & 26      & 26      \\
Conv3\_1         &  8x8   & 26      & 256    & 21      & 21         & 64      & 64      \\
Conv3\_2         &  8x8   & 24      & 256    & 18      & 18         & 64      & 64      \\
Conv3\_3         &  8x8   & 14      & 256    & 36      & 36         & 77      & 77      \\
Conv4\_1         &  4x4   & 27      & 512    & 146     & 146        & 329     & 329     \\
Conv4\_2         &  4x4   & 28      & 512    & 313     & 313      & 424     & 424     \\
Conv4\_3         &  4x4   & 38      & 512    & 223     & 223      & 438     & 438     \\
Conv5\_1         &  2x2   & 46      & 512    & 398     & 398      & 483     & 483     \\
Conv5\_2         &  2x2   & 61      & 512    & 395     & 395      & 432     & 432     \\
Conv5\_3         &  2x2   & 40      & 512    & 218     & 218        & 384     & 384     \\ \hline
p                &        &         &        & 0.5     & -        & 1       & -     \\
FLOPs (x$10^{8}$)&  3.13  &         &  -     & 1.85    & 1.85       & 1.03    & 1.03    \\
FLOPs reduction  &        &         &  -     & \textbf{41\%}    & 41\%      & \textbf{68\%}        & 68\%     \\
prune accuracy   &        &         &  -     & 88.1\%  & 82.4\%     & 72.1\%  & 60.2\%      \\
retrain accuracy &        &         & 90.2\%*& \textbf{90.3\%}  & 89.82\%     & \textbf{89.92\%}   & 89.73\%  \\ \bottomrule
\end{tabular}
\caption*{\hspace{-105mm}\footnotesize{*Baseline accuracy}}
\label{tab:VGG}
\end{table}


\begin{table}[t]
\caption{VGG on ImageNet Pruning Configuration}
\centering
\begin{tabular}{lllllll}
\toprule
        &        &     & FLOPs    & FLOPs          & prune    & retrain        \\
network & clusters & p   & (x$10^{10}$) & reduction      & accuracy & accuracy       \\ \midrule
VGG     &          & -   &  1.54      &                &          &  98.6\%*         \\
prune A &          & 0.5 &  0.84      &  \textbf{45\%}     & 51.9\%   &  \textbf{96.8\%} \\
A-L1    &          & -   &  0.84      &  45\%          & 40.7\%   &  95.7\%          \\
prune B &          & 1.5 &  0.57      &  \textbf{63\%}     & 25.1\%   &  \textbf{94.3\%} \\
A-L1    &          & -   &  0.57      &  63\%          & 21.2\%   &  93.2\%          \\ \bottomrule
\end{tabular}
\caption*{\hspace{-80mm}\footnotesize{*Baseline accuracy}}
\end{table}


\begin{table}[t]
\caption{ResNet on ImageNet Pruning Configuration}
\centering
\begin{tabular}{lllllll}
\toprule
         &        &     & FLOPs     & FLOPs          & prune    & retrain        \\
network  & clusters & p   & (x$10^{10}$)& reduction      & accuracy & accuracy       \\ \midrule
ResNet-32&          & -   &  2.31     &                &          &  97.12\%*        \\
prune A  &          & 0.2 &  1.73       &  \textbf{25\%}     &  94.75\% &  \textbf{97.50\%}\\
A-L1     &          & -   &  1.73     &   25\%         &  92.87\% &  96.25\%         \\
prune B  &          & 0.3 &  1.55       &  \textbf{33\%}     &  91.25\% &  \textbf{96.25\%}\\
A-L1     &          & -   &  1.55     &   33\%         &  90.37\% &  94.75   \%      \\ \bottomrule
\end{tabular}
\caption*{\hspace{-80mm}\footnotesize{*Baseline accuracy}}
\end{table}


\paragraph{Visualization based Filter Clustering}

In the Table~\ref{tab:1}, we first show the filter cluster distribution based on our visualization oriented clustering method.
Then, the last three columns show the pruned models of $\ell_1$ based filter pruning [1] and two clustering based parallel pruning.

  For 13 convolutional layers of VGG-16, the filters in each layer are grouped into 14$\sim$61 clusters.
  With the increasing of layer depth, the cluster amount also increases: in layer Conv5-2, the cluster is as much as 61.
  This is because the image complexity increment for the visualized graphic patterns, which minimize the similarity between two patterns.
  Therefore, the number of filter clusters also increases to adapt to meticulous filter similarities.
  Meanwhile, our proposed method clustered as much filter as possible.
  The minimum cluster ratio in all the layers remains above 85\%.
  In overall, about 93\% filters are well clustered, indicating an efficient filter redundancy analysis capability.


\paragraph{Hierarchical Filter Pruning}

Based on the filter clustering, a hierarchical filter pruning method is proposed to different levels of the neural network.
With the help of the adaptive pruning rate, each filter cluster can be pruned in an even rate, preventing overwhelmed pruning on individual clusters.
  In the Table~\ref{tab:1}, we can see that the $\ell_1$ based state-of-the-art only prunes small amount of filters, and the filters in several middle layers remained untouched.
  In order to compare with the $\ell_1$ based pruning directly, we pruning similar amount filters in each layer, which are showed by Parallel Pruning 1.
  The Parallel Pruning 2 shows our final pruning configurations.
  Compare to state-of-the-art, our proposed clustering method significantly boost the pruning ratio, especially in the middle layers, where each filter undertakes more computation cost.
  Our clustering method increased the pruned filter number up to 2082, which is $\sim$1.3$\times$ to the state-of-the-art.


  In Fig.~\ref{fig:fp}a, the effectiveness of the hierarchical pruning scheme is demonstrated.
  We can see that, with the proposed pruning method before half of the repetitive convolutional filters are pruned, all the clusters perform an very slow accuracy dropping rate.
  Meanwhile, as shown in Fig.~\ref{fig:fp}b, the $\ell_1$ based pruning scheme demonstrated fast accuracy dropping rate.
  Therefore, the visualization-oriented filter pruning demonstrated significant pruning stability and robustness.
  In other words, through the interpretable approach, the redundant filter can be more accurately identified and pruned.

\paragraph{Per Class Pruning}
As aforementioned, leveraging higher layer filters' class level preference, we also proposed the per class pruning scheme, which can effectively reconfigure the network to class specific.
  In Fig.~\ref{fig:cc}, we use the CNN to screen cat” from random test images. According to our class label on the filter clusters, we prune all the cat-unrelated filter clusters in higher layer and examine the classification results.
\begin{wrapfigure}{r}{2.5in}
  \captionsetup{justification=centering}
  \vspace{-4mm}
  \includegraphics[width=2.5in]{./_fig/6_class.pdf}
  \vspace{-5mm}
  \caption{Per-class filter pruning}
  \label{fig:cc}
  \vspace{-5mm}
\end{wrapfigure}
  Interestingly, with more cat-unrelated filter clusters pruned, the recognition results for cat can be further improved by almost 5\%, while other classes’ results are just normally decreasing.
In other words, with the per class pruning scheme, the CNN can be configured into certain class-specific detector with outstanding accuracy.
However, it is important to note that, such an adaptation scheme is built on the assumption that the CNN is used for explicit data with specific class or the under test data demonstrate certain class consistency that can be estimated for dedicated adaptation.
Fortunately, this assumption can be extensively founded with particular CNN utilization scenarios, especially on mobile systems.

\subsection{Filter Retraining Analysis}
In most state-of-the-art works for neural network compression, the retraining is an inevitable operation to compensate the inaccurately pruned filters and enhance the network performance.
  In Fig.~\ref{fig:retrain}, we explore the effectiveness and mechanism of the retraining operation through the experimental analysis on a single convolutional layer's filter distribution.

As shown in Fig.~\ref{fig:retrain}b, the $\ell_1$ ranking based pruning significantly biases the filter distribution to the higher $\ell_1$ ranking range, resulting in considerable classification accuracy drop of 3.1\%.
  Based on this pruning result, the dedicated layer-level retraining operation is applied to reconfigure all the filters, and eventually compensate the performance by 3.2\%.
  From the filter distribution change, we can see that, the compensation is actually achieved by reconstructing the pruned filters with smaller $\ell_1$ rankings (\textit{i. e.} weight significance).
  And the filter density in the higher $\ell_1$ ranking range is correspondingly reduced, indicating certain redundancy reduction.

Meanwhile, with our proposed visualization oriented filter pruning applied as shown in Fig.~\ref{fig:retrain}c, the significance of the retraining operation is relatively weakened.
  Considering the retraining operation may change the interpretation of each filter and potentially disturb the filter clustering, we apply it only once after pruning the whole network in our proposed method.
  The experiment results show that, regardless the retraining operation, the layer's filter distribution is remained uniformed across the whole  $\ell_1$ ranking range, maintaining sufficient $\ell_1$ ranking variety.
  Therefore, the visualization oriented filter pruning only cause accuracy drop of 1.9\%, which is much less compared to the $\ell_1$ ranking based pruning with the same pruning rate.
  However, the retraining also become less effective for performance compensation.

Based on the experimental analysis of filter retraining, we can see that, the network performance is not only supported by the filters with higher weight significance but also the smaller ones.
  The idea pruning process would reduce the filter redundancy for each feature representation, while maintaining certain functional integrity.
  Meanwhile the retraining process can effectively reconstruct the smaller filters and sparse the filter distribution to compensate the performance.
  However, with more interpretable and accurate redundant filter identification and pruning, the costy retraining process will become less necessary.
\begin{figure}[t]
  \centering
  \includegraphics[width=5.5in]{./_fig/7_retrain.pdf}
  \vspace{-10mm}
    \caption{Filter retraining analysis based on $\ell_1$ ranking and Euclidean Distance of the visualized filter activation:
    (a) Original convolutional filter distribution in Conv5-1 layer. Filters demonstrate uniform distribution in all range of $\ell_1$ ranking.
    (b) Filter distribution after $\ell_1$ pruning with 50\% filters pruned (denoted in blue), and dedicated retraining operation for each layer (denoted in orange).
    (c) Filter distribution after the visualization-oriented pruning with the same pruning rate (denoted in blue), and one comprehensive retraining operation for the whole network (denoted in green).}
  \label{fig:retrain}
  \vspace{-5mm}
\end{figure}

\subsection{Comparison with state-of-the-art Methods}
The performance comparison between our proposed visualization-oriented pruning method and state-of-the-art method [1] is shown in Table 2.
    From Table~\ref{tab:2}, we can see that, our proposed method significantly outperforms state-of-the-art.
    Given the same saved FLOP ratio, our proposed method (Parallel Pruning 1) has less accuracy drop.
    Meanwhile, when the retraining operation is applied to pruning, our proposed method case (Parallel Pruning 2) can significantly enhance the computation cost optimization to 45\%, which improved the state-of-the-art by 32\%.
    Moreover, when per class pruning scheme is utilized, give 58\% computation cost optimization, the accuracy is even higher than the baseline.
    Overall, the visualization-oriented filter pruning methods dramatically outperforms state-of-the-art.

\begin{table}[b]
  \caption{Comparison with state-of-the-art Methods}
  \label{sample-table}
  \centering
  \begin{tabular}{llll}
    \toprule
    Method     & Saved FLOP (\%) &  Accuracy Drop (\%) &  Retrain Drop (\%) \\
    \midrule
    $\ell_1$           &    34          &      3.1           &     0     \\
    Parallel Pruning 1     &    34          &      1.9           &     -0.1  \\
    Parallel Pruning 2     &    45          &      4.55          &     0.1   \\
    Cat                &    58          &      -4.6          &           \\
    \bottomrule
  \end{tabular}
  \label{tab:2}
\end{table}


\begin{table}[b]
\caption{VGG Pruning Configuration}
\centering
\begin{tabular}{llllllll}
\toprule
                 &        &         & base   & prune A & A-$\ell_1$ & prune B & B-$\ell_1$  \\
layer            & output & cluster & filter & filter  & filter     & filter  & filter    \\ \midrule
Conv1\_1         & 32x32  & 15      & 64     & 34      & 34         & 26      &       \\
Conv1\_2         & 32x32  & 17      & 64     & 0       & 0        & 11      &       \\
Conv2\_1         & 16x16  & 20      & 128    & 0       & 0          & 44      &       \\
Conv2\_2         & 16x16  & 31      & 128    & 0       & 0        & 6       &       \\
Conv3\_1         &  8x8   & 26      & 256    & 0       & 0          & 25      &       \\
Conv3\_2         &  8x8   & 24      & 256    & 0       & 0          & 33      &       \\
Conv3\_3         &  8x8   & 14      & 256    & 0       & 0          & 36      &       \\
Conv4\_1         &  4x4   & 27      & 512    & 256     & 256        & 172     &       \\
Conv4\_2         &  4x4   & 28      & 512    & 258     & 256      & 204     &       \\
Conv4\_3         &  4x4   & 38      & 512    & 256     & 256      & 432     &       \\
Conv5\_1         &  2x2   & 46      & 512    & 261     & 256      & 435     &       \\
Conv5\_2         &  2x2   & 61      & 512    & 264     & 256      & 405     &       \\
Conv5\_3         &  2x2   & 40      & 512    & 264     & 256        & 273     &       \\ \hline
p                &        &         &        &         &          &         &       \\
FLOPs (x$10^{6}$)& 313    &         &  -     &         &          &         &       \\
FLOPs reduction  &        &         &  -     & 34\%    & 34\%       & 49.3\%  &       \\
accuracy drop    &        &         &  -     & 1.8     & 3.1      & 4.55    &       \\
retrain accuracy &        &         & 90.2\% & 90.3\%  & 90.2\%     & 90.1\%   &      \\ \bottomrule
\end{tabular}
\label{tab:VGG}
\end{table}

\begin{table}[]
\begin{tabular}{p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}}
\hline
\multicolumn{6}{c}{CIFAR-10}                                 &  & \multicolumn{7}{c}{ImageNet-10}                                         \\ \cline{1-6} \cline{8-14}
Network   & prune  & FLOPs & FLOPs     & prune    & retrain  &  & Network   & prune & FLOPs & FLOPs     & prune     & prune    & retrain  \\
          & ration & ()    & reduction & accuracy & accuracy &  &           &       &       & reduction & reduction & accuracy & accuracy \\ \cline{1-6} \cline{8-14}
ConvNet   &        &       &           &          &          &  & VGG       &       &       &           &           &          &          \\
Prune A   &        &       &           &          &          &  &           &       &       &           &           &          &          \\
L1\_Based &        &       &           &          &          &  &           &       &       &           &           &          &          \\
Prune B   &        &       &           &          &          &  &           &       &       &           &           &          &          \\
L1\_Based &        &       &           &          &          &  &           &       &       &           &           &          &          \\ \cline{1-6} \cline{8-14}
VGG       &        &       &           &          &          &  & ResNet-32 &       &       &           &           &          &          \\
Prune A   &        &       &           &          &          &  &           &       &       &           &           &          &          \\
L1\_Based &        &       &           &          &          &  &           &       &       &           &           &          &          \\
Prune B   &        &       &           &          &          &  &           &       &       &           &           &          &          \\
L1\_Based &        &       &           &          &          &  &           &       &       &           &           &          &          \\ \hline
\end{tabular}
\end{table}

\begin{table}[t]
\caption{Convnet on Cifar10 Pruning Configuration}
\centering
\begin{tabular}{llllllllll}
\toprule
                 &        & base    & cluster& prune A & A-$\ell_1$  & prune B & B-$\ell_1$ \\
layer            & output & filter  & distribution& filter  & filter      & filter  & filter    \\ \midrule
Conv1            & 32x32  & 96      &  15    & 42      & 42          & 64      & 54     \\
Conv2            & 32x32  & 256     &  40    & 75      & 75        & 153     & 112      \\
Conv3            & 16x16  & 384     &  53    & 54      & 54        & 167     & 85     \\
Conv4            & 16x16  & 384     &  36    & 49      & 49        & 166     & 112    \\
Conv5            & 8x8    & 256     &  24    & 85      & 85        & 149     & 112      \\ \hline
Pr               & -      & -       &  -     & 0.5     & -           & 2.5     & -        \\
FLOPs (x$10^{8}$)& 8.53   & -       &  -     & 5.34    & 5.34        & 2.1     & 2.1    \\
FLOPs reduction  & -      & -       &  -     & \textbf{37\%}    & 37\%        & \textbf{75\%}    & 75\%     \\
prune accuracy   & -      & -       &  -     & 87.88\% & 83.29\%     & 65.61\% & 53.72\%  \\
retrain accuracy & -      & 90.05\%* &  -     & \textbf{90.04\%} & 89.42\%     & \textbf{88.57\%} & 87.88\%    \\ \bottomrule
\end{tabular}
\caption*{\hspace{-112mm}\footnotesize{*Baseline accuracy}}
\label{tab:Convnet}
\end{table}


\begin{table}[t]
\caption{VGG on Cifar10 Pruning Configuration}
\centering
\begin{tabular}{lllllll}
\toprule
         &             & FLOPs    & FLOPs          & prune    & retrain        \\
network  &           Pr  & (x$10^{8}$)  & reduction      & accuracy & accuracy       \\ \midrule
VGG-16   &           -   &  3.31      &                &          &  90.2\%*         \\
prune A  &           0.5 &  1.85        &  \textbf{41\%}     &  88.1\%  &  \textbf{90.3\%} \\
A-L1     &           -   &  1.85      &   41\%         &  82.4\%  &  96.25\%         \\
prune B  &           1   &  1.03        &  \textbf{68\%}     &  72.1\%  &  \textbf{89.9\%} \\
A-L1     &           -   &  1.03      &   68\%         &  60.2\%  &  89.73   \%      \\ \bottomrule
\end{tabular}
\caption*{\hspace{-65mm}\footnotesize{*Baseline accuracy}}
\end{table}


\begin{table}[]
\begin{tabular}{p{0.4in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.01in}p{0.4in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}p{0.3in}}
\hline
\multicolumn{5}{c}{CIFAR-10}                         &  & \multicolumn{6}{c}{ImageNet-10}                                 \\ \cline{1-5} \cline{7-12}
Network   & prune  & FLOPs     & prune    & retrain  &  & Network   & prune & FLOPs     & prune     & prune    & retrain  \\
          & ratio  & reduction & accuracy & accuracy &  &           & ratio & reduction & reduction & accuracy & accuracy \\ \cline{1-5} \cline{7-12}
ConvNet   &        &           &          &          &  & VGG       &       &           &           &          &          \\
Prune A   &        &           &          &          &  &           &       &           &           &          &          \\
L1\_Based &        &           &          &          &  &           &       &           &           &          &          \\
Prune B   &        &           &          &          &  &           &       &           &           &          &          \\
L1\_Based &        &           &          &          &  &           &       &           &           &          &          \\ \cline{1-5} \cline{7-12}
VGG       &        &           &          &          &  & ResNet-32 &       &           &           &          &          \\
Prune A   &        &           &          &          &  &           &       &           &           &          &          \\
L1\_Based &        &           &          &          &  &           &       &           &           &          &          \\
Prune B   &        &           &          &          &  &           &       &           &           &          &          \\
L1\_Based &        &           &          &          &  &           &       &           &           &          &          \\ \hline
\end{tabular}
\end{table}

\end{comment}
\section{Conclusion}

In this work, through activation maximization based visualization and gradient based filter functionality analysis, we firstly show that convolutional neural network filter redundancy exists in the form of functionality repetition. In other words, the functional repetitive filters could be effectively pruned from neural network to provide computation redundancy. Based on such motivation, in this paper, we propose an interpretable filter pruning method by first clustering filters with same functions together and then reducing the repetitive filters with smallest significance and contribution. Extensive experiments on CIFAR and ImageNet demonstrate the superior performance of our interpretable pruning method over state-of-the-art method, e.g. $\ell_1$ ranking based pruning. By further analyzing the functionality changing of remaining filters in the retraining process, we show that $\ell_1$ ranking based pruning actually partially destructs original neural network's functionality composition. By contrast, our method shows consistent neuron functionality during retraining process, denoting less harm to original network composition. This reveals the underlying reasons of our methods' better performance than L1 method.
\begin{figure}[hb]
  \centering
  \includegraphics[width=5.5in]{./Retrain_visua_wide.pdf}
  \caption{Filter functionality transition during retraining.}
  \label{fig:Retrain_pattern}
  \vspace{-2mm}
\end{figure}

\bibliography{iclr2019_conference}
\bibliographystyle{iclr2019_conference}

\newpage
\section{Appendix}\subsection{Network configuration}
Table~\ref{tab:ConvNet_app} and Table~\ref{tab:VGG_app} show the detailed network structure and pruning configuration of our ConvNet and the VGG-16 on CIFAR10 respectively.
The cluster distribution is the total number of clusters in each convolutional layers.

\begin{table}[h]
\caption{ConvNet on CIFAR10 Pruning Configuration}
\centering
\begin{tabular}{llllllll}
\toprule
                 &        & cluster & base   & prune A & A-$\ell_1$ & prune B & B-$\ell_1$	\\
layer            & output & distribution & filter & filter  & filter     & filter  & filter		\\ \midrule
Pr               &        &         &        & 0.5     & -       	& 2.5     & -			\\
Conv1\_1         & 32x32  & 15      & 64     & 19      & 19         & 36      & 36			\\
Conv2\_1         & 16x16  & 20      & 128    & 8       & 8        	& 38      & 38			\\
Conv3\_1         &  8x8   & 26      & 256    & 21      & 21        	& 64      & 64			\\
Conv4\_1         &  4x4   & 27      & 512    & 146     & 146       	& 329     & 329			\\
Conv5\_1         &  2x2   & 46      & 512    & 398     & 398     	& 483     & 483			\\ \bottomrule
\end{tabular}
\label{tab:ConvNet_app}
\end{table}\vspace{-2mm}\begin{table}[h]
\caption{VGG-16 on CIFAR10 Pruning Configuration}
\centering
\begin{tabular}{llllllll}
\toprule
                 &        &  cluster& base   & prune A & A-$\ell_1$ & prune B & B-$\ell_1$	\\
layer            & output & distribution& filter & filter  & filter     & filter  & filter		\\ \midrule
Pr               &        &         &        & 0.5     & -       	& 1       & -			\\
Conv1\_1         & 32x32  & 15      & 64     & 19      & 19         & 36      & 36			\\
Conv1\_2         & 32x32  & 17      & 64     & 3       & 3       	& 19      & 19			\\
Conv2\_1         & 16x16  & 20      & 128    & 8       & 8        	& 38      & 38			\\
Conv2\_2         & 16x16  & 31      & 128    & 17      & 17       	& 26      & 26			\\
Conv3\_1         &  8x8   & 26      & 256    & 21      & 21        	& 64      & 64			\\
Conv3\_2         &  8x8   & 24      & 256    & 18      & 18        	& 64      & 64			\\
Conv3\_3         &  8x8   & 14      & 256    & 36      & 36        	& 77      & 77 			\\
Conv4\_1         &  4x4   & 27      & 512    & 146     & 146       	& 329     & 329			\\
Conv4\_2         &  4x4   & 28      & 512    & 313     & 313     	& 424     & 424			\\
Conv4\_3         &  4x4   & 38      & 512    & 223     & 223     	& 438     & 438			\\
Conv5\_1         &  2x2   & 46      & 512    & 398     & 398     	& 483     & 483			\\
Conv5\_2         &  2x2   & 61      & 512    & 395     & 395     	& 432     & 432			\\
Conv5\_3         &  2x2   & 40      & 512    & 218     & 218      	& 384     & 384			\\ \bottomrule
\end{tabular}
\label{tab:VGG_app}
\end{table}\subsection{Filter Functionality Transition During Retraining}\begin{figure}[h]
  \centering
  \includegraphics[width=5.5in]{./Retrain_visua.pdf}
  \caption{Filter functionality transition during retraining.}
  \label{fig:Retrain_visua_app}
  \vspace{-2mm}
\end{figure}

\end{document}


