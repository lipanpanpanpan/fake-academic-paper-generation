\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}


\usepackage[preprint]{nips_2018}



\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{tcolorbox}
\usepackage{subfigure}
\usepackage{makecell}

\usepackage{bbm}
\usepackage{bm}% bold math
\usepackage{ifthen}
\usepackage{rays_defs_18}
\usepackage{xcolor}

\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem{example}{Example}
\newtheorem{fact}{Fact}
\newtheorem{condition}{Condition}
\newtheorem{proposition}{Proposition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{problem}{Problem}
\newtheorem{remark}{Remark}
\newtheorem{algorithm}{Algorithm}

\newcommand\sure{\mathrm{sure}}
\newcommand\SUREest{\hat \mU_{\mathrm{SURE}}}
\newcommand\PCAest{\hat \mU_{\mathrm{PCA}}}
\newcommand\img{\vx}
\newcommand\obs{\vy}
\newcommand\noise{\vw}

\usepackage{xcolor}

\newcommand{\ali}[1]{{\color{green} [ali: #1]}}

\newcommand{\rhcomment}[1]{{\color{blue} [RH: #1]}}

\title{Unsupervised Learning with \\ Stein's Unbiased Risk Estimator}


\author{
  Christopher A. Metzler\\
  Rice University\\
  Houston, TX 77005 \\
  \texttt{chris.metzler@rice.edu} \\
  \And
  Ali Mousavi\\
  Rice University\\
  Houston, TX 77005 \\
  \texttt{ali.mousavi@rice.edu} \\
  \And
  Reinhard Heckel\\
  Rice University\\
  Houston, TX 77005 \\
  \texttt{rh43@rice.edu} \\
  \And
  Richard G. Baraniuk\\
  Rice University\\
  Houston, TX 77005 \\
  \texttt{richb@rice.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
Learning from unlabeled and noisy data is one of the grand challenges of machine learning. 
As such, it has seen a flurry of research with new ideas proposed continuously. 
In this work, we revisit a classical idea: Stein's Unbiased Risk Estimator (SURE). 
We show that, in the context of image recovery, SURE and its generalizations can be used to train convolutional neural networks (CNNs) for a range of image denoising and recovery problems {\em without any ground truth data.}

Specifically, our goal is to  reconstruct an image $\img$ from a \emph{noisy} linear transformation (measurement) of the image.
We consider two scenarios: one where no additional data is available and one where we have measurements of other images that are drawn from the same noisy distribution as $\img$, but have no access to the clean images. 
Such is the case, for instance, in the context of medical imaging, microscopy, and astronomy, where noise-less ground truth data is rarely available.

We show that in this situation, SURE can be used to estimate the mean-squared-error loss associated with an estimate of $\img$. 
Using this estimate of the loss, we train networks to perform denoising and compressed sensing recovery. 
In addition, we also use the SURE framework to partially explain and improve upon an intriguing results presented by Ulyanov et al. in \cite{DeepImagePrior}: that a network initialized with random weights and fit to a single noisy image can effectively denoise that image.

\end{abstract}

\section{Introduction}%The problem
In this work we consider reconstructing an unknown image $\img \in\mathbb{R}^n$ from measurements of the form $\obs = \mH \img + \noise$, where $\obs\in\mathbb{R}^m$ are the measurements, $\mH\in\mathbb{R}^{m\times n}$ is the linear measurement operator, and $\noise \in\mathbb{R}^m$ denotes noise.
This problem arises in numerous applications, including denoising, inpainting, superresolution, deblurring, and compressive sensing.
The goal of an image recovery algorithm is to use prior information about the image's distribution and knowledge of the measurement operator $\mH$ to reconstruct $\img$.

The key determinant of an image recovery algorithm's accuracy is the accuracy of its prior. 
Accordingly, over the past decades a large variety of increasingly complex image priors have been considered, ranging from simple sparse models \cite{SureShrink}, to non-local self-similarity priors \cite{BM3D}, all the way to neural network based priors, in particular CNN based priors \cite{DnCNN}. Among these methods, CNN priors often offer the best performance.
It is widely believed that key to the success of CNN based priors is the ability to process and learn from vast amounts of training data, although recent work suggests the structure of a CNN itself encodes strong prior information \cite{DeepImagePrior}.
Given a neural network based prior, an image can be reconstructed by enforcing the prior via empirical risk minimization~\cite{hand_global_2017}, or by training a neural network to directly reconstruct the image.

CNN image recovery methods are typically trained by taking a representative set of images $\img_1, \img_2, ... \img_L$, drawn from the same distribution as $\img$, and capturing a set of measurements $\obs_1,\obs_2 ... \obs_L$, either physically or in simulation.
The network then learns the mapping $f_\theta: \obs\rightarrow \hat{\img}$ from observations back to images by minimizing a loss function; typically the mean-squared-error (MSE) between $f_\theta(\obs)$ and $\img$. 
This presents a challenge in applications where we do not have access to example images. 
Moreover, even if we have example images, we might have a large set of measurements as well, and would like to use that set to refine our reconstruction algorithm. 

In the nomenclature of machine learning, the measurements $\obs$ would be considered features and the images $\img$ the labels. Thus when $\mH=\mathbf{I}$ the training problem simplifies to learning from noisy labels. When $\mH\neq\mathbf{I}$ the training problem is to learn from noisy linear transformations of the labels.


Learning from noisy labels has been extensively studied in the context of classification; see for instance  \cite{natarajan2013learning,xiao2015learning,liu2016classification,sukhbaatar2014training,sukhbaatar2014learning}. However, the problem of learning from noisy data has been studied far less in the context of image recovery. 
In this work we show that the SURE framework can be used to 
i) denoise an image with a neural network (NN) without any training data,
ii) train NNs to denoise images from noisy training data, and 
iii) train a NN, using only noisy measurements, to solve the compressive sensing problem. 
Specifically, \cite{MCSUREtraining} also demonstrates that SURE can be used to train CNN based denoisers without ground truth data. 
However, in this work, we go significantly beyond the setup in \cite{MCSUREtraining} and show that SURE can be applied much more broadly.
\section{SURE and its Generalizations}\label{sec:SURE}

The goal of this work is to reconstruct an image $\img$ from a noisy linear observations $\obs = \mH \img + \noise$, and knowledge of the linear measurement operator $\mH$. In addition to $(\obs,\mH)$, we assume that we are given training measurements $\obs_1,\obs_2,\ldots,\obs_L$ but not the images $\img_1,\img_2,\ldots,\img_L$ that produced them (we also consider the case where no training measurements are given). 
Without access to $\img_1,\img_2,\ldots,\img_L$ we cannot fit a model that minimizes the MSE, but we can minimize a loss based on Stein's Unbiased Risk Estimator (SURE).
In this section, we introduce SURE and its generalizations.

\paragraph{SURE.}  SURE is a model selection technique that was first proposed by its namesake in \cite{SURE}. SURE provides an unbiased
estimate of the MSE of an estimator of the mean of a Gaussian distributed random vector, with unknown mean. 
Let $\vx$ denote a vector we would like to estimate from noisy observations $\vy=\vx+\vw$ where $\vw\sim \mathcal{N}(0,\sigma^2 \mI)$. Also, assume $f_\theta(\cdot)$ is a weakly differentiable function parameterized by $\theta$ which receives noisy observations $\vy$ as input and provides an estimate of $\vx$ as output. Then, according to \cite{SURE}, we can write the expectation of the MSE with respect to the random variable $\noise$ as
\begin{align}\label{eqn:SURE_loss}
    \EX[\vw]{ \frac{1}{n}\|\vx-f_\theta(\vy)\|^2}
    =
    \EX[\vw]{ \frac{1}{n}\|\vy-f_\theta (\vy)\|^2}
    -\sigma_w^2
    +\frac{2\sigma_w^2}{n}\text{div}_\vy(f_\theta (\vy)),
\end{align}
where $\text{div}(\cdot)$ stands for divergence and is defined as
\begin{align}
    \text{div}_\vy(f_\theta (\vy))=\sum_{n=1}^N \frac{\partial f_{\theta n}(\vy)}{\partial y_n}.
\end{align}Note that two terms within the SURE loss \eqref{eqn:SURE_loss} depend on the parameter $\theta$. The first term,  $\frac{1}{n}\|\vy-f_\theta (\vy)\|^2$ minimizes the difference between the estimate and the observations (bias). The second term, $\frac{2\sigma_\vw^2}{n}\text{div}_\vy(f_\theta (\vy))$ penalizes the denoiser for varying as the input is changed (variance). Thus SURE is a natural way to control the bias variance trade-off of a recovery algorithm.

The central challenge in using SURE in practice is computing the divergence. With advanced denoisers the divergence is hard or even impossible to express analytically.% Fortunately, the divergence can be estimated and this problem can be avoided by using Monte-Carlo SURE (MCSURE).\paragraph{Monte-Carlo SURE.} MC-SURE is a Monte Carlo method to estimate the divergence, and thus the SURE loss, that was proposed in \cite{MCSURE}. In particular, the authors show that for bounded functions $f_\theta$ we have
\begin{align}\label{eqn:MCdiv}
    \text{div}_\vy(f_\theta (\vy))=\lim_{\epsilon \rightarrow 0} \mathbb{E}_\vb \left\{ \vb^t \big( \frac{f_\theta(\vy+\epsilon \vb)-f_\theta(\vy)}{\epsilon} \big)\right\},
\end{align}
where $\vb$ is an i.i.d.~Gaussian distributed random vector with unit variance elements.


Following the law of large numbers, this expectation can be approximated with Monte Carlo sampling. Thanks to the high dimensionality of images, a single sample well approximates the expectation. The limit can be approximated well by using a small value for $\epsilon$; we use $\epsilon=\frac{\max(\vy)}{1000}$ throughout. This approximation leaves us with
\begin{align}\label{eqn:MCdiv_approx}
    \text{div}_\vy(f_\theta (\vy))\approx  \vb^t \left( \frac{f_\theta(\vy+\epsilon \vb)-f_\theta(\vy)}{\epsilon} \right).
\end{align}

Combining the SURE loss \eqref{eqn:SURE_loss} with the estimate of the divergence \eqref{eqn:MCdiv_approx}, enables minimization of the MSE of a denoising algorithm without ground truth data. 

\paragraph{Generalized SURE.} GSURE was proposed in \cite{GSURE} to estimate the MSE associated with estimates of $\img$ from a linear measurement $\obs = \mH \img + \noise$, where $\mH\neq \mathbf{I}$, and $\noise$ has known covariance and follows any distribution from the exponential family. % It is a generalization of SURE that derives an estimate of the MSE loss for a linear observation of a signal contaminated with noise with known covariance and any distribution from exponential family. 
For the special case of i.i.d.~Gaussian distributed noise the estimate simplifies to
\begin{align}\label{eqn:GSURE}
    \EX[\vw]{ \frac{1}{n}\|\mathbf{P}\img-\mathbf{P}f_\theta(\vy)\|^2}
    =\mathbb{E}_\vw {\Big[} \frac{1}{n}\|\mathbf{P}\vx\|^2+\frac{1}{n}\|\mathbf{P}f_\theta(\vy)\|^2+\frac{2\sigma_\vw^2}{n}\text{div}_\vy(f_\theta (\vy))
    -\frac{2}{n}f_\theta (\vy)^t\mH^\dagger \vy {\Big]},
\end{align}
where $\mathbf{P}$ denotes orthonormal projection onto the range space of $\mH$ and $\mH^\dagger$ is the pseudoinverse of $\mH$. Note that while this expression involves the unknown $\img$, it can be minimized with respect to $\theta$ without knowledge of $\img$.

\paragraph{Propogating Gradients.}
Minimizing SURE requires propagating gradients with respect to the Monte Carlo estimate of the divergence \eqref{eqn:MCdiv_approx}. This is challenging to do by hand, but made easy by TensorFlow's and PyTorch's auto-differentiation capabilities, which used throughout much of our experiments below.


\section{Denoising Without Pre-Training\label{sec:OneShotDenoising}}

CNNs offer state-of-the-art performance for many image recovery problems including super-resolution \cite{SRCNN}, inpainting \cite{yang2017high}, and denoising \cite{DnCNN}.
Typically, these networks are trained on a large dataset of images. 
However, it was recently shown that if just fit to a single corrupted image, without any pre-training, CNNs can still perform effective image recovery. 
Specifically,~the recent work {\it Deep Image Prior}\cite{DeepImagePrior} demonstrated that a randomly initialized expansive CNN, trained so that for a fixed input the output matches the corrupted image well, performs exceptionally well at the aforementioned inverse problems.

Similar to more traditional image recovery methods like BM3D \cite{BM3D}, the deep image prior method only exploits structure within a given image and does not use any external training data.
In this paper, we consider a slight variation of the original deep image prior that empirically performs better on the examples considered.
This variation---as well as the original deep prior work---requires the training to be stopped early, and the performance depends on hitting the right stopping time.
We then show that, in the context of denoising, training using the SURE loss allows us to train without early stopping and improves the performance over the original deep image prior trained with least squares loss.


The deep image prior is a CNN, denoted by $f_\theta$ and parameterized by the weight and bias vector $\theta$ that maps a latent representation $\vz$ to an image $f_\theta(\vz)$. 
The paper~\cite{DeepImagePrior} proposes to minimize
\[
E(f_\theta(\vz), \vy),
\]
over the parameters $\theta$, starting from a random initialization, using a method such as gradient descent. 
Here, $E$ is a loss function, chosen as the squared $\ell_2$-norm (i.e., $E(\vx, \vx') = \norm[2]{\vx - \vx'}^2$, and $\vz$ is the latent representation, potentially chosen at random. 


In this paper, we consider a variant of the original Deep Image Prior work, where $\vz$ is set equal to $\vy$. %, thus the network can be thought of as an autoencoder. %Instead of training on several images, we only train the network to fit a single image.
Following \cite{DeepImagePrior}, our goal is to train the network such that it fits the image but not the noise. 
If the network is trained using too few iterations, then the network does not represent the image well. 
If the network is trained until convergence, however, then the network's parameters are overfit and describe the noise along with the actual image.

Thus, there is a sweet spot, where the network minimizes the error between the true image and the reconstructed image.
This is illustrated in Figure~\ref{fig:ReconErrs}(a).
To obtain Figure~\ref{fig:ReconErrs}(a), we repeated the experiments in~\cite{DeepImagePrior} with a very large U-net architecture \cite{Unet} where we replaced $\vz$ with the the noisy observation $\vy$.
We note that this network offered performance superior the the network originally used in \cite{DeepImagePrior}, which is initialized with a random $\vz$ and not the noisy image $\vy$ itself. However, both choosing $\vz=\vy$ and choosing $\vz$ randomly performs very similar, and qualitatively both choices behave the same in that both require early stopping to achieve optimal performance.

In more detail, U-net \cite{Unet} consists of a cascade $3\times 3$ convolution and relu layers that are sequentially downsampled and then upsampled. The convolutional layers at each ``level'' of upsampling/downsampling are connected together by skip connections. The U-net we used had $128$ features at each convolution layer.
This particular recovery problem was to denoise a $512\times 512$ Mandrill image which was contaminated with additive white Gaussian noise (AWGN) with standard deviation 25.
It can be seen that after a few iterations, the network begins to overfit to the noisy image and normalized MSE (NMSE) starts to increase. 
Unfortunately, by itself the training loss gives little indication about when to stop training; it smoothly decays towards $0$.

\begin{figure}[t]
\centering
\subfigure[U-net Data Fidelity Training Loss]{\includegraphics[width=.49\textwidth]{figs/DeepPrior_TrainedwrtNoisy.png}}
\subfigure[U-net SURE Training Loss]{\includegraphics[width=.49\textwidth]{figs/DeepPrior_TrainedwrtSURE.png}}
\caption{ The training and test errors networks trained with $\frac{1}{n}\|\vy-f_\theta(\vy)\|^2$ Loss (a) and the SURE Loss \eqref{eqn:SURE_loss} (b). Without the SURE divergence term, the network starts to overfit and the NMSE worsens even while the training loss improves. Minimizing the SURE loss minimizes the NMSE.}
\label{fig:ReconErrs}
\end{figure}\subsection{SURE Deep Image Prior}

Returning to Figure \ref{fig:ReconErrs}(a), we observe that the MSE is minimized at a point where  the training loss is reasonably small, but at the same time the network divergence (scaled by $2\sigma_w^2/n$) is not too large; thus the sum of the two terms is small. 
In fact, if one plots the SURE estimate of the loss (not shown here), then it lies directly on top of the observed NMSE.

Inspired by this observation, we propose to train with the SURE loss instead of the fidelity or $\ell_2$ loss. The results are shown in~Figure~\ref{fig:ReconErrs}(b). It can be seen that not only does the network avoid overfitting, the final NMSE is superior to that achieved by optimal stopping with the fidelity loss.%, as proposed in~\cite{DeepImagePrior}.%However, since we do not know this optimal point, it is difficult to decide when to stop training.%As we demonstrate in this paper, the SURE loss enables us to solve this dilemma, and in addition, leads to even better performance.% % Table generated by Excel2LaTeX from sheet 'Sheet1'% \begin{table}[htbp]%   \centering%   \caption{Add caption}%     \begin{tabular}{lrrrrrrr}%           & \multicolumn{1}{l}{Test Time} & \multicolumn{1}{l}{Barbara} & \multicolumn{1}{l}{Boat} & \multicolumn{1}{l}{Couple} & \multicolumn{1}{l}{House} & \multicolumn{1}{l}{Mandrill} & \multicolumn{1}{l}{Bridge} \\%     CBM3D &       &       &       &       &       &       &  \\%     DNCNN &       &       &       &       &       &       &  \\%     Deep Prior &       &       &       &       &       &       &  \\%     U-net  &       &       &       &       &       &       &  \\%     \end{tabular}%%   \label{tab:denoisecolor}%% \end{table}%\begin{figure}[t]
\centering
\subfigure[Original Noisy Image]{\includegraphics[width=.24\textwidth]{figs/Mandrill_Noisy.png}}
\subfigure[CBM3D (25.9 dB, 4.84 sec)]{\includegraphics[width=.24\textwidth]{figs/BM3D_Mandrill.png}}
\subfigure[DnCNN (26.4 dB, 0.0087 sec)]{\includegraphics[width=.24\textwidth]{figs/DnCNN_Mandrill.png}}
\subfigure[Untrained U-net (26.3 dB, 72.15 sec)]{\includegraphics[width=.24\textwidth]{figs/Mandrill_Unet_Denoised.png}}
\caption{Reconstructions of $512 \times 512$ color Mandrill test image tested with AWGN with standard deviation 25. The untrained CNN offers performance similar to state-of-the-art methods but takes much longer.}
\label{fig:DeepPrior}
\end{figure}

We tested the performance of this large U-net trained only on the image to be denoised against state-of-the-art trained and untrained algorithms DnCNN~\cite{DnCNN} and CBM3D~\cite{BM3D}. Results are presented in Figure~\ref{fig:DeepPrior}. When trained with the SURE loss the U-net trained only on the image to be denoised offers performance competitive with these two methods. However, the figure also demonstrates that the deep prior method is slow and that training across additional images, as exemplified by DnCNN, is beneficial.

The experiments in this section were conducted using Torch implementations of U-net and MatConvnet implementations of DnCNN. 
The BM3D and CBM3D experiments were used Matlab implementations, based on compiled C. Public implementations of the algorithms can be found in the following locations:
DnCNN MatConvnet: \url{https://github.com/cszn/DnCNN};
U-net Torch: \url{https://github.com/cszn/DnCNN};
BM3D Matlab: \url{http://www.cs.tut.fi/~foi/GCF-BM3D/index.html#ref_software}.
The U-net was trained using the Adam optimizer \cite{ADAMopt} with a learning rate of 0.001.

Throughout the paper we report recovery accuracies in terms of PSNR, defined as 
\[
\text{PSNR}=10\log_{10}\bigg(\frac{255^2}{\|\hat{x}-x\|^2}\bigg),
\]
where all images lie in the range $[0,255]$. 
All experiments were performed on a desktop with an Intel 6800K CPU and an Nvidia Pascal Titan X GPU.

\section{Denoising with Noisy Training Data}\label{sec:Denoising}

In the previous section we showed that SURE can be used to fit a CNN denoiser without training data. This is useful in regimes where no training data whatsoever is available. However, as the previous section demonstrated, these untrained neural networks are computationally very expensive to apply.

In this section we focus on the scenario where we can capture additional {\em noisy} training data with which to train a neural network for denoising. 
We show that if provided a set of $L$\emph{noisy} images $\vy_1, \vy_2, \ldots, \vy_L$, 
training the network with the SURE loss improves upon training with the MSE loss. Specifically, we optimize the network's parameters by minimizing
\[
\sum_{\ell=1}^L  \frac{1}{n}\|\vy_\ell-f_\theta (\vy_\ell)\|^2-\sigma_w^2+\frac{2\sigma_w^2}{n}\text{div}_{\vy_\ell}\{(f_\theta (\vy_\ell))\},
\]
rather than 
\[
\sum_{\ell=1}^L \frac{1}{n}\|\vx_\ell-f_\theta(\vy_\ell)\|^2.
\]
As before we will use the Monte-Carlo estimate of the divergence \eqref{eqn:MCdiv}.

\subsection{DnCNN and Experimental Setup\label{sec:expsetupDnCNN}}\begin{figure}[t]
\centering
\subfigure[]{\includegraphics[width=.16\textwidth]{figs/Gray_2092.png}}
\subfigure[]{\includegraphics[width=.16\textwidth]{figs/Gray_23025.png}}
\subfigure[]{\includegraphics[width=.16\textwidth]{figs/Gray_35010.png}}
\subfigure[]{\includegraphics[width=.16\textwidth]{figs/Gray_48055.png}}
\subfigure[]{\includegraphics[width=.16\textwidth]{figs/Gray_65132.png}}
\subfigure[]{\includegraphics[width=.16\textwidth]{figs/Gray_94079.png}}
\caption{Six representative training images from the BSD dataset.}\label{fig:TrainingImages}

\subfigure[Barbara]{\includegraphics[width=.16\textwidth]{figs/barbara.png}}
\subfigure[Boat]{\includegraphics[width=.16\textwidth]{figs/boat.png}}
\subfigure[Couple]{\includegraphics[width=.16\textwidth]{figs/couple.png}}
\subfigure[House]{\includegraphics[width=.16\textwidth]{figs/house.png}}
\subfigure[Mandrill]{\includegraphics[width=.16\textwidth]{figs/mandrill.png}}
\subfigure[Bridge]{\includegraphics[width=.16\textwidth]{figs/streamandbridge.png}}
\caption{The six test images. They follow a similar distribution to the training images.} \label{fig:NaturalTestImages}


\end{figure}In this section, we consider the DnCNN image denoiser, trained on grayscale images pulled from Berkeley's BSD-500 dataset~\cite{BSDDataset}, and trained using the MSE and SURE loss. Example images from this dataset are presented in Figure~\ref{fig:TrainingImages}. 
The training images were cropped, rescaled, flipped, and rotated to form a set of 204\,800 overlapping $40\times 40$ patches. % The validation images were cropped to form 10\,000 non-overlapping $40\times 40$ patches. 
We tested the methods on 6 standard test images presented in Figure \ref{fig:NaturalTestImages}. %, and a set of ``unnatural'' images presented in Figure \ref{fig:UnnaturalTestImages}. Unless otherwise noted, the networks were tested using $256\times 256$ versions of the test images.

The DnCNN image denoiser \cite{DnCNN} consists of 16 sequential $3\times 3$ convolutional layers with $64$ features at each layer. Sandwiched between these layers are ReLU and batch-normalization operations. DnCNN has a single skip connection to its output and is trained using residual learning \cite{residuallearning}.

The experiments in this section were performed using a Tensorflow implementations of DnCNN available at \url{https://github.com/crisb-DUT/DnCNN-tensorflow}.
We trained all networks for 50 epochs using the Adam optimizer \cite{ADAMopt} with a training rate of 0.001 which was reduced to 0.0001 after 30 epochs. We used mini-batches of 128 patches. 

\paragraph{Results.}% We trained the DnCNN using the MSE and SURE loss. %We also trained and tested DnCNN and U-net networks that were initialized with random weights and trained to fit an image using SURE, as described in the last section. With grayscale images the performance of this method was not competitive and so these results have been omitted for space.
The results of training DnCNN using the MSE and SURE loss are presented in Figure \ref{fig:VisComparoDenoising} and Table \ref{tab:DnCNN}. 
The results show that training DnCNN with SURE on noisy data results in reconstructions almost equivalent to training with the true MSE on clean images. Moreover, both DnCNN trained with SURE and with the true MSE outperform BM3D. As expected, because calculating the SURE loss requires two calls to the denoiser, it takes roughly twice as long to train as does training with the MSE.

\begin{figure}[t]
\centering
\subfigure[Original Noisy Image]{\includegraphics[width=.24\textwidth]{figs/Stream_Noisy.png}} %26.4 dB
\subfigure[BM3D (26.0 dB, 4.01 sec)]{\includegraphics[width=.24\textwidth]{figs/Stream_BM3D.png}} %26.4 dB
\subfigure[DnCNN SURE (26.5 dB, .04 sec)]{\includegraphics[width=.24\textwidth]{figs/Stream_SURE.png}} %27.2 dB,
\subfigure[DnCNN MSE (26.7 dB, .04 sec)]{\includegraphics[width=.24\textwidth]{figs/Stream_MSE.png}} %28.1 dB
\caption{Reconstructions of $512 \times 512$ grayscale Bridge test image tested with AWGN with standard deviation 25. The CNN trained with noisy data offers performance similar to state-of-the-art methods.
}
\label{fig:VisComparoDenoising}
\end{figure}% Table generated by Excel2LaTeX from sheet 'Sheet1'\begin{table}[htbp]
  \begin{center}
    \begin{tabular}{lrrrrrrrr}
          & \multicolumn{1}{l}{\makecell{Training\\Time}} & \multicolumn{1}{l}{\makecell{Test\\Time}} & \multicolumn{1}{l}{Barbara} & \multicolumn{1}{l}{Boat} & \multicolumn{1}{l}{Couple} & \multicolumn{1}{l}{House} & \multicolumn{1}{l}{Mandrill} & \multicolumn{1}{l}{Bridge} \\
          \cmidrule{2-9}
    BM3D  & \multicolumn{1}{l}{\makecell{N/A}} & 0.82 sec & \textbf{29.8} & 28.7 & 29.2 & 32.8 & 25.8  & 26.2 \\
    \hline
    DnCNN SURE & 8.1 hrs &  0.01 sec & 29.1 & 28.9 & 29.2 & 32.4 & 26.1 & 26.6 \\
    \hline
    DnCNN MSE & 4.3 hrs & 0.01 sec & 29.5 & \textbf{29.2} & \textbf{29.6}  & \textbf{33.1 }& \textbf{26.4} & \textbf{26.8} \\
    \hline
    \end{tabular}%
    \end{center}
  \caption{\label{tab:DnCNN} Reconstruction results for $256\times 256$ images with AWGN with standard deviation 25. Even though the DnCNN SURE only has noisy training data, it performs almost as well as the DnCNN MSE training on clean data.}
\end{table}%% % Table generated by Excel2LaTeX from sheet 'Sheet1'% \begin{table}[htbp]%   \centering%   \caption{Add caption}\label{tab:DnCNN_Overall}%%     \begin{tabular}{lrrrr}%           & \multicolumn{1}{l}{Training Time} & \multicolumn{1}{l}{Test Time} & \multicolumn{1}{l}{ Average PSNR: Natural} & \multicolumn{1}{l}{Average PSNR: Unnatural} \\%     BM3D  & \multicolumn{1}{l}{N/A} & \multicolumn{1}{l}{Slow} &       &  \\%     DnCNN One SURE & \multicolumn{1}{l}{N/A} & 322   &       &  \\%     DnCNN Many SURE & 20548 & 0.7   &       &  \\%     DnCNN Many MSE & 9515  & 0.7   &       &  \\%     \end{tabular}%%  \bigskip % %\end{table}%% % Table generated by Excel2LaTeX from sheet 'Sheet1'% %\begin{table}[htbp]%   %\centering%   \caption{Add caption}\label{tab:DnCNN_natural}%%     \begin{tabular}{lrrrrrr}%           & \multicolumn{1}{l}{Barbara} & \multicolumn{1}{l}{Boat} & \multicolumn{1}{l}{Couple} & \multicolumn{1}{l}{House} & \multicolumn{1}{l}{Mandrill} & \multicolumn{1}{l}{Bridge} \\%     BM3D  &       &       &       &       &       &  \\%     DnCNN One SURE & 26.9  & 26.9  & 27.36 & 28.43 & 24.97 & 25.46 \\%     DnCNN Many SURE & 29.24 & 29.51 & 29.72 & 32.86 & 26.59 & 27.09 \\%     DnCNN Many MSE & 29.34 & 29.53 & 29.79 & 32.99 & 26.71 & 27.14 \\%     \end{tabular}%% % %\end{table}%% % \bigskip% % % Table generated by Excel2LaTeX from sheet 'Sheet1'% % %\begin{table}[htbp]% %   %\centering% %   \caption{Add caption}\label{tab:DnCNN_unnatural}%% %     \begin{tabular}{lrrrrrr}% %           & \multicolumn{1}{l}{Flinstones} & \multicolumn{1}{l}{Fingerprint} & \multicolumn{1}{l}{Yeast} & \multicolumn{1}{l}{Ecoli} & \multicolumn{1}{l}{Rice Owl} & \multicolumn{1}{l}{Pillars of Creation} \\% %     BM3D  &       &       &       &       &       &  \\% %     DnCNN One SURE & 25.67 & 25.22 &       &       &       &  \\% %     DnCNN Many SURE & 26.86 & 25.99 &       &       &       &  \\% %     DnCNN Many MSE & 26.85 & 25.96 &       &       &       &  \\% %     \end{tabular}%% \end{table}%

\section{Compressive Sensing Recovery with Noisy Measurements}\label{sec:CS}
In this section we study the problem of image recovery from linear undersampled measurements. The main novelty of this section is that we do not have training labels (i.e., ground truth images). Instead and unlike conventional learning approaches in compressive sensing (CS), we train the recovery algorithm using \emph{only noisy undersampled linear measurements}.

The proposed method is closely related to Parameterless Optimal AMP \cite{ParameterlessAMP}, which used the SURE loss to tune AMP \cite{AMP} to reconstruct signal with unknown sparsity in a known dictionary. 
The proposed method is somewhat related to blind CS \cite{BlindCS} wherein signals that are sparse with respect to an unknown dictionary are reconstructed from compressive measurements. 
At a high level, the proposed method can be considered a practical form of universal CS \cite{jalali2014minimum,BaronUniversalCS1, BaronUniversalCS2,jalali2017universal} wherein a signal's distribution is estimated from noisy linear measurements and used to reconstruct the signal. 

\subsection{LDAMP}

We used the Learned Denoising-based Approximate Message Passing (LDAMP) network proposed in \cite{LDAMP} as our recovery method. LDAMP offers state-of-the-art CS recovery when dealing with measurement matrices with i.i.d.~Gaussian distributed elements. LDAMP is an unrolled version of the DAMP~\cite{DAMP} algorithm that decouples signal recovery into a series of denoising problems solved at every layer. In other words and as shown in \eqref{eqn:LDAMP}, LDAMP receives a noisy version of $\vx$ (i.e., $\vx^k+\mathbf{A}^*\vz^k=\vx+\sigma \vnu$) at every layer and tries to reconstruct $\vx$ by eliminating the effective noise vector $\sigma \vnu$. 

The success of LDAMP stems from its Onsager correction term (i.e., the last term on the first line of \eqref{eqn:LDAMP}) that removes the bias from intermediate solutions. As a result, at each layer the effective noise term $\sigma\vnu$ follows a Gaussian distribution whose variance is accurately predicted by $({\sigma}^k)^2$\cite{MalekiThesis}.
 Learned Denoising-based AMP (LDAMP) Neural Network
 For $k=1,...~K$
\begin{eqnarray}\label{eqn:LDAMP}
\vz^k&=& \vy-\mH\vx^k+\frac{1}{m}\vz^{k-1} {\rm div} D^{k-1}_{\theta^{k-1}}(\vx^{k-1}+\mH^*\vz^{k-1}) \nonumber \\
{\sigma}^k&=& \frac{\|\vz^k\|_2}{\sqrt{m}} \nonumber \\
\vx^{k+1} &=& D^{k}_{\theta^k} (\vx^k+\mH^*\vz^k)
\end{eqnarray}
\end{tcolorbox}

In this work, we use a $K=10$ layer LDAMP network, where each layer itself contains a $16$ layer DnCNN denoiser $D^k_{\theta^k}(\cdot)$. % which itself has $16$ convolutional layers. 
Below we use $f_\theta(\cdot)$ to denote LDAMP.

\subsection{Training LDAMP and Experimental Setup}%We express LDAMP as a parameterized network $f_\theta$ that produces an estimate of the signal $\vx$ from the noisy linear measurements $\vy$. 
We compare three methods of training LDAMP. All three methods utilize layer-by-layer training, which in the context of LDAMP is minimum MSE optimal \cite{LDAMP}. 


The first method, LDAMP MSE, simply minimizes the MSE with respect to the training data.
The second method, LDAMP SURE, uses SURE to train LDAMP using only noisy measurements. 
This method takes advantage of the fact that at each layer LDAMP is solving denoising problems with known variance ${{\sigma}^k}^2$\[
\theta_{\text{SURE}}=\arg\min_\theta \sum_{\ell=1}^L  \frac{1}{n}\|\vy_\ell-f_\theta (\vy_\ell)\|^2-{{\sigma}^k}^2+\frac{2{({\sigma}^k})^2}{n}\text{div}_{\vy_\ell}(f_\theta (\vy_\ell)).
\]

The third and final method, LDAMP GSURE, uses generalized SURE to train LDAMP using only noisy measurements.  
\[
\theta_{\text{GSURE}}=\arg\min_\theta \sum_{\ell=1}^L  \frac{1}{n}\|\mathbf{P}f_\theta(\vy_\ell)\|^2+\frac{2\sigma_\vw^2}{n}\text{div}_\vy(f_\theta (\vy_\ell))
-\frac{2}{n}f_\theta (\vy_\ell)^t\mH^\dagger \vy_\ell.
\]%Note that LDAMP SURE uses the estimate ${{\sigma}^k}^2$ of the effective noise variance whereas LDAMP GSURE uses the measurement noise variance $\sigma_w^2$. 
The SURE and GSURE methods both rely upon Monte-Carlo estimation of the divergence \eqref{eqn:MCdiv}.

The experiments in this section were performed using a TensorFlow implementation of LDAMP available online at \url{https://github.com/ricedsp/D-AMP_Toolbox}. We used $m\times n$ dense Gaussian measurement matrices for our low resolution training and coded diffraction patterns, which offer $O(n\log n)$ multiplications, for our high resolution testing. For both training and testing we sampled with $\frac{m}{n}=.2$ and $\sigma_w=1$. See \cite{LDAMP} for more details about the measurement process. Other experimental settings such as batch sizes, learning rates, etc. are the same as those in Section~\ref{sec:expsetupDnCNN}.

\paragraph{Results.}% Here we compare the three versions of LDAMP with BM3D-AMP \cite{DAMP}. %While SURE could also be used to train and apply LDAMP to a single image, as in the denoising experiments in Section \ref{sec:OneShotDenoising}, this method is too slow to be of practical interest.% and so we do not test it here.%Do not test CS with SURE and single image; too slow to apply.%All three methods were trained to reconstruct data % The results are presented in Figure \ref{fig:VisualComparoCS} and Tables \ref{tab:CS1}. The results demonstrate LDAMP trained with SURE and with the MSE both perform roughly on par with BM3D-AMP and run significantly faster. 
The networks resulting from training LDAMP with the MSE, SURE, and GSURE losses are compared to BM3D-AMP \cite{DAMP} in Figure \ref{fig:VisualComparoCS} and Table \ref{tab:CS1}. The results demonstrate LDAMP networks trained with SURE and MSE both perform roughly on par with BM3D-AMP and run significantly faster. 
The results also demonstrate that LDAMP trained with GSURE offers significantly reduced performance. This can be understood by returning to the original GSURE cost \eqref{eqn:GSURE}. Minimizes GSURE minimizes the distance between $\mathbf{P}\vx$ and $\mathbf{P}f_\theta(\vy)$ not between $\vx$ and $f_\theta(\vy)$, where $\mathbf{P}$ denotes orthogonal projection onto the range space of $\mathbf{H}$.
In the context of compressive sensing, the range space is small and so these two distances are not necessarily close to one another.

\centering
\subfigure[BM3D-AMP (31.3 dB, 13.2 sec)]{\includegraphics[width=.24\textwidth]{figs/BM3D-AMP_boat_256.png}} %26.4 dB
\subfigure[LDAMP GSURE (25.7 dB, 0.4 sec)]{\includegraphics[width=.24\textwidth]{figs/LDAMP_GSURE_Boat.png}} %26.4 dB
\subfigure[LDAMP SURE (31.9 dB, 0.4 sec)]{\includegraphics[width=.24\textwidth]{figs/LDAMP_SURE_Boat.png}} %27.2 dB, 
\subfigure[LDAMP MSE (34.6 dB, 0.4 sec)]{\includegraphics[width=.24\textwidth]{figs/LDAMP_MSE_Boat.png}} %28.1 dB
\caption{Reconstructions of $256 \times 256$ Boat test image sampled at a rate of $\frac{m}{n}=0.2$ using coded diffraction pattern measurements and i.i.d.~complex Gaussian distributed measurement noise with standard deviation 1. LDAMP trained with SURE offers performance on par with BM3D-AMP. % \richb{can you fix the a  b c d spilling over 2 lines?}
}
\label{fig:VisualComparoCS}
\end{figure}% Table generated by Excel2LaTeX from sheet 'Sheet1'\begin{table}[t]
  \begin{center}
    \begin{tabular}{crrrrrrrr}
          & \multicolumn{1}{l}{\makecell{Training\\Time}} & \multicolumn{1}{l}{\makecell{Test\\Time}} & \multicolumn{1}{l}{Barbara} & \multicolumn{1}{l}{Boat} & \multicolumn{1}{l}{Couple} & \multicolumn{1}{l}{House} & \multicolumn{1}{l}{Mandrill} & \multicolumn{1}{l}{Bridge} \\
          \cmidrule{2-9}
    \makecell{BM3D-AMP}  & N/A   & 13.2 sec & \textbf{35.6}  & 31.3 & 33.2 & \textbf{39.1} & 24.1 & 25.6 \\
    \hline
    \makecell{LDAMP\\SURE} & 34.7 hrs & 0.4 sec & 30.6 & 31.9 & 31.7 & 36.6 & 26.4 & 28.0 \\
    \hline
    \makecell{LDAMP\\GSURE} & 43.2 hrs & 0.4 sec & 26.3 & 25.7 & 25.9 & 28.6 & 24.5 & 24.4 \\
    \hline
    \makecell{LDAMP\\MSE} & 20.7 hrs & 0.4 sec & 32.1 & \textbf{34.6 }& \textbf{35.0} & 38.6 & \textbf{27.3} & \textbf{28.6 }\\
    \hline
    \end{tabular}%
  \end{center}
  
  \caption{\label{tab:CS1}Reconstruction results for $256\times 256$ images sampled at $\frac{m}{n}=0.2$, a coded diffraction pattern measurement matrix, and complex i.i.d.~Gaussian noise with variance $1$.}
  
\end{table}%% % Table generated by Excel2LaTeX from sheet 'Sheet1'% \begin{table}[htbp]%   \centering%   \caption{Add caption} \label{tab:CS1}%%     \begin{tabular}{lrrrr}%           & \multicolumn{1}{l}{Training Time} & \multicolumn{1}{l}{Test Time} & \multicolumn{1}{l}{ Average PSNR: Natural} & \multicolumn{1}{l}{Average PSNR: Unnatural} \\%     BM3D-AMP  & \multicolumn{1}{l}{N/A} & \multicolumn{1}{l}{Slow} &       &  \\%     LDAMP Many GSURE & \multicolumn{1}{l}{N/A} & 322   &       &  \\%     LDAMP Many SURE & 20548 & 0.7   &       &  \\%     LDAMP Many MSE & 9515  & 0.7   &       &  \\%     \end{tabular}%% %\end{table}%% \bigskip% % Table generated by Excel2LaTeX from sheet 'Sheet1'% %\begin{table}[htbp]%   \centering%   \caption{Add caption}\label{tab:CS2}%%     \begin{tabular}{lrrrrrr}%           & \multicolumn{1}{l}{Barbara} & \multicolumn{1}{l}{Boat} & \multicolumn{1}{l}{Couple} & \multicolumn{1}{l}{House} & \multicolumn{1}{l}{Mandrill} & \multicolumn{1}{l}{Bridge} \\%     BM3D-AMP &       &       &       &       &       &  \\%     LDAMP Many GSURE & 26.9  & 26.9  & 27.36 & 28.43 & 24.97 & 25.46 \\%     LDAMP Many SURE & 29.24 & 29.51 & 29.72 & 32.86 & 26.59 & 27.09 \\%     LDAMP Many MSE & 29.34 & 29.53 & 29.79 & 32.99 & 26.71 & 27.14 \\%     \end{tabular}%% % %\end{table}%% % \bigskip% % % Table generated by Excel2LaTeX from sheet 'Sheet1'% % %\begin{table}[htbp]% %   \centering% %   \caption{Add caption}\label{tab:CS3}%% %     \begin{tabular}{lrrrrrr}% %           & \multicolumn{1}{l}{Flinstones} & \multicolumn{1}{l}{Fingerprint} & \multicolumn{1}{l}{Yeast} & \multicolumn{1}{l}{Ecoli} & \multicolumn{1}{l}{Rice Owl} & \multicolumn{1}{l}{Pillars of Creation} \\% %     BM3D-AMP &       &       &       &       &       &  \\% %     LDAMP Many GSURE & 26.9  & 26.9  & 27.36 & 28.43 & 24.97 & 25.46 \\% %     LDAMP Many SURE & 29.24 & 29.51 & 29.72 & 32.86 & 26.59 & 27.09 \\% %     LDAMP Many MSE & 29.34 & 29.53 & 29.79 & 32.99 & 26.71 & 27.14 \\% %     \end{tabular}%% \end{table}%



\section{Conclusions}

We have made three distinct contributions. 
First we showed that SURE can be used to denoise an image using a CNN without any training data. 
Second, we demonstrated that SURE can be used to train a CNN denoiser using only noisy training data. 
Third, we showed that SURE can be used to train a neural network, using only noisy measurements, to solve the compressive sensing problem. 

In the context of imaging, our work suggests a new hands-off approach to reconstruct images. Using SURE, one could toss a sensor into a novel imaging environment and have the sensor itself figure out and then apply the appropriate prior to reconstruct images.

In the context of machine learning, our work suggests that divergence may be an overlooked proxy for variance in an estimator. Thus, while SURE is applicable in only fairly specific circumstances, penalizing divergence could be applied more broadly as a tool to help attack overfitting.










{\small
	\bibliographystyle{./IEEEtran}
	\bibliography{./refs}
}










\end{document}


