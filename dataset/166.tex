\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{marvosym}
\usepackage[numbers]{natbib}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{color}
\usepackage[it,small]{caption}
\usepackage{subcaption}
\usepackage{dsfont}
\usepackage{enumitem}
\usepackage{authblk}
\newcommand{\ve}[1]{\mathbf{#1}}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\rc}[1]{\textcolor{red}{#1}}
\newcommand{\argmax}{\operatorname{arg\,max}}
\newcommand{\argmin}{\operatorname{arg\,min}}
\newcommand{\todo}[1]{\textcolor{blue}{\textbf{#1}}}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\def\cvprPaperID{1033} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\cvprfinalcopy

\pagenumbering{gobble}



  \abovedisplayskip 3.0pt plus2pt minus2pt%\belowdisplayskip\abovedisplayskip\renewcommand{\baselinestretch}{0.97}\newenvironment{packed_enum}{
\begin{enumerate}
  \setlength{\itemsep}{0pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{enumerate}}\newenvironment{packed_item}{
\begin{itemize}
  \setlength{\itemsep}{0pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}\newlength\savedwidth\newcommand\whline[1]{\noalign{\global\savedwidth\arrayrulewidth
								\global\arrayrulewidth #1} %
					   \hline
					   \noalign{\global\arrayrulewidth\savedwidth}}\renewcommand\multirowsetup{\centering}\linespread{0.95}\selectfont\newlength{\sectionReduceTop}\newlength{\sectionReduceBot}\newlength{\subsectionReduceTop}\newlength{\subsectionReduceBot}\newlength{\abstractReduceTop}\newlength{\abstractReduceBot}\newlength{\captionReduceTop}\newlength{\captionReduceBot}%\newlength{\nameReduceTop}\newlength{\subsubsectionReduceTop}\newlength{\subsubsectionReduceBot}\newlength{\horSkip}\newlength{\verSkip}\newlength{\figureHeight}\setlength{\figureHeight}{1.7in}%\newlength{\figureFraction}\setlength{\horSkip}{-.09in}\setlength{\verSkip}{-.1in}%\setlength{\figureFraction}{.195}%\setlength{\subsectionReduceTop}{-0.08in}\setlength{\subsectionReduceBot}{-0.05in}\setlength{\sectionReduceTop}{-0.08in}\setlength{\sectionReduceBot}{-0.10in}\setlength{\subsubsectionReduceTop}{-0.06in}\setlength{\subsubsectionReduceBot}{-0.05in}%%%\setlength{\figureHeight}{1.5in}\setlength{\abstractReduceTop}{-0.15in}\setlength{\abstractReduceBot}{-0.05in}%%%\setlength{\nameReduceTop}{-0.05in}\setlength{\captionReduceTop}{-0.09in}\setlength{\captionReduceBot}{-0.2in}
\begin{document}

\title{ Structural-RNN: Deep Learning on Spatio-Temporal Graphs}
\author[1,2]{Ashesh Jain}
\author[2]{Amir R. Zamir}
\author[2]{Silvio Savarese}
\author[3]{Ashutosh Saxena}
\affil[ ]{Cornell University$^1$, Stanford University$^2$, Brain Of Things Inc.$^3$}
\affil[ ]{{ashesh@cs.cornell.edu, \{zamir,ssilvio,asaxena\}@cs.stanford.edu}}

\iffalse
\title{ Structural-RNN: Deep Learning on Spatio-Temporal Graphs}
\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}
\fi

\maketitle

\begin{abstract}
Deep Recurrent Neural Network architectures, though remarkably capable at modeling sequences, lack an intuitive high-level spatio-temporal structure. That is while many problems in computer vision inherently have an underlying high-level structure and can benefit from it. Spatio-temporal graphs are a popular  tool for imposing such high-level intuitions in the formulation of real world problems. In this paper, we propose an approach for combining the power of high-level spatio-temporal graphs and sequence learning success of Recurrent Neural Networks~(RNNs). We develop a scalable method for casting an arbitrary spatio-temporal graph as a rich RNN mixture that is feedforward, fully differentiable, and jointly trainable. The proposed method is generic and principled as it can be used for transforming any spatio-temporal graph through employing a certain set of well defined steps. The evaluations of the proposed approach on a diverse set of problems, ranging from modeling human motion to object interactions, shows improvement over the state-of-the-art with a large margin. We expect this method to empower  new approaches to problem formulation through high-level spatio-temporal graphs and Recurrent Neural Networks.
\end{abstract}%This method is expected to empower a new approach to problem formulation through high-level spatio-temporal graphs and Recurrent Neural Networks and be of broad interest to the community.







\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{dra_small.pdf}
\vspace{3\sectionReduceTop}
\caption{\footnotesize{\textbf{An example of st-graph to S-RNN.} \textbf{(a)} The st-graph from Figure~\ref{fig:stgraph2} is redrawn with colors to indicate sharing of nodes and edge factors. Nodes and edges with same color share factors. Overall there are six distinct factors: 2 node factors and 4 edge factors. \textbf{(b)} S-RNN architecture has one RNN for each factor. EdgeRNNs and nodeRNNs are connected to form a bipartite graph. Parameter sharing between the human and object nodes happen through edgeRNN $\ve{R}_{E_1}$. \textbf{(c)} The forward-pass for human node $v$ involve RNNs $\ve{R}_{E_1}$, $\ve{R}_{{E}_3}$ and $\ve{R}_{V_1}$. In Figure~\ref{fig:strnn} we show the detailed layout of this forward-pass. Input features into  $\ve{R}_{E_1}$ is sum of human-object edge features $\ve{x}_{u,v}+\ve{x}_{v,w}$. \textbf{(d)} The forward-pass for object node $w$ involve RNNs $\ve{R}_{E_1}$, $\ve{R}_{{E}_2}$, $\ve{R}_{{E}_4}$ and $\ve{R}_{V_2}$. In this forward-pass,  the edgeRNN $\ve{R}_{E_1}$ only processes the edge feature $\ve{x}_{v,w}$. (Best viewed in color)}} %Therefore, during training, the edgeRNN $\ve{R}_{E_1}$ processes different linear combinations of the human-object edge features depending on the node being forward-passed.}% In this way S-RNN also captures structure in the feature space.}
\vspace{1.5\sectionReduceBot}
	\label{fig:stgraph}
\end{figure*}%\vspace{1\sectionReduceTop}\section{Structural-RNN architectures}%\vspace{1\sectionReduceBot}\label{sec:srnn}
In this section we describe our approach for building structural-RNN (S-RNN) architectures.  %The nodes of the graph represent entities that we care about and edges represent interactions between them. 
We start with a \mbox{st-graph}, decompose it into a set of factor components, then represent each factor using a RNN. 
The RNNs are interconnected in a way that the resulting architecture captures the structure and interactions of the st-graph. %space removal\vspace{1\subsectionReduceTop}\subsection{Representation of spatio-temporal graphs}\vspace{1\subsectionReduceTop}\label{sec:stgraph}
Many applications that require spatial and temporal reasoning are modeled using st-graphs~\cite{Brendel11,Douillard11,Koppula15,Zhang14b,Jain15}.
We represent a st-graph with $\mathcal{G}=(\mathcal{V},\mathcal{E}_S,\mathcal{E}_T)$,  whose structure $(\mathcal{V},\mathcal{E}_S)$ unrolls over time through edges $\mathcal{E}_T$. Figure~\ref{fig:stgraph2}\rc{a} shows an example st-graph capturing human-object interactions during an activity.  The nodes $v \in \mcal{V}$ and edges $e \in \mcal{E}_S\cup \mcal{E}_T$ of the st-graph repeats over time. In particular, Figure~\ref{fig:stgraph2}\rc{b} shows the same st-graph unrolled through time. In the unrolled st-graph, the nodes at a given time step $t$ are connected with undirected \textit{spatio-temporal} edge $e=(u,v) \in \mcal{E}_S$, and the nodes at adjacent time steps (say the node $u$ at time $t$ and the node $v$ at time $t+1$)  are connected with  undirected \textit{temporal} edge \textit{iff}$(u,v) \in \mcal{E}_T$.\footnote{For simplicity, the example st-graph in Figure~\ref{fig:stgraph2}\rc{a} considers temporal edges of the form  $(v,v) \in \mcal{E}_T$.}\iffalse
Many applications that require spatial and temporal reasoning are modeled using st-graphs~\cite{Brendel11,Douillard11,Koppula15,Zhang14b,Jain15}.
We represent a st-graph with $\mathcal{G}=(\mathcal{V},\mathcal{E}_S,\mathcal{E}_T)$,  whose structure $(\mathcal{V},\mathcal{E}_S)$ unrolls over time through edges $\mathcal{E}_T$. Figure~\ref{fig:stgraph2}\rc{a} shows an example st-graph capturing human-object interactions during an activity.  Figure~\ref{fig:stgraph2}\rc{b} shows the same st-graph unrolled over time. Note that the nodes $v \in \mcal{V}$ and edges $e \in \mcal{E}_S\cup \mcal{E}_T$ of the st-graph are \textit{temporal} in nature. In particular, the st-graph have two kinds of edges: (i) the nodes at a given time step $t$ are connected with undirected \textit{spatio-temporal} edge $e=(u,v) \in \mcal{E}_S$; (ii) the nodes at adjacent time steps (e.g. the node $u$ at time $t$ and the node $v$ at time $t+1$)  are connected with  undirected \textit{temporal} edge \textit{iff}$(u,v) \in \mcal{E}_T$.\footnote{For simplicity, the example st-graph in Figure~\ref{fig:stgraph2}\rc{a} considers temporal edges of the form  $(v,v) \in \mcal{E}_T$.}At each time step $t$, the nodes are connected with undirected \textit{spatio-temporal} edge $e=(u,v) \in \mcal{E}_S$. The node $u$ at time $t$ and the node $v$ at time $t+1$ are connected with an undirected \textit{temporal} edge \textit{iff}$(u,v) \in \mcal{E}_T$.\footnote{For simplicity, the example st-graph in Figure~\ref{fig:stgraph2}\rc{a} considers temporal edges of the form  $(v,v) \in \mcal{E}_T$.}%Therefore all edges in the unrolled st-graph are undirected. \fi

Given a st-graph and the feature vectors associated with the nodes $\ve{x}_v^t$ and edges $\ve{x}_e^t$, as shown in Figure~\ref{fig:stgraph2}\rc{b}, the goal is to predict the node labels (or real value vectors) $y_v^t$ at each time step $t$. For instance, in human-object interaction, the node features can represent the human and object poses, and edge features can their relative orientation; the node labels represent the human activity and object affordance. 
Label $y_v^t$ is affected by both its node and its interactions with other nodes (edges), leading to an overall complex system.
Such interactions  are commonly parameterized with a factor graph that conveys how a (complicated) function over the st-graph factorizes into simpler functions~\cite{Kschischang01}.
We derive our S-RNN architecture from the factor graph representation of the st-graph. Our factor graph representation has a factor function $\Psi_v(y_v,\ve{x}_v)$ for each node and a pairwise factor $\Psi_e(y_{e(1)},y_{e(2)},\ve{x}_e)$ for each edge. Figure~\ref{fig:stgraph2}\rc{c} shows the factor graph corresponding to the st-graph in ~\ref{fig:stgraph2}\rc{a}. \footnote{Note that we adopted factor graph as a tool for capturing interactions and not modeling the overall function. Factor graphs are commonly used in probabilistic graphical models for factorizing joint probability distributions. We consider them for general st-graphs and do not establish relations to its probabilistic and function decomposition properties.}%However an intuition of graphical models can be helpful in coming up with a factor graph.}\iffalse
The structure of our architecture captures the interactions in the st-graph. Such interactions  are commonly parameterized with a factor graph. It conveys how a (complicated) function over st-graph factorizes into simpler function components~\cite{Kschischang01}.  We derive our S-RNN from the factor graph representation of the st-graph. S-RNN use factor graph as a tool for capturing interactions and do not model the function represented by the factor graph.\footnote{Factor graphs are commonly used in graphical models for factorizing joint distributions. We consider factor graphs for general st-graphs. However an intuition of graphical models can be helpful in coming up with a factor graph.} Our factor graph representation has a factor (function) $\Psi_v(y_v,\ve{x}_v)$ for each node, and a pairwise factor  $\Psi_e(y_{e(1)},y_{e(2)},\ve{x}_e)$ for each edge. Figure~\ref{fig:stgraph2}\rc{c} shows the factor graph representation of the st-graph in Figure~\ref{fig:stgraph2}\rc{a}.
\fi\textbf{Sharing factors between nodes.}
Each factor in the st-graph has parameters that needs to be learned. Instead of learning a distinct factor for each node, semantically similar nodes can optionally share factors. For example, all ``object nodes'' \{$u$,$w$\} in the st-graph can share the same node factor and parameters. This modeling choice allows enforcing parameter sharing between similar nodes.
It further gives the flexibility to handle st-graphs with more nodes without increasing the number of parameters. For this purpose, we partition the nodes as $\mcal{C}_V=\{V_1,..,V_P\}$ where $V_p$ is a set of semantically similar nodes, and they all use the same node factor $\Psi_{V_p}$. In Figure~\ref{fig:stgraph}\rc{a} we re-draw the st-graph and assign same color to the nodes sharing node factors. 

Partitioning nodes on their semantic meanings leads to a natural semantic partition of the edges, $\mcal{C}_E=\{E_1,..,E_M\}$, where $E_m$ is a set of edges whose nodes form a semantic pair. Therefore, all edges in the set $E_m$  share the same edge factor $\Psi_{E_m}$. For example all ``human-object edges'' \{$(v,u), (v,w)$\} are modeled with the same edge factor.  Sharing factors based on semantic meaning makes the overall parametrization compact. In fact, sharing parameters is necessary to address applications where the number of nodes depends on the context. For example, in human-object interaction the number of object nodes vary with the environment. Therefore, without sharing parameters between the object nodes, the model cannot generalize to new environments with more objects. For modeling flexibility, the edge factors are not shared across the edges in $\mcal{E}_S$ and $\mcal{E}_T$. Hence, in Figure~\ref{fig:stgraph}\rc{a}, object-object $(w,w)\in\mcal{E}_T$ temporal edge is colored differently from object-object $(u,w) \in \mcal{E}_S$ spatio-temporal edge.%In all there are $P$ distinct node factors and $M$ distinct edge factors. It also allows for incorporating training examples with more nodes (eg. objects) and edges, as long as they belong to one of the semantic categories. \iffalse
Instead of representing each node and edge  with a distinct factor (and parameter), semantically similar edges and nodes can share factors. For example, all ``object nodes'' in the st-graph can share the same node factor, similarly all ``human-object edges'' can share the same edge factor. This modeling choice allows practitioners to encode their prior knowledge to model semantically similar nodes/edges with same parameters. Additionally, it prevents the number of parameters to grow with the size of the st-graph. For this purpose, we partition the nodes and edges into \textit{templates}$\mcal{C}_V=\{V_1,..,V_P\}$ and $\mcal{C}_E=\{E_1,..,E_M\}$ where $V_p$ is a set of nodes and $E_m$ is a set of edges, such that nodes/edges within the same set share factors. In Figure~\ref{fig:stgraph}\rc{a} we re-draw the st-graph, and assign same color to the nodes sharing node factors and edges sharing pairwise edge factors. We denote the shared factors as $\{\Psi_{V_1},..,\Psi_{V_P}\}$ and $\{\Psi_{E_1},..,\Psi_{E_M}\}$, i.e. all the nodes in set $V_p$ use the factor $\Psi_{V_p}$.  For example, the two object nodes in Figure~\ref{fig:stgraph}\rc{a} use the same node factor $\Psi_{V_2}$.  So in all there are $P$ distinct node factors and $M$ distinct edge factors.  

\begin{align}
	\nonumber \Psi(\ve{x},\ve{y})  = \prod_{t=1}^T  \left (\prod_{V_p \in C_V}\prod_{\Psi_v \in V_p} \Psi_{V_p}(y_v^t,\ve{x}_v^t;\theta_{V_p})\right) &\times \\
	\nonumber \left (\prod_{E_m \in C_E }\prod_{\Psi_e \in E_m} \Psi_{E_m}(y_{e(1)}^t,y_{e(2)}^t,\ve{x}_e^t;\theta_{E_m})\right)&\times\\
	\left (\prod_{E_n \in C_{\bar{E}} }\prod_{\Psi_{\bar{e}} \in E_n} \Psi_{E_n}(y_{\bar{e}(1)}^t,y_{\bar{e}(2)}^t,\ve{x}_{\bar{e}}^t;\theta_{E_n})\right)&
\end{align}\fi

In order to predict the label of node $v \in V_p$, we consider its node factor $\Psi_{V_p}$, and the edge factors connected to $v$ in the factor graph. We define a node factor and an edge factor as neighbors if they jointly affect the label of some node in the st-graph. More formally, the node factor $\Psi_{V_p}$ and edge factor $\Psi_{E_m}$ are \textit{neighbors}, if there exist a node $v \in {V_p}$ such that it connects to both $\Psi_{V_p}$ and $\Psi_{E_m}$ in the factor graph. We will use this definition in building S-RNN architecture such that it captures the interactions in the st-graph.

\iffalse
A node's label in the st-graph is affected by the factors the node is connected to in the factor-graph. We define {neighbor factors} to group the factors which jointly affect the node labels.  The node factor $\Psi_{V_p}$ and edge factor $\Psi_{E_m}$ are \textit{neighbors}, if there exist a node $v \in {V}$ such that it connects to both $\Psi_{V_p}$ and $\Psi_{E_m}$ in the factor graph. This definition will be instrumental in building the S-RNN architecture such that it captures the interactions in the st-graph.

\begin{align}
	\nonumber \Psi_{V_p} \;\&\; \Psi_{E_m}\; &\text{are neighbors} \Longleftrightarrow \\
	\label{eq:neighbor} &\exists v \in V_p, u \in \mcal{V}\; \text{s.t.}\; (u,v) \in E_m
\end{align}\fi\vspace{1\subsectionReduceTop}\subsection{Structural-RNN from spatio-temporal graphs}\vspace{1\subsectionReduceBot}\label{subsec:srnn}
We derive our S-RNN architecture from the factor graph representation of the st-graph. The factors in the st-graph operate in a temporal manner, where at each time step the factors observe (node \& edge) features and perform some computation on those features. In S-RNN, we represent each factor with an RNN. We refer the RNNs obtained from the node factors as nodeRNNs and the RNNs obtained from the edge factors as edgeRNNs. The interactions represented by the st-graph are captured through connections between the nodeRNNs and the edgeRNNs. 

\iffalse
We now present our S-RNN architecture to represent st-graphs described earlier.  Our contribution lies in constructively building an RNN ensemble, such that the RNNs and the connections between them are semantically related to the underlying st-graph. %Our architecture consists of multiple RNNs which are interconnected in a manner to capture the structure/interactions of the st-graph. This allows us, and will enable practitioners to learn expressive deep architectures for problems that are represented as spatio-temporal graphs. 
We derive our S-RNN architecture from the factor graph representation of the st-graph. The factors in the st-graph operate in a temporal manner, where at each time step the factors observe (node \& edge) features and perform some computation on those features. In S-RNN we represent each factor with an RNN. We refer the RNNs obtained from node factors as nodeRNNs and the RNNs obtained from edge factors as edgeRNNs. The interactions represented by the st-graph are captured through connections between the nodeRNNs and the edgeRNNs. 
\fiWe denote the RNNs corresponding to the node factor $\Psi_{V_p}$ and the edge factor $\Psi_{E_m}$ as  $\ve{R}_{V_p}$ and $\ve{R}_{E_m}$ respectively. In order to obtain a feedforward network, we connect the edgeRNNs and nodeRNNs to form a bipartite graph $\mcal{G}_R = (\{\ve{R}_{E_m}\},\{\ve{R}_{V_p}\},\mcal{E}_R)$. In particular, the edgeRNN $\ve{R}_{E_m}$ is connected to the nodeRNN $\ve{R}_{V_p}$\textit{iff} the factors $\Psi_{E_m}$  and $\Psi_{V_p}$ are \textit{neighbors} in the st-graph, i.e. they jointly affect the label of some node in the st-graph. To summarize, in Algorithm~\ref{alg:srnn} we show the steps for constructing S-RNN architecture. 
Figure~\ref{fig:stgraph}\rc{b} shows the S-RNN for the human activity represented in  Figure~\ref{fig:stgraph}\rc{a}. The nodeRNNs combine the outputs of the edgeRNNs they are connected to (i.e. its \textit{neighbors} in the factor graph), and predict the node labels. The predictions of nodeRNNs (eg. $\ve{R}_{V_1}$ and $\ve{R}_{V_2}$) interact through the edgeRNNs (eg. $\ve{R}_{E_1}$).  Each edgeRNN handles a specific semantic interaction between the nodes connected in the st-grap and models how the interactions evolve over time. In the next section, we explain the inputs, outputs, and the training procedure of S-RNN.
	\begin{algorithmic}
		\STATE \textbf{Input} $\mcal{G} = (\mcal{V},\mcal{E}_S,\mcal{E}_T)$, $C_V=\{V_1,...,V_P\}$ %, $C_E=\{E_1,...,E_M\}$
		\STATE \textbf{Output} S-RNN graph $\mcal{G}_R = (\{\ve{R}_{E_m}\},\{\ve{R}_{V_p}\},\mcal{E}_R)$
		\STATE \text{1:} Semantically partition edges $C_E=\{E_1,...,E_M\}$
		\STATE \text{2:} Find factor components $\{\Psi_{V_p},\Psi_{E_m}\}$  of $\mcal{G}$
		\STATE \text{3:} \text{Represent each $\Psi_{V_p}$ with a nodeRNN $\ve{R}_{V_p}$}
		\STATE \text{4:} \text{Represent each $\Psi_{E_m}$ with an edgeRNN $\ve{R}_{E_m}$}
		\STATE \text{5:} \text{Connect $\{\ve{R}_{E_m}\}$ and $\{\ve{R}_{V_p}\}$ to form a bipartite graph.}
		\STATE $(\ve{R}_{E_m}, \ve{R}_{V_p}) \in \mcal{E}_R$\; \textit{iff}\; $\exists v \in V_p, u \in \mcal{V}\; \text{s.t.}\; (u,v) \in E_m$
		\STATE \textbf{Return} $\mcal{G}_R = (\{\ve{R}_{E_m}\},\{\ve{R}_{V_p}\},\mcal{E}_R)$
	\end{algorithmic}
	\label{alg:srnn}
\end{algorithm}\vspace{1\subsectionReduceTop}\subsection{Training structural-RNN architecture}\vspace{1\subsectionReduceBot}
In order to train the S-RNN architecture, for each node of the st-graph the features associated with the node are fed into the architecture.  In the forward-pass for node $v \in V_p$, the input into edgeRNN $\ve{R}_{E_m}$ is the temporal sequence of edge features $\ve{x}_e^t$ on the edge $e \in E_m$, where edge $e$ is incident to node $v$ in the st-graph.   The nodeRNN $\ve{R}_{V_p}$ at each time step concatenates the node feature $\ve{x}_v^t$ and the outputs of edgeRNNs it is connected to, and predicts the node label. At the time of training, the errors in prediction are back-propagated through the nodeRNN and edgeRNNs involved during the forward-pass. That way, S-RNN non-linearly combines the node and edge features associated with the nodes in order to predict the node labels.

Figure~\ref{fig:stgraph}\rc{c} shows the forward-pass through S-RNN for the human node. Figure~\ref{fig:strnn} shows a detailed architecture layout of the same forward-pass. The forward-pass involves the edgeRNNs $\ve{R}_{E_1}$ (human-object edge) and $\ve{R}_{E_3}$ (human-human edge). Since the human node $v$ interacts with two object nodes \{$u$,$w$\},
we pass the summation of the two edge features as input to $\ve{R}_{E_1}$. The summation of features, as opposed to concatenation, is important to handle {variable number} of object nodes with a {fixed architecture}. Since the object count varies with environment, it is challenging to represent variable context with a fixed length feature vector. Empirically, adding features works better than mean pooling. We conjecture that addition retains the object count and the structure of the st-graph, while mean pooling averages out the number of edges.  The nodeRNN $\ve{R}_{V_1}$ concatenates the (human) node features with the outputs from edgeRNNs, and predicts the activity at each time step. 
\begin{figure}[t]
	\centering
	\includegraphics[width=.9\linewidth]{strnn.pdf}
	\vspace{1.5\sectionReduceTop}
	\caption{\footnotesize{\textbf{Forward-pass for human node $v$.} Shows the architecture layout corresponding to the Figure~\ref{fig:stgraph}\rc{c} on unrolled st-graph. (View in color)}}
	\label{fig:strnn}
	\vspace{2\sectionReduceBot}
\end{figure}% the input into the edgeRNN modeling this edge is the sum of two human-object edge feature vectors. The nodeRNN modeling the human node concatenates the node features with the output representations from edgeRNNs, and predicts the human activity. \textbf{Parameter sharing and structured feature space.} An important aspect of S-RNN is sharing of parameters across the node labels. Parameter sharing between node labels happen when an RNN is common in their forward-pass. For example in Figure~\ref{fig:stgraph}\rc{c}~and~\ref{fig:stgraph}\rc{d}, the edgeRNN $\ve{R}_{E_1}$ is common in the forward-pass for the human node and the object nodes. Furthermore, the parameters of $\ve{R}_{E_1}$ gets updated through back-propagated gradients from both the object and human nodeRNNs. In this way, $\ve{R}_{E_1}$ affects both the human and object node labels. 

Since the human node is connected to multiple object nodes, the input into edgeRNN $\ve{R}_{E_1}$ is always a linear combination of  human-object edge features. This imposes an structure on the features processed by $\ve{R}_{E_1}$. More formally, the input into $\ve{R}_{E_1}$ is the inner product $\ve{s}^T\ve{F}$, where $\ve{F}$ is the feature matrix storing the edge features $\ve{x}_e$ s.t. $e \in E_1$. Vector $\ve{s}$ captures  the structured feature space. Its entries are in \{0,1\} depending on the node being forward-passed. In the example above $\ve{F} = [\ve{x}_{v,u}\;\; \ve{x}_{v,w}]^T$. For the human node $v$, $\ve{s}=[1\; 1]^T$, while for the object node $u$, $\ve{s}=[1\; 0]^T$.
To summarize, S-RNN is a feed-forward  mixture of RNNs. Each RNN represents one factor of the st-graph, and the connections between the RNNs captures the structure of the st-graph -- a nodeRNN and edgeRNN are connected if their corresponding factors are \textit{neighbors} in the factor graph.% This enables the S-RNN architecture to capture the interactions between the node 

In this section we describe our approach for building structural-RNN (S-RNN) architectures for spatial and temporal reasoning. Many applications that  require this form of reasoning are modeled using spatio-temporal graphs (st-graphs)~\cite{X}. In literature, st-graphs are commonly modeled with probabilistic graphical models such as CRF. In our approach, we treat st-graphs as general graphs whose nodes represent entities that we care for and edges represent interactions between them. We start with a st-graph, we then represent its structure using multiple RNNs. The RNNs are interconnected in a way the resulting architecture captures the structure and interactions of the st-graph. 

In this section we describe our approach for building structural-RNN (S-RNN) architectures for spatial and temporal reasoning. Many applications that  require this form of reasoning are modeled using spatio-temporal graphs (st-graphs)~\cite{X}. The nodes of the graph represent entities that we care about and edges represent interactions between them.\footnote{In literature, st-graphs are commonly given probabilistic interpretation such as CRF. We treat them as general graphs representing interactions between entities.} In our approach we start with a st-graph, we then represent its structure using multiple RNNs. The RNNs are interconnected in a way the resulting architecture captures the structure and interactions of the st-graph. 
\fi\iffalse\noindent\textbf{\underline{Neighbor factors:}} We define the node factor $\Psi_{V_p}$ and edge factor $\Psi_{E_m}$ to be \textit{neighbors}, if there exist a node $v \in \mcal{V}$ such that it connects to both $\Psi_{V_p}$ and $\Psi_{E_m}$ in the factor graph. This definition will be instrumental in building the S-RNN architecture such that it captures the interactions in the st-graph.

Instead of representing each node and edge  with a distinct factor (and parameter), we encourage sharing between semantically similar nodes and edges. For this purpose, we partition nodes and edges into \textit{templates}$\mcal{C}_V=\{V_1,..,V_P\}$ and $\mcal{C}_E=\{E_1,..,E_M\}$ where $V_p$ is a set of nodes and $E_m$ is a set of edges, such that nodes/edges within the same set share factors. This allows us to encode the intuition that semantically similar nodes/edges should be modeled similarly with same parameters. In Figure~\ref{fig:stgraph}\rc{a} we re-draw the st-graph, and assign same color to the nodes sharing node factors and edges sharing pairwise edge factors. We denote the shared factors as $\{\Psi_{V_1},..,\Psi_{V_P}\}$ and $\{\Psi_{E_1},..,\Psi_{E_M}\}$, i.e. all the nodes in set $V_p$ use the factor $\Psi_{V_p}$.  For example, the two object nodes in Figure~\ref{fig:stgraph}\rc{a} use the same node factor $\Psi_{V_2}$.  So in all there are $P$ distinct node factors and $M$ distinct edge factors.  


A spatio-temporal graph represents interaction between entities which are represented as nodes in the graph. We represent a spatio-temporal graph (``st-graph'') with $\mathcal{G}=(\mathcal{V},\mathcal{E}_S,\mathcal{E}_T)$  whose spatial structure $(\mathcal{V},\mathcal{E}_S)$ unrolls over time through its temporal edges $\mathcal{E}_T$. Figure~\ref{fig:stgraph2}\rc{a} shows a  st-graph capturing interactions of a human with two objects during an activity.  Figure~\ref{fig:stgraph2}\rc{b} shows the st-graph unrolled over time through the temporal edges. At each time step $t$ the nodes $u,v \in \mcal{V}$ in the graph are connected with undirected edge $e=(u,v) \in \mcal{E}_S$. The nodes at time $t$ and $t+1$ are connected with temporal edge $e \in \mcal{E}_T$.


Along with the st-graph are associated observations which are represented as features. The feature vector associated with node $v$ and edges $e \in E_S \cup E_T$ at time $t$ are denoted as $\ve{x}_v^t$ and $\ve{x}_e^t$, as shown in Figure~\ref{fig:stgraph2}\rc{b}. In human-object interaction the nodes features for example can represent the human and object pose, and edge features can represent their relative orientation.  Given the features of the st-graph, the goal is to predict the node labels $y_v^t$ at each time step. In order to address this problem, we first parameterize the st-graph and then propose our S-RNN architecture which builds on the parameterization. 

We parameterize the st-graph by its factor graph which has a factor $\Psi_v(y_v,\ve{x}_v)$ for each node and a pairwise factor $\Psi_e(y_{e(1)},y_{e(2)},\ve{x}_e)$ for each edge in the st-graph. For a given assignment of labels and features, the function evaluated by the st-graph is a product of all the factors. Figure~\ref{fig:stgraph2}\rc{c} shows the factor graph representation of the st-graph shown in Figure~\ref{fig:stgraph2}\rc{a}. Furthermore, instead of learning a new factor for every node and edge, nodes (and edges) which are semantically similar can share factors. Sharing factors allows us to encode the intuition that semantically similar nodes/edges should be modeled with same parameters. In Figure~\ref{fig:stgraph}\rc{a} we re-draw the st-graph and show this by assigning same color to nodes sharing the node factors and edges sharing the pairwise edge factors. For example the two object nodes share the node factor. On this basis, the nodes and edges are partitioned into \textit{templates}$\mcal{C}_V=\{V_1,..,V_P\}$ and $\mcal{C}_E=\{E_1,..,E_M\}$ where $V_p$ is a set of nodes and $E_m$ is a set of edges, such that nodes/edges within the same set share factors. We denote the shared factors as $\{\Psi_{V_1},..,\Psi_{V_P}\}$ and $\{\Psi_{E_1},..,\Psi_{E_M}\}$. So in all there are $P$ distinct node factors and $M$ distinct edge factors. 

We define node factor $\Psi_{V_p}$ and edge factor $\Psi_{E_m}$ as \textit{neighbors} if there exist a node $v \in V_p$ and its edge $(u,v) \in {E}_m$. 
We now present our structural-RNN (S-RNN) architecture to represent the spatio-temporal graph described in the previous section. Our architecture consists of multiple RNNs which are connected in a way that the overall architecture captures the structure of the st-graph. Our main contribution lies in constructively building a structural-RNN architecture in which the RNNs and the connections between them are semantically related to the underlying spatio-temporal graph. This allows us, and will enable practitioners, to learn expressive deep architectures for problems that are represented as spatio-temporal graphs. 

In spatio-temporal graphs at each time step the node factor $\Psi_{V_p}$ and edge factors $\Psi_{E_m}$ observe node and edge features and perform some computation on those features. Our S-RNN architecture parameterizes the node factors and edge factors of the st-graph with RNNs. We refer the RNNs obtained from node factors as nodeRNNs and the RNNs obtained from edge factors as edgeRNNs.  The structure of the st-graph is captured through connections between the edgeRNNs and nodeRNNs. Each edgeRNN handles a specific kind of semantic interaction between nodes of the st-graph, and each nodeRNN combines the outputs of the edgeRNNs it is connected to and predicts the node labels.  The resulting architecture is called the structural-RNN.  


We denote the RNNs corresponding to node factor $\Psi_{V_p}$ and edge factor $\Psi_{E_m}$ as  $\ve{R}_{V_p}$ and $\ve{R}_{E_m}$ respectively. The edgeRNNs and nodeRNNs are connected to form a bipartite graph. Figure~\ref{fig:stgraph}b shows the S-RNN for the human activity represented by the st-graph shown in  Figure~\ref{fig:stgraph}a. The edgeRNN $\ve{R}_{E_m}$ is connected to the nodeRNN $\ve{R}_{V_p}$\textit{iff} the factors $\Psi_{E_m}$  and $\Psi_{V_p}$ are \textit{neighbors} in the st-graph. The key insight in connecting edgeRNNs and nodeRNNs is to appropriately fuse the node and edge features before label prediction. In order to predict the label of node $v \in V_p$ the edgeRNNs first process the edge features on the edges connected to the node. The nodeRNN $\ve{R}_{V_p}$ then combines the outputs from edgeRNNs and predicts the label of the node. This is in contrast to the parametrization of the st-graph discussed in Section~\ref{sec:stgraph} where the node and edge factors only interact through product (or through summations in log-space). In the following section we explain the inputs and outputs of RNNs, and the training procedure.

\subsection{Training structural RNN architecture}

In order to train the S-RNN architecture, for each node of the st-graph the edge and node features associated with the node are fed into the architecture.  In the forward-pass through the architecture for node $v \in V_p$, the input into edgeRNN $\ve{R}_{E_m}$ is a temporal sequence of features on the edge $e \in E_m$ connected to node $v$. The output of edgeRNNs are high-level representations of the edge features.  The nodeRNN $\ve{R}_{V_p}$ at each time step  concatenates the node feature $\ve{x}_v^t$ with the high-level representations from the edgeRNNs and predicts the node label. 

At the time of training, the errors in prediction are back-propagated through the nodeRNN and edgeRNNs involved during the forward-pass. In this way our architecture non-linearly combines the node and edge features associated with the node.  Figure~\ref{fig:stgraph}c shows the forward-pass through S-RNN for human node. The edgeRNNs modeling the human-object interaction and human-human temporal interaction process the edge features connected to the human node. Since the human node is connected to two objects, the human-object edge features are accumulated and fed into the edgeRNN as a single input vector. The nodeRNN modeling the human node concatenates the human node features with the high-level representations from edgeRNNs and outputs the node label (i.e. human activity).

\textbf{Parameter and feature sharing.} An important aspect of S-RNN lies in sharing of parameters and features across node labels. Sharing happens through the edgeRNNs which are connected to multiple nodeRNNs in the bipartite graph. For example in Figure~\ref{fig:stgraph}b the edgeRNN $\ve{R}_{E_1}$ modeling the human-object interaction is connected to both the human and object nodeRNNs. Such edgeRNNs capture the interaction between the labels, and during training their parameters gets updated through back-propagated gradients from multiple nodeRNNs they are connected to. The edge features into such edgeRNNs further captures the structure of the st-graph. This is depicted in Figure~\ref{fig:stgraph} (c \& d) where the edgeRNN $\ve{R}_{E_1}$ processes different linear combinations of the human-object edge features. Therefore, sharing parameters and features between the output labels allows S-RNN to model the structure of the st-graph and learn interactions between the output labels.

\caption{\textbf{An example spatio-temporal graph of human activity.} (a) St-graph showing a human interacting with two objects. It has two kinds of edges, spatial edges $\mcal{E}_S$ and temporal edges $\mcal{E}_T$. Later we will present our structural-RNN architecture for modeling such spatio-temporal reasoning. (b) Unrolling the st-graph through temporal edges. The nodes and edges are labelled with the feature vectors associated with them. (c) Factor graph parameterization of the st-graph. Each node and edge in the st-graph has a corresponding factor.}\fi


We present results on three diverse spatio-temporal problems to ensure generic applicability of S-RNN, shown in Figure~\ref{fig:stgraphexp}. The applications include: (i) modeling human motion~\cite{Fragkiadaki15} from motion capture data~\cite{H36m}; (ii) human activity detection and anticipation~\cite{Koppula13b,Koppula13}; and (iii) maneuver anticipation from real-world driving data~\cite{Jain15}. 

Human body is a good example of separate but well related components. Its motion involves complex spatio-temporal interactions between the components (arms, legs, spine), resulting in sensible motion styles like walking, eating etc.  In this experiment, we represent the complex motion of humans over st-graphs and learn to model them with S-RNN.  We show that our structured approach outperforms the state-of-the-art unstructured deep architecture~\cite{Fragkiadaki15} on motion forecasting from motion capture (mocap) data. Several approaches based on Gaussian processes~\cite{Urtasun08,Wang08}, Restricted Boltzmann Machines (RBMs)~\cite{Taylor06,Taylor10,Sutskever09}, and RNNs~\cite{Fragkiadaki15} have been proposed to model human motion. Recently, Fragkiadaki et al.~\cite{Fragkiadaki15} proposed an encoder-RNN-decoder (ERD) which gets state-of-the-art forecasting results on H3.6m mocap data set~\cite{H36m}. 
\centering
\includegraphics[width=.9\linewidth]{stgraph_examples_exp.pdf}
\vspace{1.5\sectionReduceTop}
\caption{\footnotesize{\textbf{Diverse spatio-temporal tasks}. We apply S-RNN to the following three diverse spatio-temporal problems. (View in color)}}
\vspace{2\sectionReduceBot}
\label{fig:stgraphexp}
\end{figure}\noindent\textbf{S-RNN architecture for human motion.} Our S-RNN architecture follows the st-graph shown in Figure~\ref{fig:stgraphexp}\rc{a}. According to the st-graph, the spine interacts with all the body parts, and the arms and legs interact with each other. The st-graph is automatically transformed to S-RNN following Section~\ref{subsec:srnn}. The resulting S-RNN have three nodeRNNs, one for each type of body part (spine, arm, and leg), four edgeRNNs for modeling the spatio-temporal interactions between them, and three edgeRNNs for their temporal connections. For edgeRNNs and nodeRNNs we use FC(256)-FC(256)-LSTM(512) and LSTM(512)-FC(256)-FC(100)-FC($\cdot$) architectures, respectively, with skip input and output connections~\cite{Graves13}. The outputs of nodeRNNs are skeleton joints of different body parts, which are concatenated to reconstruct the complete skeleton. In order to model human motion, we train S-RNN to predict the mocap frame at time $t+1$  given the frame at time $t$. Similar to~\cite{Fragkiadaki15}, we gradually add noise to the mocap frames during training. This simulates curriculum learning~\cite{Bengio09} and helps in keeping the forecasted motion close to the manifold of human motion. As node features we use the raw joint values expressed as exponential map~\cite{Fragkiadaki15}, and edge features are concatenation of the node features. We train all RNNs jointly to minimize the Euclidean loss between the predicted mocap frame and the ground truth. See supplementary material on the project web page~\cite{SuppSRNN} for training details.

\begin{figure}[t]
\centering
\includegraphics[width=.9\linewidth]{eating_activity.pdf}
\caption{\footnotesize{\textbf{Forecasting eating activity on test subject}. On aperiodic activities, ERD and LSTM-3LR struggle to model human motion. S-RNN, on the other hand, mimics the ground truth in the short-term and generates human like motion in the long term. Without (w/o) edgeRNNs the motion freezes to some mean standing position. See the video~\cite{SuppSRNN}.}}
\label{fig:eating_videos}
\vspace{1.5\sectionReduceBot}
\end{figure}%We compare S-RNN with the ERD architecture~\cite{Fragkiadaki15}, which is currently the state-of-the-art in motion forecasting on H3.6m data set~\cite{H36m}.\noindent\textbf{Evaluation setup.} We compare S-RNN with the state-of-the-art ERD architecture~\cite{Fragkiadaki15} on H3.6m mocap data set~\cite{H36m}. We also compare with a 3 layer LSTM architecture (LSTM-3LR) which~\cite{Fragkiadaki15} use as a baseline.\footnote{We reproduce ERD and LSTM-3LR architectures following~\cite{Fragkiadaki15}. The authors implementation were not available at the time of submission.} For motion forecasting we follow the experimental setup of~\cite{Fragkiadaki15}. We downsample H3.6m by two and train on 6 subjects and test on  subject S5.   To forecast, we first feed the architectures with (50) \textit{seed} mocap frames, and then forecast the future (100) frames.  Following~\cite{Fragkiadaki15}, we consider  walking, eating, and smoking activities. In addition to these three, we also consider discussion  activity. 

Forecasting is specially challenging on activities with complex aperiodic human motion. In H3.6m data set, significant parts of eating, smoking, and discussion activities are aperiodic, while walking activity is mostly periodic. Our evaluation demonstrates the benefits of having an underlying structure in three important ways:
(i) We present visualizations and  quantitative results on complex aperiodic activities (\cite{Fragkiadaki15} evaluates only on periodic walking motion); (ii) We forecast human motion for almost twice longer than state-of-the-art~\cite{Fragkiadaki15}. This is very challenging for aperiodic activities; and finally (iii) We show S-RNN interestingly learns semantic concepts, and demonstrate its modularity by generating hybrid human motion. Unstructured deep architectures like~\cite{Fragkiadaki15} does not offer such modularity.
\centering
\caption{\footnotesize{\textbf{Motion forecasting angle error}. \{80, 160, 320, 560, 1000\} msecs after the seed motion. The results are averaged over 8 seed motion sequences for each activity on the test subject.}}% S-RNN outperforms both ERD~\cite{Fragkiadaki15} and LSTM-3LR majority of times.}
\vspace{0\captionReduceBot}
\resizebox{0.8\linewidth}{!}{
\centering
\begin{tabular}{r|ccccc}\hline
\multirow{2}{*}{Methods}&\multicolumn{3}{c}{Short-term forecast}&\multicolumn{2}{c}{Long-term forecast}\\
&80ms&160ms&320ms&560ms&1000ms\\\hline
&\multicolumn{5}{c}{Walking activity}\\\hline
ERD~\cite{Fragkiadaki15}&1.30&1.56&1.84&2.00&2.38\\
LSTM-3LR&1.18&1.50&1.67&\textbf{1.81}&2.20\\
S-RNN&\textbf{1.08}&\textbf{1.34}&\textbf{1.60}&1.90&\textbf{2.13}\\\hline
&\multicolumn{5}{c}{Eating activity}\\\hline
ERD~\cite{Fragkiadaki15}&1.66&1.93&2.28&2.36&\textbf{2.41}\\
LSTM-3LR&1.36&1.79&2.29&2.49&2.82\\
S-RNN&\textbf{1.35}&\textbf{1.71}&\textbf{2.12}&\textbf{2.28}&2.58\\\hline
&\multicolumn{5}{c}{Smoking activity}\\\hline
ERD~\cite{Fragkiadaki15}&2.34&2.74&3.73&3.68&3.82\\
LSTM-3LR&2.05&2.34&3.10&3.24&3.42\\
S-RNN&\textbf{1.90}&\textbf{2.30}&\textbf{2.90}&\textbf{3.21}&\textbf{3.23}\\\hline
&\multicolumn{5}{c}{Discussion activity}\\\hline
ERD~\cite{Fragkiadaki15}&2.67&2.97&3.23&3.47&2.92\\
LSTM-3LR&2.25&2.33&2.45&{2.48}&2.93\\
S-RNN&\textbf{1.67}&\textbf{2.03}&\textbf{2.20}&\textbf{2.39}&\textbf{2.43}\\\hline
\end{tabular}
}
\vspace{1.5\sectionReduceBot}
\label{tab:3derror}
\end{table}\noindent\textbf{Qualitative results on motion forecasting.} Figure~\ref{fig:eating_videos} shows forecasting 1000ms of human motion on ``eating'' activity -- the subject drinks while walking. S-RNN stays close to the ground-truth in the short-term and generates human like motion in the long-term. On removing edgeRNNs, the parts of human body become independent and stops interacting through parameters. Hence without edgeRNNs the skeleton freezes to some mean position. LSTM-3LR suffers with a drifting problem. On many test examples it drifts to the mean position of walking human (\cite{Fragkiadaki15} made similar observations about LSTM-3LR). The motion generated by ERD~\cite{Fragkiadaki15} stays human-like in the short-term but it drifts away to non-human like motion in the long-term. This was a common outcome of ERD on complex aperiodic activities, unlike S-RNN. Furthermore, ERD produced human motion was non-smooth  on many test examples. See the video on the project web page for more examples~\cite{SuppSRNN}. 

\iffalse
S-RNN models human motion well. It stays close to the ground-truth in the short-term and generates human like motion in the long-term. Figure~\ref{fig:eating_videos}  also shows the importance of edgeRNNs, without them the human skeleton freezes to some mean position. LSTM-3LR suffers with a drifting problem. On many test examples it drifts to the mean position of walking human (\cite{Fragkiadaki15} made similar observations about LSTM-3LR). The motion generated by ERD~\cite{Fragkiadaki15} stays close to the ground-truth in the short-term but it drifts away to non-human like motion in the long-term. Furthermore, ERD produced  human motion was non-smooth on many test examples. 
We follow the evaluation metric of Fragkiadaki et al.~\cite{Fragkiadaki15} and present the 3D angle error between the forecasted mocap frame and the ground truth in Table~\ref{tab:3derror}.  Qualitatively, ERD models human motion better than LSTM-3LR. However, in the short-term, it does not mimic the ground-truth as well as LSTM-3LR. Fragkiadaki et al.~\cite{Fragkiadaki15} also note this trade-off between ERD and LSTM-3LR. On the other hand, S-RNN outperforms both LSTM-3LR and ERD on short-term motion forecasting on all activities. S-RNN therefore mimics  the ground truth in the short-term and generates human like motion in the long term. In this way it well handles both short and long term forecasting. Due to stochasticity in human motion, long-term forecasts ($>$ 500ms) can significantly differ from the ground truth but still depict human-like motion. For this reason, the long-term forecast numbers in Table~\ref{tab:3derror} are not a fair representative of algorithms modeling capabilities. We also observe that discussion is one of the most challenging aperiodic activity for all algorithms. 

\iffalse%%%%%%%%%%%%%%%%%% This part is commentted out %%%%%%%%%%%%%%%%%%%of motion forecasting is tricky due to stochasticity in human motion. For this reason most of the previous works present qualitative results. 
We follow the evaluation metric of Fragkiadaki et al.~\cite{Fragkiadaki15} and present the 3D angle error between the forecasted mocap frame and the ground truth in Table~\ref{tab:3derror}.  For short-term motion forecasting, S-RNN outperforms both LSTM-3LR and ERD on all activities. While S-RNN produces realistic human-like motion, it also mimics the ground truth in the short-term. Qualitatively ERD models human motion better than LSTM-3LR. However, in the short-term it does mimic the ground-truth as well as LSTM-3LR. Fragkiadaki et al.~\cite{Fragkiadaki15} also note this trade-off between ERD and LSTM-3LR. Therefore S-RNN gives us the best of both world. It mimics  the ground truth in the short-term and generates human like motion in the long term. Due to stochasticity in human motion, long-term forecasts ($>$ 500ms) can significantly differ from the ground truth but they can still depict human-like motion. For this reason, the long-term forecast numbers in Table~\ref{tab:3derror} are not a fair representative of algorithms modeling capabilities. We also observe that discussion is one of the most challenging aperiodic activity for all algorithms. 
We now present several insights into S-RNN architecture and demonstrate the modularity of the architecture which enables it to generate hybrid human motions. 
\includegraphics[width=\linewidth]{memory_cell_new.pdf}
\vspace{2.5\sectionReduceTop}
\caption{\footnotesize{\textbf{S-RNN memory cell visualization.} (\textbf{Left}) A cell of the leg nodeRNN fires (red) when ``putting the leg forward''. (\textbf{Right}) A cell of the arm nodeRNN fires for ``moving the hand close to the face''. We visualize the same cell for eating and smoking activities. (\textbf{See the video}~\cite{SuppSRNN}) }}
\vspace{2.5\sectionReduceBot}
\label{fig:memory_cell}
\end{figure}\textbf{Visualization of memory cells.} We investigated if S-RNN memory cells represent meaningful semantic sub-motions. Semantic cells were earlier studied on other problems~\cite{Karpathy15}, we are the first to present it on a vision task and human motion. In Figure~\ref{fig:memory_cell} (left) we show a cell in the leg nodeRNN learns the semantic motion of \textit{moving the leg forward}. The cell fires positive (red color) on the forward movement of the leg and negative (blue color) on its backward movement. As the subject walks, the cell alternatively fires for the right and the left leg. Longer activations in the right leg corresponds to the longer steps taken by the subject with the right leg. Similarly, a cell in the arm nodeRNN learns the concept of \textit{moving hand close to the face}, as shown in Figure~\ref{fig:memory_cell} (right). The same cell fires whenever the subject moves the hand closer to the face during eating or smoking. The cell remains active  as long as the hand stays close to the face. See the  video~\cite{SuppSRNN}.

\iffalse
 Our architecture allows us to factor an overall complex motion into individual body parts. In Figure~\ref{fig:memory_cell} (left) we show a cell in the leg nodeRNN that \textit{learns the semantic concept of walking}. The cell fires positive (red color) on the forward movement of the leg and negative (blue color) on its backward movement. As the subject walks, the cell alternatively fires for the right and the left leg. Longer activations in the right leg corresponds to the longer steps taken by the subject with the right leg. Similarly, a cell in the arm nodeRNN \textit{learns the concept of moving hand close to the face}, as shown in Figure~\ref{fig:memory_cell} (right). The same cell fires whenever the subject moves the hand closer to the face during eating or smoking. The cell remains active  as long as the hand stays close to the face. %In Figure~\ref{fig:memory_cell} (right) we show the same cell on different activities and with varying number of movements of hand to the face.\fi\textbf{Generating hybrid human motion.} We now demonstrate the flexibility of our modular architecture by generating novel yet meaningful motions which are not in the data set. Such modularity is of interest and has been explored to generate diverse motion styles~\cite{Taylor09}. As a result of having an underlying high-level structure, our approach allows us to exchange RNNs between the S-RNN architectures trained on different motion styles. We leverage this to create a novel S-RNN architecture which generates a hybrid motion of a \textit{human jumping forward on one leg}, as shown in Figure~\ref{fig:loss} (Left). For this experiment we modeled the left and right leg with different nodeRNNs.  We trained two independent S-RNN models -- a slower human and a faster human (by down sampling data) -- and swapped the left leg nodeRNN of the trained models. The resulting faster human, with a slower left leg, jumps forward on the left leg to keep up with its twice faster right leg.\footnote{Imagine your motion forward if someone holds your right leg and runs!} Unstructured architectures like ERD~\cite{Fragkiadaki15} does not offer this kind of flexibility.  

Figure~\ref{fig:loss} (Right) examines the test and train error with iterations. Both S-RNN and ERD converge to similar training error, however S-RNN generalizes better with a smaller test error for next step prediction. Discussion in supplementary.

\centering
\vspace{.5\sectionReduceTop}
\includegraphics[width=0.66\linewidth]{jumping_human.pdf}
\includegraphics[width=0.32\linewidth]{lossPlot.pdf}
\vspace{1\sectionReduceTop}
\caption{\footnotesize{\textbf{(Left)} \textbf{Generating hybrid motions} \textbf{(See the video~\cite{SuppSRNN})}. We demonstrate flexibility of S-RNN by generating a hybrid motion of a ``human jumping forward on one leg''.   \textbf{(Right)} \textbf{Train and test error}. S-RNN generalizes better than ERD with a smaller test error. }}
\vspace{1\sectionReduceBot}
\label{fig:loss}
\end{figure}\begin{table*}[t]
\centering
\vspace{.75\sectionReduceTop}
\caption{\footnotesize{\textbf{Maneuver Anticipation on 1100 miles of real-world driving data}. S-RNN is derived from the st-graph shown in Figure~\ref{fig:stgraphexp}\rc{c}. Jain et al.~\cite{Jain15} use the same st-graph but models it in a probabilistic frame with AIO-HMM. The table shows average \textit{precision}, \textit{recall} and \textit{time-to-maneuver}. Time-to-maneuver is the interval between the time of algorithm's prediction and the start of the maneuver. Algorithms are compared on the features from~\cite{Jain15}. }}
\vspace{1\sectionReduceBot}
\resizebox{0.9\linewidth}{!}{
\centering
\begin{tabular}{cr|ccc|ccc|ccc}
&  &\multicolumn{3}{c}{Turns}&\multicolumn{3}{|c}{Lane change}&\multicolumn{3}{|c}{All maneuvers}\\
\cline{1-11}
\multicolumn{2}{c|}{\multirow{2}{*}{Method}} & \multirow{2}{*}{$Pr$ (\%)}  & \multirow{2}{*}{$Re$ (\%)} & Time-to-  & \multirow{2}{*}{$Pr$ (\%)} & \multirow{2}{*}{$Re$ (\%)} & Time-to-  & \multirow{2}{*}{$Pr$ (\%)} & \multirow{2}{*}{$Re$ (\%)}  & Time-to- \\ 
& & & &  maneuver (s) &  & &  maneuver (s) &  & & maneuver (s)\\\hline
&SVM 		&	64.7 &	47.2 &	2.40 	&	73.7 &	57.8 &	2.40		&	43.7 &	37.7 & 1.20\\
&AIO-HMM	~\cite{Jain15}	 		&	{80.8}	&		75.2	&	4.16 &	{83.8}	&	79.2	&	3.80 		&	{77.4}	&	{71.2}	&	3.53 \\
 & S-RNN w/o edgeRNN  & 75.2 & 75.3  & 3.68 & 85.4  & \textbf{86.0} & 3.53 & 78.0 & 71.1 & 3.15 \\
&S-RNN  & \textbf{81.2}  & \textbf{78.6} & 3.94 & \textbf{92.7} & 84.4  & 3.46 & \textbf{82.2}  & \textbf{75.9} & 3.75 \\\hline
\end{tabular}
}
\vspace{2\sectionReduceTop}
\label{tab:maneuver}
\end{table*}\vspace{1\subsectionReduceTop}\subsection{Human activity detection and anticipation}\vspace{1\subsectionReduceBot}
In this section we present S-RNN for modeling human activities. We consider the CAD-120~\cite{Koppula13b} data set where the activities involve rich human-object interactions. Each activity consist of a sequence of sub-activities (e.g. moving, drinking etc.) and  objects affordance (e.g., reachable, drinkable etc.), which evolves as the activity progresses. Detecting and anticipating the sub-activities and affordance enables personal robots to assist humans. However, the problem is challenging as it involves complex interactions -- humans interact with multiple objects during an activity, and  objects  also interact with each other (e.g. pouring water from ``glass'' into a ``container''), which  makes it a particularly good fit for evaluating our method. Koppula et al.~\cite{Koppula13,Koppula13b} represents such rich spatio-temporal interactions with the st-graph shown in Figure~\ref{fig:stgraphexp}\rc{b}, and models it with a spatio-temporal CRF. In this experiment, we show that modeling the same st-graph with S-RNN yields  superior results. We use the node and edges features from~\cite{Koppula13b}.

Figure~\ref{fig:stgraph}\rc{b} shows our S-RNN architecture  to model the st-graph. Since the number of objects varies with environment, factor sharing between the object nodes and the human-object edges becomes crucial. In S-RNN, $\ve{R}_{V_2}$ and $\ve{R}_{E_1}$ handles all the object nodes and the human-object edges respectively. This allows our fixed S-RNN architecture to handle varying size st-graphs. For edgeRNNs we use a single layer LSTM of size 128, and for nodeRNNs we use LSTM(256)-softmax($\cdot$). At each time step, the human nodeRNN outputs the sub-activity label (10 classes), and the object nodeRNN outputs the affordance (12 classes). Having observed the st-graph upto time $t$, the goal is to \textit{detect} the sub-activity and affordance labels at the current time $t$, and also \textit{anticipate} their future labels of the time step $t+1$. For detection  we train S-RNN on the labels of the current time step. For anticipation we train the architecture to predict the labels of the next time step, given the observations upto the current time. We also train a \textit{multi-task} version of S-RNN, where we add two softmax layers to each nodeRNN and jointly train for anticipation and detection.

\iffalse
In this section we present S-RNN for modeling human activities. We consider the activities in CAD-120~\cite{Koppula13b} data set. Each activity consist of a sequence of sub-activities (e.g. moving, drinking etc.) involving human-object interactions.  Koppula et al.~\cite{Koppula13,Koppula13b} represents these interactions with the st-graph shown in Figure~\ref{fig:stgraphexp}\rc{b}. They model the st-graph as an spatio-temporal CRF. The human node in the graph is labeled with the sub-activity (10 classes) and the object nodes are labeled with the affordance (12 classes). The number of object nodes in the st-graph varies with the task, subject and environment. Figure~\ref{fig:stgraph}\rc{b} shows our S-RNN architecture  to model the st-graph. All object nodes in the st-graph are handled by the nodeRNN $\ve{R}_{V_2}$. Hence a fixed S-RNN architecture can handle varying number of objects.
\fi%\caption{\footnotesize{\textbf{Maneuver Anticipation on 1100 miles of driving data}. S-RNN is derived from the st-graph shown in Figure~\ref{fig:stgraphexp}\rc{c}. Jain et al.~\cite{Jain15} use the same st-graph but models it in a probabilistic frame with AIO-HMM. The table shows average \textit{precision}, \textit{recall} and \textit{time-to-maneuver} are computed from 5-fold cross-validation. Algorithms are compared on the features from Jain et al.~\cite{Jain15}.} }\begin{table}[t]
\centering
\caption{\footnotesize{\textbf{Results on CAD-120~\cite{Koppula13b}}. S-RNN architecture derived from the st-graph in Figure~\ref{fig:stgraphexp}\rc{b} outperforms Koppula et al.~\cite{Koppula13,Koppula13b} which models the same st-graph in a probabilistic framework. S-RNN in multi-task setting (joint detection and anticipation) further improves the performance. }}
\vspace{1\sectionReduceBot}
\resizebox{1\linewidth}{!}{
\centering
\begin{tabular}{r|cc|cc}
&\multicolumn{2}{c|}{Detection F1-score}& \multicolumn{2}{c}{Anticipation F1-score}\\\hline
{\multirow{2}{*}{Method}} & {\multirow{1}{*}{Sub-}} & Object & {\multirow{1}{*}{Sub-}} & Object\\
&activity (\%)&Affordance (\%)&activity (\%)&Affordance (\%)\\\hline
Koppula et al.~\cite{Koppula13,Koppula13b}&80.4&81.5&37.9&36.7\\
S-RNN w/o edgeRNN &82.2&82.1&64.8&72.4\\
S-RNN &\textbf{83.2}&88.7&62.3&80.7\\
S-RNN (multi-task)&82.4&\textbf{91.1}&\textbf{65.6}&\textbf{80.9}\\\hline
\end{tabular}
}
\vspace{2.5\sectionReduceBot}
\label{tab:cad120}
\end{table}
Table~\ref{tab:cad120} shows the detection and anticipation F1-scores averaged over all the classes.  S-RNN significantly improves over Koppula et al. on both anticipation~\cite{Koppula13} and detection~\cite{Koppula13b}. On anticipating object affordance S-RNN F1-score is 44\% more than~\cite{Koppula13}, and 7\% more on detection. S-RNN does not have any Markov assumptions like spatio-temporal CRF, and therefore, it better models the long-time dependencies needed for anticipation. The table also shows the importance of edgeRNNs in handling spatio-temporal components. EdgeRNN transfers the information from the human to objects, which helps is predicting the object labels. Therefore, S-RNN without the edgeRNNs poorly models the objects. This signifies the importance of edgeRNNs and also validates our design.   Finally, training S-RNN in a multi-task manner  works best in majority of the cases. In Figure~\ref{fig:eating_anticipation} we show the visualization of an eating activity. We show one representative frame from each sub-activity and our corresponding predictions. 

\vspace{1\subsectionReduceTop}\subsection{Driver maneuver anticipation}\vspace{1\subsectionReduceBot}
We finally present S-RNN for another application which involves anticipating maneuvers several seconds before they happen.  Jain et al.~\cite{Jain15} represent this problem with the st-graph shown in Figure~\ref{fig:stgraphexp}\rc{c}. They model the st-graph as a probabilistic Bayesian network (AIO-HMM~\citep{Jain15}). The st-graph represents the interactions between the observations outside the vehicle (eg. the road features), the driver's maneuvers, and the observations inside the vehicle (eg. the driver's facial features). We model the same st-graph with S-RNN architecture using the node and edge features from Jain et al.~\cite{Jain15}. Table~\ref{tab:maneuver} shows the performance of different algorithms on this task.  S-RNN performs better than the  state-of-the-art AIO-HMM~\cite{Jain15} in every setting. See supplementary material for the discussion and details~\cite{SuppSRNN}.

\begin{figure}[t]
\centering
\includegraphics[width=.99\linewidth]{cad120_edits.pdf}
\vspace{1.5\sectionReduceTop}
\caption{\footnotesize{\textbf{Qualitative result on eating activity on CAD-120.} Shows multi-task S-RNN detection and anticipation results. For the sub-activity at time $t$, the labels are anticipated at time $t-1$. (Zoom in to see the image) }}
\vspace{2.7\sectionReduceBot}
\label{fig:eating_anticipation}
\end{figure}\iffalse

We now present S-RNN for another application which involves anticipating maneuvers several seconds before they happen. For example, anticipating a future lane change maneuver several seconds before the wheel touches the lane markings. This problem requires spatial and temporal reasoning of the driver, and the sensory observations from inside and outside of the car. Jain et al.~\cite{Jain15} represent this problem with the st-graph shown in Figure~\ref{fig:stgraphexp}\rc{c}. They model the st-graph as a probabilistic Bayesian network called AIO-HMM. The st-graph represents the interactions between the observations outside the vehicle (eg. the road features), the driver's maneuvers, and the observations inside the vehicle (eg. the driver's facial features). We model the same st-graph with S-RNN architecture using the node and edge features from Jain et al.~\cite{Jain15}.

The nodeRNN models the driver, and the two edgeRNNs model the interactions between the driver and the observations inside the vehicle, and the observations outside the vehicle. The driver node is labeled with the future maneuver and, the observation nodes do not carry any label. The output of the driver nodeRNN is softmax probabilities of the following five maneuvers: \{\textit{Left lane change, right lane change, left turn, right turn, straight driving}\}.  Our nodeRNN architecture is RNN(64)-softmax(5), and edgeRNN is LSTM(64).


We train S-RNN on the features provided by Jain et al.~\cite{Jain15} on their 1100 miles of natural driving data set. The algorithms are evaluated on their precision and recall in anticipating maneuvers  under the following three prediction
settings: (i) Lane change: algorithms only anticipate lane changes. This setting is relevant for freeway driving; (ii) Turns: algorithms only anticipate turns; and (iii) All maneuvers: algorithms anticipate all five maneuvers. Table~\ref{tab:maneuver} shows the performance of different algorithms on this task.  S-RNN performs better than the previous state-of-the-art AIO-HMM~\cite{Jain15} in every setting.  It improves the precision  by 5\% and recall by 4\% with predicting all five maneuvers. Both AIO-HMM and S-RNN model the same st-graph but using different techniques. The table also shows that the performance decreases if we remove edgeRNNs and simply feed the concatenation of edge features into the nodeRNN. This emphasizes importance of the edgeRNNs,  and the need for separately modeling different kinds of edge interactions. 
\fi


We proposed a generic and principled approach for combining high-level
spatio-temporal graphs with sequence modeling success of RNNs. We make use of
factor graph, and factor sharing in order to obtain an RNN mixture that is
scalable and applicable to any problem expressed over st-graphs. Our RNN mixture
captures the rich interactions in the underlying st-graph. We demonstrated
significant improvements with S-RNN on three diverse spatio-temporal problems
including: (i) human motion modeling; (ii) human-object interaction; and (iii)
driver maneuver anticipation. By visualizing the memory cells we showed that
S-RNN learns certain semantic sub-motions, and demonstrated its modularity by
generating novel human motion.\footnote{We acknowledge NRI \#1426452,
ONR-N00014-14-1-0156, MURI-WF911NF-15-1-0479 and Panasonic Center grant \#122282.}\iffalse
We proposed a generic and principled approach for combining high-level spatio-temporal graphs with sequence modeling success of RNNs. We make use of factor graph, and factor sharing in order to obtain an RNN mixture that is scalable and applicable to any problem expressed over st-graphs. Our RNN mixture captures the rich interactions in the underlying st-graph through connections between the RNNs. It learns the dependencies between the output labels by sharing RNNs between the outputs. 

We demonstrated significant improvements with S-RNN on three diverse spatio-temporal problems. We showed that representing human motion over st-graph and learning via S-RNN outperforms state-of-the-art  RNN based methods. We further showed, on two context-rich spatio-temporal problems: (i) human-object interaction; and (ii) driver maneuver anticipation; that learning S-RNN from their st-graphs outperforms the existing state-of-the-art non-deep learning based methods on the same st-graph. By visualizing the memory cells we showed that S-RNN learns certain semantic sub-motions, and demonstrated its modularity by generating hybrid human motions. Future work includes combining S-RNN with CNNs for spatio-temporal feature learning~\citep{Tran15}, and developing inference methods on S-RNN for structured-output prediction.
\fi

{\small
\bibliographystyle{ieee}
\bibliography{shortstrings,references}
}


\end{document}




