\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{graphicx, subfigure}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{url}
\usepackage{hyperref}
\usepackage{collectbox}
\usepackage{cite}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Object cosegmentation using deep Siamese network}


\author{\IEEEauthorblockN{Prerana Mukherjee\IEEEauthorrefmark{1},
Brejesh Lall\IEEEauthorrefmark{1}  and
Snehith Lattupally\IEEEauthorrefmark{1}
}
\IEEEauthorblockA{\IEEEauthorrefmark{1}Dept of EE, IIT Delhi, India. \\Email: \{eez138300, brejesh, eet152695  \}@ee.iitd.ac.in} }


\maketitle

\begin{abstract}
Object cosegmentation addresses the problem of discovering similar objects from multiple images and segmenting them as foreground simultaneously. In this paper, we propose a novel end-to-end pipeline to segment the similar objects simultaneously from relevant set of images using supervised learning via deep-learning framework. We experiment with multiple set of object proposal generation techniques and perform extensive numerical evaluations by training the Siamese network with generated object proposals. Similar objects proposals for the test images are retrieved using the ANNOY (Approximate Nearest Neighbor) library and deep semantic segmentation is performed on them. Finally, we form a collage from the segmented similar objects based on the relative importance of the objects. 
\end{abstract}

\begin{IEEEkeywords}
Cosegmentation, Siamese Network, Multiscale Combinatorial Grouping, Nearest Neighbor
\end{IEEEkeywords}

\section{Introduction \label{sec:intro}}
Automated foreground segregation and localization of objects constitute the fundamental problem in computer vision tasks. Further the lack of sufficient information about the foreground objects makes it highly complex to deal with it. The exploitation of the commonness prior and the joint processing of similar images (containing objects of same category) can aid in the process of such object related tasks. Cosegmentation refers to such class of problems which deals with the segmentation of the \textit{common objects} from a given set of images without any priori knowledge about the foreground. It was first hypothesized in \cite{vicente2011object} that in most cases the common objects for cosegmentation represent the `objects of interest' which appear in the images instead of common background details. These objects have huge variations in terms of scale, viewpoint, rotation, illumination, location and affine changes. In other cases, it may be highly occluded by other objects. Even same class of objects may drastically differ in appearance resulting in high intra-class variation. 

The works in \cite{wang2015robust, li2016object, Huang2016ObjectCB} solve the generic object cosegmentation by applying the localization constraint that in all the images the common object will always belong to the salient region. Some of the methods are confined to the cosegmentation between image pairs \cite{rother2006cosegmentation, wang2013image, joulin2012multi} while others require some user intervention \cite{batra2010icoseg, kowdle2010imodel}. Further \cite{rother2006cosegmentation, mukherjee2009half} pose it as segmenting only those objects that are exactly similar. These approaches are unable to handle the intra-class variations or other synthetic changes or noise which might be present in case of images that are downloaded from Internet. In recent years, with increase in the computational power and access to widespread availability
of semantic annotations for object classes, deep learning has achieved dramatic break-through in various applications. Siamese Network has also been extensively used for many vision applications. They have been used to learn the similarity metrics by aligning the similar objects together and dissimilar objects far away. This motivates us to solve the cosegmentation problem using the high-level features extracted using deep networks. We propose to couple the similarity based clustering and cosegmentation task so that they can coexist and benefit from each other synergistically.   


In this work, we pose cosegmentation as a clustering problem using the Siamese network. For a given set of images, we train the Siamese twin architecture to assess the similarity of two equally sized patches. These patches are the object proposals of an image. Co-segmenting the objects using trained model is done using high-level features utilizing  fully convolutional networks \cite{long2015fully} rather than low-level features like SIFT, HOG etc. Finally, we create a visual summary from the segmented images based on their similarity score in the respective class. In view of the above discussions, the major contributions of this paper are:

\begin{enumerate}
\item Cosegmentation is posed as a clustering problem to align the similar objects using Siamese network and segmenting them. We also train the Siamese network on non-target classes with no to little fine-tuning 
and test the generalization capability to target classes.
\item Generation of visual summary of similar images based on relative relevance.
\end{enumerate}

Rest of the paper is organized as follows. 
In Sec. \ref{sec:Methodology}, we describe the proposed approach in detail. In Sec. \ref{sec:results}, we present the results and discussions. Finally, we conclude the paper in Sec. \ref{sec:conclusion}.







\section{Methodology} \label{sec:Methodology}
In the following subsections, we describe the components of the proposed method. Fig. \ref{fig:block-diagram} shows the overall pipeline of the proposed method.
\subsection{Siamese network}
For given set of images, we first generate the object proposals using different object proposal techniques as described in Sec. \ref{sec:results}. The generated object proposals are given to Siamese Network for training. Siamese Networks are useful in finding similarities and relationship between different structures. The Siamese configuration consists of two convolutional neural networks (CNNs) with shared weights with a contrastive loss layer. 
\begin{figure*}[ht]
\centering
\fbox{
\includegraphics[scale=0.75]{flow.pdf}}
\caption{Overall Architecture}
\label{fig:block-diagram}
\end{figure*}
The input to the Siamese network are two input patches (object proposals) along with a similarity label. Similarity label `1' indicates that patches are similar while `0' indicates dissimilar patches. Two CNNs generate a N-Dimensional feature vector in forward pass. The N-Dimensional vectors are fed to the contrastive loss layer which helps in adjusting the weights such that positive samples are closer and negative samples are far from each other. Contrastive loss function penalizes the positive samples that are far away and negative samples that are closer. Let us consider two  patches $(x_1,x_2)$ that are fed to Siamese network. Let us assume the N-Dimension vectors generated by convnets as $f(x_1)$ and $f(x_2)$. $Y$ be the binary label $Y\epsilon\{0,1\}$, $Y$=1 for similar pairs and 0 otherwise. Margin $m$ is defined for the contrastive layer such that positive samples are at a distance less than margin and negative samples are at a distance greater than margin. Thus, the contrastive loss function is given as,
\begin{equation}
     \textit{L}(W, Y, x_1,x_2)=Y\frac{1}{2} D_W^2 + (1-Y) \frac{1}{2}\{max(0,m-D_W^2)\}
\end{equation} 
where $D_W^2=\left \| f(x_1)-f(x_2) \right \|^2$ is the Euclidean distance between the two feature vectors of the input patches. The outputs from the fully-connected layers are fed to contrastive layers, which measures the distance between two features. The weights $W$ are adjusted such that the loss function is minimized.

After training the Siamese Network, we deployed the trained model on test images. First we extracted the object proposals for the test images. A N-Dimensional feature vector is generated for each of the proposals. In our experiments, we used 256-Dimensional feature vector. The features generated for test image proposals using trained Siamese network are fed to Annoy (Approximate Nearest Neighbor) Library \footnote{https://github.com/spotify/annoy}. It measures the Euclidean distance or Cosine distance between vectors. It works by building up a tree using random projections. A hyper-plane is generated at every intermediate node in the tree which divides the space into two parts. The hyperplane is chosen such that it is equidistant from the chosen two sample points. Annoy library allows to tune two parameters, number of trees and number of nodes to be checked while searching. The features extracted from the test image proposals are given to ANNOY library. Annoy assigns indices to each of the features. To retrieve nearest neighbor for any of the feature, it measures the Euclidean distance to all other features and indices of neighbors are assigned in the increasing order of their Euclidean distance. It has many advantages as compared to other nearest neighbor search algorithms. These include (i) small memory footprint (ii) Annoy creates data structures (read-only) which can be shared among multiple processes.
\subsection{Segmentation}
\label{sec:segment}
Segmentation is performed on the retrieved similar object proposals. We used Fully convolutional Networks for semantic segmentation proposed by Jonathan Long et al.\cite{long2015fully}. Convolutional networks are used as powerful tools for extracting the hierarchy of features. It was the first approach to generate pixel-wise predictions using supervised learning.  The contemporary classification networks are adapted to the segmentation tasks by transferring the learned representations. It utilizes a skip architecture which combines the semantic information from deep (coarse information) and shallow (fine appearance information) layers. %The fully connected (FC) layers are converted into fully convolutional layers to generate coarse output maps. The basic difference between FC layers and convolutional layers is that each neuron in convolutional layer is connected to a local region whereas neurons in FC layers are connected to all the activations in the previous layers. FC layers are converted into convolutional layers by using a filter of size equal to size of input volume to FC layer. 
The fully connected layers can also be considered as convolutions with kernels covering entire image. 
Transforming the FC layers into convolutional layers converts the classification network to generate a heat map. However, the generated output maps are of reduced size as compared to the input size. So, dense predictions are made from coarse maps by upsampling. Upsampling is performed by backward convolution (also called as deconvolution) with stride as $f$. Skip layers are added to fuse semantic and appearance information. 
\subsection{Visual Summary based on relative importance}
A visual summary is created from the segmented proposals. While retrieving the similar object proposals using ANNOY library, we preserved the Euclidean distances corresponding to each of the proposals. A basic collage is formed with 10 slots constituting the most similar proposal (least Euclidean distance) getting a larger block. The remaining segmented objects are placed in the other slots and a background is added to the image.
\section{Experimental Results}
\label{sec:results}
In this section, we discuss the empirical results on two publicly available benchmark co-segmentation datasets. We describe the datasets used followed by implementation details and baseline. Caffe \cite{jia2014caffe} is used for the constructing the Siamese network.

\textbf{Datasets.} MSRC dataset \cite{shotton2006textonboost} consists of 14 categories. Each category consists of 30 images of dimension 213x320. iCoseg dataset \cite{batra2010icoseg} consists of 38 categories. Each category consists of about 20 to 30 images, which are of 300x500 size.

\textbf{Baselines and Parameter setting.} We report results with two baselines. The first baseline involves training the Siamese network with pretrained ILSVRC \cite{russakovsky2015imagenet} models. The weights are fine-tuned for target classes as in the datasets and then segmentation is performed on the clustered test set data. In the second baseline, we train the network on non-target classes and test the generalization ability on target classes. We evaluated on two objective measures: Precision ($\bar{\textit{P}}$) and Jaccard Similarity ($\bar{\textit{J}}$). $\bar{\textit{P}}$ indicates the fraction of the pixels in the segmented image common with the ground truth. $\bar{\textit{J}}$ is the intersection over union measure with the ground truth images.    
\begin{figure}[hbtp]
\centering{
\includegraphics[scale=0.5]{flow_5_.pdf}

\caption{Performance analysis of various object proposal generation methods with proposed architecture.}
\label{fig:objectprops}
}
\end{figure}

\begin{figure}[hbtp]
\centering{
\includegraphics[scale=0.4]{flow_6_.pdf}

\caption{Visualization of iCoseg and MSRC Training set using t-SNE}
\label{fig:tsne}
}
\end{figure}
We generated the object proposals using different methods and evaluated the performance on these metrics. The techniques used are Multiscale Combinatorial Grouping (MCG) \cite{pont2017multiscale}, Selective Search (SS) \cite{uijlings2013selective}, Objectness (Obj) \cite{alexe2012measuring}, SalProp \cite{mukherjee2017salprop} and Edgeboxes \cite{zitnick2014edge}. We further perform a non-maximal suppression and near duplicate rejection in the proposal set. We preserved the top-10 object proposals, so that all the object instances in the images are covered. We used GoogLeNet architectures \cite{szegedy2015going} for training the Siamese in our experiments. We used transfer learning, in which we initialized the weights with pre-trained model weights. We then fine-tuned the weights using back propagation technique. Siamese network is trained and the N-Dimensional (N=256) features are extracted for the test images. The N-Dimensional features are fed to ANNOY and similar object proposals are retrieved. The parameters used for Annoy library include number of trees, $ n_{trees}$=350 and number of nodes to inspect during searching $search_k$=50. Similar object proposals are segmented using FCN based semantic segmentation as discussed in Sec. \ref{sec:segment}. We trained the Siamese architecture by employing the standard backpropagation on feed-forward nets by stochastic gradient descent with momentum to adjust the weights. The mini-batch size was set to 128, with an equal learning rate for all layers set to 0.01. The number of iterations is set as 100,000, contrastive loss margin as 1. 

We also trained the Siamese network on datasets which contains similar (but not same) classes to iCoseg and MSRC datasets. We used Pascal \cite{everingham2010pascal}, Animals\cite{afkham2008joint} and Coseg-Rep \cite{dai2013cosegmentation} datasets to train the Siamese model and tested on iCoseg and MSRC datasets. Initially, we randomly selected positive and negative pairs for training the Siamese network. However, once most of the pairs are correctly learned, then using those pairs, Siamese cannot learn anymore. So, to address this issue, we used strategy of aggressive mining \cite{simo2015discriminative} for preparing hard negative and positive pairs. 

\textbf{Results.} We divided iCoseg dataset into 80\% training samples and 20\% as testing set for each class. For MSRC dataset the split was 70\%-30\% (training-test). The results of the $\bar{\textit{P}}$ and $\bar{\textit{J}}$ are shown in Fig.\ref{fig:objectprops}. It can be observed that  Siamese network fed with MCG proposals outperforms all other object proposal generation techniques with the closest being SalProp followed by SS, Obj and Edgeboxes. For both the datasets, the average precision and Jaccard index over all the classes with MCG proposals is higher than SalProp technique with a gap on an average being 2.48\% and 1.84\% in $\bar{\textit{P}}$ and $\bar{\textit{J}}$ respectively. The 256-D feature vector of the training set are visualized using t-SNE (t-Distributed Stochastic Neighbor Embedding) as shown in Fig. \ref{fig:tsne}. Firstly for high dimensional data, a probability distribution is built such that similar objects gets selected with high probability and dissimilar points have very low probability of being selected. In the second step, similar to a high-dimensional map, probability distribution over the points in the low-dimensional map is constructed. The color-coding in the t-SNE plots corresponds to the number of object classes in the respective datasets. Siamese net with post-processing helps in better separation of the classes compared to before fine-tuning. As can be seen, the results of clusters of classes are well separated with only few cluster of confusion. 

We computed the average precision and jaccard similarity and compared with the other state-of-the-art methods in Tab. \ref{perficoseg}-\ref{perfmsrc}. On testing with complete iCoseg dataset, we achieve a gain in $\bar{\textit{P}}$ of 27.27\% (Joulin et al. \cite{joulin2010discriminative}), 23.52\% (Kim et al. \cite{kim2012multiple}). Quan et al. \cite{quan2016object} outperform with a margin of $\bar{\textit{P}}$ :9.67\% and $\bar{\textit{J}}$ : 13.15\% compared to the proposed technique (Siamese (MCG) + FCN segmentation). Similarly with MSRC dataset, we achieve a gain in $\bar{\textit{P}}$ of 20\% (Joulin et al. \cite{joulin2010discriminative}), 9.09\% (Jian et al. \cite{sun2016learning}), 44.82\% (Kim et al. \cite{kim2012multiple}) and in $\bar{\textit{J}}$ of 15.51\% (Yong Li\cite{li2016object}). Rubinstein et al.\cite{Rubinstein13Unsupervised} outperform with a margin of $\bar{\textit{P}}$ :8.6\% and $\bar{\textit{J}}$ : 1.47\% compared to the proposed technique (Siamese (MCG) + FCN segmentation). In FCN segmentation, we used VGGNet architecture, with FC layers replaced with convolutional layers. Deconvolutional layers are fixed using bilinear interpolation. We have abstained from using any auxiliary training and use the pretrained weights to avoid over-fitting in the FCN segmentation network. The object proposals that are similar and clustered together are fed as input to FCN segmentation to obtain the co-segmentation results. We consider only those object proposals for the cosegmentation task which have an intersection over union (IoU) score $IoU\geq0.5$. Since, the segmentation is performed on the tight object proposals it segments the regions specific to the object class only and thus refrains from performing semantic segmentation over entire image. Owing to the performance boost by aggressive mining we achieve an average gain of 4\% and 2.52\% on both the datasets in $\bar{\textit{P}}$ and $\bar{\textit{J}}$ respectively over training with MCG proposals (Siamese (MCG) + FCN segmentation). Fig. \ref{fig:segmented} shows the qualitative results on few example classes in the datasets. However, it is important to note here that since we perform cosegmentation on the subset of images owing to the retrieval results we observe that there is a drop in performance with respect to few reported techniques. The advantage of the proposed technique over other compared techniques(\cite{Rubinstein13Unsupervised,li2016object,joulin2010discriminative,kim2012multiple,quan2016object,meng2016cosegmentation,faktor2013co}) involve: (i) co-segmenting without explicit knowledge of localization prior in the form of saliency map (ii) co-segmentation pipeline being formulated as clustering followed by segmentation of the similar object classes thus eliminating the need for providing as input the relevant set of class-specific images as required in graph-based co-segmentation techniques. The proposed method takes less than 100ms for the generation of 256-dimensional features from trained Siamese network.

\begin{figure}[hbtp]
\centering{
\includegraphics[scale=0.35]{flow_7_.pdf}

\caption{Visual segmentation results on iCoseg and MSRC datasets. First three rows are classes of iCoseg (Cheetah, Panda, Taj-Mahal) and next two rows are MSRC (Car, Cow).}
\label{fig:segmented}
}
\end{figure}

\begin{figure}[hbtp]
\centering{
\includegraphics[scale=0.4]{flow_8_.pdf}

\caption{Example of Collage results for Chair class (MSRC).}
\label{fig:collage}
}
\end{figure}



\begin{table}[]
\centering
\caption{Comparison of Average precision and Jaccard Similarity with state-of-the-art methods. ('-' indicates that the metric has not been provided in the respective paper) on iCoseg dataset}
\label{perficoseg}
\begin{tabular}{||c| c |c||}
\hline
\textbf{Method} & $\bar{\textit{P}}$ & $\bar{\textit{J}}$\\ \hline
\textbf{Rubinstein}\cite{Rubinstein13Unsupervised}                                       & 0.88                             & 0.674                            \\\hline
\textbf{Joulin}\cite{joulin2010discriminative}                                           & 0.66                             & -                                \\\hline
\textbf{Kim}\cite{kim2012multiple}                                              & 0.68                             & -                                \\\hline
\textbf{Keuttel}\cite{kuettel2012segmentation}                                          & 0.91                             & -                                \\\hline
\textbf{Quan}\cite{quan2016object}                                            & 0.93                             & 0.76                             \\\hline
\textbf{Fanman Meng}\cite{meng2016cosegmentation}                                      & -                                & 0.71                             \\\hline
\textbf{Faktor}\cite{faktor2013co}                                           & 0.92                             & 0.70                             \\\hline
\multicolumn{3}{||c||}{\textbf{Trained on 80\% and tested with 20\%}}                                                                   \\\hline
\textbf{Method} & $\bar{\textit{P}}$ & $\bar{\textit{J}}$\\\hline
\textbf{Siamese (Edgeboxes) + FCN segmentation}                                   & 0.76                             & 0.61                             \\\hline
\textbf{Siamese (Obj) + FCN segmentation}                                         & 0.78                             & 0.62                             \\\hline
\textbf{Siamese (SS) + FCN segmentation}                                          & 0.79                             & 0.64                             \\\hline
\textbf{Siamese (SalProp) + FCN segmentation}                                     & 0.79                             & 0.64                             \\\hline
\textbf{Siamese (MCG) + FCN segmentation}                                         & 0.81                             & 0.654                            \\\hline\hline
\multicolumn{3}{||c||}{\textbf{Trained on 80\% and tested with 100\%}}   \\                                                  \hline             
\textbf{Method} & $\bar{\textit{P}}$ & $\bar{\textit{J}}$\\\hline
\textbf{Siamese (Edgeboxes) + FCN segmentation}                                   & 0.76                             & 0.62                             \\\hline
\textbf{Siamese (Obj) + FCN segmentation}                                         & 0.81                             & 0.64                             \\\hline
\textbf{Siamese (SS) + FCN segmentation}                                          & 0.83                             & 0.65                             \\\hline
\textbf{Siamese (SalProp) + FCN segmentation}                                     & 0.83                             & 0.65                             \\\hline
\textbf{Siamese (MCG) + FCN segmentation}                                         & 0.84                             & 0.66                             \\\hline\hline
\multicolumn{3}{||c||}{\textbf{\begin{tabular}[c]{@{}l@{}}Trained on Pascal+animals+coseg-rep and tested\\ on iCoseg\end{tabular}}} \\\hline
\textbf{Method} & $\bar{\textit{P}}$ & $\bar{\textit{J}}$\\\hline
\textbf{Siamese (MCG) + FCN segmentation}                                         & 0.73                             & 0.59                             \\\hline
\textbf{Siamese (MCG) + FCN segmentation-Aggressive mining}                       & 0.76                             & 0.61     \\              \hline         
\end{tabular}
\end{table}

\begin{table}[]
\centering
\caption{Comparision of Average precision and Jaccard Similarity with state-of-the-art methods. ('-' indicates that the metric has not been provided in the respective paper) on MSRC dataset}
\label{perfmsrc}
\begin{tabular}{||c| c |c||}
\hline
\textbf{Method} & $\bar{\textit{P}}$ & $\bar{\textit{J}}$\\ \hline
\textbf{Rubinstein}\cite{Rubinstein13Unsupervised}                                       & 0.92                            & 0.68                            \\\hline
\textbf{Joulin}\cite{joulin2010discriminative}                                           & 0.70                             & -                                \\\hline
\textbf{Jian Sun}\cite{sun2016learning}                                           & 0.77                             & 0.54                                \\\hline
\textbf{Faktor}\cite{faktor2013co}                                           & 0.89                             & 0.73                             \\\hline
\textbf{Kim}\cite{kim2012multiple}                                              & 0.58                             & -                                \\\hline
\textbf{Yong Li}\cite{li2016object}                                          & -                            & 0.58                                \\\hline\hline
 \multicolumn{3}{||c||}{\textbf{Trained on 70\% and tested with 30\%}}                                                                   \\\hline
\textbf{Method} & $\bar{\textit{P}}$ & $\bar{\textit{J}}$\\\hline
\textbf{Siamese (Edgeboxes) + FCN segmentation}                                   & 0.77                             & 0.62                             \\\hline
\textbf{Siamese (Obj) + FCN segmentation}                                         & 0.80                            & 0.63                             \\\hline
\textbf{Siamese (SS) + FCN segmentation}                                          & 0.81                             & 0.63                             \\\hline
\textbf{Siamese (SalProp) + FCN segmentation}                                     & 0.81                             & 0.64                             \\\hline
\textbf{Siamese (MCG) + FCN segmentation}                                         & 0.83                             & 0.65                            \\\hline\hline
\multicolumn{3}{||c||}{\textbf{Trained on 70\% and tested with 100\%}}   \\                                                  \hline             
\textbf{Method} & $\bar{\textit{P}}$ & $\bar{\textit{J}}$\\\hline
\textbf{Siamese (Edgeboxes) + FCN segmentation}                                   & 0.765                             & 0.64                             \\\hline
\textbf{Siamese (Obj) + FCN segmentation}                                         & 0.81                             & 0.65                             \\\hline
\textbf{Siamese (SS) + FCN segmentation}                                          & 0.82                             & 0.65                             \\\hline
\textbf{Siamese (SalProp) + FCN segmentation}                                     & 0.82                             & 0.66                             \\\hline
\textbf{Siamese (MCG) + FCN segmentation}                                         & 0.84                             & 0.67                            \\\hline\hline
\multicolumn{3}{||c||}{\textbf{\begin{tabular}[c]{@{}l@{}}Trained on Pascal+animals+coseg-rep and tested\\ on iCoseg\end{tabular}}} \\\hline
\textbf{Method} & $\bar{\textit{P}}$ & $\bar{\textit{J}}$\\\hline
\textbf{Siamese (MCG) + FCN segmentation}                                         & 0.76                             & 0.60                             \\\hline
\textbf{Siamese (MCG) + FCN segmentation-Aggressive mining}                       & 0.79                             & 0.61     \\              \hline         
\end{tabular}
\end{table}
We create a visual summary of the co-segmented similar objects. We preserved the Euclidean distances while retrieving the similar objects. Image is divided into different blocks and objects are placed such that the object with least Euclidean distance is at the center. A proper background is added to improve the visual appearance. Fig. \ref{fig:collage} shows the sample collage results formed the Chair class in MSRC. A 512x512 image is divided into 10 blocks consisting a blue sky back-ground to form collage. Future work would be aimed to further improve the segmentation results and utilization of more cues for the relevance ranking in the collage-generation.

\section{Conclusion} \label{sec:conclusion}
We addressed object cosegmentation and posed it as a clustering problem using deep Siamese network to align the similar images which are segmented using semantic segmentation. Additionally, we compared the performance of various object proposal generation schemes on Siamese architecture. We performed extensive evaluation on iCoseg and MSRC dataset and demonstrated that the deep features can encode the commonness prior and thus provide a more discriminative representation for the features.  

\bibliographystyle{IEEEtran}
\bibliography{sigproc}

\end{document}


