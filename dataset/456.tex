\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage{booktabs}
\usepackage{mathtools}
\usepackage{algorithm,algorithmic}
\usepackage[utf8]{inputenc}
\usepackage{tabularx}
\usepackage{color}

\usepackage[font={footnotesize}]{caption}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\HB}[1]{{\color{blue}{HB: #1}}} % Harkirat comments
\newcommand{\MON}[1]{{\color{green}{MON: #1}}} % Harkirat comments
\newcommand{\footnoteremember}[2]{
\footnote{#2}
\newcounter{#1}
\setcounter{#1}{\value{footnote}}
}
\newcommand{\footnoterecall}[1]{
\footnotemark[\value{#1}]
}

\def\cvprPaperID{1102} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}



\ifcvprfinal\pagestyle{empty}\fi

\begin{document}
\title{Meta Learning Deep Visual Words for Fast Video Object Segmentation}

\author{Harkirat Singh Behl\textsuperscript{\,*}
\qquad
Mohammad Najafi\textsuperscript{\,*}
\qquad
Philip H.S. Torr\\
University of Oxford\\
{\tt\small \{harkirat, monaj, phst\}@robots.ox.ac.uk}
}
\twocolumn[{%
\maketitle
\renewcommand\twocolumn[1][]{#1}%
\begin{center}
    \centering
    \includegraphics[width=\textwidth]{teaser.pdf}
    \captionof{figure}{{\bf Video object segmentation by unsupervised learning of a dictionary of deep visual words as object representation.} The proposed method meta learns a dictionary of deep visual words for representing object parts in the video (bottom), in order to perform video object segmentation (top) by assigning each pixel to the most relevant visual word. Note that no ground-truth for the visual words are provided at training or test time. The algorithm yields a meaningful set of visual words, which implicitly represent object parts, and are robust over time.
    \label{fig:teaser}}
\end{center}%
}]


\let\thefootnote\relax\footnotetext{\textsuperscript{*}Joint first authorship.}
\begin{abstract}
\vspace{-0.15in}
Meta learning has attracted a lot of attention recently. In this paper, we propose a fast and novel meta learning based method for video object segmentation that quickly adapts to new domains without any fine-tuning. The proposed model performs segmentation by matching pixels to object parts. The model represents object parts using deep visual words, and meta learns them with the objective of minimizing the object segmentation loss. This is however not straightforward as no ground-truth information is available for the object parts. We tackle this problem by iteratively performing unsupervised learning of the deep visual words, followed by supervised learning of the segmentation problem, given the visual words. Our experiments show that the proposed method performs on-par with state-of-the-art methods, while being computationally much more efficient.

\end{abstract}


\vspace{-0.15in}\section{Introduction}%Meta learning, or learning to learn, is an efficient way to tackle the problem of few-shot learning, which is the problem of learning new concepts from just a few training examples~\cite{NIPS2016_6385, NIPS2017_6996, MAML}. More specifically, meta learning systematically analyzes the learning process on a variety of similar problems/tasks, such that learning a new problem/task becomes easier.%This is achieved by dividing the problem into two sub problems: learning the common knowledge across tasks, and learning the task specific knowledge from the task at hand.%Thus it achieves rapid learning of a new concept by learning good generalization from previously seen tasks.%Field of meta learning has attracted massive attention recently \cite{vanschoren2018metalearning, MAML, NIPS2016_6385, NIPS2017_6996} in the machine learning community, but it has not been explored much on bigger and more complex, computer vision problems.
Meta learning is a method for learning to learn~\cite{Schmidhuber, bengio:1995:oban}, which can be used in few-shot learning problems~\cite{Hugo, NIPS2016_6385, NIPS2017_6996}. By analyzing the learning process over multiple tasks, a meta learning algorithm learns how to learn a new but similar task quickly and more efficiently.
Within this paper, we aim to explore meta learning in the context of video object segmentation.


Video object segmentation is defined as the task of segmenting one or multiple objects in a video. The most recent setup for this task, as defined by the DAVIS challenge \cite{Caelles_arXiv_2018, davis_2017}, is when the ground-truth masks of objects in the first frame are provided, and the goal is to segment them in rest of the video. 
It is a very challenging task, because at different time points, the objects of interest may be observed in different configurations due to e.g. partial occlusion, disappearing and re-appearing, shape deformation, and pose and scaling variation.
Thus, a good object representation should be robust to these intra-object variances, and be able to quickly adapt to new object configurations in time.

\begin{figure}[t!]
	\centering
	\includegraphics[width=\textwidth/2]{Picture1.png}
	\caption{{\bf Accuracy vs. Speed on DAVIS-2017 validation Dataset.} This figure illustrates the accuracy (mean $\mathcal{J}$) and speed (FPS) of various methods. The FPS axis is in log scale.}
	\label{fig:chart}   
\end{figure}

The key idea is that these intra-object variations can be accounted for by assigning a pixel to only one relevant part of the object, rather than forcing it to resemble the entire object (e.g. matching a pixel from leg to the leg only). In other words, segmentation will be better if the matching is done to object parts.
We introduce a set of deep visual words into our model to represent the object parts, and let the model learn these visual words (in an unsupervised manner), with the objective of minimizing the object segmentation loss.
It can be seen in Fig.~\ref{fig:teaser} and Fig.~\ref{fig:onlineadaptation} that the model can learn meaningful visual words which correspond to different object parts, which might be quite dissimilar, even though all are parts of the same object. %Thus, this result supports our idea.%Therefore, the model is able to do quick adaptation in video object segmentation, by matching the pixels to these learned visual words.%Our idea is to account for these intra-object variations by corresponding a pixel to only one relevant part of the object, rather than forcing it to resemble the entire object (e.g. matching a pixel from leg to the leg only). In other words, segmentation will be better if the matching is done to object parts.%However these object parts are unknown during training and test time. Thus, we introduce a set of deep visual words into our model to represent them, and let the model learn these visual words (in an unsupervised manner), with the objective of minimizing the object segmentation loss.%It can be seen in Fig.~\ref{fig:teaser} and Fig.~\ref{fig:onlineadaptation} that the model can learn meaningful visual words which correspond to different object parts, which might be quite dissimilar, even though all are parts of the same object. This result supports our idea.%Then, the model is able to do quick adaptation in video object segmentation, by matching the pixels to these learned visual words.%This is a form of meta learning as our algorithm learns to learn deep visual words from the first frame of the video, to minimize an object segmentation loss over the rest of the video. %%This meta learning process is not straightforward since the visual words are not shown to the model during training. %We tackle this problem by presenting the meta learning procedure as an iterative meta-training process of unsupervised learning of deep visual words, followed by supervised learning of the object segmentation model given the learned visual words. %This procedure can be seen in Fig. \ref{fig:ourmodel}. Finally, the model is able to do quick adaptation in video object segmentation, by matching the pixels to these learned meaningful visual words.

In this paper we present an iterative meta learning based algorithm, which learns the visual words from the first frame of the video using unsupervised learning. The algorithm then minimizes an object segmentation loss over the rest of the video, given the learned visual words.
This procedure can be seen in Fig. \ref{fig:ourmodel}.
Finally, the model is able to do quick adaptation in video object segmentation, by matching the pixels to these learned meaningful visual words.


State-of-the-art methods for video object segmentation often fine-tune a pre-trained segmentation network on the first frame of the video \cite{OSVOS,onavos,masktrack,Bao_2018_CVPR,Li_2018_ECCV}, and some perform further online fine-tuning~\cite{onavos}, to adapt better to the objects of interest .
However, the fine-tuning process is very time consuming ($\sim 700$s to $3$h per video) \cite{OSVOS,lucid}, because of which the best performing methods on DAVIS video-object segmentation challenge \cite{davis_2017, Caelles_arXiv_2018} work at $\sim 15$~seconds/frame.
Thus, these methods are not very practical for online applications including autonomous cars and robots, where frame processing needs to be fast.


In contrast, our model is very simple and intuitive and does not include sophisticated modules that are often incorporated in more complex methods. Nevertheless, it performs on par with them by achieving a promising score of $\mathcal{J}\&\mathcal{F}=67.3\%$ on DAVIS-2017, without any fine-tuning on the first frame of the video, while running at $\sim$ 3.5 fps, which is 1-2 orders of magnitude faster than them as shown in Fig.~\ref{fig:chart}. 
Furthermore, our model enables objects to be represented by newer visual words on demand, as they change in shape or pose over time. This leads to a much more efficient domain adaptation, compared to the previous methods~\cite{onavos}, where adaptation was addressed by fine tuning network parameters over time.
I) We show that efficient adaptation for video object segmentation can be achieved by learning to learn;
 We further present state-of-the-art results on two challenging video object segmentation benchmarks, among the methods without fine-tuning on the first frame of the test video;
II) Our approach is very simple and intuitive, making it an appealing approach to video object segmentation.
III) Our model constructs robust and meaningful visual words without ever seeing any visual words during training.
\section{Related Work}\label{sec:relwork}\paragraph{Video object segmentation papers}
One of the widely used techniques for video object segmentation is to train a deep fully convolutional network (FCN)~\cite{fcn} for foreground/background object segmentation on a training dataset, and then further adapt it to the test video through a fine-tuning process,
using dense ground-truth object masks that are available in the first frame of the video ~\cite{OSVOS, onavos, hu2017maskrnn, Cheng_2017_ICCV, masktrack, lucid, Yoon_2017_ICCV, Ci_2018_ECCV}. 
However, fine-tuning process dramatically slows down these techniques, making them unsuitable for online applications.
Our algorithm performs on-par with these methods, and is much faster as it does not need any fine-tuning.

A group of methods are proposed based on mask propagation \cite{masktrack, Oh_2018_CVPR}, where the segmentation network is guided by the predicted objects masks over time. These methods are vulnerable to object occlusion and may lose the track of objects at some points. 
Li et al.~\cite{Li_2018_ECCV} addressed this problem by using re-id modules and traversing the video back-and-forth to recover any potential missed object prediction, however their method is not suitable for online and streaming applications.
There have been successful attempts to incorporate motion cues into video object segmentation systems, using {\it Optical Flow}\cite{hu2017maskrnn, Hu_2018_CVPR, Xiao_2018_CVPR, Bao_2018_CVPR, Cheng_2017_ICCV, Jain_2017_CVPR, Tsai_2016_CVPR, Yeo_2017_CVPR, Jang_2017_CVPR}. However, Optical Flow maps are expensive to compute ($\sim 0.5$s per frame using {\it FlowNet2}~\cite{flownet2}) and slow down the algorithm.
Unsupervised methods have also tackled the video object segmentation problem \cite{Li_2018_ECCV_un, Hu_2018_ECCV_un, Li_2018_CVPR, Tokmakov_2017_ICCV, Koh_2017_CVPR, Jain_2017_CVPR}, but they are not very well suited for multiple object segmentation and tracking, which is the aim of this paper.%, as there is no supervision to distinguish between different objects/instances in the video.%FAST METHODS% blazingly fast
Pixel-to-pixel matching has been used in \cite{Chen_2018_CVPR, Hu_2018_ECCV, DAVIS2018-Interactive-2nd, Yoon_2017_ICCV} to transfer the ground-truth information from the first frame to all subsequent frames of the video.
Chen et al.~\cite{Chen_2018_CVPR} formulated the segmentation task as a pixel retrieval problem. In particular, they retrieved a set of similar pixels from the training pool and then applied a k-nearest-neighbour classifier, to predict the class label of a test pixel.
Siamese networks have been used for pixel label transfer, using {\it soft matching layer}~\cite{Hu_2018_ECCV} and {\it stochastic pooling layer}~\cite{DAVIS2018-Interactive-2nd}.
Yoon et al. \cite{Yoon_2017_ICCV} proposed a pixel level matching module to match the image with the reference frame. 
The problem with methods based on pixel-to-pixel matching is that they either have to explore all the pixels space, which is computationally expensive, or they might alleviate this problem by discarding many training pixels, which may lead to information loss.


Similar to our work, is \cite{Cheng_2018_CVPR}, in which the authors performed video object segmentation via tracking object parts. More specifically, they first extract a fixed set of object parts using region proposals, and track their corresponding bounding boxes independently for the entire video. Next, they perform Region of Interest (ROI) segmentation within each bounding box to extract the object part from the background. Finally, they apply similarity based part aggregation to discard false positives.
In contrast to \cite{Cheng_2018_CVPR}, we do not put any constraint on the object parts, and all the visual words in our model are learned automatically through the meta-learning algorithm. Note that no ground-truth information is available for learning the visual words, and they are computed using an unsupervised approach, as described in Sec.~\ref{sec:Model}. As shown in Fig.~\ref{fig:teaser}, the computed visual words are quite meaningful, even though we have not explicitly used any location information to learn them. 
Moreover, our model facilitates online adaptation, simply by updating the pool of visual words that are computed by the model itself, through time.
In addition, unlike \cite{Cheng_2018_CVPR}, our model achieves promising results, without any post-processing module such as Conditional Random Field (CRF), thanks to our end-to-end meta-training process. 

\paragraph{Meta learning methods}
Meta learning has not been explored much for video object segmentation.
Yang et al.~\cite{Yang_2018_CVPR} proposed a meta learning based method for fast adaptation of a deep segmentation network, where, unlike our approach, the parameters of the segmentation network are updated at the test time using different network modulators. 

Our meta learning algorithm could be viewed as a generalization of Prototypical networks~\cite{NIPS2017_6996} and Matching networks~\cite{NIPS2016_6385}. In the Prototypical networks, the training data from each class is represented using only one single prototype, which as shown in our ablation study Table \ref{tab:abltation_dictionary_size}, is not sufficient for visual data representation in the complex tasks of object segmentation. 
On the other end of spectrum is Matching networks~\cite{NIPS2016_6385}, where all training data samples play a role in the classification/matching task, regardless of how redundant or noisy they are. This deteriorates the performance of the system both in terms of accuracy and run-time speed.
The proposed method in this paper combats these problems by introducing visual words into the meta learning process, which inherently represent object parts. It is a challenging task as there is no supervision for determining the visual words in the objects, and we propose to learn them in an unsupervised fashion. 
OSVOS \cite{Caelles_2017_CVPR} proposed to fine-tune a segmentation model on the first frame. This technique has been widely used in the community. Fine-tuning gives significant boost in performance, and has been widely used \cite{onavos,hu2017maskrnn, Cheng_2017_ICCV, masktrack, Jampani_2017_CVPR,lucid, Yoon_2017_ICCV, Ci_2018_ECCV}. But this also makes the methods slow and cannot be used for real online applications.
MaskTrack \cite{masktrack} proposed to formulate it as a mask propagation problem, i.e guiding the segmentation network with the mask from the previous frame.
Jampani et al. \cite{Jampani_2017_CVPR} explored bilateral filtering for propagating information in time.
This approach has been widely used. But propagation based methods are not capable of dealing with occlusion. To overcome this limitation, Li et. al. \cite{Li_2018_ECCV} proposed to use re-identification along with the propagation module of \cite{masktrack} to deal with occlusions. They have excellent results but method is offline and uses future frames and really slow.
It is important to note that we do not explicitly propagate the mask.
Some people have tried to use motion information efficiently \cite{Hu_2018_CVPR, Xiao_2018_CVPR, Cheng_2017_ICCV, Jain_2017_CVPR, Tsai_2016_CVPR}. This is something that we have not tried yet. This is complementary to our work and can further help improve performance.
Bao et al.\cite{Bao_2018_CVPR} proposed to use a spatio-temporal MRF to utilize the temporal information, which gives huge improvements. This method cannot be used for online applications.
Some methods\cite{Koh_2018_ECCV, Yeo_2017_CVPR, Yang_2017_CVPR} have proposed to independently extract instances in each frame and then link them in time.
Han et al. \cite{Han_2018_CVPR} formulate it as a markov decision process and apply re-inforcement learning algorithms.
Some unsupervised methods have also been proposed \cite{Li_2018_ECCV_un, Hu_2018_ECCV_un, Li_2018_CVPR, Tokmakov_2017_ICCV, Koh_2017_CVPR, Jain_2017_CVPR}. These methods work even without the mask of the object on the first frame, for the segmentation of the primary object.

Some recent methods have focused on making fast methods for this problem. Chen et al. \cite{Chen_2018_CVPR} formulate it as a pixel-wise retrieval problem, i.e, for each pixel in frame $t$ they find the most similar pixel in the reference frame.
Hu et al. \cite{Hu_2018_ECCV} keep all pixels in fg and bg to match with.n they then use top k pixels in template to find score for each pixel in query frame.
Yang et al. \cite{Yang_2018_CVPR} deal with fast adaptation. They propose to keep a separate modulator network that can tell how to modify the weights, they use three modulators, conditional batch normalization to modify batch-norm layers, a spatial modulator and a visual modulator. Not at all intuitive.
Yoon et al. \cite{Yoon_2017_ICCV} proposed a pixel level matching module to match the image with the reference frame. Oh et al. \cite{Oh_2018_CVPR} also do pixel level matching with both reference mask and propagation. Their contribution involves the refinement module which produces very well refined segmentation outputs without taking extra crf inference time. They also use a complex training mechanism by first training on simulated samples from 2 datasets, and then training on video using recurrence. 
Model is for one object, for multiple objects they have to do it independently, thus speed increases linearly with number of instances in video.
They are unable to tackle occlusions because they rely on mask propagation.

Jang et al. \cite{Jang_2017_CVPR} use three decoders and then a MRF on top.
Cheng et al. \cite{Cheng_2018_CVPR} propose to do video object segmentation via tracking parts. They first extract parts using region proposals and track part bounding boxes independently. Then do segmentation withing each part based bounding box and in the final step apply a similarity based part aggregation.
All these components are individually done.
They then use standard instance segmentation network and train it on parts extracted from DAVIS dataset. Then after getting segmentation inside each bb. they again match these part segments with inital object part segments.
They use a fg/bg regularization, spatial propagation network and another region based refinement for DAVIS-17 to get the results. do it independently for each object.
spee shown without these extra components.
they have 5-7 predefined parts. we don't use spatial information. These parts are constructed from large number of intra-class samples.
The formation of these parts is learnt during tracking and it has learned to form the best parts from just the first frame.** Our approach is about learning how to learn these parts just from the first frame.
can capture texture as you can take as many parts.
this is not just happening at test time but at train time.
They do it independently for each instance which cannot scale to more difficult scenarios.

\paragraph{Previous multi-modal methods}\paragraph{Meta learning methods}
In the case with one mode, our model is same as the prototypical networks, but in the case where there are more than multiple modes

\iffalse\textbf{NNs with external memory and attention}
Non-static classification
This is basically attention mechanism. Instead of the fact that the memory is composed of the clusters of each object with label of the object attached to it, and the attention mechanism attends to one at a time. The online update can be seen as updating the memory of the network with time, as more is being added to the memory.
\begin{equation}
\hat{y_i} = \sum_{ck} a(\mu_{ck},x_i) c
\label{eq:nn_memory}
\end{equation}
where the attention mechanism $a(\mu_{ck},x_i)$ is as specified in Eq.2-4. So, unlike matching networks which stored all the examples from support set into the memory, or the prototypical networks that stored the mean of examples of a class, we store the respective clusters for the class.
\fi\fi
	\centering
	\includegraphics[width=\textwidth/2]{meta-learning.pdf}
	\caption{{\bf Formulation of video object segmentation as a meta learning problem.} Each video presents a new task; to learn from the ground truth object masks on the first frame (support set), to segment them on the rest of the frames in the video (query set).}
	\label{fig:meta-learning}  
\end{figure}\section{Method}\label{sec:method}
In this section, we first describe the formulation of video object segmentation problem as a meta-learning problem. Next, we explain our model and meta training strategy. Then we discuss our online adaptation approach.

\begin{figure*}[t!]
	\centering
	\includegraphics[width=\textwidth]{fig.pdf}
	\caption{{\bf Overview of the proposed method.} The first frame of the video (reference frame), which forms the support set $\mathcal{S}$ in our meta learning setup, passes through a deep segmentation network $f(\theta)$ to compute a 128D embedding vector for each pixel. Then a dictionary of deep visual words are learned by clustering these embeddings for each objects in the reference frame (Eq.~\ref{eq:unsupervised}). Then, pixels of the query frame are classified as one of the objects based to their similarities to the visual words (Eq.~\ref{eq:cluster_prob} and Eq.~\ref{eq:label_prob}). The model is meta-trained by alternately learning the visual words given model parameters $\theta$, and learning model parameters given the visual words.}
	\label{fig:ourmodel}   
\end{figure*}\subsection{Video Object Segmentation as Meta-Learning}\label{sec:metaformulation}%Meta-learning, or learning to learn, is often defined as learning from a number of tasks in the training set to learn a new task in the test set \cite{MAML, NIPS2017_6996}. Let each task $\mathcal{T}$ be a supervised learning problem \cite{MAML} designed for a pair of support set $\mathcal{S}$ and query set $\mathcal{Q}$, i.e. $\mathcal{T} = \{\mathcal{S},\mathcal{Q}\}$. %The objective of meta-learning~\cite{Schmidhuber, bengio:1995:oban} is to learn how to learn a new but similar task quickly and more efficiently. It does so by analyzing the learning process over multiple tasks.

Meta-learning, or learning to learn, is often defined as learning from a number of tasks in the training set, to become better at learning a new task in the test set \cite{MAML, NIPS2017_6996}.
In the context of meta-learning for video object segmentation, the task is to learn from the ground-truth masks of the objects in the first frame of the video (support set) to segment and track them in rest of the video (query set).
Our meta learning objective is to learn model parameters $\theta$ on a variety of tasks (videos), which are sampled from the distribution $p(\mathcal{T})$ of training tasks (i.e. meta-training set), such that the learned model performs well on a new unseen task (test video).
Let the performance of the model on the $n^{th}$ task, $\mathcal{T}_n$, be rendered by the loss $\mathcal{L}_{\mathcal{T}_n}(\theta)$. 
Then, the meta-training objective becomes:

\theta^*
&= \argmin_\theta \sum_{\mathcal{T}_n \sim p(\mathcal{T})} \mathcal{L}_{\mathcal{T}_n}(\theta).
\label{eq:loss} \numberthis
\end{align*}% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%performing object segmentation on each video is a new task. On each video, the task is to learn about the objects of interest from the ground-truth masks in the first frame, and then segment and track them in rest of the video.
Fig.~\ref{fig:meta-learning} illustrates  our meta learning setup for video object segmentation based on this definition.
In this setup, the support set $\mathcal{S}$ is the set of all labeled pixels in the first frame, $ \mathcal{S} = \{ x_i, y_i \}_{i=1}^{N}$. Here $x_i$ represents the pixel $i$ in the first frame, $y_i \in \mathcal{C} = \{ 1,...,C \}$ is the ground truth class label of pixel $x_i$, $N$ is the number of labeled pixels in the frame, and $C$ is the number of object classes that need to be tracked and segmented in the video. Similarly the query set is defined by $ \mathcal{Q} = \{ x_j, y_j \}_{j=1}^{N\times F}$,  where $x_j$ denotes pixel $j$ from the video, $y_j$ represents the ground-truth class label for pixel $j$, and $F$ is the number of frames in the video (excluding the first frame). 
The output of each task $\mathcal{T}$ is the set of predicted class labels for the pixels in $ \mathcal{Q}$, i.e. $\mathcal{\hat{Y}} = \{\hat{y}_j\}_{j=1}^{N\times F}$. 

\iffalse
The objective of meta-learning~\cite{Schmidhuber, bengio:1995:oban} is to learn model parameters $\theta$ on a variety of tasks sampled from a distribution $p(\mathcal{T})$, such that the learned model performs better on a new unseen task from the same distribution. 
Here each task $\mathcal{T}$ can be a learning problem \cite{MAML} designed for a pair of support set $\mathcal{S}$ and query set $\mathcal{Q}$, i.e. $\mathcal{T} = \{\mathcal{S},\mathcal{Q}\}$. 
Let the performance of the model on the $n^{th}$ task, $\mathcal{T}_n$, be rendered by a loss $\mathcal{L}_{\mathcal{T}_n}(\theta)$. $\mathcal{L}_{\mathcal{T}_n}(\theta)$ is defined as a function of model parameters, and computed given the predicted labels and ground-truth labels of samples in query set $ \mathcal{Q}$. Then, the meta-training objective becomes:
\theta^*
&= \argmin_\theta \sum_{\mathcal{T}_n \sim p(\mathcal{T})} \mathcal{L}_{\mathcal{T}_n}(\theta).
\label{eq:loss} \numberthis
\end{align*}% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\begin{figure*}[t!]
	\centering
	\includegraphics[width=\textwidth]{images/fig2.pdf}
	\caption{{\bf Overview of the proposed method.} The first frame of the video (reference frame), which forms the support set $\mathcal{S}$ in our meta learning setup, passes through a deep segmentation network $f(\theta)$ to compute a 128D embedding vector for each pixel. Then a dictionary of deep visual words are learned by clustering these embeddings for each objects in the reference frame (Eq.~\ref{eq:unsupervised}). Then, pixels of the query frame are classified as one of the objects based to their similarities to the visual words (Eq.~\ref{eq:cluster_prob} and Eq.~\ref{eq:label_prob}). The model is meta-trained by alternately learning the visual words given model parameters $\theta$, and learning model parameters given the visual words.}
	\label{fig:ourmodel}  
\end{figure*}%Here, $\mathcal{S}=\{(x_i, y_i)\}_{i=1}^{|\mathcal{S}|}$ and $\mathcal{Q}=\{(x_j, y_j)\}_{j=1}^{|\mathcal{Q}|}$ are the set of support and query examples, respectively. Moreover, 
In the context of meta-learning for video object segmentation, performing object segmentation on each video is a new task. With each task as to learn from the ground-truth object masks in the first frame of a video, to segment and track objects of interest in the rest of the video.\HB{Fix this} Fig.~\ref{fig:meta-learning} illustrates  our meta learning setup for video object segmentation based on this definition.
In this setup, the support set $\mathcal{S}$ is the set of all labeled pixels in the first frame, $ \mathcal{S} = \{ x_i, y_i \}_{i=1}^{N}$. Here $x_i$ represents the pixel $i$ in the first frame, $y_i \in \mathcal{C} = \{ 1,...,C \}$ is the ground truth class label of pixel $x_i$, $N$ is the number of labeled pixels in the frame, and $C$ is the number of object classes that need to be tracked and segmented in the video. Similarly the query set is defined by $ \mathcal{Q} = \{ x_j, y_j \}_{j=1}^{N\times F}$,  where $x_j$ denotes pixel $j$ from the video, $y_j$ represents the ground-truth class label for pixel $j$, and $F$ is the number of frames in the video (excluding the first frame). 
The output of each task $\mathcal{T}$ is the set of predicted class labels for the pixels in $ \mathcal{Q}$, i.e. $\mathcal{\hat{Y}} = \{\hat{y}_j\}_{j=1}^{N\times F}$. 
\fi

Next, we describe our model for estimating the outputs of each task, i.e. the object label for every pixel in the query frames of the video.

\subsection{Model}\label{sec:Model}
In order to predict the object class for each pixel in the query set $\mathcal{Q}$, we need to learn a representation for each object, using the information provided in the support set $\mathcal{S}$. In this paper, we propose to represent the objects in each video using a dictionary of deep visual words (Fig.~\ref{fig:ourmodel}). Each pixel in the query set is then classified into one of the object classes, based on the deep visual word it is assigned to.
This process is described in more detail in Sec.~\ref{sec:unsupervised} and Sec.~\ref{sec:supervised}. 

Learning visual words is however a challenging task, as they do not come with any ground-truth information of the object parts. Therefore, the assignment of pixels to the visual words and consequently, the pixel-to-object assignment becomes an ill-posed problem. To address this, we propose a meta training algorithm, where we alternate between the unsupervised learning of deep visual words and supervised learning problem of pixel classification. More specifically, our model learns to learn a better classifier by optimizing these visual words.

We initially pass the first frame of the video, which is the support set $\mathcal{S}$, through a deep segmentation network $f(\theta)$ to compute the encoding for each pixel $x_i$ in $\mathcal{S}$, i.e., $f_\theta(x_i)$. 

Next, we compute a set of deep visual words for all the pixels in each object class. In particular, let $\mathcal{S}_c$ be the set of pixels in $\mathcal{S}$ with class $c$. Each set $\mathcal{S}_c$ is partitioned into K clusters $\mathcal{S}_{c1}, ...., \mathcal{S}_{cK}$ using the k-means algorithm~\cite{kmeans}, with $\mu_{ck}$ being the respective centroids of the clusters, using the following objective:

\begin{subequations}\label{eq:unsupervised}
	\begin{equation}
	\mathcal{S}_{c1}, ...., \mathcal{S}_{cK} = \argmin_{\mathcal{S}_{c1}, ...., \mathcal{S}_{cK}} \sum_{k=1}^{K}\sum_{x_i \in \mathcal{S}_{ck}} \left\lVert f_\theta(x_i) - \mu_{ck} \right\rVert _2^2, \\
	\end{equation}
	\text{where}
	\begin{equation}\label{eq:mu}
	\mu_{ck} = \frac{1}{\lvert \mathcal{S}_{ck} \rvert}\sum_{x_i \in \mathcal{S}_{ck}} f_\theta(x_i).
	\end{equation}
\end{subequations}

In other words, we represent the distribution of the pixels within each set $\mathcal{S}_c$ in the deep embedding space with a group of deep visual words $\mathcal{M}_c = \{\mu_{c1},...,\mu_{cK}\}$. 
Once the deep visual words for each object are constructed, the probability of assigning a pixel $x_j \in \mathcal{Q}$ to the $k^{th}$ visual word from object class $c$ is computed using a non-parametric softmax classifier as follows:

p(c_k|x_j) =  \frac{\exp\big(d(\mu_{ck}, f_{\theta}(x_j))\big)}{\sum_{\mu_{i} \in \mathcal{M}}\exp\big(d(\mu_{i}, f_{\theta}(x_j))\big)},
\label{eq:cluster_prob}
\end{equation}% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
where $\mathcal{M} = \bigcup_{c=1}^{C} \mathcal{M}_c$ is the dictionary of deep visual words for all objects present in the video, and $d$ is the cosine similarity function.

We argue that, it is sufficient for a pixel to be very similar to at least one of the visual words within the object, in order for that pixel to be labelled as that object class.
Hence, the probability of pixel $x_j$ being a part of object class $c$ is defined as:
p(y_j=c|x_j) = \frac{\operatorname*{max}_{k \in \{1,..,K\}}p(c_k|x_j)}{\sum_{c'=1}^{C} \operatorname*{max}_{k \in \{1,..,K\}}p(c'_k|x_j)},
\label{eq:label_prob}
\end{equation}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

where the maximum operation selects the most similar visual word from each class $c$ to pixel $x_j$. The key point is that we allow the model to account for intra-class variations by assigning a pixel to only one relevant visual word in the class, rather than forcing it to resemble all visual words. This is important because, as we see in Fig.~\ref{fig:teaser} and Fig.~\ref{fig:onlineadaptation}, the model learns meaningful visual words that correspond to different object parts, which might be quite dissimilar, even though all are parts of the same object. 

Next, we define a loss function for this pixel-wise classification problem, i.e., video object segmentation. Let $\mathcal{T}_{n}$ be the task in our meta training process. The loss for predicting class label $\hat{y}_j$ for pixels $x_j$ in the query set is defined as:
\begin{multlined}
\mathcal{L}_{\mathcal{T}_{n}}=- \frac{1}{|\mathcal{Q}|} \sum_{j=1}^{|\mathcal{Q}|} \log \big[p(\hat{y}_j=y_j|x_j)\big] \\
- \frac{1}{|\mathcal{Q}|(C-1)} \sum_{j=1}^{|\mathcal{Q}|} \sum_{c=1, c \neq y_j}^{C} \log \big[1 - p(\hat{y}_j=c|x_j)\big],
\label{eq:loss_image}
\end{multlined}
\end{equation}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
where $|\mathcal{Q}|$ denotes the size of the query set (i.e. total number of pixels in the video $N\times F$) and $y_j$ is the ground-truth class label for pixel $x_j$, as defined in Sec.~\ref{sec:metaformulation}. 

The first term in Eq.~\ref{eq:loss_image} encourages the probability of the correct class to be high, whereas the second term tries to reduce the probability for all other classes. In other words, the proposed loss function attempts to pull each pixel closer towards the most similar visual word from the correct object class by maximizing the probability in Eq.~\ref{eq:label_prob} for the ground-truth class. At the same time, it aims to push the pixel away from the visual words of other classes. 

Each iteration of our meta training algorithm is composed of the unsupervised learning process, where deep visual words are learned over the support set $\mathcal{S}$, followed by the supervised learning step, in which the model parameters $\theta$ are updated by minimizing the loss function in Eq.~\ref{eq:loss_image}, according to Eq.~\ref{eq:loss}. In other words, the model learns to learn deep visual words from the first frame of the video, to minimize an object segmentation loss over the rest of the video.

The proposed approach to meta learning is very flexible, in contrast to previous work, where each class is represented either by all data points from that class in the support set (as in Matching networks~\cite{NIPS2016_6385}), or by only one single prototype for that class (as in Prototypical networks~\cite{NIPS2017_6996}).
Our model leverages the whole set of $\mathcal{S}$ to build a more robust and less noisy representation for all pixels in $\mathcal{S}$, using deep visual words.

Fig.~\ref{fig:ourmodel} depicts the structure of the proposed model. We have used a ResNet-101~\cite{resnet} architecture with dilated convolutions~\cite{deeplab} as our segmentation network $f(\theta)$ to compute $f_\theta(x_i)$, though any state-of-the-art segmentation network can be used here as well. Deep segmentation networks ensure that the structural dependencies are leveraged when computing the encoding, which is crucial because we are dealing with a structured prediction problem. Thus, $f_\theta(x_i) \equiv f_\theta(x_i, \mathcal{S})$.
The model architecture is described in more detail in Sec.~\ref{sec:exp}.

	\caption{\HB{THIS is important}Training of Multimodal Networks}
	\begin{algorithmic}[1]
		\renewcommand{\algorithmicrequire}{\textbf{Input:}}
		\renewcommand{\algorithmicensure}{\textbf{Output:}}
		\REQUIRE $p(\mathcal{T})$ distribution over the Training tasks
		\ENSURE  Updated parameters $\theta$
		\\ \textit{Initialisation} : Pretrained weights $\theta_0$
		\STATE Sample a batch of Tasks $\mathcal{T}_n \sim p(\mathcal{T})$
		\\ \textit{For each Task:}
		\FOR{$i = l-2$ to $0$}
		\STATE find the modes from the support set/first frame
		\STATE Initialize Loss = 0
		\STATE Sample some query frames
		\\ \textit{For each Query frame:}
			\FOR{$i = l-2$ to $0$}
			\STATE Compute the loss $\mathcal{L}_{\mathcal{T}_{nt}}$
			\STATE **Backpropagate the loss to accumulate the gradients / **Loss = Loss + this loss
			\ENDFOR
		\ENDFOR
		\STATE Update the parameters
		\RETURN $\theta$ 
	\end{algorithmic} \label{algo:training}
\end{algorithm}\fi\subsection{Online Adaptation}\label{sec:onlineadaptation}
An important attribute for video object segmentation algorithms is the ability to update online as the video progresses. This is vital because the objects of interest, as well as the background scene, might undergo significant deformation and changes in shape and appearance. 
\iffalse
Previous works have addressed the problem of online adaptation by either updating the model parameters \cite{OSVOS, onavos}, or by updating the pool of reference data points / support set \cite{Chen_2018_CVPR, Hu_2018_ECCV}. The former approach is prohibitive as it involves at least a few iterations of forward and backward pass in the network for every round of online update, which dramatically slows down the video object segmentation task. The latter, which is mostly used in Siamese-based methods \cite{Chen_2018_CVPR, Hu_2018_ECCV}, however keeps model parameters intact and updates the reference training samples whose class labels are to be transferred to the unseen examples. 
\fi

In this work we perform online adaptation by updating the set of visual words that represent objects. In particular, given a dictionary of deep visual words $\mathcal{M}$, captured up to the frame $t_j$, we predict the segmentation map in frame $t_{j+\delta}$, and treat it as a new support set $ \mathcal{S^\delta} = \{ x_i^\delta, y_i^\delta \}_{i=1}^{N}$, where $y_i^\delta$ is the predicted object class for pixel $x_i^\delta$.
Next, we compute an updated set of deep visual words $\mathcal{M^\delta}$ from the new support set using k-means algorithm as described in Sec.~\ref{sec:unsupervised}, and compute their corresponding cluster centroid representations by
\begin{equation}
\mu_{ck}^\delta = \frac{1}{\lvert \mathcal{S}_{ck}^\delta \rvert}\sum_{x_i \in \mathcal{S}_{ck}^\delta} f_\theta(x_i).
\label{online_mu}
\end{equation}

At this point, depending on how much the scene and objects have distorted in shape and appearance between frames $t_j$ and $t_{j+\delta}$, these updated visual word representations could be quite similar or different to the previous ones. We update the main visual word set $\mathcal{M}$ with the new set $\mathcal{M}^\delta$, if there are $m^\delta \in \mathcal{M}_c^\delta$ and $m \in \mathcal{M}_c$, for which, $d(\mu_{m}^\delta, \mu_m) < 0.5$. In other words, if the objects undergo some deformation within the time interval $\delta$, assuming $\delta$ is chosen moderately, we still expect the new visual words to resemble the main group of visual words $\mathcal{M}$ to some degree. However, if $d(\mu_{m}^\delta, \mu_m) > 0.5$, i.e. if there is a significant difference between the representation of the new visual words and the ones in $\mathcal{M}$, it could be an indication of potential incorrect segmentation, which has led to assignment of irrelevant visual words $\mathcal{M}_c^\delta$ in the class $c$. As a result, they are discarded and will not be added to $\mathcal{M}$.

Note that during online adaptation, none of the existing visual words within $\mathcal{M}$ are discarded, because each object may revert to its original shape and appearance during a video sequence. This is where the max in Eq.~\ref{eq:label_prob} shows its merit, since no matter how many new visual words are added to the dictionary of class conditional deep visual words, the algorithm still picks up the most relevant one.

It is critical for online adaptation to use reliable and confident predictions of objects masks, in order to learn new visual words from them.
We address this problem by applying a simple outlier removal process to the prediction outputs. More specifically, we refine the predictions of each object at every frame by, first finding its isolated predicted regions, and discarding the ones that have no intersection with predicted mask of the object in the previous frame. This process simply encodes the spatio-temporal consistency of the object masks over time. The effect of outlier removal on the performance of the system is investigated in the ablation study, Sec.~\ref{sec:ablation}. %\HB{Still Unclear}%We tried online k-means, but it did not work as well.%The online update can be seen as updating the memory of the network with time, as more is being added to the memory.%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}\label{sec:exp}
We evaluate our method on DAVIS-2017~\cite{davis_2017}, which is one of the primary benchmarks for assessing video object segmentation techniques. We perform a thorough analysis and ablation study on this dataset, and compare our method with state-of-the-arts, in terms of accuracy, as well as speed, which is a very important criterion for practical real-world applications. Furthermore, we report the results of evaluating our method on DAVIS-2016~\cite{DAVIS} dataset, and compare it with state-of-the-art techniques. 
Our model architecture uses a Deeplab-ResNet-101~\cite{deeplab} segmentation network as the {\it encoder}. This encoder maps an input frame of size $[H,W]$ to an embedding of size $[H/8, W/8, 2048]$. This embedding is then upsampled to a $[H/2, W/2, 128]$ feature volume, where the number of channels in the feature space is dropped from $2048$ down to $128$ in order to keep the model computationally feasible. This is done using a {\it decoder} network, which is comprised of a bilinear upsampling layer in conjunction with a transposed convolution layer~\cite{dumoulin2016guide}.

	\centering
	\includegraphics[width=\textwidth]{segmentation.pdf}

	\vspace{-0.1in}
	\caption{ Qualitative segmentation outputs for some very challenging videos from DAVIS-2017 (each row), obtained using our model \textbf{without any fine-tuning}.} \label{fig:qualitative_segmentation}
\end{figure*}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\subsection{Model Pre-training}\label{sec:pretraining}
We first initialize our encoder network using the pre-trained Microsoft COCO weights \cite{coco}, as done in other works \cite{OSVOS-S, onavos, DAVIS2018-Interactive-2nd}. Next, we pre-train our encoder-decoder network $f(\theta)$ using static images, following the training strategy in \cite{DAVIS2018-Interactive-2nd}. 
More specifically, we feed the network with images from Pascal instance segmentation dataset~\cite{pascal} as inputs, and use a binary cross-entropy loss function for training a Siamese network.
We then used the parameters of the pre-trained model as initialization for meta learning experiments. 

\subsection{Meta Learning}\label{sec:metalearning}
In order to meta train the model, we follow the {\it episodic} training procedure, which is the standard practice in meta learning-based approaches~\cite{NIPS2016_6385, NIPS2017_6996, MAML}. Each training episode is formed by sampling a support set $\mathcal{S}$ and a relevant query set $\mathcal{Q}$. The idea of episodic training is to, at each training iteration, mimic the inference procedure, where given the information provided by the support set, the query set should be classified. In this work, we build each episode by first randomly sampling a video from the training pool; treat the pixels of the first frame of the video as $\mathcal{S}$; and randomly selecting a set of query frames from the rest of the video and treat their pixels as $\mathcal{Q}$.

Following the proposed method in Sec.~\ref{sec:method}, we compute the loss according to Eq.~\ref{eq:loss_image} for the query set of each episode, and train the model.

We report our results based on two standard metrics: {\it Jaccard} index ($\mathcal{J}$) and boundary F-score ($\mathcal{F}$).

We meta trained the proposed model, which was already pre-trained as described in Sec.~\ref{sec:pretraining}, on $60$ video sequences taken from DAVIS-2017 training set. 

During training, we learned $10$ visual words from the support set of each episode, to represent each object (inc. background). However, during inference each object is represented by $50$ visual words. Note that due to the complexity of the background scene compared to the foreground objects, four times more visual words are extracted from background. 
    \centering
    \small
    \resizebox{\columnwidth}{!}{
	\begin{tabular}{cccccccc} 
	    \hline
		Method & FT & PP & OF & ${\mathcal{J}}${\footnotesize (\%)} & ${\mathcal{F}}${\footnotesize (\%)} &  ${\mathcal{J}}\&\mathcal{F}${\footnotesize (\%)} & Time(s)\\ \hline
		MaskRNN~\cite{hu2017maskrnn}  &\cmark & &\cmark & 60.5 & -- & -- & 9s \\
		OSMN~\cite{Yang_2018_CVPR} &\cmark &\cmark & & 60.8 & -- & -- & --\\
        OnAVOS~\cite{onavos} &\cmark &\cmark & & 61.6 & 69.1 & 65.3 & 13s \\
        $\textrm{OnAVOS}^{\dagger}$~\cite{onavos} &\cmark &\cmark & & 64.5 & 71.2 & 67.8 & 30s \\
		VideoMatch~\cite{Hu_2018_ECCV}  &\cmark & & & 61.4 & -- & -- & 2.62s \\
		ReID~\cite{Li_2018_ECCV}  &\cmark & &\cmark & 67.3 & 71.0 & 69.1 & 2.33s \\
		$\textrm{OSVOS}^S$~\cite{OSVOS}  &\cmark &\cmark & & { 64.7} & { 71.3} & { 68.0} & -- \\
		$\textrm{CINM}^{\dagger}$~\cite{Bao_2018_CVPR}  &\cmark &\cmark &\cmark & { 67.2} & { 74.4} & { 70.7} & $\sim$ 108s \\
		$\textrm{PReMVOS}^{\dagger}$~\cite{DAVIS2018-Semi-Supervised-1st}  &\cmark & &\cmark & {\bf 74.3} & {\bf 82.2} & {\bf 78.2} & $\sim$ 70s \\
		\hline
        OnAVOS~\cite{onavos} & & & & 39.5 & -- & -- & 3.78s \\
		MaskRNN~\cite{hu2017maskrnn} & & &\cmark & 45.5 & -- & -- & 0.6s \\
		VideoMatch~\cite{Hu_2018_ECCV} & & & & 56.5 & -- & -- & 0.35s \\
		RGMP~\cite{Oh_2018_CVPR} & & & & {\bf 64.8} & 68.6 & 66.7 & -- \\
	    $\textrm{Ours}^-$ & & & &{55.8}&{63.1} &{ 59.5}  & {0.17s} \\
	    Ours & & & &{63.9}&{\bf 70.7} &{\bf 67.3}  &{ 0.29s} \\
	    \hline
	\end{tabular}
	}
	\caption{{\bf Results on DAVIS-2017 validation Dataset.} FT: Fine-Tuning on the first frame of the test video; PP: Post-Processing; OF: Optical Flow; ${\mathcal{J}}\&\mathcal{F}$: The mean of $\mathcal{J}$ and $\mathcal{F}$ metrics; Time(s): The time (in seconds) spent on each frame on average; $\textrm{Ours}^-$: Our model without online adaptation; $\dagger$: Ensemble of models are used. }
	\label{table:davis-2017-others}
\end{table}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We then evaluated the model on DAVIS-2017 validation set, which consists of 30 videos. This is a very challenging dataset, as some videos contain up to five dynamic objects to be tracked and segmented. 
Table~\ref{table:davis-2017-others} shows the performance of our method in comparison with the state-of-the-art methods. 
It is evident from this table that our method works on-par with the best performing methods, while performing at an encouraging speed of $0.29$ second per frame which is 1-2 orders of magnitude faster. Also, our method gives the best accuracy ($\mathcal{J}$\&$\mathcal{F}=0.673$) among the methods without fine-tuning on the first video frame.
Note that our method does not utilize any fine-tuning over the first video frame, and can be applied to any test video straight-away without any over-head. 
In addition, we do not leverage Optical Flow information~\cite{Dosovitskiy_2015_ICCV} or any post-processing module, and this keeps the proposed method fast for practical applications. 

Fig.~\ref{fig:qualitative_segmentation} shows qualitative results computed using the proposed method for some videos from DAVIS-2017 validation set. 
Furthermore, Fig.~\ref{fig:chart} depicts an accuracy-vs-run-time diagram, which reveals that the proposed method offers a very good compensation between accuracy and speed, compared to other techniques. 

    \centering
    \small
    \resizebox{\columnwidth}{!}{
	\begin{tabular}{cccccccc} 
	    \hline
		Method & FT & PP & OF & ${\mathcal{J}}${\footnotesize (\%)} & ${\mathcal{F}}${\footnotesize (\%)} &  ${\mathcal{J}}\&\mathcal{F}${\footnotesize (\%)} & Time(s)\\ \hline
		MSK~\cite{masktrack} &\cmark &\cmark &\cmark & 79.7 & 75.4 & 77.5 & 12s\\
		MaskRNN~\cite{hu2017maskrnn}  &\cmark & &\cmark & 80.7 & 80.9 & 80.8 & 0.6s \\
        OnAVOS~\cite{onavos} &\cmark &\cmark & & 86.1 & 84.9 & 85.5 & 13s \\
		OSVOS~\cite{OSVOS} &\cmark &\cmark & & 79.8 & 80.6 & 80.2 & 9s \\
		DRL~\cite{Han_2018_CVPR} &\cmark & & & 84.1 & 84.6 & 84.3 & -- \\
		Lucid~\cite{lucid} &\cmark &\cmark &\cmark & 84.8 & -- & -- & $\sim$ 190s \\
		$\textrm{OSVOS}^S$~\cite{OSVOS} &\cmark &\cmark & & {\bf 85.6} & { 87.5} & { 86.5} & 4.5s \\
		$\textrm{PReMVOS}^{\dagger}$~\cite{DAVIS2018-Semi-Supervised-1st}  &\cmark & &\cmark & { 85.5} & {\bf 88.6} & {\bf 87.0} & $\sim$ 70s \\
		\hline
		OTP~\cite{Cheng_2018_CVPR} & &\cmark & & {\bf 82.4} & 79.5 & 80.9 & 1.8s \\ 
		CTN~\cite{Jang_2017_CVPR} & & &\cmark & 73.5 & 69.3 & 71.4 & 1.33s \\ 
		VPN~\cite{Jampani_2017_CVPR} & & & & 70.2 & 65.5 & 67.8 & 0.63s \\ 
		OTP~\cite{Cheng_2018_CVPR} & & & & 77.9 & 76.0 & 76.9 & 0.60s \\ 
		BVS~\cite{bvs} & & & & 60.0 & 58.8 & 59.4 & 0.37s \\ 
		PML~\cite{Chen_2018_CVPR} & & & & 79.3 & 75.5 & 77.4 & 0.27s\\
		OSMN~\cite{Yang_2018_CVPR} & & & & 74.0 & -- & -- & 0.14s\\
		RGMP~\cite{Oh_2018_CVPR} & & & & {81.5} & 82.0 & 81.7 & 0.13s \\
	    Ours & & & & 81.4 & {\bf 82.6} & {\bf 82.0} & 0.25s \\
	    \hline
	\end{tabular}
	}
	\caption{{\bf Results on DAVIS-2016 validation Dataset.} FT: Fine-Tuning on the first frame of the test video; PP: Post-Processing; OF: Optical Flow; ${\mathcal{J}}\&\mathcal{F}$: The mean of $\mathcal{J}$ and $\mathcal{F}$ metrics; Time(s): The time (in seconds) spent on each frame on average; $\dagger$: Ensemble of models are used. }
	\label{table:davis-2016-others}
\end{table}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\vspace{-0.2in}\paragraph{DAVIS-2016}\mbox{}\\
This dataset is a subset of DAVIS-2017, and includes $30$ training videos and $20$ validation videos. Moreover, the segmentation task is simplified to single-object tracking and segmentation. For this experiment, the pre-trained model was meta trained on DAVIS-2016 training set, where the episode configuration and optimization parameters were chosen similar to the DAVIS-2017 experiment. As shown in Table~\ref{table:davis-2016-others}, our method achieves state-of-the-art performance among the fast approaches (i.e. approaches that do not fine-tune their model parameters on the first frame of the video), and is at par with the best-performing slow methods.

In this section, we study the influence of different components and modules of our algorithm on the performance of the system.% All the ablation studies are carried out on DAVIS-2017 validation set.%\vspace{-0.1in}\paragraph{Effect of Pre-Training and Meta Learning}
Table~\ref{tab:ablation_training} reveals the contribution of each training stage in the final performance. Note that the initial model that only uses MS-COCO initializations for the encoder and random weights for the decoder, already performs on par with some recent techniques in Table~\ref{table:davis-2017-others} that are well trained for video object segmentation tasks. This indicates the potential of our algorithm for visual word extraction and matching, and its ability to work well without much training effort. Nevertheless, it can still be further improved by training it on relevant data, as shown in Table~\ref{tab:ablation_training}.
\vspace{-0.2in}\paragraph{Effect of the Size of Visual Word Dictionary}
Here we show how the performance of our method varies with the size of the visual word dictionary. The learned visual words for each object are in fact an approximate representation for the distribution of the pixels within that object. More complex objects with high intra-object variations require more visual words for a good representation, and one single prototype (as in \cite{NIPS2017_6996}) may not be able to describe the entire pixel distribution of each object. Table~\ref{tab:abltation_dictionary_size} indicates the effect of number of visual words for each object on the performance of the object segmentation system. Evidently based on this table, our method outperforms prototypical networks~\cite{NIPS2017_6996} for video object segmentation task. 
	\centering
	\includegraphics[width=\textwidth/2]{online.pdf}
	\vspace{-0.2in}
	\caption{{\bf The effect of online adaptation on the representation of dynamic objects.} As the object pose changes over time, newer visual words (denoted by gray and light-blue colors) are learned and added to the dictionary of visual words. }
	\label{fig:onlineadaptation}   
\end{figure}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\begin{table}[t]
    \centering
    \small
    \begin{tabular}{cc}
    \hline
    Model Initialization                  & $\mathcal{J}${\footnotesize (\%)} \\ \hline
    MS-COCO                & 50.7\\
    + Pascal VOC    & 53.4\\
    + Meta Training on DAVIS 2017      & {\bf 63.9}\\
    \hline
    \end{tabular}
    \caption{{\bf The effect of different model initializations; DAVIS-2017 validation dataset.} Note that in the first row, only the encoder of our segmentation network is initialized with COCO weights and the decoder is randomly initialized. Despite that, the model still performs on par with some recent techniques, thanks to the representation power of visual words.}
    \label{tab:ablation_training}
\end{table}\begin{table}[t]
    \centering
    \small
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{cccccc}
    \hline
    Dictionary Size ($K$) & 1 & 5 & 10 & 20 & 50  \\ \hline
    $\mathcal{J}${\footnotesize (\%)} & 49.9 & 54.5 & 54.8 & 54.9 & 55.8\\
    \hline
    \end{tabular}
    }
    \caption{{\bf The effect of the size of visual word dictionary on model performance (without online adaptation); DAVIS-2017 validation dataset.} This table indicates the significance of visual words in the performance of the video object segmentation task. $K$ denotes the number of visual words that are extracted from each object class. Note that except for the first case ($K=1$), in all other cases $4\times K$ visual words are extracted from the background class, due to its complexity.}
    \label{tab:abltation_dictionary_size}
    \vspace{-0.1in}
\end{table}\begin{table}[t]
    \centering
    \small
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{cccccccc}
    \hline
    Interval ($\delta$) & NA & 30 & 20 & 10 & 5 & 2 & 1  \\ \hline
    $\mathcal{J}${\footnotesize (\%)} & 55.8 & 58.6 & 59.2 & 61.3 & 63.9 & 63.7 & 62.8\\
    \hline
    \end{tabular}
    }
    \caption{{\bf The effect of online adaptation on model performance; DAVIS-2017 validation dataset.} In this table, $\delta$ denotes the frame interval before every step of online adaptation. Choosing a moderate value for the online adaptation frequency is important. Very small values of $\delta$ might quickly enlarge the size of the visual word dictionary and potentially fill it with a lot of noisy visual words, obtained from the incorrect predicted masks. NA: No online adaptation.}
    \label{tab:onlineadaptation}
\end{table}\begin{table}[t]
    \centering
    \small
	\begin{tabular}{ccc} 
	    \hline
		Outlier Removal & Online Adaptation & ${\mathcal{J}}${\footnotesize (\%)} \\ \hline
		\xmark &\xmark & 55.8  \\
		\cmark &\xmark & 56.8  \\ 
		\xmark &\cmark & 60.4  \\ 
		\cmark &\cmark & 63.9  \\ 
	    \hline
	\end{tabular}
	\vspace{-0.1in}
	\caption{{\bf The effect of outlier removal on online adaptation; DAVIS-2017 validation dataset.} This table shows how refining the predicted object masks using the outlier removal process improves the online adaptation performance. }
	\label{table:outlierremoval}
	\vspace{-0.2in}
\end{table}\paragraph{Effect of Online Adaptation}
Online adaptation, as described in Sec.~\ref{sec:onlineadaptation}, attempts to update the dictionary of visual words that are used for matching and label transfer. Table~\ref{tab:onlineadaptation} illustrates how updating the dictionary $\mathcal{M}$ improves the performance of the system. By updating $\mathcal{M}$ more frequently (i.e. for smaller values of update interval $\delta$), we let the system smoothly adapt to dynamic scenes and fast moving objects. However, very small values of $\delta$ might quickly enlarge the size of the visual word dictionary and potentially fill it with a lot of noisy visual words, obtained from the incorrect predicted masks. 

Note that during online adaptation, model parameters are fixed and no forward/backward pass is performed in the network. Hence, our online adaptation scheme is still suitable for fast video object segmentation systems, unlike e.g. \cite{onavos}, where online adaptation incurs a computation time of $13$s per frame.
Fig.~\ref{fig:onlineadaptation} illustrates how online adaptation updates the dictionary of visual words by incrementally learning new object parts automatically, and adding them to the set of visual words $\mathcal{M}$.
Table~\ref{table:outlierremoval} indicates the effect the outlier removal step in the performance of online adaptation algorithm.

In this section, we initially introduce the evaluation metrics that were used in this paper, next we describe the network structure in detail, then elaborate on the training and label transfer stages, and lastly we present the experiments and obtained results.

\paragraph{Evaluation Metrics}

Following the standard benchmarks for video object segmentation, we report our
results based on two metrics: {\it Jaccard} index ($\mathcal{J}$) and boundary
F-score ($\mathcal{F}$). The former ($\mathcal{J}$) is defined as the
intersection-over-union (IoU) of the predicted object mask and its respective
ground-truth mask. The latter ($\mathcal{F}$) indicates how accurate the
boundaries of the predicted mask are with respect to the ground-truth
boundaries~\cite{davis_2017}. 
One can obtain per-object evaluation results by computing the average of these
two metrics over the video that contains the object. The final evaluation result
for a dataset is defined as the average of per-object performances.
\fi\paragraph{Network Structure}\paragraph{Training and Label Transfer}\iffalse
As shown in Figure~\ref{fig:siamese}, our network is built on a pair of deep
fully convolutional neural networks whose parameters are shared.
Each branch of the network is a Deeplab-Resnet-101 model (pre-trained on
ImageNet~\cite{imagenet} and Microsoft-coco image dataset~\cite{coco}, as in~\cite{onavos, Lix}), connected
to a decoder in order to recover spatial information of the embeddings and learn
similarity at a higher resolution.

More specifically, the Deeplab-Resnet-101 model, produces feature maps whose
resolutions are eight times smaller than the resolution of input images. As a
result, many details are diminished and the boundary information are not
preserved properly. To address this problem, the decoder network upsamples the
feature maps and yields high-resolution embeddings. The decoder consists of a
transposed convolution layer (with $\text{stride}=2$) and a non-parametric
upsampling layer (again with $\text{stride}=2$). We tried other decoder configurations (\eg,
upsampling with $\text{stride}=4$, or two transposed convolution layers), but none of
them led to a better performance. Upsampling the feature maps to the size of
input data was not feasible as the embeddings could not fit into our single GPU
memory.

	
	\includegraphics[width=0.19\textwidth]{500FT_100GT/cows/paper/00001.png}
	\hspace*{\fill} % separation between the subfigures
	\includegraphics[width=0.19\textwidth]{500FT_100GT/cows/paper/00024.png}
	\hspace*{\fill} % separation between the subfigures
	\includegraphics[width=0.19\textwidth]{500FT_100GT/cows/paper/00051.png}
	\hspace*{\fill} % separation between the subfigures
	\includegraphics[width=0.19\textwidth]{500FT_100GT/cows/paper/00084.png}
	\hspace*{\fill} % separation between the subfigures
	\includegraphics[width=0.19\textwidth]{500FT_100GT/cows/paper/00103.png}
	
	\includegraphics[width=0.19\textwidth]{500FT_100GT/soapbox/paper/00012.png}
	\hspace*{\fill} % separation between the subfigures
	\includegraphics[width=0.19\textwidth]{500FT_100GT/soapbox/paper/00047.png}
	\hspace*{\fill} % separation between the subfigures
	\includegraphics[width=0.19\textwidth]{500FT_100GT/soapbox/paper/00058.png}
	\hspace*{\fill} % separation between the subfigures
	\includegraphics[width=0.19\textwidth]{500FT_100GT/soapbox/paper/00079.png}
	\hspace*{\fill} % separation between the subfigures
	\includegraphics[width=0.19\textwidth]{500FT_100GT/soapbox/paper/00092.png}
	
	\includegraphics[width=0.19\textwidth]{500FT_100GT/bmx-trees/paper/00014.png}
	\hspace*{\fill} % separation between the subfigures
	\includegraphics[width=0.19\textwidth]{500FT_100GT/bmx-trees/paper/00032.png}
	\hspace*{\fill} % separation between the subfigures
	\includegraphics[width=0.19\textwidth]{500FT_100GT/bmx-trees/paper/00045.png}
	\hspace*{\fill} % separation between the subfigures
	\includegraphics[width=0.19\textwidth]{500FT_100GT/bmx-trees/paper/00056.png}
	\hspace*{\fill} % separation between the subfigures
	\includegraphics[width=0.19\textwidth]{500FT_100GT/bmx-trees/paper/00074.png}
	
	\vspace{-5pt}
	\caption{ Qualitative segmetation outputs for some sample videos from DAVIS-2016 (each row), obtained using our model after fine-tuning. } \label{fig:500ft_davis}
\end{figure*}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\fi\iffalse
At each iteration, a video was randomly
selected from the training pool, and then a pair of frames were drawn randomly
from the video to feed the network. The target of the network was constructed by
checking whether or not any two pixels in the image pair refer to the the same
object. As explained in Sec.~\ref{sec:pooling}, only a small fraction of the
labelled pixels are used at each iteration ($k=100$ pixels in our
experiments). In the results, we show that selecting more pixels does not
affect the performance, while it slows down the training process.
The network is trained for $100000$ iterations, with the base learning rate of
$2.5\times 10^{-4}$ for the branches and a learning rate factor of two for the
decoder ($\text{lr}_{\text{decoder}}=5 \times 10^{-4}$, with a momentum of $0.9$ and weight decay of $0.0005$. In addition, learning rates are decreased polynomially, with a power of $0.9$.

Note that during training, we have access to the dense ground-truth maps of both
input images to our network. However, at the test/evaluation time, the second
branch of the network is fed by a query image, and we utilise the learned
similarity model to transfer the ground-truth labels of the training input image
in the top branch (first frame of the video) to the query image (any subsequent
frame).

Label transfer, as explained in Sec.~\ref{sec:label_transfer}, is merely a label
matching operation, which does not leverage any prior information over the image
pixels. Hence we can further improve the output of this step by passing it
through a dense CRF~\cite{densecrf}, where Flow information together with RGB
cues, are utilised as pairwise consistency constraints.
\vspace{-5pt}\fi\subsection{Ablation Studies}\begin{table}[t]
    \centering
    \small
    \begin{tabular}{c|c|c|c}
    \hline
    Train clusters & Test clusters & train mIou & val mIou \\ \hline
    \hline
    1  & 1  & -- & -- \\
    1  & 50 & -- & -- \\
    50 & 1  & -- & -- \\
    50 & 50 & -- & -- \\
    \hline
    \end{tabular}
    \caption{This table shows how the results are affected by changing the number of clusters at train and test time. Case 1 is when only one mean is used for each object both at test and train time. Case2 shows that taking more than one cluster in each object at test time improves results, which shows the point of keeping multiple representations. Case 3 shows how the results improve by increasing the number of clusters during the training time. This case shows that we are able to use unsupervised techniques to improve results.}
    \label{tab:unsup_training}
\end{table}\paragraph{Influence of the number of modes on testing and training}\paragraph{Influence of components during the training}\begin{table}[t]
    \centering
    \small
    \begin{tabular}{|c|c|c|c|}
    \hline
    Model                   & train mIou    & val mIou \\ \hline
    \hline
    Baseline                & --            & --  \\
    Using Meta learning     & --            & --  \\
    Unsupervised Knowledge  & --            & --  \\
    Online update training  & --            & --  \\
    \hline
    \end{tabular}
    \caption{This table shows effect of different training components}
    \label{tab:unsup_training}
\end{table}\paragraph{Influence of Components during test time}\begin{table}[t]
    \centering
    \small
    \begin{tabular}{|c|c|c|c|}
    \hline
    Model                   & train mIou    & val mIou & Speed\\ \hline
    \hline
    Baseline                & --            & --        & --\\
    with outlier removal    & --            & --        & --\\
    with online update      & 50            & --        & --\\
    with CRF and Flow       & --            & --        & --\\
    \hline
    \end{tabular}
    \caption{This table shows how the 2 training mechanisms we propose affect the results.}
    \label{tab:unsup_training}
\end{table}\subsection{Comparison to State-of-art}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\begin{table}[t]
    \centering
    \small
	\begin{tabular}{|ccccc|ccc|}
	    \hline
		Method & FT & DAVIS16 & YoutubeObjs & Time(s)\\ \hline
		\hline
		MSK~\cite{masktrack}&\xmark & 69.9      & --        & --\\
		VPN~\cite{vpn}      &\xmark & 65.5      &70.2       &67.8 \\
		Ours                &\xmark &{\bf 75.4} &{\bf 76.5} &{\bf 76.0} \\
		MSK~\cite{masktrack}&\xmark & 77.5      &79.7       &75.4 \\
		OSVOS~\cite{OSVOS}  &\xmark & 80.6      &79.8       &80.2 \\
        OnAVOS~\cite{onavos}&\xmark & 81.1      &81.7       &81.4 \\
		Lucid~\cite{lucid}  &\xmark & 83.5      &{\bf 84.8} &82.3 \\
		\hline
		\hline
	    Ours                &\xmark &{\bf 83.6} &84.1       &{\bf 83.1} \\
	    \hline
	\end{tabular}
	\caption{{\bf Results on DAVIS-2016 validation and YoutubeObjs Dataset.} The reported numbers are in (\%) units.}
	\label{table:davis-2017}
\end{table}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\begin{table*}[t]
    \centering
    \small
	\begin{tabular}{|ccccc|cc|c|} 
	    \hline
		Method & FT & OF & CRF & On & $\mathcal{J}$ mean & $\mathcal{F}$ mean & Time(s)\\ \hline
		\hline
		MSK~\cite{masktrack}&\xmark &\xmark &\xmark & -- & 69.9 & -- & --\\
		VPN~\cite{vpn}&\xmark &\xmark &\xmark & -- &65.5&70.2&67.8 \\
		Ours&\xmark &\xmark &\xmark & --     &{\bf 75.4}&{\bf 76.5}&{\bf 76.0} \\ \hline
		\hline
		MSK~\cite{masktrack}&\xmark &\xmark &\xmark & -- &77.5&79.7&75.4 \\
		OSVOS~\cite{OSVOS}  &\xmark &\xmark &\xmark & -- &80.6&79.8&80.2 \\
        OnAVOS~\cite{onavos}&\xmark &\xmark &\xmark & -- &81.1&81.7&81.4 \\
		Lucid~\cite{lucid}  &\xmark &\xmark &\xmark & -- &83.5&{\bf 84.8}&82.3 \\
	    Ours                &\xmark &\xmark &\xmark & -- &{\bf 83.6}&84.1&{\bf 83.1} \\
	    \hline
	\end{tabular}
	\caption{{\bf Results on DAVIS-2017 validation Dataset.} The reported numbers are in (\%) units. {D.A.}:Domain Adaptation. Note that in case of DAVIS-2017 dataset, we did not have access to the results of other methods without fine-tuning, so all the results are in this table are with domain adaptation.}
	\label{table:davis-2017}
\end{table*}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\subsection{Analysis of the modes learned by the model}\begin{itemize}
    \item Our method is learning to learn how to generate parts so that it performs well on the corresponding query set.
    \item we can see that without ever giving it any part labelling it forms very sensible parts.
    \item it is beyond parts because the best performance is attained at 50 clusters. So it tries to capture the texture correctly and not just limited to parts.
    \item on one end it becomes pixelwise mathcing/matching networks and on the other end it becomes prototypical networks.
    \item Future work involves being able to come up with the number of parts for each task and object on the go.
\end{itemize}\iffalse\fi\iffalse%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\begin{figure*}[h]
	
	\includegraphics[width=0.19\textwidth]{0FT_100GT/drift-straight/paper/00001.png}
	\hspace*{\fill} % separation between the subfigures
	\includegraphics[width=0.19\textwidth]{0FT_100GT/motocross-jump/paper/00024.png}
	\hspace*{\fill} % separation between the subfigures
	\includegraphics[width=0.19\textwidth]{0FT_100GT/breakdance/paper/00001.png}
	\hspace*{\fill} % separation between the subfigures
	\includegraphics[width=0.19\textwidth]{0FT_100GT/soapbox/paper/00069.png}
	\hspace*{\fill} % separation between the subfigures
	\includegraphics[width=0.19\textwidth]{0FT_100GT/dance-twirl/paper/00073.png}
	
	\includegraphics[width=0.19\textwidth]{500FT_100GT/drift-straight/paper/00001.png}
	\hspace*{\fill} % separation between the subfigures
	\includegraphics[width=0.19\textwidth]{500FT_100GT/motocross-jump/paper/00024.png}
	\hspace*{\fill} % separation between the subfigures
	\includegraphics[width=0.19\textwidth]{500FT_100GT/breakdance/paper/00001.png}
	\hspace*{\fill} % separation between the subfigures
	\includegraphics[width=0.19\textwidth]{500FT_100GT/soapbox/paper/00069.png}
	\hspace*{\fill} % separation between the subfigures
	\includegraphics[width=0.19\textwidth]{500FT_100GT/dance-twirl/paper/00073.png}
	
	\vspace{-5pt}
	\caption{ Some examples where domain adaptation (second row) has improved the results over the pre-trained model (first row). } \label{fig:0ft_500ft}
\end{figure*}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\fi\iffalse
Our method exhibits its strength among other techniques particularly when domain
adaptation is not an option due to, \eg, time constraints. Nevertheless, we
carried out an experiment to show the potential of our method when equipped with
test domain adaptation.
To this end, we fine-tuned our model on the first frame of each test video, as
described in Sec.~\ref{sec:domain_adaptation} for $500$ iterations. As shown in
Table~\ref{table:davis-2016}, we achieved $\mathcal{J}=84.1\%$ and $\mathcal{J}=83.1\%$,
which are on par with the state-of-the-art. Note that \cite{lucid} uses a very sophisticated and complex data augmentation technique and also ensembles multiple models to get the best possible results. This has led to a very long fine-tuning procedure ($3.5h$ per video). On the contrary, our model reaches the state-of-the-art by fine-tuning on each video for less than eight minutes. In addition, once fine-tuning is over, our model can process each frame at a rate 1 frame per second, whereas in \cite{lucid}, each frame is processed in 5s. Figure~\ref{fig:500ft_davis} depicts the result of evaluating our model after domain adaptation on some videos in the DAVIS-2016 dataset.



As reported in Table~\ref{table:davis-2016}, we evaluated our method on the Test-dev split of DAVIS-2017 dataset and outperformed {\it OnAVOS}\cite{onavos} by a good margin. Note that {\it Lucid}\cite{lucid} used an ensemble of four different models based on image and flow information and also leveraged an off-the-shelf state-of-the-art semantic segmentation module (PSPNet~\cite{pspnet}) to achieve this result. In fact, our model (with a global mean of $64\%$) outperforms each of the individual models used in the ensemble ($63.5\%$, $61.1\%$, $61.3\%$, $59.9\%$)~\cite{lucid}.



An advantage of fine-tuning is for tackling the problem of instance
segmentation, where the pre-trained model might not be able to distinguish
between multiple instances of the same semantic class in a new video.
Figure~\ref{fig:0ft_500ft} demonstrates a few cases where our pre-trained model has failed
to segment the object of interest properly from background due to e.g. the presence
of other instances of that object class in the image. However, through
fine-tuning on the first frame, the model has learned to better differentiate
between the main object and its surrounding.
\fi\fi%Furthermore, we applied online fine-tuning to address the problems that are made due to the sudden changes in the shapes and appearances of the scenes and objects along the video. To this aim, we should select a set of training pixels within the current frame from each class (foreground/background). In order to choose training pixel candidates for the foreground class, we first masked the current prediction at time $t$ with the surrounding bounding box of the prediction at time $t-1$ to discard the potential false positives that might appear due to new incoming objects in the scene. Then we performed an erosion operation on the prediction mask of the current frame via a square kernel of size (5,5) and then randomly drew 100 pixels from it. As a result, the pixels that are well within the object are considered for updating its embedding.%On the other hand, new pixel candidates for the background class (negative class) are selected via two approaches. One way is to simply apply a dilation operation on the foreground mask and selecting pixels that are outside this region. This was done using a square kernel of size (15,15). Note that some foreground regions might have not been predicted in the current frame and therefore some negative pixel candidates might be selected from these regions following the above scheme. In order to address this problem, a history of the last $N_{prev}=3$ predictions are recorded and negative pixel candidates are picked from the exterior of the dilated union of these masks. On top of this, if at some point a new incoming object in the scene is mistakenly predicted as foreground, more negative pixel candidates should be selected so that the system could faster learn to assign them a background label in the subsequent frames. %Each round of online fine-tuning consists of one iteration of pixel candidate selection and model update. In order to prevent the network from forgetting the fully reliable Ground-Truth information of the first frame, two iterations of fine-tuning based on the first frame (as in Sec.~\ref{sec:ffft}) are scheduled after each online update. The model equipped with online fine-tuning offers the best mean-IoU of ...\% which beats current state-of-the-art~\cite{OnAVOS} by ?\%. %In particular, the goal is to train the model on the fly such that as object varies in shape, brightness and size during the video, the embedding space is refined such that the new forms of the object are still similar to its initial form in the first frame. Moreover, if the background changes dramatically compared to the beginning of the video, or if at some point a new instance/object enters the scene, the model should be able to distinguish them well from the object of interest at any time.%We assume that at the test time, we only have access to a sparse ground-truth map of the first frame rather than its dense annotation. In this experiment, we use the ground-truth information of only 10 pixels from each class in the first frame, and then transfer their labels to all the pixels of the query image based on their corresponding similarity scores. The final class score of each query pixel will be computed by taking the mean of all transferred scores from the first frame. The result (after CRF) was a mean-IoU of 78\% which was about 7 percent lower than state-of-the-art. However, note that we have not used any dynamic information (e.g. optical flow), objectness cues, or fine-tuning yet.  \label{sec:experiments}



\clearpage
\newpage{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}
\end{document}


