\documentclass[5p,twocolumn,10pt,times]{elsarticle}
\usepackage{amsmath}
\usepackage{color}
\usepackage{hyperref}
\addtolength{\textheight}{8mm}
\addtolength{\textwidth}{4mm}
\addtolength{\voffset}{-10mm}
\addtolength{\hoffset}{-3mm}

\bibliographystyle{elsarticle-num}
\begin{document}
\baselineskip11pt

\begin{frontmatter}

\title{Deep Spectral Descriptors: Learning the point-wise correspondence metric via Siamese deep neural networks}


\author{Zhiyu Sun}
\author{Yusen He}
\author{Andrey Gritsenko}
\author{Amaury Lendasse}
\author{Stephen Baek}
\cortext[mycorresponingauthor]{Corresponding author}
\ead{stephen-baek@uiowa.edu}
\address{Department of Mechanical and Industrial Engineering, The University of Iowa, IA, United States}


\begin{abstract} 
A robust and informative local shape descriptor plays an important role in mesh registration. In this regard, spectral descriptors that are based on the spectrum of the Laplace-Beltrami operator have gained a spotlight among the researchers for the last decade due to their desirable properties, such as isometry invariance. Despite such, however, spectral descriptors often fail to give a correct similarity measure for non-isometric cases where the metric distortion between the models is large. Hence, they are in general not suitable for the registration problems, except for the special cases when the models are near-isometry. In this paper, we investigate a way to develop shape descriptors for non-isometric registration tasks by embedding the spectral shape descriptors into a different metric space where the Euclidean distance between the elements directly indicates the geometric dissimilarity. We design and train a Siamese deep neural network to find such an embedding, where the embedded descriptors are promoted to rearrange based on the geometric similarity. We found our approach can significantly enhance the performance of the conventional spectral descriptors for the non-isometric registration tasks, and outperforms recent state-of-the-art methods reported in literature.
\end{abstract}

\begin{keyword} Deep Siamese networks, Spectral descriptor, Point-wise correspondence, Surface registration
\end{keyword}

\end{frontmatter}


\section{Introduction}
\label{sec:1}
 Deformable registration of surfaces is a fundamental problem in computer-aided design and computer graphics. The problem is critical for a wide range of applications such as shape interpolation \cite{Baek2015,Kilian:2007:GMS:1275808.1276457}, statistical shape analysis and modeling \cite{allen2003space,Baek2012}, geometry transfer \cite{sumner2004deformation}, and such. Among others, finding the point-wise correspondence between surfaces is the main challenge for a successful registration, which, however, is known to be a NP-hard problem. To this end, a well defined local shape descriptor and an accurate similarity metric can increase the success rate of a registration algorithm significantly, as they provide a tool for quantifying the similarity between two points computationally.

For the last decade, a family of spectral descriptors (e.g., \cite{Reuter2006,rustamov2007laplace,sun2009concise,aubry2011wave,bronstein2010scale}) that utilizes the eigendecomposition of the Laplace-Beltrami operator  \cite{rosenberg1997} has drawn lots of attention among the researchers in relevant fields. This is because the spectrum of the Laplace-Beltrami operator has many desirable properties, such as isometry invariance, multiscaleness, parametrization independence, and so on \cite{reuter2005laplace,sun2009concise}. In fact, the spectral descriptors in general show superior performances compared to other types of descriptors \cite{lian2013comparison}, especially for the tasks such as shape retrieval \cite{li2014spatially,bronstein2010scale}, mesh segmentation \cite{aubry2011pose,fang2011heat} and isometric matching \cite{ovsjanikov2010one}. However, despite of all the desirable properties, they often struggle in most of the surface registration tasks, especially when the disparity between the surfaces is relatively large. This is because the spectral descriptors, by definition, are highly sensitive to the metric distortion. Therefore, even though they work generally well with the isometric registration tasks (e.g., surface scans of the same individual with different postures), but not so well with the deformable registration tasks (e.g., registering a skinny person to an obese person).

In order to mitigate such limitations of the spectral descriptors, we introduce a novel approach to embed the spectral descriptors to a new metric space using deep neural networks (DNNs) in such a way that the Euclidean distance between the descriptors in the new embedding directly provides a desirable similarity metric for deformable registration tasks. To achieve so, we design a Siamese architecture of DNNs, in which two parallel sets of neural networks sharing the same coefficients are used to train the suitable embedding. The Siamese DNN is designed to find the optimal embedding of spectral descriptors by rearranging them in accordance of their correspondence among each other. In our experiment of detecting correspondence between human bodies, we found the newly embedded shape descriptors through our Siamese DNN have a superior performance in both aspects of the absolute number of correct matches and the distribution of the matches over entire shape.
\section{Related works}
\subsection{Spectral Descriptors}

Spectral shape analysis is a branch of computational geometry that analyzes digital geometry using the spectrum of a linear operator defined on a surface. Among a variety of choices, the Laplace-Beltrami operator \cite{rosenberg1997} that generalizes the Laplacian on Riemannian manifolds has gained significant highlights because of several desirable properties. One such property is the isometry invariance of its eigenvalues \cite{levy2006}. Since a lot of deformations in real-world can be characterized as an isometry or a near-isometry, the isometry invariance property of the eigenvalues of the Laplace-Beltrami operator gives advantage for many applications, including shape retrieval \cite{reuter2005laplace,jain2007spectral,li2014spatially,bronstein2010scale}, correspondence matching \cite{dubrovina2010matching,ovsjanikov2010one}, segmentation \cite{reuter2009discrete,aubry2011pose}, and etc.

Based upon this idea, Reuter \textit{et al}. \cite{Reuter2006} defined an ordered set of numerical signatures of a shape that captures the unique fingerprints of a given geometry. The signatures, which they named ShapeDNA, essentially are an ascending sequence of eigenvalues of the Laplace-Beltrami operator. They proved such an encoding of a geometry data provides an effective analytic tool for quantifying the geometric dissimilarity between different shapes in such a way that is proportional to the metric distortion.

Similarly, Rustamov \cite{rustamov2007laplace} defined the Global Point Signature (GPS), also by utilizing the eigendecomposition of the Laplace-Beltrami operator. He defined the GPS at each point on the surface as a vector value whose elements are the eigenfunctions of different modes scaled by the corresponding eigenvalues.

More notably, Sun \textit{et al}. \cite{sun2009concise} proposed the Heat Kernel Signature (HKS) physically analogous to the different heat diffusion characteristics depending on the geometric shape of surfaces. They introduced the heat kernel equation that formulates the isotropic heat diffusion process on a manifold surface by using the Laplace-Beltrami spectrum, and defined a shape signature as a collection of function values of the heat kernel. For each point on the surface, the heat kernel function is sampled at $n$ different time scales, forming an $n$-dimensional feature vector, to define the HKS. Due to the effectiveness of the HKS, a lot of variations of HKS have been introduced later on, including the works such as scale-invariant HKS (SIHKS) \cite{bronstein2010scale}.

In the similar spirit of HKS, Aubry \textit{et al}. \cite{aubry2011wave} proposed the wave kernel signature (WKS) that is based on the quantum mechanical characterization of the wave propagation on manifolds. The WKS represents the average probability of measuring a quantum mechanical particle at a specific location. This is achieved by solving the Shr\"{o}dinger's equation, whose solution is represented also by the Laplace-Beltrami spectrum in a similar form to the HKS.

Many relevant literature (see e.g., \cite{li2014spatially,lian2013comparison}) reports that spectral descriptors outperform other types of shape representation methods for shape retrieval tasks in general, since they are invariant to the isometry and are proportional to a deformation, or a metric change. Especially, signatures such as HKS and WKS are also known to be multiscale in a sense that they inherently capture both the local and global shape characteristics through different time scales.

However, the spectral descriptors are not quite suitable for the dense correspondence problems or deformable registration problems that involve large, non-isometric deformations. This is because, for non-isometric registration cases, the spectral descriptors tend to fail in recognizing the corresponding points between two models due to a large difference in local metrics. 

To mitigate such limitations, Litman \textit{et al}. \cite{litman2014learning} introduced a machine learning approach for constructing a shape descriptor for the correspondence problems with large deformations. They generalized the well-known HKS and WKS equations into a generic form, and parametrized the generic form with a number of coefficients. They then proved that the Euclidean distance between the generic form of spectral descriptors are essentially the weighted distance with a metric tensor comprised of those coefficients. Therefore, by learning the optimal set of coefficients that generates the optimal metric maximizing the gap between true matching pairs and non-matching pairs, they derive an optimized spectral descriptor (OSD). The OSD made a significant contribution to the field by overcoming the limitations of conventional spectral descriptors to some extent. Similar approaches in attempts to learn informative descriptors that are used directly in the context of shape matching were adopted in several works \cite{windheuser2014optimal,corman2014supervised,rodola2014dense}. 

\subsection{Deep Neural Networks in Matching}

Deep neural networks (DNNs) are a type of artificial neural networks that has multiple hidden layers as opposed to shallow neural networks where there is only one hidden layer. As like many other neural network approaches, DNNs try to find and generalize underlying rules and features of data on its own, from the observations on a training dataset, instead of using hard-coded or hand-crafted rules that are in general vulnerable to exceptions. This in fact is particularly useful for the tasks involving geometry data, such as computer vision, digital signal processing, and digital geometry processing, where features hold a highly sophisticated nature, and hence, manually coding sets of rules is extremely difficult or not necessarily available.

Indeed, in recent several years, there has been a phenomenal progress in the relevant fields of DNNs. Especially for the matching tasks, a number of works that utilize DNNs for deriving geometric correspondence metrics have been reported. For instance, \cite{simo2015discriminative,kumar2016learning,balntas2016pn} employed the convolutional neural networks (CNNs) to define shape descriptors on two-dimensional (2D) images. Instead of being analytically defined, shape descriptors in these works are derived from the CNNs through a number of observations on training images. The descriptors developed in such a way are shown to outperform the conventional image descriptors that are hand-crafted. 

Quite recently, the trend of CNN-based matching frameworks started to be transferred to 3D domains. Wei \textit{et al.} \cite{wei2016dense} proposed using deep convolutional neural network to train a feature descriptor for correspondence of non-rigid shapes. Yi \textit{et al.} \cite{yi2016syncspeccnn} developed SyncSpecCNN by adapting the idea of graph convolutions on the spectral domain in order to achieve semantic segmentations of 3D shapes. Masci \textit{et al.} \cite{Masci_2015_ICCV_Workshops} proposed a geodesic convolutional neural networks (GCNN) as a generalization of CNN to Riemannian manifolds based on geodesic local patches. Using the polar coordinate system defined on each geodesic patch, the local features are extracted and passed through a cascade of filters and operators. The matching error can be minimized by optimizing the coefficients of the filters and the combination weights of the operators respectively. Based on the GCNN, Boscaini \textit{et al.} \cite{boscaini2016learning} constructed an  anisotropic convolutional neural networks (ACNN) to further improve the matching performance of GCNN through the development of a patch operator using anisotropic heat kernels. 

\subsection{Contribution}
In this work, we proposes a novel approach to learn shape descriptors for correspondence matching tasks between largely deformable shapes (non-isometric). A Siamese deep neural network is constructed to embed the spectral shape descriptors (e.g., GPS, HKS, WKS) into a different metric space that geometric dissimilarity can be directly represented in terms of the Euclidean distances between elements. Compared with several state-of-the-art methods, our method showed a comparable performance to the best among them in terms of the overall matching accuracy with a superior robustness. For some extreme cases that the metric distortion between shapes is significantly large, our method showed an unique strength over all the other methods.

\section{Method}
\label{sec:2}
\subsection{Overview}
The goal of our method is to develop new shape descriptors by embedding the spectral descriptors into a new metric space such that the Euclidean distance between them directly provides a desirable similarity measure for deformable registration tasks. We first compute the spectral descriptors on each point of a 3D model. These spectral descriptors are then fed into our Siamese neural network as an input, in which the features are embedded into a new metric space. The output of the Siamese network is the similarity measure as well as a mapping from the space of original shape descriptors to the new metric space we desire.

\subsection{Spectral Descriptors}
\label{sec:spectral}
In this section, we describe the spectral descriptors that are to be used as inputs for our neural network. We used our own implementation for the computation of each type of the spectral descriptors and followed the standard parameters suggested in the original papers.

\textbf{Global Points Signature} \cite{rustamov2007laplace}: Given a point on a 2-manifold, the GPS at the point is defined as:
\begin{equation}
    \text{GPS}(x) = \left[
    \frac{\phi_1(x)}{\sqrt{\lambda_1}},\frac{\phi_2(x)}{\sqrt{\lambda_2}}, \ldots, \frac{\phi_n(x)}{\sqrt{\lambda_n}} \right]^\top,
\end{equation}
where $\lambda_k$ and $\phi_k$ are the $k$-th eigenvalue and eigenfunction of the Laplace-Beltrami operator defined on the manifold respectively. An adequate number of eigenvalues suggested in \cite{rustamov2007laplace} is $n=25$. 

\textbf{Heat Kernel Signature} \cite{sun2009concise}: Given a 2-manifold, the heat flow on the manifold can be approximated by the heat kernel function:
\begin{equation}
    H_t(x,y)=\sum_{k=0}^{\infty}\ e^{-\lambda_k t}\phi_k(x)\phi_k(y).
    \label{eq:heatkernel}
\end{equation}
Physically, the function returns the amount of heat diffused from a point $x$ to a point $y$ on the manifold during a certain time $t$. From this, the HKS is defined as a series of heat kernel values $H_t(x,x)$ measured at discrete samples of time $t_1, t_2, \ldots, t_n$:
\begin{equation}
    \text{HKS}(x) = \left[ H_{t_1}(x,x),H_{t_2}(x,x),...,H_{t_n}(x,x) \right]^\top.
\end{equation}

In \cite{sun2009concise}, the authors suggest using the first 300 eigenvalues and eigenvectors for the approximation of Equation~\ref{eq:heatkernel}. They also suggest uniformly sampling $n=100$ time samples in logarithmic scale over the time interval from $4\ln 10/\lambda_{300}$ to $4\ln 10/\lambda_2$.

\textbf{Wave Kernel Signature} \cite{aubry2011wave}: Given a 2-manifold, the propagation of a quantum particle on the manifold is governed by the Schr\"{o}dinger's wave function, whose solution is given as follows:
\begin{equation}
    \psi_E(x,t) =\sum_{k=0}^{\infty}\ e^{i \lambda_k t}\phi_i(x)f_E(\lambda_k),
\end{equation}
where $i$ is the imaginary number and $f_E^2$ is an energy probability distribution with expectation value $E$. In practice, the energy probability distribution is approximated by the log-normal distribution, $e^{\frac{-(\rho-\ln \lambda_k)^2}{2\sigma^2}}$, where $\rho$ is the energy scale.

Here, the $l_2$ norm of the $\psi_E(x,t)$ physically has a meaning that the probability of measuring the particle at a point $x$ on the manifold at time $t$. The average probability is then achieved by integrating the norm over time:
\begin{equation}
    P_\rho(x) =\lim_{T\to\infty}\frac{1}{T}\int_{0}^{T}\|\psi_E(x,t)\|^2 dt =\sum_{k=0}^{\infty}\phi_k^2(x)f_E^2(\lambda_k).
    \label{eq:wavekernel}
\end{equation}

From this, the wave kernel signature is defined as a series of the probability values in different energy scales $\rho_1, \rho_2, \hdots, \rho_n$:
\begin{equation}
    \text{WKS}(x) = \left[ P_{\rho_1}(x),P_{\rho_2}(x),...,P_{\rho_n}(x) \right]^\top.
\end{equation}

Similar to HKS, first 300 eigenvalues is used to approximate Equation~\ref{eq:wavekernel}, and the $n=100$ energy scale values are uniformly sampled over an interval from $\ln(\lambda_1)$ to $\ln(\lambda_{300})$, in \cite{aubry2011wave}.


\subsection{Embedding Space}
\label{sec:embedding}

Originally, spectral descriptors are embedded in a canonical Euclidean $n$-space equipped with the Euclidean metric (i.e., $l_2$ norm). That is, each of the descriptors corresponds to a point in an $n$-dimensional Euclidean space, and the Euclidean distance between two points indicates the difference between the spectral descriptors, and thus, the geometric dissimilarity between the corresponding surface points.

Essentially, our goal of training the neural network is to find a better, and hopefully the optimal, embedding of the spectral descriptors in a different metric space then the canonical Euclidean $n$-space. Hence, as a first step, it is critical to determine the dimensionality of the new embedding space in such a way that it preserves most of geometric information encoded in the original spectral descriptors while keeping the dimensionality as concise as possible. In fact, although the original spectral descriptors are highly informative, they in general contain a great deal of redundancy as well. We prove this hypothesis by conducting an analysis on the intrinsic dimensionality of the spectral descriptors.

Simplest way of achieving such is to perform principal component analysis (PCA) on a set of spectral descriptors \cite{kirby2000geometric,wold1987principal}. First, we collect 10,000 random samples from our database. Then, we randomly pick a point from the samples and find $k$-nearest neighbors to the selected point. Next, we conduct PCA on the set of $k$-nearest neighbors to estimate the local tangent space around the selected point. Finally, we analyze the residual variances with respect to the number of principal components, and estimate the intrinsic dimension of the local tangent space from it. Technically, we find $d$ number of principal components that covers larger than 99\% of the total variance, that is $\sum_{i}^{d} \lambda_i \ge 0.99\sum_{i}^{n} \lambda_i$ where $\lambda_i$ is the eigenvalue associated with the $i$-th principal component. We repeat this process multiple times to statistically determine the intrinsic dimension of the spectral descriptors. More sophisticated methods (see e.g., \cite{martin1979multivariate,tenenbaum2000global,roweis2000nonlinear}) may give better estimation on the intrinsic dimensionality of the data, we found them not critical from our experiments.


\begin{figure}
	\begin{center}
			\includegraphics[width=\columnwidth]{./img/dimension.png}
	\end{center}
	\caption{Intrinsic dimensionality analysis. A significant drop of residual variance can be observed throughout the first five principal components. The trend is consistent when the number of nearest neighbors ($k$) is between 6 and 25. Hence, the intrinsic dimension of the spectral descriptors can be assumed to 5. We determine the dimension of the embedding space to be 15, which is three times bigger than the actual intrinsic dimension, in order to give some more flexibility of learning.}
	\label{fig:dimension}
\end{figure}

Figure~\ref{fig:dimension} shows graphs of the average residual variances for GPS, HKS, and WKS. As can be noticed from the figure, the residual variances drop significantly up to the first five or so components, and the later components are close to zero. Such a trend was consistent when we varied the number of nearest neighbors $k$ from 6 to 25. Interestingly, the same tendency was commonly observed across the different types of the spectral descriptors. We therefore conclude that the intrinsic dimension of the spectral descriptors is 5 and define the dimensionality of the embedding space to be 15, which is three times larger than the actual dimensionality to give enough degrees of freedom for distortion of the data manifold.

\subsection{Siamese Neural Network}
\label{network}
A ``Siamese'' architecture \cite{bromley1993signature} is an effective way of designing neural networks for comparative analysis. In Siamese network, two identical neural networks are placed in parallel, and the output layers of these networks are merged and are fed into another layers of neural network. The Siamese pairs share the same coefficients such that the weight values of the neurons and the biases are all identical between the pairs. The Siamese neural network is more advantageous than the other similar architectures. They can be trained with fewer parameters since the weights are shared across the pair of neural networks. In addition, it is rational to use similar neural networks to process similar inputs (e.g., 3D models). Features with the same semantics extracted by deep learning techniques can be compared with each other easily \cite{koch2015siamese}.

\begin{figure*}
	\begin{center}
			\includegraphics[width=0.8\textwidth]{./img/siamese.png}
	\end{center}
	\caption{A schematic diagram of the Siamese architecture used in this paper. Spectral descriptors computed at different points are fed into each branch of the Siamese network. The pair of the networks is identical, and shares the same coefficients (i.e., weights $W$, and bias $b$). The outputs of the Siamese pair, which mathematically are the spectral descriptors embedded into a different metric space, are then compared with Euclidean metric (or $l_2$ distance), which then gives a measure of similarity.}.
	\label{fig:siamese}
\end{figure*}


\begin{figure}
	\begin{center}
			\includegraphics[width=0.9\columnwidth]{./img/layers.png}
	\end{center}
	\caption{The structure of 5-layer DNN used in our method.}
	\label{fig:layers}
\end{figure}

In order to learn the optimal embedding of the spectral descriptors, we design a Siamese architecture as shown in Figure~\ref{fig:siamese}. First, each of the Siamese pairs takes a spectral descriptor defined at a given point as an input. Hence, the number of neurons on the input layer equals to the dimension of the spectral descriptor to be used. On the other hand, the output layer for each of the pairs is the final embedding of the spectral descriptors, and has a dimensionality of 15 as we discussed in Section~\ref{sec:embedding}. In-between the input and the output layers, there is a repetition of a fully-connected hidden layer and the rectified linear unit (ReLU) layer (Figure~\ref{fig:layers}). Finally, the two output layers from the both side of the Siamese pair are then merged and are fed into the main body, which is purposed to find the distance metric in the embedding space. It is a common practice to place another layers of neural network as a main body, however, in our case, we just set a simple unit for computing the Euclidean distance between the two embedded descriptors and transforming it to a similarity metric.

The most critical part of our design of the Siamese network is the hidden layers at each of the Siamese pairs. There are two main design parameters in concern for the design of the hidden layers: how many layers we require and; how many neurons we should have for each of the layers. Unfortunately, there are no known guideline that works well for most of the cases. In this reason, we conducted several sessions of experiments by varying the number of hidden layers and the number of neurons associated to each of it.

Empirically, we found that more than two fully-connected hidden layers tend to overfit the training data provided, and unnecessarily complicates the training process of the neural network. On the other hand, shallow networks with a single hidden layer showed reasonable performance in general, but not quite good as the two fully-connected hidden layer cases.

Given these, we also conducted a series of analysis to determine the adequate number of neurons for each of the fully-connected layers. Considering that HKS and WKS are 100-dimensional vectors and the output embedding is 15-dimension, we varied the number of neurons from 15 to 100 and trained the neural network for each of the cases. In the case of GPS, whose initial dimensionality is 25, we varied the values from 15 to 25. To evaluate the performance for different cases, we compared the loss (LSS), true-negative ratio (TNR), false-negative ratio (FNR), and the misclassification error rate (ERR), whose details will be discussed in Section~\ref{sec:evalcrit}.

Figure~\ref{fig:nnanal} shows the result of our study. The horizontal axis represents the number of neurons in the first fully-connected hidden layer, and the vertical axis represents the second fully-connected hidden layer. Note that the upper triangle of the plot is empty because we excluded the cases where the next layers have the larger number of neurons than the previous layers. From such an analysis, we selected 78 and 32 as the adequate number of neurons for HKS and WKS, and 20 and 18 for GPS.


\begin{figure*}
	\begin{center}
			\includegraphics[width=\textwidth]{./img/HKS_NN_Analysis.png}
	\end{center}
	\caption{Visualization of grid analysis for selecting an adequate number of neurons. Samples were collected every 2 units in each direction, and the values were interpolated for the visualization of the contour diagrams. From this, we choose 78 and 32 as the number of neurons for HKS and GPS, and 20 and 18 for GPS.}
	\label{fig:nnanal}
\end{figure*}



\subsection{Training}
\label{sec:training}

For the training of the neural network, we used the Dyna dataset \cite{Dyna} to generate our training samples. The Dyna dataset is composed of over 40,000 human body meshes of ten subjects spanning a range of body shapes under various body postures. Each mesh contains 6,980 vertices. The meshes are obtained by aligning the body scans of the 10 subjects. The body scans are collected by using a custom-built multi-camera active stereo system, which captures 14 assigned motions (e.g. running, jumping, shaking, etc.) at 60 frames per second. The system outputs 3D meshes of a size around 150,000 vertices in average. During the scanning, each subject wore tight pants and a bathing cap, and a sports bra for female subjects.

Such dense sets of vertices are then registered by conforming a template mesh to each of the scanned meshes. In this way, the topology of each mesh becomes compatible to each other, and the vertices with the same index get to correspond to each other geometrically. We exploit such by using them as the ground truth for the correspondence of a pair of vertices. On each of the meshes in Dyna database, we computed the spectral descriptors in a way that is described in Section~\ref{sec:spectral}, and stored their values as a data matrix.

For each batch at the training stage, we randomly select two models from the entire database, and pick 512 pairs of vertices at random. For each batch, the first half of the pairs are chosen from among the correct matching pairs, and the second half of the pairs are from the non-matching pairs.

The neural network then takes the pairs of spectral descriptors and updates its coefficients as a result of training. For the optimization, we found no significant difference between different types of optimization methods, but we found the Adam optimizer \cite{kingma2014adam} performs well in general. For the learning rate we used initially 0.015 and let it exponentially decay by a factor of 0.9999 for each of the training steps. The objective of the optimization is to maximize the margin between non-matching pairs. This can be achieved by minimizing the sum of the following error terms over all training samples $k$.

\begin{equation}
\begin{aligned}
    E(f_k, g_k)& =  y_k\| D(f_k)-D(g_k) \|^2 \\
    &+ (1-y_k)\max \left(0, C-\|D(f_k)-D(g_k)\|^2 \right),
\end{aligned}
\label{eq:error}
\end{equation}
where $(f_k, g_k)$ are the $k$-th pair, $y_k$ is a Boolean value indicating the correspondence, $D(\cdot)$ is an embedding derived from the Siamese branch, and $\max (\cdot, \cdot)$ is a function comparing two values and returning the larger number. The constant $C$ is the minimum margin value between the non-matching pairs, and we set $C=5$.

Here, one practical consideration that needs to be done for the selection of the Boolean value $y_k$ is that even the non-matching pairs might have a highly similar geometry. For instance, a point on the left thumb would probably be highly similar in geometry with a point on the right thumb. However, they are technically non-matching pair. If such cases happen in the training dataset, it may confuse the neural network since it is a sort of conflicting example. In this reason, instead of setting $y_k$ strictly either 0 or 1, we set some random value between 0 to 0.2 for the non matching pairs. The correct matching pairs are still kept to $y_k=1$.

To produce the result presented in this paper, we trained the neural network with the total of 10,000 iterations, and hence the total number of training samples were 5,120,000 (batch size $\times$ number of iterations).

\subsection{Evaluation criteria}
\label{sec:evalcrit}

For the quantitative measurement of the trained performance of our method, we use following criteria:

\textbf{Loss} is the sum of error terms in Equation~\ref{eq:error} over test dataset. The loss is an indicator of how well the non-matching pairs are separated from each other in the embedding space.

\textbf{True-Negative Rate (TNR)} is the percentage of error that the correct matching pairs are classified as non-matching pairs. For the threshold of classification, we use half of the margin $C$ in Equation~\ref{eq:error}. Therefore, TNR shows what percentage of matching pairs are separated wider than $0.5 C$.

\textbf{False-Positive Rate (FPR)} is the percentage of error that the non-matching pairs are classified as correct matching pairs. We use the same threshold with the above case. FPR shows what percentage of non-matching pairs are within the margin of $0.5C$.

\textbf{Misclassification Rate (ERR)} is the percentage of error that the pairs are classified to other than their ground truth. Conceptually, it is the union of TNR and FPR, and it is calculated as the sum of these two quantities.

\section{Result}
\label{sec:result}

\subsection{Matching Performance}
\label{sec:3}
From the Dyna dataset, we randomly selected a pair of models from two different individuals that were not used for training of the Siamese DNN. We computed the spectral descriptors as well as the proposed descriptors on the models and matched their vertices one to the other through the nearest neighbor search using the simple Euclidean distance. We have rejected non-matching pairs, i.e., the pairs with the Euclidean distance over the threshold value $0.5C$. In addition, we also rejected the pairs with the geodesic distortion more than 5\% of the shape diameter. We then counted the number of remaining matches, which could be considered as the correct matching pairs. We did not use any sophisticated algorithm for the calculation of the matching pairs, such as the ones presented in \cite{litman2014learning,leordeanu2005spectral}. Instead, we directly compared the Euclidean distances among the descriptors such that a more straight forward comparison could be done on the quality of the metric.

Figure~\ref{fig:bestmatch} and Table~\ref{tbl:DynaTest} show the result of such analysis. Overall, it is observed a significant improvement with the proposed method in terms of the absolute numbers of correct matching pair. Among them, GPS showed the largest improvement in average, followed by HKS and WKS. In overall matching performance, DeepHKS showed the most desirable result, followed by DeepWKS and DeepGPS.

Note even for the cases where the improvement in absolute number was not so significant or even negative, the quality of matching still has been improved in a sense that the pairs in deep signature results cover larger area than the original descriptors. For instance, the second row in Table~\ref{tbl:DynaTest} shows a relatively small improvement in HKS (2,209 $\rightarrow$ 2,300), and even a decrease of numbers in WKS (2,124 $\rightarrow$ 1,325) in terms of the absolute number of correct matching pairs. However, as can be observed from the corresponding images in Figure~\ref{fig:bestmatch}, the quality of matching is much better in DeepHKS and DeepWKS, such that the matching pairs are spread well over the larger area (see e.g., areas around breast and belly). This is, in fact, because some of the body parts such as hands, feet, and faces, are near-isometry across the models. In this reason, the matching results of the original spectral descriptors are mostly concentrated around those body parts, whereas the matching results of the deep spectral descriptors are distributed widely over the entire body area, even including highly non-isometric areas.


\begin{center}
\begin{table*}
\begin{tabular}{l|l|r|r|r|r|r|r}
    Source model & Target model & GPS & HKS & WKS & DGPS & DHKS & DWKS \\
    \hline
    50002\_one\_leg\_jump\textbackslash00455 & 50027\_jumping\_jacks\textbackslash00228 & 0 & 211 & 669 & 631 & 1,585 & 706 \\
    50009\_running\_on\_spot\textbackslash00165 & 50004\_punching\textbackslash00170 & 147 & 2,209 & 2,124 & 2,374 & 2,300 & 1,325 \\
    50020\_chicken\_wings\textbackslash00090 & 50022\_one\_leg\_jump\textbackslash00266 & 84 & 1,145 & 1,328 & 3,478 & 2,654 & 1,595 \\
    50026\_light\_hopping\_loose\textbackslash00168 & 50020\_knees\textbackslash00374 & 25 & 627 & 1,072 & 1,072 & 2,229 & 1,794 \\
    50007\_jiggle\_on\_toes\textbackslash00233 & 50022\_shake\_arms\textbackslash00302 & 206 & 455 & 1,162 & 998 & 1,637 & 1,684 \\
    50002\_hips\textbackslash00515 & 50007\_shake\_shoulders\textbackslash00214 & 17 & 1,474 & 1,943 & 921 & 1,975 & 1,565
\end{tabular}

\caption{Statistics of correct matching pairs for the results presented in Figure~\ref{fig:bestmatch}. Each of the rows in the table corresponds to each of the rows in Figure~\ref{fig:bestmatch} in the same order. The left two entries of each row show the id number of an individual and the name and the frame number of a motion, following the same naming convention in Dyna database. Different id numbers indicate different body shapes, and different motion names and frame numbers indicate different body postures. The rest of the entries show the number of correct matching pairs for each of the methods.}
\label{tbl:DynaTest}
\end{table*}
\end{center}


\begin{figure*}
	\begin{center}
			\includegraphics[width=\textwidth]{./img/bestmatch.png}
	\end{center}
	\caption{Visualization of the matches within the geodesic distortion of 5\% of the shape diameter. Each row shows a comparison between the GPS, HKS, and WKS, and their embeddings, namely DeepGPS, DeepHKS, and DeepWKS, respectively. Different models in different rows are randomly picked from the test dataset for the visualization. For a fair comparison, no sophisticated algorithm such as \cite{leordeanu2005spectral} is used, but a simple nearest neighbor search based on the Euclidean distances between the shape descriptors.}
	\label{fig:bestmatch}
\end{figure*}

\subsection{Comparison with other methods}

\label{sec:4}
In addition to the aforementioned performance evaluation, we further verify the performance of our method by comparing with the OSD \cite{litman2014learning}, the GCNN \cite{Masci_2015_ICCV_Workshops} and the ACNN \cite{boscaini2016learning}. For such evaluation, we formulate a dataset containing $100$ different models by randomly picking $10$ meshes of each subject over all assigned motions across the total $10$ subjects in the Dyna dataset. From the formulated dataset, we used $60$ mesh models of 6 different subjects for training. The rest $40$ mesh models of the other $4$ subjects were kept for testing. In order to make the comparison fair, we used the $100$ dimensional HKS computed as described in Section~\ref{sec:spectral} as an identical network input for GCNN, ACNN and our Siamese DNN. Under such setting, we have a clear guide to compare the proposed methods by seeing how much extent they can improve some existing shape descriptor for a specific task (correspondence matching for deformable shapes).

To drill-down to details, we conducted four groups of analyses to evaluate the correspondence matching performance of each method. For all groups, the source models were selected from the mesh models belongs to the same subject "50020" which has the smallest body size among the four subjects in our test dataset. For the first group, the target models were selected from the same subject "50002", but with different postures corresponding to each source one. From the second to the fourth group, the target models are selected from the rest three subjects ("50021", "50025", "50002") in an order of increasing body shape disparities compared with the subject "50020", respectively. Each of the group contains $10$ different pairs of models (source and target) that are randomly selected under the restriction of aforementioned manner. 

For better visualization, we randomly sampled 10\% of all vertices uniformly distributed over the entire mesh. In the Dyna dataset, some of the body segments such as head, hands, and feet were rigid and thus remain unchanged across individuals due to the nature of how the dataset had been generated (see \cite{Dyna} for the detail). Thus, these areas were excluded from the evaluation for more precise comparison between different methods. For each pair of the source and target models, matching accuracy was calculated as the percentage value of counted correct matching pairs of vertices (within the geodesic distortion of 5\% of the shape diameter) over the total number of sampled vertices. Table~\ref{tbl:DynaTest2} summarizes the correspondences matching performance of each proposed method for the four groups of analysis as described. Figure~\ref{fig:bestmatch3} visualizes the correct matches of a representative pair for each of the group, with the corresponding matching accuracy shown in the figure. The visualization of additional pairs in each of the group is provided in the supplementary materials.

From Figure \ref{fig:bestmatch3}, it can be visually recognized that the body shape disparities between the paired subjects gradually increased from Group $1$ to Group $4$. For the Group $1$, while the deformation between source and target models is isometric (same subject with different posture), the HKS has the best performance in terms of the correspondence matching accuracy. As the amount of metric distortion increases (Group 2,3 4), the matching accuracy of the HKS significantly drops, which is consistent with our prior knowledge according to the limitation of spectral descriptors. From here, the proposed methods (OSD, GCNN, ACNN and DeepHKS) start to demonstrate their values in improving the HKS for large deformation (non-isometric) cases. For Group 2 and Group 3, the ACNN performed the best in terms of the overall matching accuracy, followed by our method (DeepHKS) that showed a comparable performance with ACNN. For Group 4, while the amount of metric distortion between models became tremendous (highly non-isometric), our method significantly outperformed all the other methods. It is also noteworthy that for all the groups, our method showed a consistent level of the relative standard deviation (RSD) in each group, as well as the smallest compared with all the other methods, which indicates a more reliable and robust performance of our method.

\begin{center}
\begin{table*}
\begin{tabular}{l|l|rr|rr|rr|rr|rr}
     & & \multicolumn{2}{c|}{HKS} & \multicolumn{2}{c|}{OSD} & \multicolumn{2}{c|}{GCNN} & \multicolumn{2}{c|}{ACNN} & \multicolumn{2}{c}{DHKS} \\
    Group ID & Subjects (source\textbackslash target) & Mean & RSD & Mean & RSD & Mean & RSD & Mean & RSD & Mean & RSD \\
    \hline
     1 & 50020\textbackslash 50020 & 54.18\% & 0.15 & 42.84\% & 0.22 & 15.69\% & 0.16 & 26.46\% & 0.10 & 34.73\% & 0.05 \\
     2 & 50020\textbackslash 50021 & 10.71\% & 0.20 & 20.13\% & 0.18 & 20.76\% & 0.21 & 33.03\% & 0.11 & 28.91\% & 0.08 \\
     3 & 50020\textbackslash 50025 & 5.88\% & 0.28 & 17.18\% & 0.14 & 22.25\% & 0.17 & 31.93\% & 0.09 & 26.15\% & 0.08 \\
     4 & 50020\textbackslash 50002 & 0.12\% & 0.53 & 1.80\% & 0.27 & 0.54\% & 0.52 & 1.05\% & 0.28 & 10.12\% & 0.10
\end{tabular}
\caption{Statistics for four groups of analyses. The mean matching accuracy and the relative standard deviation (RSD) over the $10$ pairs of selected models in each of the group are presented.}
\label{tbl:DynaTest2}
\end{table*}
\end{center}

\begin{figure*}
	\begin{center}
			\includegraphics[width=\textwidth]{./img/Comparison.png}
	\end{center}
	\caption{Comparison among HKS, OSD, GCNN, ACNN and DeepHKS. Each row corresponds to a representative pair of models in each of the group presented in Table~\ref{tbl:DynaTest2}. For each of the pair, source model is rendered in yellow and target model in blue.}
	\label{fig:bestmatch3}
\end{figure*}
\section{Conclusion}
\label{sec:5}
In this paper, we proposed an approach to learn the similarity metric between the spectral descriptors for deformable registration tasks. We designed a Siamese neural network architecture carefully, based on the analysis of its design parameters. The neural network designed in this paper was able to find an optimal embedding of the spectral descriptors in a new metric space, in which a direct comparison based on the Euclidean distance already gives an good point-wise matching result between non-isometric models. The method was verified on a database containing a variety of body shapes and postures, where it showed significant improvement of the original spectral descriptors in non-isometric cases. In the task of improving a conventional spectral descriptor (HKS \cite{sun2009concise}) for detecting correspondence between non-isometric shapes, our method (DeepHKS) showed a comparable performance in terms of the overall matching accuracy with the best robustness compared with the other state-of-the-art methods (OSD \cite{litman2014learning}, GCNN \cite{Masci_2015_ICCV_Workshops}, ACNN \cite{boscaini2016learning}). For some relatively  highly non-isometric cases, our method significantly outperformed all the other methods.

A question might be raised if concatenating different spectral descriptors all together and apply the same Siamese learning procedure would produce any better result. We found that concatenating GPS, HKS, and WKS did not necessarily lead to a noticeable improvement of the performance immediately, possibly because of the increased dimensionality of the input layer, complicating the training procedure and the convergence of the neural network coefficients. We have not conducted a detailed study to find the optimal network parameters in such cases, as it would be out of scope of this paper. However, it is reasonable to assume having more descriptor components would provide richer information on geometric characteristics, and a further investigation on this would be valuable.

As another future work, it would be interesting to explore possible ways of improving our method to achieve scale-invariance. Current formulation is dependent upon scaling of the model. Furthermore, transferring the learned embedding from one training set to another would also be a possible direction for a future research work.
\section*{References}

\begin{thebibliography}{10}
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi
\expandafter\ifx\csname href\endcsname\relax
  \def\href#1#2{#2} \def\path#1{#1}\fi

\bibitem{Baek2015}
S.-Y. Baek, J.~Lim, K.~Lee, Isometric shape interpolation, Computers \&
  Graphics 46~(1) (2015) 257--263.
\newblock \href{http://dx.doi.org/10.1016/j.cag.2014.09.025}{\path{doi:10.1016/j.cag.2014.09.025}}.

\bibitem{Kilian:2007:GMS:1275808.1276457}
M.~Kilian, N.~J. Mitra, H.~Pottmann,
  \href{http://doi.acm.org/10.1145/1275808.1276457}{Geometric modeling in shape
  space}, in: ACM SIGGRAPH 2007 Papers, SIGGRAPH '07, ACM, New York, NY, USA,
  2007.
\newblock \href{http://dx.doi.org/10.1145/1275808.1276457}{\path{doi:10.1145/1275808.1276457}}.
\newline\urlprefix\url{http://doi.acm.org/10.1145/1275808.1276457}

\bibitem{allen2003space}
B.~Allen, B.~Curless, Z.~Popovi{\'c}, The space of human body shapes:
  reconstruction and parameterization from range scans, ACM transactions on
  graphics (TOG) 22~(3) (2003) 587--594.

\bibitem{Baek2012}
S.-Y. Baek, K.~Lee, Parametric human body shape modeling framework for
  human-centered product design, Computer-Aided Design 44~(1) (2012) 56--67.
\newblock \href{http://dx.doi.org/10.1016/j.cad.2010.12.006}{\path{doi:10.1016/j.cad.2010.12.006}}.

\bibitem{sumner2004deformation}
R.~W. Sumner, J.~Popovi{\'c}, Deformation transfer for triangle meshes, ACM
  Transactions on Graphics (TOG) 23~(3) (2004) 399--405.

\bibitem{Reuter2006}
M.~Reuter, F.-E. Wolter, N.~Peinecke,
  \href{http://dx.doi.org/10.1016/j.cad.2005.10.011}{Laplace-beltrami spectra
  as 'shape-dna' of surfaces and solids}, Comput. Aided Des. 38~(4) (2006)
  342--366.
\newblock \href{http://dx.doi.org/10.1016/j.cad.2005.10.011}{\path{doi:10.1016/j.cad.2005.10.011}}.
\newline\urlprefix\url{http://dx.doi.org/10.1016/j.cad.2005.10.011}

\bibitem{rustamov2007laplace}
R.~M. Rustamov, Laplace-beltrami eigenfunctions for deformation invariant shape
  representation, in: Proceedings of the fifth Eurographics symposium on
  Geometry processing, Eurographics Association, 2007, pp. 225--233.

\bibitem{sun2009concise}
J.~Sun, M.~Ovsjanikov, L.~Guibas, A concise and provably informative
  multi-scale signature based on heat diffusion, Computer graphics forum 28~(5)
  (2009) 1383--1392.

\bibitem{aubry2011wave}
M.~Aubry, U.~Schlickewei, D.~Cremers, The wave kernel signature: A quantum
  mechanical approach to shape analysis, in: 2011 IEEE International Conference
  on Computer Vision Workshops (ICCV Workshops), 2011, pp. 1626--1633.

\bibitem{bronstein2010scale}
M.~M. Bronstein, I.~Kokkinos, Scale-invariant heat kernel signatures for
  non-rigid shape recognition, in: Computer Vision and Pattern Recognition
  (CVPR), 2010 IEEE Conference on, IEEE, 2010, pp. 1704--1711.

\bibitem{rosenberg1997}
S.~Rosenberg, The Laplacian on a Riemannian manifold: an introduction to
  analysis on manifolds, no.~31, Cambridge University Press, 1997.

\bibitem{reuter2005laplace}
M.~Reuter, F.-E. Wolter, N.~Peinecke, Laplace-spectra as fingerprints for shape
  matching, in: Proceedings of the 2005 ACM symposium on Solid and physical
  modeling, ACM, 2005, pp. 101--106.

\bibitem{lian2013comparison}
Z.~Lian, A.~Godil, B.~Bustos, M.~Daoudi, J.~Hermans, S.~Kawamura, Y.~Kurita,
  G.~Lavou{\'e}, H.~Van~Nguyen, R.~Ohbuchi, et~al., A comparison of methods for
  non-rigid 3d shape retrieval, Pattern Recognition 46~(1) (2013) 449--461.

\bibitem{li2014spatially}
C.~Li, A.~B. Hamza, Spatially aggregating spectral descriptors for nonrigid 3d
  shape retrieval: a comparative survey, Multimedia Systems 20~(3) (2014)
  253--281.

\bibitem{aubry2011pose}
M.~Aubry, U.~Schlickewei, D.~Cremers, Pose-consistent 3d shape segmentation
  based on a quantum mechanical feature descriptor, in: Joint Pattern
  Recognition Symposium, 2011, pp. 122--131.

\bibitem{fang2011heat}
Y.~Fang, M.~Sun, M.~Kim, K.~Ramani, Heat-mapping: A robust approach toward
  perceptually consistent mesh segmentation, in: Computer Vision and Pattern
  Recognition (CVPR), 2011 IEEE Conference on, IEEE, 2011, pp. 2145--2152.

\bibitem{ovsjanikov2010one}
M.~Ovsjanikov, Q.~M{\'e}rigot, F.~M{\'e}moli, L.~Guibas, One point isometric
  matching with the heat kernel, Computer Graphics Forum 29~(5) (2010)
  1555--1564.

\bibitem{levy2006}
B.~L{\'e}vy, Laplace-beltrami eigenfunctions towards an algorithm that"
  understands" geometry, in: Shape Modeling and Applications, 2006. SMI 2006.
  IEEE International Conference on, IEEE, 2006, pp. 13--13.

\bibitem{jain2007spectral}
V.~Jain, H.~Zhang, A spectral approach to shape-based retrieval of articulated
  3d models, Computer-Aided Design 39~(5) (2007) 398--407.

\bibitem{dubrovina2010matching}
A.~Dubrovina, R.~Kimmel, Matching shapes by eigendecomposition of the
  laplace-beltrami operator, in: Proc. 3DPVT, Vol.~2, 2010.

\bibitem{reuter2009discrete}
M.~Reuter, S.~Biasotti, D.~Giorgi, G.~Patan{\`e}, M.~Spagnuolo, Discrete
  laplace--beltrami operators for shape analysis and segmentation, Computers \&
  Graphics 33~(3) (2009) 381--390.

\bibitem{litman2014learning}
R.~Litman, A.~M. Bronstein, Learning spectral descriptors for deformable shape
  correspondence, IEEE transactions on pattern analysis and machine
  intelligence 36~(1) (2014) 171--180.

\bibitem{windheuser2014optimal}
T.~Windheuser, M.~Vestner, E.~Rodol{\`a}, R.~Triebel, D.~Cremers, Optimal
  intrinsic descriptors for non-rigid shape analysis., in: BMVC, 2014.

\bibitem{corman2014supervised}{\'E}.~Corman, M.~Ovsjanikov, A.~Chambolle, Supervised descriptor learning for
  non-rigid shape matching, in: European Conference on Computer Vision,
  Springer, 2014, pp. 283--298.

\bibitem{rodola2014dense}
E.~Rodol{\`a}, S.~Rota~Bulo, T.~Windheuser, M.~Vestner, D.~Cremers, Dense
  non-rigid shape correspondence using random forests, in: Proceedings of the
  IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp.
  4177--4184.

\bibitem{simo2015discriminative}
E.~Simo-Serra, E.~Trulls, L.~Ferraz, I.~Kokkinos, P.~Fua, F.~Moreno-Noguer,
  Discriminative learning of deep convolutional feature point descriptors, in:
  Proceedings of the IEEE International Conference on Computer Vision, 2015,
  pp. 118--126.

\bibitem{kumar2016learning}
B.~Kumar, G.~Carneiro, I.~Reid, et~al., Learning local image descriptors with
  deep siamese and triplet convolutional networks by minimising global loss
  functions, in: Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition, 2016, pp. 5385--5394.

\bibitem{balntas2016pn}
V.~Balntas, E.~Johns, L.~Tang, K.~Mikolajczyk, Pn-net: conjoined triple deep
  network for learning local image descriptors, arXiv preprint
  arXiv:1601.05030.

\bibitem{wei2016dense}
L.~Wei, Q.~Huang, D.~Ceylan, E.~Vouga, H.~Li, Dense human body correspondences
  using convolutional networks, in: Proceedings of the IEEE Conference on
  Computer Vision and Pattern Recognition, 2016, pp. 1544--1553.

\bibitem{yi2016syncspeccnn}
L.~Yi, H.~Su, X.~Guo, L.~Guibas, Syncspeccnn: Synchronized spectral cnn for 3d
  shape segmentation, arXiv preprint arXiv:1612.00606.

\bibitem{Masci_2015_ICCV_Workshops}
J.~Masci, D.~Boscaini, M.~M. Bronstein, P.~Vandergheynst, Geodesic
  convolutional neural networks on riemannian manifolds, in: The IEEE
  International Conference on Computer Vision (ICCV) Workshops, 2015.

\bibitem{boscaini2016learning}
D.~Boscaini, J.~Masci, E.~Rodol{\`a}, M.~Bronstein, Learning shape
  correspondence with anisotropic convolutional neural networks, in: Advances
  in Neural Information Processing Systems, 2016, pp. 3189--3197.

\bibitem{kirby2000geometric}
M.~Kirby, Geometric data analysis: an empirical approach to dimensionality
  reduction and the study of patterns, John Wiley \& Sons, Inc., 2000.

\bibitem{wold1987principal}
S.~Wold, K.~Esbensen, P.~Geladi, Principal component analysis, Chemometrics and
  intelligent laboratory systems 2~(1-3) (1987) 37--52.

\bibitem{martin1979multivariate}
N.~Martin, H.~Maes, Multivariate analysis, Academic press, 1979.

\bibitem{tenenbaum2000global}
J.~B. Tenenbaum, V.~De~Silva, J.~C. Langford, A global geometric framework for
  nonlinear dimensionality reduction, science 290~(5500) (2000) 2319--2323.

\bibitem{roweis2000nonlinear}
S.~T. Roweis, L.~K. Saul, Nonlinear dimensionality reduction by locally linear
  embedding, science 290~(5500) (2000) 2323--2326.

\bibitem{bromley1993signature}
J.~Bromley, J.~W. Bentz, L.~Bottou, I.~Guyon, Y.~LeCun, C.~Moore,
  E.~S{\"a}ckinger, R.~Shah, Signature verification using a ``siamese'' time
  delay neural network, IJPRAI 7~(4) (1993) 669--688.

\bibitem{koch2015siamese}
G.~Koch, R.~Zemel, R.~Salakhutdinov, Siamese neural networks for one-shot image
  recognition, in: ICML Deep Learning Workshop, Vol.~2, 2015.

\bibitem{Dyna}
G.~Pons-Moll, J.~Romero, N.~Mahmood, M.~J. Black, Dyna: A model of dynamic
  human shape in motion, ACM Transactions on Graphics, (Proc. SIGGRAPH) 34~(4)
  (2015) 120:1--120:14.

\bibitem{kingma2014adam}
D.~Kingma, J.~Ba, Adam: A method for stochastic optimization, arXiv preprint
  arXiv:1412.6980.

\bibitem{leordeanu2005spectral}
M.~Leordeanu, M.~Hebert, A spectral technique for correspondence problems using
  pairwise constraints, in: Computer Vision, 2005. ICCV 2005. Tenth IEEE
  International Conference on, Vol.~2, IEEE, 2005, pp. 1482--1489.

\end{thebibliography}


\end{document}




