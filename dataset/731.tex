\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{comment}
\usepackage{multirow}
\usepackage{float}
\usepackage{tikz}
\usepackage{calc}
\usepackage{adjustbox}
\usepackage{pdfpages}
\usepackage{comment}

\definecolor{newcolor}{rgb}{0,0.4,0}
\definecolor{oldcolor}{rgb}{0.7,0.7,0.7}



\ifdefined\SHOWDIFF
  \includecomment{NEW}
  \includecomment{OLD}
  \specialcomment{NEW}{\begingroup\color{newcolor}}{\endgroup}
  \specialcomment{OLD}{\begingroup\color{oldcolor}}{\endgroup}
\else
  \includecomment{NEW}
  \excludecomment{OLD}
\fi

\IfFileExists{../images/teaser.pdf}{%
\graphicspath{{../images/}}
}{
\graphicspath{{./images/}}
}


\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{2080} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ifcvprfinal\pagestyle{empty}\fi
\begin{document}
\newlength{\capvspace}
\setlength{\capvspace}{-0.7em}

\newlength{\figvspace}
\setlength{\figvspace}{-0.5em}

\definecolor{dgreen}{rgb}{0,.7,0}
\definecolor{ben}{rgb}{0.9,.4,0}
\definecolor{dred}{rgb}{.7,0,0}
\definecolor{dblue}{rgb}{0,0,0.7}
\definecolor{alexey}{rgb}{0.7,0,1}
\newcommand{\TODO}[1]{{\color{dblue}[TODO #1]}}

\newcommand{\tcb}[1]{\textcolor{dred}{\scriptsize #1}}    % Thomas's comments
\newcommand{\tca}[1]{\textcolor{alexey}{\scriptsize #1}}    % Alexey's comments
\newcommand{\tcu}[1]{\textcolor{ben}{\scriptsize #1}}    % Benjamin's comments
\newcommand{\tch}[1]{\textcolor{dgreen}{\scriptsize #1}}    % Huizhong's comments
\newcommand{\tcj}[1]{\textcolor{dblue}{\scriptsize #1}}    % Jonas' comments

\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\img}{I}
\newcommand{\depth}{D}
\newcommand{\motion}{M}
\newcommand{\preddepth}{\widehat{D}}
\newcommand{\predmotion}{\widehat{M}}
\newcommand{\weights}{w}
\newcommand{\loss}{\mathcal{L}}

\newcommand{\no}{\textcolor{dred}{no}}
\newcommand{\yes}{\textcolor{dgreen}{yes}}

\newcommand{\fig}[1]{\mbox{Fig.~\ref{#1}}}
\newcommand{\tab}[1]{\mbox{Tab.~\ref{#1}}}


\newcommand{\bb}[1]{\textbf{#1}}

\newcommand{\compw}{0.09}
\newcommand{\compc}{\textcolor{dred}}

\newcommand{\incgraphics}[3]{%
  \begin{tikzpicture}%
    \node[inner sep=0pt] (mynode) {\adjustimage{#1}{#2}};%
    \node at (mynode) [inner sep=0,scale=1] {\strut #3};%
  \end{tikzpicture}%
}
\newcommand{\incgraphicsbelow}[3]{%
  \begin{tikzpicture}%
    \node[inner sep=0pt] (mynode) {\adjustimage{#1}{#2}};%
    \node at (mynode.south) [inner sep=0,scale=1,below=1mm] {\strut #3};%
  \end{tikzpicture}%
}
\newcommand{\incgraphicsabove}[4][1mm]{%
  \begin{tikzpicture}%
    \node[inner sep=0pt] (mynode) {\adjustimage{#2}{#3}};%
    \node at (mynode.north) [inner sep=0,scale=1,above=#1] {\strut #4};%
  \end{tikzpicture}%
}

\newcommand{\incgraphicsbrbox}[3]{%
  \begin{tikzpicture}%
    \node[inner sep=0pt] (mynode) {\adjustimage{#1}{#2}};%
    \node at (mynode.south east) [inner sep=0.25mm,scale=1,above left=0.15ex, fill=white, opacity=0.5, text opacity=1] {#3};%
  \end{tikzpicture}%
}


\title{DeMoN: Depth and Motion Network for Learning Monocular Stereo}

\author{Benjamin Ummenhofer\textsuperscript{*,1} \qquad Huizhong Zhou\textsuperscript{*,1}\\
{\tt\small \{ummenhof, zhouh\}@cs.uni-freiburg.de}
\and
Jonas Uhrig\textsuperscript{1,2}\quad Nikolaus Mayer\textsuperscript{1}\quad Eddy Ilg\textsuperscript{1}\quad Alexey Dosovitskiy\textsuperscript{1}\quad Thomas Brox\textsuperscript{1}\\[1mm]
\textsuperscript{1}University of Freiburg \qquad \textsuperscript{2}Daimler AG R\&D\\
{\tt\small \{uhrigj, mayern, ilg, dosovits, brox\}@cs.uni-freiburg.de}
}

\maketitle
\renewcommand*{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Equal contribution}
\global\csname @topnum\endcsname 0
\global\csname @botnum\endcsname 0

\begin{abstract}
In this paper we formulate structure from motion as a learning problem.
We train a convolutional network end-to-end to compute depth and camera motion from successive, unconstrained image pairs.
The architecture is composed of multiple stacked encoder-decoder networks, the core part being an iterative network that is able to improve its own predictions.
The network estimates not only depth and motion, but additionally surface normals, optical flow between the images and confidence of the matching.
A crucial component of the approach is a training loss based on spatial relative differences.
Compared to traditional two-frame structure from motion methods, results are more accurate and more robust.
In contrast to the popular depth-from-single-image networks, DeMoN learns the concept of matching and, thus, better generalizes to structures not seen during training.
\end{abstract}



\section{Introduction}

\begin{figure}
\begin{center}
  \includegraphics[width=0.43\textwidth]{teaser.pdf}
\end{center}
\vspace{\capvspace}%
\caption{%
Illustration of DeMoN. The input to the network is two successive images from a monocular camera. The network estimates the depth in the first image and the camera motion.
}
\label{fig:teaser}
\vspace{\figvspace}
\end{figure}


Structure from motion (SfM) is a long standing task in computer vision. 
Most existing systems, which represent the state of the art, are carefully engineered pipelines consisting of several consecutive processing steps.
A fundamental building block of these pipelines is the computation of the structure and motion for two images. 
Present implementations of this step have some inherent limitations. %do not leverage all available information.
For instance, it is common to start with the estimation of the camera motion before inferring the structure of the scene by dense correspondence search. 
Thus, an incorrect estimate of the camera motion leads to wrong depth predictions. 
Moreover, the camera motion is estimated from sparse correspondences computed via keypoint detection and descriptor matching. 
This low-level process is prone to outliers and does not work in non-textured regions. 
Finally, all existing SfM approaches fail in case of small camera translation. 
This is because it is hard to integrate priors that could provide reasonable solutions in those degenerate cases.

In this paper, we succeed for the first time in training a convolutional network to jointly estimate the depth and the camera motion from an unconstrained pair of images. 
This approach is very different from the typical SfM pipeline in that it solves the problems of motion and dense depth estimation jointly.
We cannot yet provide a full learning-based system for large-scale SfM, but the two-frame case is a crucial first step towards this goal. 
In the longer term, the learning approach has large potential, since it integrates naturally all the shape from X approaches: multi-view, silhouettes, texture, shading, defocus, haze. 
Moreover, strong priors on objects and structure can be learned efficiently from data and regularize the problem in degenerate cases; see \fig{fig:small_baseline} for example. 
This potential is indicated by our results for the two-frame scenario, where the learning approach clearly outperforms traditional methods. 




Convolutional networks recently have shown to excel at depth prediction from a single image \cite{eigen_predicting_2015,eigen_predicting_2014,liu_learning_2015}.
By learning priors about objects and their shapes these networks reach remarkably good performance in restricted evaluation scenarios such as indoor or driving scenes. 
However, single-image methods have more problems generalizing to previously unseen types of images.
This is because they do not exploit stereopsis. 
\fig{fig:generalization} shows an example, where depth from a single image fails, because the network did not see similar structures before. 
Our network, which learned to exploit the motion parallax, does not have this restriction and generalizes well to very new scenarios. 

To exploit the motion parallax, the network must put the two input images in correspondence.
We found that a simple encoder-decoder network fails to make use of stereo: when trained to compute depth from two images it ends up using only one of them.
Depth from a single image is a shortcut to satisfy the training objective
without putting the two images into correspondence and deriving camera motion and depth from these correspondences.

In this paper, we present a way to avoid this shortcut and elaborate on it to obtain accurate depth maps and camera motion estimates. The key to the problem is an architecture that alternates optical flow estimation with the estimation of camera motion and depth; see \fig{fig:encoder_decoder_pair}. 
In order to solve for optical flow, the network \emph{must} use both images. To this end, we adapted the FlowNet architecture \cite{dosovitskiy_flownet_2015} to our case. 
Our network architecture has an iterative part that is comparable to a recurrent network, since weights are shared. 
Instead of the typical unrolling, which is common practice when training recurrent networks, we append predictions of previous training iterations to the current minibatch.
This training technique saves much memory and allows us to include more iterations for training. 
Another technical contribution of this paper is a special gradient loss to deal with the scale ambiguity in structure from motion. The network was trained on a mixture of real images from a Kinect camera, including the SUN3D dataset \cite{xiao_sun3d_2013}, and a variety of rendered scenes that we created for this work.


\section{Related Work}

Estimation of depth and motion from pairs of images goes back to Longuet-Higgins~\cite{longuet-higgins_computer_1981}. The underlying 3D geometry is a consolidated field, which is well covered in textbooks \cite{Hartley2004,Faugeras1993}.
State-of-the-art systems~\cite{frahm_building_2010,wu_towards_2013} allow for reconstructions of large scenes including whole cities. They consist of a long pipeline of methods, starting with 
descriptor matching for finding a sparse set of correspondences between images~\cite{lowe_distinctive_2004}, followed by estimating the essential matrix to determine the camera motion. Outliers among the correspondences are typically filtered out via RANSAC \cite{fischler_random_1981}.
Although these systems use bundle adjustment \cite{triggs_bundle_2000} to jointly optimize camera poses and structure of many images, they 
depend on the quality of the estimated geometry between image pairs for initialization.
Only after estimation of the camera motion and a sparse 3D point cloud, dense depth maps are computed by exploiting the epipolar geometry \cite{collins_space-sweep_1996}. 
LSD-SLAM \cite{engel14eccv} deviates from this approach by jointly optimizing semi-dense correspondences and depth maps. It considers multiple frames from a short temporal window but does not include bundle adjustment.
DTAM \cite{newcombe_dtam:_2011} can track camera poses reliably for critical motions by matching against dense depth maps.
However, an external depth map initialization is required, which in turn relies on classic structure and motion methods.

Camera motion estimation from dense correspondences has been proposed by Valgaerts et al.~\cite{valgaerts_dense_2012}. 
In this paper, we deviate completely from these previous approaches by training a single deep network that includes computation of dense correspondences, estimation of depth, and the camera motion between two frames. 

Eigen et al.~\cite{eigen_predicting_2015} trained a ConvNet to predict depth from a single image.
Depth prediction from a single image is an inherently ill-posed problem which can only be solved using priors and semantic understanding of the scene~-- tasks ConvNets are known to be very good at.
Liu et al.~\cite{liu_learning_2015} combined a ConvNet with a superpixel-based conditional random field, yielding improved results.
Our two-frame network also learns to exploit the same cues and priors as the single-frame networks, but in addition it makes use of a pair of images and the motion parallax between those. 
This enables generalization to arbitrary new scenes. 

ConvNets have been trained to replace the descriptor matching module in aforementioned SfM systems~\cite{Exemplar_CNN_PAMI, zagoruyko_2015}.
The same idea was used by {\v{Z}}bontar and LeCun~\cite{zbontar_computing_2015} to estimate dense disparity maps between stereo images. 
Computation of dense correspondences with a ConvNet that is trained end-to-end on the task, was presented by Dosovitskiy et al.~\cite{dosovitskiy_flownet_2015}. Mayer et al.~\cite{mayer_sceneflownet_2016} applied the same concept to dense disparity estimation in stereo pairs.
We, too, make use of the FlowNet idea \cite{dosovitskiy_flownet_2015}, but in contrast to \cite{mayer_sceneflownet_2016,zbontar_computing_2015}, the motion between the two views is not fixed, but must be estimated to derive depth estimates.
This makes the learning problem much more difficult. 
\begin{figure*}
\begin{center}
\includegraphics[width=1.0\textwidth]{network_overview.pdf}
\end{center}
\vspace{-0.9em}%
\vspace{\capvspace}%
\caption{%
Overview of the architecture. DeMoN takes an image pair as input and predicts the depth map of the first image and the relative pose of the second camera. The network consists of a chain of encoder-decoder networks that iterate over optical flow, depth, and egomotion estimation; see \fig{fig:encoder_decoder_pair} for details. The refinement network increases the resolution of the final depth map.
}
\label{fig:network_overview}
\end{figure*}

\begin{figure*}
\begin{center}
\includegraphics[page=1,width=\textwidth]{encoder_decoder.pdf}
\end{center}
\vspace{-0.8em}%
\vspace{\capvspace}%
\caption{%
Schematic representation of the encoder-decoder pair used in the bootstrapping and iterative network.
Inputs with gray font are only available for the iterative network.
The first encoder-decoder predicts optical flow and its confidence from an image pair and previous estimates.
The second encoder-decoder predicts the depth map and surface normals. A fully connected network appended to the encoder estimates the camera motion $\vect r, \vect t$ and a depth scale factor $s$.
The scale factor $s$ relates the scale of the depth values to the camera motion.
}
\label{fig:encoder_decoder_pair}
\vspace{\figvspace}
\end{figure*}

Flynn et al.~\cite{flynn_deepstereo_2015} implicitly estimated the 3D structure of a scene from a monocular video using a convolutional network.
They assume known camera poses~-- a large simplification which allows them to use the plane-sweeping approach to interpolate between given views of the scene.
Moreover, they never explicitly predict the depth, only RGB images from intermediate viewpoints.

Agrawal et al.~\cite{agrawal_egomotion_2015} and Jayaraman \& Grauman~\cite{Jayaraman2015egomotion} applied ConvNets to estimating camera motion. The main focus of these works is not on the camera motion itself, but on learning a feature representation useful for recognition. The accuracy of the estimated camera motion is not competitive with classic methods. 
Kendall et al.~\cite{kendall_modelling_2015} trained a ConvNet for camera relocalization~--- predicting the location of the camera within a known scene from a single image.
This is mainly an instance recognition task and requires retraining for each new scene. 
All these works do not provide depth estimates. 


\section{Network Architecture}

The overall network architecture is shown in \fig{fig:network_overview}.
\mbox{DeMoN} is a chain of encoder-decoder networks solving different tasks. 
The architecture consists of three main components: the bootstrap net, the iterative net and the refinement net. 
The first two components are pairs of encoder-decoder networks, where the first one computes optical flow while the second one computes depth and camera motion; see \fig{fig:encoder_decoder_pair}. The iterative net is applied 
recursively to successively refine the estimates of the previous iteration. 
The last component is a single encoder-decoder network that generates the final upsampled and refined depth map.% and camera motion.

\bb{Bootstrap net.}
The bootstrap component gets the image pair as input and outputs the initial depth and motion estimates. 
Internally, first an encoder-decoder network computes optical flow and a confidence map for the flow (the left part of \fig{fig:encoder_decoder_pair}). 
The encoder consists of pairs of convolutional layers with 1D filters in $y$ and $x$-direction.
Using pairs of 1D filters as suggested in \cite{szegedy_rethinking_2015} allows us to use spatially large filter while keeping the number of parameters and runtime manageable.
We gradually reduce the spatial resolution with a stride of $2$ while increasing the number of channels.
The decoder part generates the optical flow estimate from the encoder's representation via a series of up-convolutional layers with stride $2$ followed by two convolutional layers.
It outputs two components of the optical flow field and an estimate of their confidence.
Details on the loss and the training procedure are described in Section~\ref{sec:training}.

The second encoder-decoder, shown in the right part of \fig{fig:encoder_decoder_pair}, takes as input the optical flow, its confidence, the image pair, and the 
second image warped with the estimated flow field.
Based on these inputs it estimates depth, surface normals, and camera motion.
The architecture is the same as above, apart from the extra 3 fully connected layers that compute the camera motion and a scaling factor for the depth prediction.
The latter reflects the inherent connection between depth and motion predictions due to scale ambiguity; see Section~\ref{sec:parametrization}. 

\begin{table}
  \begin{center}
    \setlength{\tabcolsep}{0.1cm}{ \small
    \begin{tabular}{|c|ccc|}
      \hline
       Method              &  L1-inv &  sc-inv &  L1-rel \\
      \hline
      Single image         &   0.080 &   0.159 &   0.696 \\
      Na{\"i}ve image pair &   0.079 &   0.165 &   0.722 \\
      DeMoN                &   \bb{0.012} & \bb{0.131} & \bb{0.097} \\
      
    \hline
    \end{tabular}
    }
  \end{center}
  \vspace{0.4em}%
  \vspace{\capvspace}%
  \caption{%
Na{\"i}ve two-frame depth estimation does not perform better than depth from a single image on any of the error measures (smaller is better). 
The architecture of DeMoN forces the network to use both images, yielding a large performance improvement. 
}
  \label{tbl:ablation_naive_two_frame}
  \vspace{\figvspace}
  \vspace{-0.2em}
\end{table}

By feeding optical flow estimate into the second encoder-decoder we let it make use of motion parallax.
\tab{tbl:ablation_naive_two_frame} shows that an encoder-decoder network trained to estimate depth and camera motion  directly from an image pair (na{\"i}ve image pair) fails to make use of stereo cues and performs on par with a single-image network.  
DeMoN, on the other hand, performs significantly better.


\bb{Iterative net.}
The iterative net is trained to improve existing depth, normal, and motion estimates.
The architecture of this encoder-decoder pair is identical to the bootstrap net, but it takes additional inputs. 
We convert the depth map and camera motion estimated by the bootstrap net or a previous iteration of the iterative net into an optical flow field, and feed it into the first encoder-decoder together with other inputs.
Likewise, we convert the optical flow to a depth map using the previous camera motion prediction and pass it along with the optical flow to the second encoder-decoder.
In both cases the networks are presented with a prediction proposal generated from the predictions of the previous encoder-decoder.

\fig{fig:predictions_iterations} shows how the optical flow and depth improve with each iteration of the network. 
The iterations enable sharp discontinuities, improve the scale of the depth values, and can even correct wrong estimates of the initial bootstrapping network.
The improvements largely saturate after 3 or 4 iterations.
Quantitative analysis is shown in the supplementary material.

During training we simulate 4 iterations by appending predictions from previous training iterations to the minibatch.
Unlike unrolling, there is no backpropagation of the gradient through iterations.
Instead, the gradient of each iteration is described by the losses on the well defined network outputs:  optical flow, depth, normals, and camera motion.
Compared to backpropagation through time this saves a lot of memory and allows us to have a larger network and more iterations. 
\begin{NEW}
A similar approach was taken by Li~\etal~\cite{li_iterative_2016}, who train each iteration in a separate step and therefore need to store predictions as input for the next stage.
We also train the first iteration on its own, but then train all iterations jointly which avoids intermediate storage.
\end{NEW}

\bb{Refinement net.}
While the previous network components operate at a reduced resolution of $64\times48$ to save parameters and to reduce training and test time, the final refinement net upscales the predictions to the full input image resolution ($256\times192$).  
It gets as input the full resolution first image and the nearest-neighbor-upsampled depth and normal field.
\fig{fig:refinement_comparison} shows the low-resolution input and the refined high-resolution output.

A forward pass through the network with 3 iterations takes 110ms on an Nvidia GTX Titan X. 
Implementation details and exact network definitions of all network components are provided in the supplementary material.



\begin{figure}
\begin{center}
  \begin{tikzpicture}[scale=.25]
    \node at (5.0,3) {\small Bootstrap};
    \node at (11.2,3) {\small Iterative 1};
    \node at (17.4,3) {\small 2};
    \node at (23.6,3) {\small 3};
    \node at (29.8,3) {\small GT};
    \node at (1.2,0) [rotate=90] {\small Depth};
    \node at (5,0) {\includegraphics[width=0.085\textwidth]{png/iter/depth/34_iter0_predicted_depth.png}};
    \node at (11.2,0) {\includegraphics[width=0.085\textwidth]{png/iter/depth/34_iter1_predicted_depth.png}};
    \node at (17.4,0) {\includegraphics[width=0.085\textwidth]{png/iter/depth/34_iter2_predicted_depth.png}};
    \node at (23.6,0) {\includegraphics[width=0.085\textwidth]{png/iter/depth/34_iter3_predicted_depth.png}};
    \node at (29.8,0) {\includegraphics[width=0.085\textwidth]{png/iter/depth/34_depth.png}};
    \node at (1.2,-4.6) [rotate=90] {\small Flow};
    \node at (5.0,-4.6) {\includegraphics[width=0.085\textwidth]{png/iter/flow/34_x_iter0_predicted_flow.png}};
    \node at (11.2,-4.6) {\includegraphics[width=0.085\textwidth]{png/iter/flow/34_x_iter1_predicted_flow.png}};
    \node at (17.4,-4.6) {\includegraphics[width=0.085\textwidth]{png/iter/flow/34_x_iter2_predicted_flow.png}};
    \node at (23.6,-4.6) {\includegraphics[width=0.085\textwidth]{png/iter/flow/34_x_iter3_predicted_flow.png}};
    \node at (29.8,-4.6) {\includegraphics[width=0.085\textwidth]{png/iter/flow/34_x_flow.png}};
  \end{tikzpicture}
\end{center}
\vspace{\capvspace}%
\caption{
\bb{Top:} Iterative depth refinement. 
The bootstrap net fails to accurately estimate the scale of the depth.
The iterations refine the depth prediction and strongly improve the scale of the depth values. 
The L1-inverse error drops from $0.0137$ to $0.0072$ after the first iteration.  
\bb{Bottom:} Iterative refinement of optical flow.
Images show the $x$ component of the optical flow for better visibility. 
The flow prediction of the bootstrap net misses the object completely.
Motion edges are retrieved already in the first iteration and the endpoint error is reduced from 0.0176 to 0.0120.
}
\label{fig:predictions_iterations}
\vspace{\figvspace}
\end{figure}


\begin{figure}
\begin{center}
  \incgraphicsabove[0mm]{width=0.15\textwidth}{png/refine/42_iter3_predicted_depth.png}{\small Prediction}
  \incgraphicsabove[0mm]{width=0.15\textwidth}{png/refine/42_iter3_refined_predicted_depth.png}{\small Refined prediction}
  \incgraphicsabove[0mm]{width=0.15\textwidth}{png/refine/42_depth.png}{\small Ground Truth}
\end{center}
\vspace{\capvspace}%
\caption{%
The refinement net generates a high-resolution depth map ($256\times192$) from the low-resolution estimate ($64\times48$) and the input image.
The depth sampling preserves depth edges and can even repair wrong depth measurements. %\TODO{put sculpture image with repaired hole here.. Jonas: no, that one has too much (very!) bad background. Any other good examples?}
}
\label{fig:refinement_comparison}
\vspace{\figvspace}
\end{figure}



\section{Depth and Motion Parameterization}
\label{sec:parametrization}
The network computes the depth map in the first view and the camera motion to the second view.
We represent the relative pose of the second camera with $\vect r, \vect t \in \mathbb R^3$.
The rotation $\vect r = \theta \vect v$ is an angle axis representation with angle $\theta$ and axis $\vect v$.
The translation $\vect t$ is given in Cartesian coordinates.

It is well-known that the reconstruction of a scene from images with unknown camera motion can be determined only up to scale.
We resolve the scale ambiguity by normalizing translations and depth values such that $\Vert \vect t \Vert = 1$.
This way the network learns to predict a unit norm translation vector.


Rather than the depth $z$, the network estimates the inverse depth $\xi = 1/z$.
The inverse depth allows representation of points at infinity and accounts for the growing localization uncertainty of points with increasing distance.% with a smaller penalty.
To match the unit translation, our network predicts a scalar scaling factor $s$, which we use to obtain the final depth values $s\xi$.



\section{Training Procedure}
\label{sec:training}

\subsection{Loss functions}

The network estimates outputs of very different nature: high-dimensional (per-pixel) depth maps and low-dimensional camera motion vectors. 
The loss has to balance both of these objectives, and stimulate synergy of the two tasks without over-fitting to a specific scenario. 

\bb{Point-wise losses.}
We apply point-wise losses to our outputs: inverse depth $\xi$, surface normals $\vect n$, optical flow $\vect w$, and optical flow confidence $\vect c$.
For depth we use an L1 loss directly on the inverse depth values:
\begin{equation}
\label{eq:loss_depth}
\loss_\text{depth} = \textstyle{\sum_{i,j}} \vert s\xi(i,j) - \hat{\xi}(i,j) \vert, 
\end{equation}
with ground truth $\hat\xi$.
Note that we apply the predicted scale $s$ to the predicted values $\xi$.

For the loss function of the normals and the optical flow we use the (non-squared) L2 norm to penalize deviations from the respective ground truths $\hat{\vect n}$ and $\hat{\vect w}$
\begin{equation}
\begin{aligned}
\loss_\text{normal} &= \textstyle{\sum_{i,j}} \left\Vert \vect n(i,j) - \hat{\vect n}(i,j) \right\Vert_2 \\
\loss_\text{flow} &= \textstyle{\sum_{i,j}} \left\Vert \vect w(i,j) - \hat{\vect w}(i,j) \right\Vert_2.
\end{aligned}
\end{equation}
For optical flow this amounts to the usual endpoint error.




 We train the network to assess the quality of its own flow prediction by predicting a confidence map for each optical flow component.
 The ground truth of the confidence for the x component is 
 \begin{equation}
 \hat{c}_x(i,j) = e^{- \vert\vect w_x(i,j) - \hat{\vect w}_x(i,j)\vert},
 \end{equation}
 and the corresponding loss function reads as
 \begin{equation}
 \loss_\text{flow confidence} = \textstyle{\sum_{i,j}} \left\vert c_x(i,j) - \hat{c}_x(i,j) \right\vert.
 \end{equation}

\bb{Motion losses.}
We use a minimal parameterization of the camera motion with 3 parameters for rotation $\vect r$ and translation $\vect t$ each.
The losses for the motion vectors are
\begin{equation}
\begin{aligned}
\loss_\text{rotation} &= \Vert \vect r - \hat{\vect r} \Vert_2 \\
\loss_\text{translation} &= \Vert \vect t - \hat{\vect t} \Vert_2.
\end{aligned}
\end{equation}
The translation ground truth is always normalized such that $\Vert \hat{\vect t} \Vert_2 = 1$, while the magnitude of $\hat{\vect r}$ encodes the angle of the rotation.

\bb{Scale invariant gradient loss.}
We define a discrete scale invariant gradient $\vect g$ as
\begin{equation}
\vect g_h[f](i,j) = \left( \tfrac{f(i+h,j) - f(i,j)}{\vert f(i+h,j) \vert + \vert f(i,j) \vert}, \tfrac{f(i,j+h) - f(i,j)}{\vert f(i,j+h) \vert + \vert f(i,j) \vert} \right)^\top.
\end{equation}
Based on this gradient we define a scale invariant loss that penalizes relative depth errors between neighbouring pixels:
\begin{equation}
\label{eq:loss_grad}
\loss_{\text{grad}\, \xi} = \sum_{h \in \{1,2,4,8,16\}} \sum_{i,j} \left\Vert \vect g_h[\xi](i,j) - \vect g_h[\hat\xi](i,j)\right\Vert_2.
\end{equation}
To cover gradients at different scales we use 5 different spacings $h$.
This loss stimulates the network to compare depth values within a local neighbourhood for each pixel.
It emphasizes depth discontinuities, stimulates sharp edges in the depth map and increases smoothness within homogeneous regions as seen in \fig{fig:gradient_comparison}.
Note that due to the relation $\vect g_h[\xi](i,j) = - \vect g_h[z](i,j)$ for $\xi, z > 0$, the loss is the same for the actual non-inverse depth values $z$.

We apply the same scale invariant gradient loss to each component of the optical flow.
This enhances the smoothness of estimated flow fields and the sharpness of motion discontinuities.

\bb{Weighting.} We individually weigh the losses to 
balance their importance.
The weight factors were determined empirically and are listed in the supplementary material.


\subsection{Training Schedule}
The network training is based on the Caffe framework~\cite{jia_caffe:_2014}. We train our model from scratch with Adam~\cite{kingma_adam:_2014} using a momentum of 0.9 and a weight decay of 0.0004. 
The whole training procedure consists of three phases.%: bootstrap, iterative and refinement training.

First, we sequentially train the four encoder-decoder components in both bootstrap and iterative nets for 250k iterations each with a batch size of 32.
While training an encoder-decoder we keep the weights for all previous components fixed.
For encoder-decoders predicting optical flow, the scale invariant loss is applied after 10k iterations.


Second, we train only the encoder-decoder pair of the iterative net.
In this phase we append outputs from previous three training iterations to the minibatch.
In this phase the bootstrap net uses batches of size 8. 
The outputs of the previous three network iterations are added to the batch, which yields a total batch size of 32 for the iterative network. 
We run 1.6 million training iterations.

Finally, the refinement net is trained for 600k iterations with all other weights fixed. 
The details of the training process, including the learning rate schedules, are provided in the supplementary material.

\section{Experiments}


\subsection{Datasets}

\bb{SUN3D} \cite{xiao_sun3d_2013} provides a diverse set of indoor images together with depth and camera pose.
The depth and camera pose on this dataset are not perfect.
Thus, we sampled image pairs from the dataset and automatically discarded pairs with a high photoconsistency error. We split the dataset so that the same scenes do not appear in both the training and the test set.

\bb{RGB-D SLAM} \cite{sturm12iros} provides high quality camera poses obtained with an external motion tracking system.
Depth maps are disturbed by measurement noise, and we use the same preprocessing as for SUN3D. We created a training and a test set. 

\bb{MVS} includes several outdoor datasets. We used the Citywall and Achteckturm datasets from \cite{fuhrmann2014mve} and the Breisach dataset \cite{UB15} for training and the datasets provided with COLMAP \cite{schoenberger2016sfm,schoenberger2016mvs} for testing. The depth maps of the reconstructed scenes are often sparse and can comprise reconstruction errors.

\bb{Scenes11} is a synthetic dataset with generated images of virtual scenes with random geometry, which provide perfect depth and motion ground truth, but lack realism. 

Thus, we introduce the \bb{Blendswap} dataset which is based on 150 scenes from \texttt{blendswap.com}.
The dataset provides a large variety of scenes, ranging from cartoon-like to photorealistic scenes.
The dataset contains mainly indoor scenes. We used this dataset only for training. 

\bb{NYUv2} \cite{NYU} provides depth maps for diverse indoor scenes but lacks camera pose information.
We did not train on NYU and used the same test split as in Eigen et al.~\cite{eigen_predicting_2015}.
In contrast to Eigen et al., we also require a second input image that should not be identical to the previous one. Thus, we automatically chose the next image that is sufficiently different from the first image according to a threshold on the difference image.

In all cases where the surface normals are not available, we generated them from the depth maps.
We trained DeMoN specifically for the camera intrinsics used in SUN3D and adapted all other datasets by cropping and scaling to match these parameters.

\subsection{Error metrics}
While single-image methods aim to predict depth at the actual physical scale, two-image methods typically yield the scale relative to the norm of the camera translation vector.
Comparing the results of these two families of methods requires a scale-invariant error metric.
We adopt the scale-invariant error of \cite{eigen_predicting_2014}, which is defined as
\begin{equation}
\textstyle \text{sc-inv}(z,\hat{z}) = \sqrt{\tfrac{1}{n}\sum_i d_i^2 - \tfrac{1}{n^2} \left(\sum_i d_i\right)^2},
\end{equation}
with $ d_i = \log z_i - \log \hat{z}_i$.
For comparison with classic structure from motion methods we use the following measures:
\begin{align}
\text{L1-rel}(z,\hat{z}) &= \tfrac{1}{n} \textstyle{\sum_i} \frac{\vert z_i - \hat{z}_i\vert}{\hat{z}_i}\\
\text{L1-inv}(z,\hat{z}) &= \tfrac{1}{n} \textstyle{\sum_i} \vert \xi_i - \hat{\xi}_i\vert = \tfrac{1}{n} \textstyle{\sum_i} \left\vert \frac{1}{z_i} - \frac{1}{\hat{z}_i}\right\vert 
\end{align}
L1-rel computes the depth error relative to the ground truth depth and therefore reduces errors where the ground truth depth is large and increases the importance of close objects in the ground truth.
L1-inv behaves similarly and resembles our loss function for predicted inverse depth values \eqref{eq:loss_depth}.

For evaluating the camera motion estimation, we report the angle (in degrees) between the prediction and the ground truth for both the translation and the rotation. 
The length of the translation vector is $1$ by definition.

The accuracy of optical flow is measured by the average endpoint error (EPE), that is, the Euclidean norm of the difference between the predicted and the true flow vector, averaged over all image pixels. The flow is scaled such that the displacement by the image size corresponds to $1$.


\subsection{Comparison to classic structure from motion}

We compare to several strong baselines implemented by us from state-of-the-art components (``Base-*'').
For these baselines, we estimated correspondences between images, either by matching SIFT keypoints (``Base-SIFT'') or with the FlowFields  optical flow method from Bailer et al.~\cite{bailer_flow_2015} (``Base-FF'').
Next, we computed the essential matrix with the normalized 8-point algorithm~\cite{hartley_defense_1997} and RANSAC.
To further improve accuracy we minimized the reprojection error using the \emph{ceres} library \cite{agarwal_ceres}.
Finally, we generated the depth maps by plane sweep stereo and used the approach of Hirschmueller et al.~\cite{hirschmuller_accurate_2005} for optimization.
We also report the accuracy of the depth estimate when providing the ground truth camera motion (``Base-Oracle'').
(``Base-Matlab'') and (``Base-Mat-F'') are implemented in Matlab. (``Base-Matlab'') uses Matlab implementations of the KLT algorithm \cite{Tomasi91detectionand,lucas_iterative_1981,Shi_1994_3266} for correspondence search while (``Base-Mat-F'') uses the predicted flow from DeMoN. The essential matrix is computed with RANSAC and the 5-point algorithm \cite{nister_efficient_2004} for both.


\begin{table}
  \begin{minipage}{0.73\linewidth}
  \begin{flushleft}
    \setlength{\tabcolsep}{0.1cm}{ \scriptsize
    \begin{tabular}{|c|c|ccc|cc|}
      \hline
      
                                                                           &             & \multicolumn{3}{c|}{Depth}             & \multicolumn{2}{c|}{Motion}    \\           
      \hline                                                               
                                                                           & Method      &  L1-inv      &  sc-inv    &  L1-rel    & rot        & trans             \\
      \hline                                                                                                                                                       
      \hline                                                                                                                                                       
      \parbox[t]{2mm}{\multirow{6}{*}{\rotatebox[origin=c]{90}{MVS}}}      & Base-Oracle & \bb{0.019}   & \bb{0.197} & \bb{0.105} &  0         &  0                \\
                                                                           & Base-SIFT   &   0.056      &   0.309    &   0.361    &  21.180    &  60.516           \\
                                                                           & Base-FF     &   0.055      &   0.308    &   0.322    & \bb{4.834} &  17.252           \\
                                                                           & Base-Matlab &     -        &     -      &     -      &  10.843    &  32.736           \\
                                                                           & Base-Mat-F  &     -        &     -      &     -      &   5.442    &  18.549
                                                                           \\                                                                           
                                                                           & DeMoN       &   0.047      &   0.202    &   0.305    &   5.156    &  \bb{14.447}      \\
      \hline                                                                                                                                                       
      \hline                                                                                                                                                       
      \parbox[t]{2mm}{\multirow{6}{*}{\rotatebox[origin=c]{90}{Scenes11}}} & Base-Oracle &   0.023      &   0.618    &   0.349    &  0         &  0                \\
                                                                           & Base-SIFT   &   0.051      &   0.900    &   1.027    &  6.179     &  56.650           \\
                                                                           & Base-FF     &   0.038      &   0.793    &   0.776    &  1.309     &  19.425           \\
                                                                           & Base-Matlab &     -        &     -      &     -      &  0.917     &  14.639           \\
                                                                           & Base-Mat-F  &     -        &     -      &     -      &  2.324     &  39.055
                                                                           \\                                                                           
                                                                           & DeMoN       &   \bb{0.019} & \bb{0.315} & \bb{0.248} & \bb{0.809} &  \bb{8.918}       \\
      \hline                                                                                                                                                       
      \hline                                                                                                                                                       
      \parbox[t]{2mm}{\multirow{6}{*}{\rotatebox[origin=c]{90}{RGB-D}}}    & Base-Oracle &  \bb{0.026}  &   0.398    &   0.336    &  0         &  0                \\
                                                                           & Base-SIFT   &   0.050      &   0.577    &   0.703    & 12.010     &  56.021           \\
                                                                           & Base-FF     &   0.045      &   0.548    &   0.613    &  4.709     &  46.058           \\
                                                                           & Base-Matlab &      -       &     -      &     -      &  12.831    &  49.612
                                                                           \\
                                                                           & Base-Mat-F  &     -        &     -      &     -      &   2.917    &  22.523
                                                                           \\
                                                                           & DeMoN       &   0.028      & \bb{0.130} & \bb{0.212} & \bb{2.641} &  \bb{20.585}      \\
      \hline                                                                                                                                                       
      \hline                                                                                                                                                                      
      \parbox[t]{2mm}{\multirow{6}{*}{\rotatebox[origin=c]{90}{Sun3D}}}    & Base-oracle &   0.020      &   0.241    &   0.220    & 0          &  0                \\
                                                                           & Base-SIFT   &   0.029      &   0.290    &   0.286    & 7.702      &  41.825           \\
                                                                           & Base-FF     &   0.029      &   0.284    &   0.297    & 3.681      &  33.301           \\
                                                                           & Base-Matlab &      -       &     -      &     -      & 5.920      &  32.298  
                                                                           \\
                                                                           & Base-Mat-F  &     -        &     -      &     -      &   2.230    &  26.338
                                                                           \\
                                                                           & DeMoN       &   \bb{0.019} & \bb{0.114} & \bb{0.172} & \bb{1.801} &  \bb{18.811}      \\
      \hline
      \hline                                                                                                                                                                      
      \parbox[t]{2mm}{\multirow{6}{*}{\rotatebox[origin=c]{90}{NYUv2}}}    & Base-oracle &     -        &     -      &     -      &     -      &    -              \\
                                                                           & Base-SIFT   &     -        &     -      &     -      &     -      &    -              \\
                                                                           & Base-FF     &     -        &     -      &     -      &     -      &    -              \\
                                                                           & Base-Matlab &     -        &     -      &     -      &     -      &    -             
                                                                           \\
                                                                           & Base-Mat-F  &     -        &     -      &     -      &     -      &    -
                                                                           \\
                                                                           & DeMoN       &     -        &     -      &     -      &     -      &    -              \\
      \hline

    \end{tabular}
    }
  \end{flushleft}
  \end{minipage}
  \begin{minipage}{0.15\linewidth}
  \begin{flushright}
    \vspace*{-.033cm}%
    \setlength{\tabcolsep}{0.1cm}{ \scriptsize
      \begin{tabular}{|c|c|}
      \hline
                  & Depth        \\
      \hline
      Method      & sc-inv       \\
      \hline                     
      \hline                     
                  &              \\
                  &              \\
      Liu indoor  &  0.260       \\
      Liu outdoor &  0.341       \\
      Eigen VGG   &  0.225       \\
      DeMoN       & \bb{0.203}   \\
      \hline                     
      \hline   
                  &              \\
                  &              \\
      Liu indoor  &  0.816       \\
      Liu outdoor &  0.814       \\
      Eigen VGG   &  0.763       \\
      DeMoN       & \bb{0.303}   \\
      \hline                     
      \hline  
                  &              \\
                  &              \\
      Liu indoor  &  0.338       \\
      Liu outdoor &  0.428       \\
      Eigen VGG   &  0.272       \\
      DeMoN       & \bb{0.134}   \\
      \hline                     
      \hline  
                  &              \\
                  &              \\
      Liu indoor  &  0.214       \\
      Liu outdoor &  0.401       \\
      Eigen VGG   &  0.175       \\
      DeMoN       & \bb{0.126}   \\
      \hline
      \hline 
                  &              \\
                  &              \\
      Liu indoor  &  0.210       \\
      Liu outdoor &  0.421       \\
      Eigen VGG   & \bb{0.148}   \\
      DeMoN       &  0.180       \\
      \hline
    \end{tabular}
    }
  \end{flushright}
  \end{minipage}
  \vspace{0.3cm}
  \caption{
\bb{Left:} Comparison of two-frame depth and motion estimation methods. 
Lower is better for all measures. 
For a fair comparison with the baseline methods, we evaluate depth only at pixels visible in both images. 
For Base-Matlab depth is only available as a sparse point cloud and is therefore not compared to here.
We do not report the errors on NYUv2 since motion ground truth (and therefore depth scale) is not available. 
\bb{Right:} Comparison to single-frame depth estimation. Since the scale estimates are not comparable, we report only the scale invariant error metric.
}
  \label{tbl:results_unscaled}
\vspace{\figvspace}
\end{table}

\tab{tbl:results_unscaled} shows that DeMoN outperforms all baseline methods both on motion and depth accuracy by a factor of $1.5$ to $2$ on most datasets. 
The only exception is the MVS dataset where the motion accuracy of DeMoN is on par with the strong baseline based on FlowFields optical flow.
This demonstrates that traditional methods work well on the texture rich scenes present in MVS, but do not perform well for example on indoor scenes, with large homogeneous regions or small baselines where priors may be very useful.
Besides that, all Base-* methods use images at the full resolution of $640\times480$, while our method uses downsampled images of $256\times192$ as input.
\begin{NEW}
Higher resolution gives the Base-* methods an advantage in depth accuracy, but on the other hand these methods are more prone to outliers.
For detailed error distributions see the supplemental material.
\end{NEW}
Remarkably, on all datasets except for MVS the depth estimates of DeMoN are better than the ones a traditional approach can produce given the ground truth motion.
This is supported by qualitative results in \fig{fig:depth_comparison}.
We also note that DeMoN has smaller motion errors than (``Base-Mat-F''), showing its advantage over classical methods in motion estimation.

In contrast to classical approaches, we can also handle cases without and with very little camera motion, see Fig.~\ref{fig:small_baseline}.
  We used our network to compute camera trajectories by simple concatenation of the motion of consecutive frames, as shown in \fig{fig:trajectory_teddy}.
The trajectory shows mainly translational drift. 
We also did not apply any drift correction which is a crucial component in SLAM systems, but results convince us that DeMoN can be integrated into such systems.



\begin{figure}
\begin{center}
  \begin{tikzpicture}[scale=.25]
    \node at (0,0) {\includegraphics[width=0.11\textwidth]{png/small_baseline/IMG_4352.png}};
    \node at (8,0) {\includegraphics[width=0.11\textwidth]{png/small_baseline/IMG_4352_IMG_4352_norm_img_2_ref_scaled.png}};
    \node at (16,0) {\includegraphics[width=0.11\textwidth]{png/small_baseline/IMG_4352_IMG_4354_norm_img_2_ref.png}};
    \node at (24,0) {\includegraphics[width=0.11\textwidth]{png/small_baseline/IMG_4352_IMG_4356_norm_img_2_ref.png}};
    \node at (8,-6.1) {\includegraphics[width=0.11\textwidth]{png/small_baseline/IMG_4352.png}};
    \node at (16,-6.1) {\includegraphics[width=0.11\textwidth]{png/small_baseline/IMG_4354.png}};
    \node at (24,-6.1) {\includegraphics[width=0.11\textwidth]{png/small_baseline/IMG_4356.png}};
    \draw (-2.5,-7.5) -- ++(6,4);
    \node at (-0.9,-3.8) {\small Reference};
    \node at (3.1,-6.6) [rotate=90] {\small Second};
  \end{tikzpicture}
\end{center}
\vspace{\capvspace}%
\caption{%
Qualitative performance gain by increasing the baseline between the two input images for DeMoN. The depth map is produced with the top left reference image and the second image below. The first output is obtained with two identical images as input, which is a degenerate case for traditional structure from motion. 
}
\label{fig:small_baseline}
\vspace{\figvspace}%
\end{figure}


\begin{figure}
  \begin{center}
    \includegraphics[width=\linewidth]{png/trajectory/teddy/teddy-trajectory.png}\\
    \vspace{-0.5em}
    \small First frame \hspace*{1.3cm} Frontal view \hspace*{1.1cm} Top view
  \end{center}
\vspace{\capvspace}%
\caption{%
Result on a sequence of the RGB-D SLAM dataset~\cite{sturm12iros}. 
The accumulated pairwise pose estimates by our network (red) are locally consistent with the ground truth trajectory (black).
The depth prediction of the first frame is shown. 
The network also separates foreground and background in its depth output.}
  \label{fig:trajectory_teddy}
\vspace{\figvspace}
\vspace{-0.3em}
\end{figure}



\subsection{Comparison to depth from single image}

To demonstrate the value of the motion parallax, we additionally compare to the single-image depth estimation methods by Eigen \& Fergus~\cite{eigen_predicting_2015} and Liu et al.~\cite{liu_learning_2015}.
We compare to the improved version of the Eigen \& Fergus method, which is based on the VGG network architecture, and to two models by Liu et al.: one trained on indoor scenes from the NYUv2 dataset (``indoor'') and another, trained on outdoor images from the Make3D dataset~\cite{Saxena05learningdepth} (``outdoor'').

The comparison in \fig{fig:depth_comparison} shows that the depth maps produced by DeMoN are more detailed and more regular than the ones produced by other methods. This becomes even more obvious when the results are visualized as a point cloud; see the videos in the supplemental material. 

On all but one dataset, DeMoN outperforms the single-frame methods also by numbers, typically by a large margin. Notably, a large improvement can be observed even on the indoor datasets, Sun3D and RGB-D, showing that the additional stereopsis complements the other cues that can be learned from the large amounts of training data available for this scenario.
Only on the NYUv2 dataset, DeMoN is slightly behind the method of Eigen \& Fergus. 
This is because the comparison is not totally fair: the network of Eigen \& Fergus as well as Liu indoor was trained on the training set of NYUv2, whereas the other networks have not seen this kind of data before. 



\begin{figure}
\begin{center}
\begin{tikzpicture}[xscale=1.55,yscale=1.18]
  \node at (0,4.75) {\small Image};
  \node at (1,4.75) {\small GT};
  \node at (2,4.75) {\small Base-O};
  \node at (3.005,4.75) {\small Eigen};
  \node at (4.01,4.75) {\small DeMoN};

  \node at (-.7,4) [rotate=90] {\footnotesize Sun3D};
  \node at (-.7,3) [rotate=90] {\footnotesize RGBD};
  \node at (-.7,2) [rotate=90] {\footnotesize MVS};
  \node at (-.7,1) [rotate=90] {\footnotesize Scenes11};
  \node at (-.7,0) [rotate=90] {\footnotesize NYUv2};

  \node at (0,4) {\includegraphics[width=0.088\textwidth]{png/comp/ours/sun3d/36_a_image_pair.png}};
  \node at (1,4) {\includegraphics[width=0.088\textwidth]{png/comp/ours/sun3d/36_depth.png}};
  \node at (2,4) {\includegraphics[width=0.088\textwidth]{png/comp/sfm/sun3d/36_predicted_depth.png}};
  \node at (3.005,4) {\includegraphics[width=0.089\textwidth]{png/comp/eigen/sun3d/20_predicted_depth.png}};
  \node at (4.01,4) {\includegraphics[width=0.088\textwidth]{png/comp/ours/sun3d/36_iter3_refined_predicted_depth.png}};
85
  \node at (0,3) {\includegraphics[width=0.088\textwidth]{png/comp/ours/rgbd_new/0_a_image_pair.png}};
  \node at (1,3) {\includegraphics[width=0.088\textwidth]{png/comp/ours/rgbd_new/0_depth.png}};
  \node at (2,3) {\includegraphics[width=0.088\textwidth]{png/comp/sfm/rgbd_new/0_predicted_depth.png}};
  \node at (3.005,3) {\includegraphics[width=0.089\textwidth]{png/comp/eigen/rgbd_new/0_predicted_depth.png}};
  \node at (4.01,3) {\includegraphics[width=0.088\textwidth]{png/comp/ours/rgbd_new/0_iter3_refined_predicted_depth.png}};
85
  \node at (0,2) {\includegraphics[width=0.088\textwidth]{png/comp/ours/mvs/5_a_image_pair.png}};
  \node at (1,2) {\includegraphics[width=0.088\textwidth]{png/comp/ours/mvs/5_depth.png}};
  \node at (2,2) {\includegraphics[width=0.088\textwidth]{png/comp/sfm/mvs/5_predicted_depth.png}};
  \node at (3.005,2) {\includegraphics[width=0.089\textwidth]{png/comp/eigen/mvs/5_predicted_depth.png}};
  \node at (4.01,2) {\includegraphics[width=0.088\textwidth]{png/comp/ours/mvs/5_iter3_refined_predicted_depth.png}};
85
  \node at (0,1) {\includegraphics[width=0.088\textwidth]{png/comp/ours/random/19_a_image_pair.png}};
  \node at (1,1) {\includegraphics[width=0.088\textwidth]{png/comp/ours/random/19_depth.png}};
  \node at (2,1) {\includegraphics[width=0.088\textwidth]{png/comp/sfm/random/19_predicted_depth.png}};
  \node at (3.005,1) {\includegraphics[width=0.089\textwidth]{png/comp/eigen/random/19_predicted_depth.png}};
  \node at (4.01,1) {\includegraphics[width=0.088\textwidth]{png/comp/ours/random/19_iter3_refined_predicted_depth.png}};
85
  \node at (0,0) {\includegraphics[width=0.088\textwidth]{png/nyu_clouds/303_a_image_pair.png}};
  \node at (1,0) {\includegraphics[width=0.088\textwidth]{png/nyu_clouds/303_depth.png}};
  \node at (2,0) {\includegraphics[width=0.088\textwidth]{png/comp/sfm/nyu/373_predicted_depth.png}};
  \node at (3.005,0) {\includegraphics[width=0.089\textwidth]{png/nyu_clouds/303_predicted_depth_eigen.png}};
  \node at (4.01,0) {\includegraphics[width=0.088\textwidth]{png/nyu_clouds/303_iter3_refined_predicted_depth_demon.png}};
\end{tikzpicture}
\end{center}
\vspace{-0.4em}%
\vspace{\capvspace}%
\caption{
\bb{Top:} Qualitative depth prediction comparison on various datasets. The predictions of DeMoN are very sharp and detailed.
The Base-Oracle prediction on NYUv2 is missing because the motion ground truth is not available. 
Results on more methods and examples are shown in the supplementary material. %\bb{Bottom:} Visualization of the points clouds on NYUv2 example.
}
\label{fig:depth_comparison}
\vspace{\figvspace}
\vspace{-0.5em}
\end{figure}





\vspace{-0.3em}%
\subsubsection{Generalization to new data}
\label{sec:generalization}


Scene-specific priors learned during training may be useless or even harmful when being confronted with a scene that is very different from the training data. In contrast, the geometric relations between a pair of images are independent of the content of the scene and should generalize to unknown scenes. 
To analyze the generalization properties of DeMoN, we compiled a small dataset of images showing uncommon or complicated scenes, for example abstract sculptures, close-ups of people and objects, images rotated by 90 degrees.% More details are provided in the supplementary material.

\fig{fig:generalization} and \tab{tbl:generalization} show that DeMoN, as to be expected, generalizes better to these unexpected scenes than single-image methods. It shows that the network has learned to make use of the motion parallax. 


\begin{figure}
\begin{center}
\incgraphicsabove{width=0.09\textwidth,trim={0.04\Width} {0.045\Height} {0.04\Width} {0.045\Height},clip}{png/generalization/rgb_img/IMG_1285.png}{\footnotesize Image}
\incgraphicsabove{width=0.09\textwidth,trim={0.04\Width} {0.045\Height} {0.04\Width} {0.045\Height},clip}{png/generalization/gt_depth/IMG_1285_norm_img.png}{\footnotesize GT}
\incgraphicsabove{width=0.09\textwidth}{png/generalization/eigen/IMG_1285_norm_img_0_scaled.png}{\footnotesize Eigen}
\incgraphicsabove{width=0.09\textwidth,trim={0.04\Width} {0.045\Height} {0.04\Width} {0.045\Height},clip}{png/generalization/liu/IMG_1285_norm_img_0.png}{\footnotesize Liu}
\incgraphicsabove{width=0.09\textwidth,trim={0.04\Width} {0.045\Height} {0.04\Width} {0.045\Height},clip}{png/generalization/ours_refined/IMG_1285_IMG_1281_norm_img_2_ref.png}{\footnotesize DeMoN}\\
\vspace{0.1em}
\adjustimage{width=0.09\textwidth,trim={0.04\Width} {0.045\Height} {0.04\Width} {0.045\Height},clip}{png/generalization/rgb_img/IMG_4022.png}
\adjustimage{width=0.09\textwidth,trim={0.04\Width} {0.045\Height} {0.04\Width} {0.045\Height},clip}{png/generalization/gt_depth/IMG_4022_norm_img.png}
\adjustimage{width=0.09\textwidth}{png/generalization/eigen/IMG_4022_norm_img_0_scaled.png}
\adjustimage{width=0.09\textwidth,trim={0.04\Width} {0.045\Height} {0.04\Width} {0.045\Height},clip}{png/generalization/liu/IMG_4022_norm_img_0.png}
\adjustimage{width=0.09\textwidth,trim={0.04\Width} {0.045\Height} {0.04\Width} {0.045\Height},clip}{png/generalization/ours_refined/IMG_4022_IMG_4023_norm_img_2_ref.png}\\
\vspace{0.1em}
\adjustimage{width=0.09\textwidth,trim={0.04\Width} {0.045\Height} {0.04\Width} {0.045\Height},clip}{png/generalization/rgb_img/IMG_4058.png}
\adjustimage{width=0.09\textwidth,trim={0.04\Width} {0.045\Height} {0.04\Width} {0.045\Height},clip}{png/generalization/gt_depth/IMG_4058_norm_img.png}
\adjustimage{width=0.09\textwidth}{png/generalization/eigen/IMG_4058_norm_img_0_scaled.png}
\adjustimage{width=0.09\textwidth,trim={0.04\Width} {0.045\Height} {0.04\Width} {0.045\Height},clip}{png/generalization/liu/IMG_4058_norm_img_0.png}
\adjustimage{width=0.09\textwidth,trim={0.04\Width} {0.045\Height} {0.04\Width} {0.045\Height},clip}{png/generalization/ours_refined/IMG_4058_IMG_4059_norm_img_2_ref.png}\\
\vspace{0.1em}
\adjustimage{width=0.09\textwidth,trim={0.04\Width} {0.045\Height} {0.04\Width} {0.045\Height},clip}{png/generalization/rgb_img/IMG_4117.png}
\adjustimage{width=0.09\textwidth,trim={0.04\Width} {0.045\Height} {0.04\Width} {0.045\Height},clip}{png/generalization/gt_depth/IMG_4117_norm_img.png}
\adjustimage{width=0.09\textwidth}{png/generalization/eigen/IMG_4117_norm_img_0_scaled.png}
\adjustimage{width=0.09\textwidth,trim={0.04\Width} {0.045\Height} {0.04\Width} {0.045\Height},clip}{png/generalization/liu/IMG_4117_norm_img_0.png}
\adjustimage{width=0.09\textwidth,trim={0.04\Width} {0.045\Height} {0.04\Width} {0.045\Height},clip}{png/generalization/ours_refined/IMG_4117_IMG_4119_norm_img_2_ref.png}\\
\vspace{0.3em}
\newcommand{\incgraphicslabelinsidebottom}[3]{%
  \begin{tikzpicture}%
    \node[inner sep=0pt] (mynode) {\adjustimage{#1}{#2}};%
    \node at (mynode.south) [inner sep=0,scale=1,above=-3mm] {\strut #3};%
  \end{tikzpicture}%
}
\incgraphicslabelinsidebottom{width=0.14\textwidth}{png/generalization/point_clouds/close_benjamin_eigen_white.png}{\footnotesize Eigen}
\incgraphicslabelinsidebottom{width=0.14\textwidth}{png/generalization/point_clouds/close_benjamin_liu_indoor_white.png}{\footnotesize Liu}
\incgraphicslabelinsidebottom{width=0.14\textwidth}{png/generalization/point_clouds/close_benjamin_ours_refined_white.png}{\footnotesize DeMoN}
\end{center}
\vspace{-0.4em}%
\vspace{\capvspace}%
\caption{%
Visualization of DeMoN's generalization capabilities to previously unseen configurations. Single-frame methods have severe problems in such cases, as most clearly visible in the point cloud visualization of the depth estimate for the last example.  %From left to right: RGB image, inverse
}
\label{fig:generalization}
\vspace{\figvspace}
\vspace{0.2em}
\end{figure}


\begin{table}
   \begin{center}
     \setlength{\tabcolsep}{0.06cm}{ \small
     \begin{tabular}{|c|ccc|}
       \hline
       Method             &   L1-inv    &   sc-inv     &   L1-rel     \\
       \hline
       Liu \cite{liu_learning_2015}     &    0.055    &    0.247       &    0.194 \\
       Eigen \cite{eigen_predicting_2015} &    0.062  &    0.238       &    0.185 \\
       DeMoN              & \bf{0.041}  & \bf{0.183}  & \bf{0.130}              \\
       \hline
     \end{tabular}
    }
   \end{center}
\vspace{\capvspace}%
\vspace{0.2em}%
   \caption{%
   Quantitative generalization performance on previously unseen scenes, objects, and camera rotations, 
using a self-recorded and reconstructed dataset. 
Errors after optimal log-scaling. The best model of Eigen et al. \cite{eigen_predicting_2015} for this task is based on VGG, for Liu et al. \cite{liu_learning_2015}, the model trained on Make3D \cite{make3d_2009} performed best. DeMoN achieved best performance after two iterations.
   }%
   \label{tbl:generalization}
\vspace{\figvspace}
\vspace{-0.4em}
 \end{table}






\subsection{Ablation studies}
\label{sec:ablation_study}
Our architecture contains some design decisions that we justify by the following ablation studies. 
All results have been obtained on the Sun3D dataset with the bootstrap net.


\bb{Choice of the loss function.}
\tab{tbl:ablation_loss} shows the influence of the loss function on the accuracy of the estimated depth and motion. Interestingly, while the scale invariant loss greatly improves the prediction qualitatively (see \fig{fig:gradient_comparison}), it has negative effects on depth scale estimation. 
This leads to weak performance on non-scale-invariant metrics and the motion accuracy. 
Estimation of the surface normals slightly improves all results.
Finally, the full architecture with the scale invariant loss, normal estimation, and a loss on the flow, leads to the best results. 

\bb{Flow confidence.}
Egomotion estimation only requires sparse but high-quality correspondences. \tab{tbl:ablation_confidence} shows that given the same flow, egomotion estimation improves when given the flow confidence as an extra input. Our interpretation is that the flow confidence helps finding most accurate correspondences. 

\begin{figure}
\newcommand{\incgraphicslabelinsidetopleft}[3]{%
  \begin{tikzpicture}%
    \node[inner sep=0pt] (mynode) {\adjustimage{#1}{#2}};%
    \node at (mynode.north west) [inner sep=0pt,minimum width=1em,scale=1,below right=-0.5pt,fill=white,fill opacity=0.6, text opacity=1] {\strut #3};%
  \end{tikzpicture}%
}%
\begin{center}
  \incgraphicslabelinsidetopleft{width=0.11\textwidth}{png/grad/nogradient/0_predicted_depth.png}{\small a}
  \incgraphicslabelinsidetopleft{width=0.11\textwidth}{png/grad/normal/0_predicted_depth.png}{\small b}
  \incgraphicslabelinsidetopleft{width=0.11\textwidth}{png/grad/gradient/0_predicted_depth.png}{\small c}
  \incgraphicslabelinsidetopleft{width=0.11\textwidth}{png/grad/gradient/0_depth.png}{\small d}%
\end{center}
\vspace{\capvspace}%
\caption{%
Depth prediction comparison with different outputs and losses.
\bb{(a)} Just L1 loss on the absolute depth values.
\bb{(b)} Additional output of normals and L1 loss on the normals.
\bb{(c)} Like (b) but with the proposed gradient loss.
\bb{(d)} Ground truth.
}
\label{fig:gradient_comparison}
\vspace{\figvspace}
\end{figure}

\begin{table}
  \begin{center}
    \setlength{\tabcolsep}{0.1cm}{ \small
    \begin{tabular}{|ccc||ccc|cc|}
      \hline
            & &  & \multicolumn{3}{|c|}{Depth} & \multicolumn{2}{|c|}{Motion} \\
      \hline
      grad & norm & flow &  L1-inv &  sc-inv &  L1-rel &  rot &  tran  \\
      \hline
      \no    & \no   & \no   &   0.040 &   0.211 &   0.354 &  3.127 &  30.861    \\
      \yes   & \no   & \no   &   0.057 &   0.159 &   0.437 &  4.585 &  39.819   \\
      \no    & \yes  & \no   &   0.037 &   0.190 &   0.336 &  2.570 &  29.607   \\
      \no    & \yes  & \yes  & \bb{0.029} &   0.184 &  \bb{0.266} &  \bb{2.359} &  \bb{23.578}   \\
      \yes   & \yes  & \yes  & 0.032 & \bb{0.150} & 0.276 & 2.479 & 24.372 \\
      \hline
    \end{tabular}
    }
  \end{center}
\vspace{\capvspace}%
\vspace{0.2em}%
  \caption{%
The influence of the loss function on the performance. 
The gradient loss improves the scale invariant error, but degrades the scale-sensitive measures. 
Surface normal prediction improves the depth accuracy. 
A combination of all components leads to the best tradeoff.
  }
  \label{tbl:ablation_loss}
\vspace{\figvspace}
\end{table}

\begin{table}
  \begin{center}
    \setlength{\tabcolsep}{0.1cm}{ \small
    \begin{tabular}{|c||ccc|cc|c|}
      \hline
                 & \multicolumn{3}{c|}{Depth} & \multicolumn{2}{c|}{Motion} & Flow \\
      \hline
      Confidence &  L1-inv &  sc-inv &  L1-rel &  rot &  tran &  EPE  \\
      \hline
      \no    &  0.030 &  0.028 &  0.26 &     2.830 &      25.262 &    0.027 \\
      \yes   &  0.032 &  0.027 &  0.28 &     2.479 &      24.372 &    0.027 \\
    \hline
    \end{tabular}
    }
  \end{center}
\vspace{\capvspace}%
\vspace{0.2em}%
  \caption{The influence of confidence prediction on the overall performance of the different outputs.}
  \label{tbl:ablation_confidence}
\vspace{\figvspace}
\vspace{-0.5em}
\end{table}





\section{Conclusions and Future Work}
DeMoN is the first deep network that has learned to estimate depth and camera motion from two unconstrained images. 
Unlike networks that estimate depth from a single image, DeMoN can exploit the motion parallax, which is a powerful cue that generalizes to new types of scenes, and allows to estimate the egomotion.  
This network outperforms traditional structure from motion techniques on two frames, since in contrast to those, it is trained end-to-end and learns to integrate other shape from X cues. 
When it comes to handling cameras with different intrinsic parameters it has not yet reached the flexibility of classic approaches.
The next challenge is to lift this restriction and extend this work to more than two images.
As in classical techniques, this is expected to significantly improve the robustness and accuracy.


\vspace{-0.7em}
\paragraph{Acknowledgements}
We acknowledge funding by the ERC Starting Grant VideoLearn, the DFG 
grant BR-3815/5-1, and the EU project Trimbot2020.

\clearpage{\small
\bibliographystyle{ieee}
\bibliography{DepthMotionNet}
}

\IfFileExists{./depthmotionnet_supplement.pdf}{%
\clearpage
\includepdf[pages=-]{./depthmotionnet_supplement.pdf}
}{}

\end{document}


