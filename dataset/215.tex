\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{rotating}
\usepackage{multirow}
\newcommand{\ignore}[1]{}


\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission


\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

\title{Towards Hiding Adversarial Examples from Network Interpretation}

\author{
     Akshayvarun Subramanya\footnotemark[1] \qquad
     Vipin Pillai\thanks{Equal contribution} \qquad Hamed Pirsiavash\\
    University of Maryland, Baltimore County (UMBC)\\
    \tt\small{\{akshayv1, vp7,hpirsiav\}@umbc.edu} \\
}


\maketitle

\begin{abstract}

Deep networks have been shown to be fooled rather easily using adversarial attack algorithms.
Practical methods such as adversarial patches have been shown to be extremely effective in causing misclassification. However, these patches can be highlighted using standard network interpretation algorithms, thus revealing the identity of the adversary.
We show that it is possible to create adversarial patches which not only fool the prediction, but also change what we interpret regarding the cause of prediction. We show that our algorithms can empower adversarial patches, by hiding them from network interpretation tools. We believe our algorithms can facilitate developing more robust network interpretation tools that truly explain the network's underlying decision making process.


\end{abstract}

\section{Introduction}
Deep learning has achieved great results in many domains including computer vision. However, it is still far from being deployed in many real-world applications due to reasons including:

{\bf (1) Explainable AI (XAI):} Explaining the prediction of deep networks is a challenging task simply because they are complex models with large number of parameters. Recently, XAI has become a trending research area in which the goal is to develop reliable interpretation algorithms that can explain the underlying decision making process. Designing such algorithms is a challenging task and considerable research \cite{simonyan2013deep,zhou2016learning,selvaraju2016grad} has been done to describe \textit{local explanations} - explaining the model's output for a given input \cite{baehrens2010explain}. We will be focusing on such methods in our work. Most of the these algorithms rely on studying the gradient of the output of a machine learning model with respect to its input.


{\bf (2) Adversarial examples:} Many works have shown that deep networks are vulnerable to adversarial examples, which are carefully constructed samples created by adding imperceptible perturbations to the original input to change the final decision of the network. This is important for two reasons: (a) Such vulnerabilities could be used by adversaries to fool AI algorithms when they are deployed in real-world applications such as Internet of Things (IoT) \cite{mosenia2017comprehensive} or self-driving cars \cite{sitawarin2018darts}. (b) Studying these attacks can lead to better understanding of how deep networks work and possibly improve generalization on new environments.

\begin{figure*}[!t]
  \begin{center}
\includegraphics[scale = 0.5]{teaser2.pdf}
  \caption{We show that Grad-CAM highlights the patch location in the image perturbed by regular targeted adversarial patches \cite{brown2017adversarial} (top row). Our modified attack algorithm goes beyond fooling the final prediction by hiding the patch in the Grad-CAM visualization, making it difficult to investigate the cause of the mistake. Note that Grad-CAM visualizes the cause of target category.}
  \end{center}
\label{teaser}
\end{figure*}

We specifically focus on adversarial patches rather than regular adversarial examples since patches are a more practical form of attack, and also the cause of the misclassification is strictly limited to the patch area. Hence, it is not trivial for the attacker to mislead the interpretation to highlight non-patch regions without perturbing them.



Consider an example of adversarial attack using adversarial patches as seen in \cite{brown2017adversarial}, we show that an interpretation algorithm like Grad-CAM \cite{selvaraju2016grad} usually highlights the location of such an adversarial patch making it clear that the image patch was responsible for misclassification. We are interested in designing stronger attack algorithms that not only change the prediction but also mislead the interpretation of the model to hide the attack from investigation. As an example, assume an adversary (one in a crowd of pedestrians) is wearing a t-shirt with a printed adversarial patch on the back that fools a self-driving car leading to an accident. Now, a simple investigation with standard network interpretation methods may reveal which pedestrian in the scene was the cause of the wrong decision, and thereby identifying the adversary. However, we show that it is possible for the adversary to learn a patch without revealing their identity (patch location) and thus escape scrutiny.

We do this by encouraging the optimization to change the interpretation of the network when constructing the corresponding adversarial example. Our work highlights that using a well-designed attack algorithm, an adversary can construct sophisticated adversarial examples which not only change the prediction but also remove any trace of corruption when inspected by network interpretation algorithms.



\section{Related work}{\bf Adversarial examples:}
Adversarial examples were discovered by Szegedy et al. \cite{intriguing-arxiv-2013} who showed that state-of-the-art machine learning classifiers can be fooled comprehensively by simple backpropagation algorithms. Goodfellow et al. \cite{explainingharnessing-arxiv-2014} improved this by Fast Gradient Sign Method (FGSM) that needs only one iteration of optimization. The possibility of extending these examples to the real world was shown in \cite{world-arxiv-2016,sharif2016accessorize} and recently, \cite{athalye2017synthesizing} showed that adversarial examples could be robust to affine transformations. Madry et al. \cite{madry2017towards} proposed Projected Gradient Descent (PGD) which has been shown to be the best first-order adversary for fooling classifiers.
Kindermans et al. \cite{kindermans2017reliability} showed how saliency methods are unreliable by adding constant shift to input data and checking against different saliency methods. In our work, we show that it is not only possible to fool the classifier using an adversary, but also hide it from standard network interpretation algorithms.

\begin{figure*}[!t]
  \begin{center}
  \begin{tabular}{| c c c c c|}
\hline
Original & Regular adv. patch & Regular adv. patch & Ours & Ours\\
 & & GCAM &  &GCAM\\
 \hline
\vspace{-.08in}
&&&&\\
\begin{sideways} Target: Bridegroom \end{sideways}
\includegraphics[width=.17\textwidth]{img_orig_ILSVRC2012_val_00008855.JPEG}&
\includegraphics[width=.17\textwidth]{img_adv_patch_ILSVRC2012_val_00008855_orig_693_target_982_pred_982_prob_99.png}&
\includegraphics[width=.17\textwidth]{mask_adv_patch_ILSVRC2012_val_00008855_orig_693_target_982_pred_982.JPEG}&
\includegraphics[width=.17\textwidth]{img_our_patch_ILSVRC2012_val_00008855_orig_693_target_982__pred_982_prob_09.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_ILSVRC2012_val_00008855_orig_693_target_982_pred_982.png}\\
Paddle & Bridegroom & Bridegroom & Bridegroom & Bridegroom \\
\begin{sideways} Target: Pomegranate \end{sideways}
\includegraphics[width=.17\textwidth]{img_orig_ILSVRC2012_val_00000088.JPEG}&
\includegraphics[width=.17\textwidth]{img_adv_patch_ILSVRC2012_val_00000088_orig_058_target_957_pred_957_prob_99.png}&
\includegraphics[width=.17\textwidth]{mask_adv_patch_ILSVRC2012_val_00000088_orig_058_target_957_pred_957.JPEG}&
\includegraphics[width=.17\textwidth]{img_our_patch__ILSVRC2012_val_00000088_orig_058_target_957__pred_957_prob_05.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_ILSVRC2012_val_00000088_orig_058_target_957_pred_957.png}\\
Water snake & Pomegranate & Pomegranate & Pomegranate & Pomegranate \\
\begin{sideways} \quad \quad Target: Fig \end{sideways}
\includegraphics[width=.17\textwidth]{img_orig_ILSVRC2012_val_00003784.JPEG}&
\includegraphics[width=.17\textwidth]{img_reg_patch_ILSVRC2012_val_00003784_orig_902_target_952_pred_952_prob_99.png}&
\includegraphics[width=.17\textwidth]{mask_reg_patch_ILSVRC2012_val_00003784_orig_902_target_952_pred_952.JPEG}&
\includegraphics[width=.17\textwidth]{img_our_patch_ILSVRC2012_val_00003784_orig_902_target_952__pred_952_prob_04.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_ILSVRC2012_val_00003784_orig_902_target_952_pred_952.png}\\
Whistle & Fig & Fig & Fig & Fig \\
\begin{sideways} \quad \quad Target: Tray \end{sideways}
\includegraphics[width=.17\textwidth]{img_orig_ILSVRC2012_val_00006196.JPEG}&
\includegraphics[width=.17\textwidth]{img_reg_patch_ILSVRC2012_val_00006196_orig_474_target_868_pred_868_prob_99.png}&
\includegraphics[width=.17\textwidth]{mask_reg_patch_ILSVRC2012_val_00006196_orig_474_target_868_pred_868.JPEG}&
\includegraphics[width=.17\textwidth]{img_our_patch_ILSVRC2012_val_00006196_orig_474_target_868__pred_868_prob_03.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_ILSVRC2012_val_00006196_orig_474_target_868_pred_868.png}\\
Cardigan  & Tray & Tray & Tray & Tray \\
\hline
\end{tabular}
\vspace{.05in}
  \caption{Comparison of Grad-CAM visualization results for targeted patch attacks  using our method (`Ours') vs regular adversarial patch (`AP'). The predicted label is written under each image, the attack was successful for all images, and Grad-CAM is always computed for the target category. Note that the patch is not highlighted in the right column.}
\label{fig_patch_target1}
  \end{center}
\end{figure*}
{\bf Adversarial patches:}
Adversarial Patches \cite{brown2017adversarial,karmon2018lavan} were introduced recently as a more practical version of adversarial attacks where we restrict the spatial dimensions of the perturbation, but remove the imperceptibility constraint. These patches can be printed and `pasted' on top of an image to mislead classification networks. We improve this by hiding the patch location from network interpretation tools.

{\bf Interpretation of deep networks:}
As neural networks are getting closer towards deployment in real world applications, it is important that their results are interpretable.
Researchers have proposed various algorithms in this direction. One of the earliest attempt was done in \cite{simonyan2013deep} where they calculate the derivative of the network's outputs w.r.t the input to compute class specific saliency maps. Zhou et al. \cite{zhou2014object} calculates the change in the network output when a small portion of the image (say $11\times11$ pixels) is covered by a random occluder. We call this \textbf{Occluding Patch}. CAM \cite{zhou2016learning} used weighted average map for each image based on their activations. The most popular one that we consider in this paper is called \textbf{Grad-CAM} \cite{selvaraju2016grad}, a gradient based method which provides visual explanations for any neural network architecture. Kunpeng \etal \cite{kunpeng2018gain} recently improved upon Grad-CAM using Guided attention mechanism with state-of-the-art results on segmentation tasks. Although the above methods have shown great improvement in explaining the network's decision, our work highlights that it is important to ensure that they are robust enough to adversaries as well.




\begin{figure*}[h]
  \begin{center}
  \begin{tabular}{| c c c c c|}
\hline  Original & Regular adv. patch & Regular adv. patch & Ours & Ours\\
 & & GCAM &  &GCAMs\\
\hline
\vspace{-.08in}
&&&&\\
\begin{sideways} \quad \quad Target: Sofa \end{sideways}
\includegraphics[width=.17\textwidth]{img_orig_000038.jpg}&
\includegraphics[width=.17\textwidth]{img_reg_patch_000038_orig_014_target_017_pred_017_prob_100.png}&
\includegraphics[width=.17\textwidth]{mask_reg_patch_000038_orig_014_target_017_pred_017.JPEG}&
\includegraphics[width=.17\textwidth]{img_our_patch_000038_orig_014_target_017__pred_017_prob_86.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_000038_orig_014_target_017_pred_017.png}\\
Person & Sofa & Sofa & Sofa & Sofa \\
\begin{sideways} \quad \quad Target: Chair \end{sideways}
\includegraphics[width=.17\textwidth]{img_orig_000145.jpg}&
\includegraphics[width=.17\textwidth]{img_reg_patch_000145_orig_018_target_008_pred_008_prob_100.png}&
\includegraphics[width=.17\textwidth]{mask_reg_patch_000145_orig_018_target_008_pred_008.JPEG}&
\includegraphics[width=.17\textwidth]{img_our_patch_000145_orig_018_target_008__pred_008_prob_97.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_000145_orig_018_target_008_pred_008.png}\\
Train & Chair & Chair & Chair & Chair \\
\begin{sideways} \quad \quad Target: Boat \end{sideways}
\includegraphics[width=.17\textwidth]{img_orig_000168.jpg}&
\includegraphics[width=.17\textwidth]{img_reg_patch_000168_orig_012_target_003_pred_003_prob_100.png}&
\includegraphics[width=.17\textwidth]{mask_reg_patch_000168_orig_012_target_003_pred_003.JPEG}&
\includegraphics[width=.17\textwidth]{img_our_patch_000168_orig_012_target_003__pred_003_prob_91.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_000168_orig_012_target_003_pred_003.png}\\
Horse & Boat & Boat & Boat & Boat \\
\begin{sideways} \quad Target: Sofa \end{sideways}
\includegraphics[width=.17\textwidth]{img_orig_002007.jpg}&
\includegraphics[width=.17\textwidth]{img_reg_patch_002007_orig_000_target_017_pred_017_prob_100.png}&
\includegraphics[width=.17\textwidth]{mask_reg_patch_002007_orig_000_target_017_pred_017.JPEG}&
\includegraphics[width=.17\textwidth]{img_our_patch_002007_orig_000_target_017__pred_017_prob_99.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_002007_orig_000_target_017_pred_017.png}\\
Aeroplane & Sofa & Sofa & Sofa & Sofa \\
\hline
\end{tabular}
\vspace{.05in}
  \caption{Comparison of Grad-CAM visualization results for targeted patch attacks for the least likely target category using our method (`Ours') vs regular adversarial patch (`Adv Patch') on the GAIN{$_{ext}$} \cite{kunpeng2018gain} network for VOC dataset. The predicted label is written under each image, the attack was successful for all images, and Grad-CAM is always computed for the target category. GAIN{$_{ext}$} is particularly designed to produce better Grad-CAM visualizations using direct supervision of the Grad-CAM output.}
\label{fig_patch_target1_GAIN}
  \end{center}
\end{figure*}


\begin{figure*}[h]
  \begin{center}
  \begin{tabular}{| c c c c c|}
\hline  Original & GCAM & Occluding Patch & GCAM & Occluding Patch \\
& Regular adv. Patch & Regular adv. Patch & Ours & Ours \\
\hline
\vspace{-.08in}
&&&&\\
\begin{sideways} Target: Dining Table \end{sideways}
\includegraphics[width=.17\textwidth]{img_orig_002081.jpg}&
\includegraphics[width=.17\textwidth]{mask_reg_patch_002081_orig_019_target_010_pred_010.JPEG}&
\includegraphics[width=.17\textwidth]{occ_reg_patch_adv_img_002081_10.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_002081_orig_019_target_010_pred_010.png}&
\includegraphics[width=.17\textwidth]{occ_our_patch_adv_img_002081_10.png}\\
TV / Monitor & Dining Table & Dining Table & Dining Table & Dining Table \\
\begin{sideways} Target: TV / Monitor \end{sideways}
\includegraphics[width=.17\textwidth]{img_orig_002245.jpg}&
\includegraphics[width=.17\textwidth]{mask_reg_patch_002245_orig_005_target_019_pred_019.JPEG}&
\includegraphics[width=.17\textwidth]{occ_reg_patch_adv_img_002245_19.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_002245_orig_005_target_019_pred_019.png}&
\includegraphics[width=.17\textwidth]{occ_our_patch_adv_img_002245_19.png}\\
Bus & TV / Monitor & TV / Monitor & TV / Monitor & TV / Monitor \\
\begin{sideways} \quad  Target: Bicycle \end{sideways}
\includegraphics[width=.17\textwidth]{img_orig_003019.jpg}&
\includegraphics[width=.17\textwidth]{mask_reg_patch_003019_orig_007_target_001_pred_001.JPEG}&
\includegraphics[width=.17\textwidth]{occ_reg_patch_adv_img_003019_1.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_003019_orig_007_target_001_pred_001.png}&
\includegraphics[width=.17\textwidth]{occ_our_patch_adv_img_003019_1.png}\\
Cat & Bicycle & Bicycle & Bicycle & Bicycle \\
\begin{sideways}  Target: Potted Plant \end{sideways}
\includegraphics[width=.17\textwidth]{img_orig_003069.jpg}&
\includegraphics[width=.17\textwidth]{mask_reg_patch_003069_orig_016_target_015_pred_015.JPEG}&
\includegraphics[width=.17\textwidth]{occ_reg_patch_adv_img_003069_15.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_003069_orig_016_target_015_pred_015.png}&
\includegraphics[width=.17\textwidth]{occ_our_patch_adv_img_003069_15.png}\\
Sheep & Potted Plant & Potted Plant & Potted Plant & Potted Plant \\
\hline
\end{tabular}
\vspace{.05in}
  \caption{Transfer of Grad-CAM visualization attack to Occluding Patch visualization. Here, we use targeted patch attacks for the least likely target category using our method (`Ours') vs regular adversarial patch (`Adv Patch') on the GAIN{$_{ext}$} \cite{kunpeng2018gain} network for VOC dataset. The predicted label is written under each image, the attack was successful for all images, and Grad-CAM and Occluding Patch visualizations are always computed for the target category. Note that the patch is hidden in both visualizations in columns 4 and 5.}
\label{fig_transfer}
  \end{center}

\end{figure*}

\section{Method}
We propose algorithms to learn adversarial patches that when pasted on the input image, can change the interpretation of the model's final decision. We will focus on Grad-CAM \cite{selvaraju2016grad}, which is one of the popular network interpretation methods in designing our algorithms and then, will show that our results generalize to other interpretation algorithms as well.

{\bf Background on Grad-CAM visualization} %\label{attacking_gcam_viz}

Consider a deep network for image classification task, e.g., VGG, and an image $x_0$. We feed the image to the network and get the final output $y$  where $y^c$ is the logit or class-score for the $c$'th  class. To interpret the network's decision for category $c$, we want to generate heatmap $G^c$ for a convolutional layer, e.g, {\em conv5}, which when up-sampled to the size of input image, highlights the regions of the image that have significant effect in producing higher values in $y^c$. We denote $A^k_{ij}$ as the activations of the $k$'th neuron at location $(i,j)$ of the chosen layer. Then, as in \cite{selvaraju2016grad}, we define:
$$\alpha^c_k = \frac{1}{Z}\sum_{i}\sum_{j}{\frac{\partial y^c}{\partial A^k_{ij}}}$$

\noindent where $Z$ is a normalizer and then calculate the heatmap as follows :
$$G_{ij}^c = max(0, \sum_k\alpha^c_kA^k_{ij})$$

Finally, we normalize it to get: $$\hat{G}^c := \frac{G^c}{|G^c|_1}$$.

\begin{figure*}[h]
  \begin{center}
  \begin{tabular}{| c c c c c|}
    \hline
Original & Regular adv. patch & Regular adv. patch & Ours & Ours\\
 & & GCAM &  &GCAM\\
  \hline
\vspace{-.08in}
&&&&\\
\includegraphics[width=.17\textwidth]{img_orig_ILSVRC2012_val_00000075.JPEG}&
\includegraphics[width=.17\textwidth]{img_reg_patch_ILSVRC2012_val_00000075_orig_080_pred_086_prob_74.png}&
\includegraphics[width=.17\textwidth]{mask_reg_patch_ILSVRC2012_val_00000075_orig_080_pred_086.JPEG}&
\includegraphics[width=.17\textwidth]{img_our_patch_ILSVRC2012_val_00000075_orig_080__pred_429_prob_02.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_ILSVRC2012_val_00000075_orig_080_pred_429.png}\\
Black grouse & Partridge & Partridge & Baseball & Baseball \\
\includegraphics[width=.17\textwidth]{img_orig_ILSVRC2012_val_00000681.JPEG}&
\includegraphics[width=.17\textwidth]{img_reg_patch_ILSVRC2012_val_00000681_orig_699_pred_999_prob_63.png}&
\includegraphics[width=.17\textwidth]{mask_reg_patch_ILSVRC2012_val_00000681_orig_699_pred_999.JPEG}&
\includegraphics[width=.17\textwidth]{img_our_patch_ILSVRC2012_val_00000681_orig_699__pred_093_prob_12.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_ILSVRC2012_val_00000681_orig_699_pred_093.png}\\
Panpipe & Bath tissue & Bath tissue & Hornbill & Hornbill \\
\includegraphics[width=.17\textwidth]{img_orig_ILSVRC2012_val_00000903.JPEG}&
\includegraphics[width=.17\textwidth]{img_reg_patch_ILSVRC2012_val_00000903_orig_919_pred_619_prob_66.png}&
\includegraphics[width=.17\textwidth]{mask_reg_patch_ILSVRC2012_val_00000903_orig_919_pred_619.JPEG}&
\includegraphics[width=.17\textwidth]{img_our_patch_ILSVRC2012_val_00000903_orig_919__pred_706_prob_01.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_ILSVRC2012_val_00000903_orig_919_pred_706.png}\\
Street sign & Lampshade & Lampshade & Patio & Patio \\
\includegraphics[width=.17\textwidth]{img_orig_ILSVRC2012_val_00001288.JPEG}&
\includegraphics[width=.17\textwidth]{img_reg_patch_ILSVRC2012_val_00001288_orig_089_pred_326_prob_97.png}&
\includegraphics[width=.17\textwidth]{mask_reg_patch_ILSVRC2012_val_00001288_orig_089_pred_326.JPEG}&
\includegraphics[width=.17\textwidth]{img_our_patch_ILSVRC2012_val_00001288_orig_089__pred_994_prob_02.png}&
\includegraphics[width=.17\textwidth]{mask_our_patch_ILSVRC2012_val_00001288_orig_089_pred_994.png}\\
Cockatoo  & Lycaenid & Lycaenid & Stinkhorn & Stinkhorn \\
  \hline

\end{tabular}
\vspace{.05in}
  \caption{Comparison of Grad-CAM visualization results for non-targeted patch attacks using our method (`Ours') vs regular adversarial patch (`AP'). The predicted label is written under each image, the non-targeted attack was successful for all images, and Grad-CAM is always computed for the predicted category.}
\label{fig_patch_nontarget1}
  \end{center}

\end{figure*}
\begin{figure*}[h]
  \begin{center}
  \begin{tabular}{| c c c c c|}
    \hline
Original & Regular adv. patch & Regular adv. patch & Ours & Ours\\
 & & GCAM &  &GCAM\\
  \hline
\vspace{-.08in}
&&&&\\
\includegraphics[width=.17\textwidth]{o_697.jpg}&
\includegraphics[width=.17\textwidth]{re.png}&
\includegraphics[width=.17\textwidth]{rm_000697_orig_014_pred_015.png}&
\includegraphics[width=.17\textwidth]{adv_000697_orig_014_pred_015_prob_99.png}&
\includegraphics[width=.17\textwidth]{am_000697_orig_014_pred_015.png}\\
Person & Pottedplant & Pottedplant & Pottedplant & Pottedplant \\
\includegraphics[width=.17\textwidth]{o_96.jpg}&
\includegraphics[width=.17\textwidth]{reg_000096_orig_014_pred_015_prob_100.png}&
\includegraphics[width=.17\textwidth]{rm_000096_orig_014_pred_015.png}&
\includegraphics[width=.17\textwidth]{adv_000096_orig_014_pred_015_prob_100.png}&
\includegraphics[width=.17\textwidth]{am_000096_orig_014_pred_015.png}\\
Person & Pottedplant & Pottedplant & Pottedplant & Pottedplant \\
\includegraphics[width=.17\textwidth]{o_2406.jpg}&
\includegraphics[width=.17\textwidth]{reg_002406_orig_006_pred_015_prob_100.png}&
\includegraphics[width=.17\textwidth]{rm_002406_orig_006_pred_015.png}&
\includegraphics[width=.17\textwidth]{adv_002406_orig_006_pred_015_prob_83.png}&
\includegraphics[width=.17\textwidth]{am_002406_orig_006_pred_015.png}\\
Car & Pottedplant &Pottedplant & Pottedplant & Pottedplant \\
\includegraphics[width=.17\textwidth]{o_5816.jpg}&
\includegraphics[width=.17\textwidth]{reg_005816_orig_013_pred_015_prob_100.png}&
\includegraphics[width=.17\textwidth]{rm_005816_orig_013_pred_015.png}&
\includegraphics[width=.17\textwidth]{adv_005816_orig_013_pred_015_prob_100.png}&
\includegraphics[width=.17\textwidth]{am_005816_orig_013_pred_015.png}\\
Motorbike  & Pottedplant & Pottedplant & Pottedplant & Pottedplant\\
\hline

\end{tabular}
\vspace{.05in}
  \caption{Comparison of Grad-CAM visualization results for universal targeted patch attacks using our method (`Ours') vs regular adversarial patch (`AP'). The top-1 predicted label is written under each image, the universal attack was successful for all VOC images, and Grad-CAM is always computed for the predicted category. The target category chosen was ``Pottedplant''.}
\label{fig_patch_universal}
  \end{center}

\end{figure*}
{\bf Background on adversarial patches:}

Consider an input image $x_0$ and a predefined constant binary mask $m$ that is $1$ on the location of the patch and $0$ everywhere else. We want to find an adversarial patch $z$ that changes the output of the network to catgegory $t$ when pasted on the image, so we solve:
$$ z = \argmin_{z} \ell_{ce}(x \odot (1-m)+z \odot m; t)$$

\noindent where $\ell_{ce}(.; t)$ is the cross entropy loss for the target category $t$
and $\odot$ is the element-wise product. This results in adversarial patches similar to \cite{brown2017adversarial}.




\subsection{Misleading interpretation in targeted mode:}
\label{sec:target}
As shown in Figure \ref{fig_patch_target1}, when Grad-CAM of the target category is used to investigate the cause of misclassification  , it highlights the patch very strongly revealing the cause of the attack. This is expected as the adversary is restricted to perturbing the patch area and the patch is the cause of the final misclassifcation towards target category. %We are interested in advancing adversarial patches to hide the patch in the interpretation of the prediction.
To fool the network's interpretation so that the adversarial patch  is not highlighted at the interpretation of the final prediction, we add an additional term to our loss function in learning the patch to suppress the total activation of the visualization at the patch location $m$. Hence, assuming the perturbed image $\tilde x = x_0 \odot (1-m)+z \odot m$, we optimize:
\begin{equation}
\begin{split}
     \argmin_{z}\Big[\ell_{ce}(\tilde x; t) + \lambda \sum_{ij} \big(\hat{G}^t(\tilde x) \odot m\big)\Big ]
\end{split}
\end{equation}

\noindent where $t$ is the target category and $\lambda$ is the hyper-parameter to trade-off the effect of two loss terms. We choose the target label randomly across all classes excluding the original prediction similar to  ``step rnd" method in \cite{atscale-arxiv-2016}.
\subsection{Misleading interpretation in non-targeted mode}

A similar approach can be used to develop a non-targeted attack by maximizing the cross entropy loss of the correct category. This can be considered a weaker form of attack since we have no control over the final category which is predicted after adding the patch. In this case, our optimization problem becomes:


\begin{equation}
\begin{split}
     \argmin_{z}\Big[\max(0, M -\ell_{ce}(\tilde x; c)) + \lambda \sum_{ij} \big(\hat{G}^a(\tilde x) \odot m\big)\Big ] \\ \text{where} \quad a=\argmax_k{p(k)}. \nonumber
\end{split}
\end{equation}


\noindent where $c$ is the predicted category for the original image, $p(k)$ is the probability of category $k$, and $a$ is the top prediction at every iteration. Since cross entropy loss is not upper-bounded, it can dominate the optimization, so we use contrastive loss \cite{hadsell2006dimensionality} to ignore cross entropy  when the probability of $c$ is less than the chance level, thus $M=-log(p_0)$ where $p_0$ is the chance probability (e.g., 0.001 for ImageNet). Note that the second term is using the visualization of the current top category $a$.

To optimize above loss functions, we use an iterative approach similar to  projected gradient decent (PGD) algorithm \cite{madry2017towards}. We initialize $z^0$ randomly and then iteratively update it by: $\displaystyle z^{n+1} = z^n - \eta Sign({\frac{\partial \ell}{\partial z}})$ with learning rate $\eta$. At each iteration, we project $z$ to the feasible region by clipping it to the dynamic range of the image values.
\begin{table*}[!t]
\centering
 \begin{tabular}{||c || c | c | c | c | c | c ||}
 \hline
 \multirow{2}{*}{Method} & Top-1 Acc(\%) & \multicolumn{2}{|c|}{Non-Targeted} & \multicolumn{3}{|c|}{Targeted} \\ [0.5ex]
 \cline{3-7}
 & & Acc (\%) & Energy Ratio (\%) & Acc (\%) & Target Acc (\%) & Energy Ratio(\%)\\
 \hline\hline
 Adversarial Patch \cite{brown2017adversarial} & 74.24 & 0.06 & 38.95 & 0.02 & 99.98 & 58.39   \\
 \hline
 Our Patch & 74.24 & 0.05 & \textbf{2.00} & 2.95 &77.88 & \textbf{5.21} \\
 \hline
\end{tabular}
\newline
    \caption{Comparison of heatmap energy within the 8\% patch area for the adversarial patch \cite{brown2017adversarial} and our patch. Accuracy denotes the fraction of images that had the same final predicted label as the original image. Target Accuracy denotes the fraction of images where the final predicted label has changed to the randomly chosen target label. %Note that the Target accuracy is not defined for the Non-Targeted attack.
    }
    \label{fig:comparison_patch_heatmap}
\end{table*}
\begin{table}[!t]
\centering
 \begin{tabular}{||c || c | c ||}
 \hline
 \multirow{2}{*}{Method} & \multicolumn{2}{|c|}{Targeted} \\ [0.5ex]
 \cline{2-3}
 & \footnotesize{Target Acc (\%)} & \footnotesize{Energy Ratio (\%)}\\
 \hline\hline
 Adversarial Patch \cite{brown2017adversarial} & 94.34 & 29.02   \\
 \hline
 Our Patch & 94.70 & \textbf{2.45} \\
 \hline
\end{tabular}
\newline
    \caption{Comparison of Grad-CAM heatmap energy within the 8\% patch area for the adversarial patch \cite{brown2017adversarial} and our patch trained on the GAIN{$_{ext}$} \cite{kunpeng2018gain} for VOC dataset. The heatmap has far less energy in the patch area that the adversary can change.
    }
    \label{fig:comparison_patch_heatmap_GAIN}
\end{table}
\begin{table}[!t]
\centering
 \begin{tabular}{||c || c | c ||}
 \hline
 \multirow{2}{*}{Method} & \multicolumn{2}{|c|}{Targeted} \\ [0.5ex]
 \cline{2-3}
 & \footnotesize{Target Acc (\%)} & \footnotesize{Energy Ratio (\%)}\\
 \hline\hline
 Adversarial Patch \cite{brown2017adversarial} &  99.97& 61.91  \\
 \hline
 Our Patch & 92.28 & \textbf{0.56} \\
 \hline
\end{tabular}
\newline
    \caption{Comparison of heatmap energy for the universal attack case.
    }
    \label{fig:comparison_patch_univ_heatmap_GAIN}
\end{table}
\begin{table}[!t]
\centering
 \begin{tabular}{||c || c | c ||}
 \hline
 Method & \small{Targeted Attack Energy Ratio (\%)}\\[0.5ex]
 \hline\hline
 Adversarial Patch \cite{brown2017adversarial} & 61.59  \\
 \hline
 Our Patch & \textbf{24.19}  \\
 \hline
\end{tabular}
\newline
    \caption{Comparison of Occluding Patch \cite{zhou2014object} heatmap energy within the 8\% patch area for the adversarial patch \cite{brown2017adversarial} and our patch trained on the GAIN{$_{ext}$} \cite{kunpeng2018gain} for VOC dataset. Note that we still use Grad-CAM in training and evaluate on Occluding Patch. This shows our attack generalizes from Grad-CAM to occluding patch.
    }
    \label{table:comparison_patch_transfer_heatmap_GAIN}
\end{table}
\subsection{Misleading interpretation for guided attention models}
\label{improv_adv_exmaples_attention_models}
Recently, there have been works \cite{singh2017hide}, \cite{wei2017object} which focus on improving the attention maps of the predicted objects when training a classifier. Kunpeng \etal \cite{kunpeng2018gain} further improve upon this by providing supervision on the network's attention in an end-to-end way. This is done by designing loss functions that guide the network's focus on the entire area critical to the task of interest. We perform the targeted attack as described in section \ref{sec:target} on the GAIN{$_{ext}$} model from \cite{kunpeng2018gain}. Since the GAIN{$_{ext}$} model was fine-tuned for the PASCAL VOC dataset, we perform a targeted attack to change the prediction to the least likely category from the predictions and also ensure that the Grad-CAM heatmap does not overlap with the patch. We also observe that patches created our method transfer to Occluding Patch \cite{zhou2016learning} as seen in Figure \ref{fig_transfer}.  This shows thateven if the patch is optimized to fool only one visualization, it also results in fooling other visualizations as well, which is to the advantage of the adversary.
\subsection{Universal Patches}
\label{universal_attention}
Universal attack is a much stronger form of attack wherein we train a patch that generalizes across images in fooling towards a particular category. Such an attack shows that it is possible to fool an unknown test image using a patch learnt using the training data. Similar to \ref{improv_adv_exmaples_attention_models}, we consider the GAIN$_{ext}$ model which is fine-tuned on PASCAL VOC dataset. We fix the target category as ``Pottedplant'' and learn the patch as a form of targeted attack as explained in \ref{sec:target} which ensures mis-classification towards the target category along with the heatmap on the patch area being minimal. This is the most practical form of attack, since the adversary needs to train the patch just once, which would be strong enough to fool multiple test scenarios.\%\subsection{Misleading interpretation of regular adversarial examples}





















\section{Experiments}
 We use pre-trained VGG-19 \cite{simonyan2014very} with batch normalization implemented in PyTorch.\\



{\bf 4.1. Misleading interpretation of adversarial patches:}
\label{exp_misleading_adv_patches}
For the adversarial patch experiments described in the method section, we use a patch of size 64x64 on the top-left corner for 50,000 images of size 224x224 ($\sim$ 8\% of the image area) in the validation set of ImageNet \cite{deng2009imagenet} ILSVRC2012. We do 750 iterations with $\eta = 0.001$. To evaluate how much the patch is highlighted in the visualization, we construct the visualization heatmap $\hat{G}^c$ for the mistaken category, and calculate the ratio of the total energy at the patch location to that of the whole image. We call this metric ``energy ratio". It will be $0$ if the patch is not highlighted at all and $1$ if the heatmap is completely concentrated inside the patch. The quantitative results are shown in Table \ref{fig:comparison_patch_heatmap}.








{\bf 4.2. Misleading interpretation for guided attention models:}
We use the GAIN{${_{ext}}$} model fine-tuned on the VOC dataset from \cite{kunpeng2018gain} and VOC test set for these experiments. We use a patch of size 64x64 on the top-left corner for 4952 images of size 224x224 ($\sim$ 8\% of the image area) in the test set of the PASCAL VOC dataset. We do 750 iterations with $\eta = 0.1$. Since each image in the VOC dataset can contain more than one category, we use the least likely predicted category to perform the targeted patch attack. We evaluate using the same method as described in Section
{\textcolor{red}{4.1}}. %\ref{exp_misleading_adv_patches}.
The results of the evaluation are described in Table \ref{fig:comparison_patch_heatmap_GAIN}.

{\bf 4.3. Generalization beyond Grad-CAM:} We also show that our patches learned using Grad-CAM are hidden in the visualizations generated by Occluding Patch \cite{zhou2014object} method. In occluding patch method, we visualize the change in the final score of the model by sliding a small black box on the image. Larger decrease in the score means more important regions and hence they contribute more to the heatmap. The results are shown in Table \ref{table:comparison_patch_transfer_heatmap_GAIN} and Figure \ref{fig_transfer}.


{\bf 4.4. Universal Patches:}
For these experiments, we learn a patch of size 64x64 on the top-left but use the training set from PASCAL VOC dataset. The optimization performed is similar to the above section. We use 50 iterations per image with $\eta=0.05$ and $\lambda = 0.09$. As described in \ref{universal_attention}, we choose ``Pottedplant'' as the target category and the evaluation was done on test set. The results for these can be found in Figure \ref{fig_patch_universal} and Table \ref{fig:comparison_patch_univ_heatmap_GAIN} . We observe high fooling rates for both the methods, but our method has considerably low energy focused inside the patch area. Note that only the region of the patch is perturbed and everything else is untouched.

\ignore{

\subsection{Misleading interpretation of regular adversarial examples}
We use 5,000 uniformly random images from the validation set of ImageNet dataset. We do 500 iterations with $\eta = 0.05$. We use two metrics for quantitative evaluation of our method described in Section \ref{improv_adv_exmaples}:
{\bf (1) Localization error:} We use the Top-1 error of object localization metric similar to \cite{selvaraju2016grad} from the ImageNet competition. A higher number means a good attack where objects cannot be detected in the visualization.
{\bf (2) Histogram intersection:} We calculate the Grad-CAM heatmap of the original image and the adversarial image, normalize them to sum to one, and calculate the histogram intersection between them. A lower number means a good attack where there is not much of overlap between visualizations before and after the attack. Examples are shown in Figure \ref{figCAM_1}, Figure \ref{figPGD_1} and Table \ref{table:PGD}.

Moreover, Figure \ref{figCAM_1} also shows transferability of our attack across different visualization tools. It shows that our adversarial image which is tuned for Grad-CAM visualization, fools CAM \cite{zhou2016learning} and occluding patch \cite{zhou2014object} visualizations as well. In the occluding patch method, we visualize the change in the final score of the model by sliding a small black box on the image. Larger decrease in the score means more important regions and hence they contribute more to the heatmap. We use SqueezeNet for these experiments since CAM is not supported in networks without Global Average Pooling layer (GAP) \cite{zhou2016learning}.

\begin{table*}[h]
\centering
\begin{tabular}{||c ||c | c |c| c|}
 \hline
 Image & Accuracy (\%) & Loc. Error(\%) & Histogram Intersection\\ [1ex]
 \hline\hline
 Original & - & 66.68 & 1.0 \\
 \hline
 PGD Adversarial & 0.26 & 67.74 & 0.78 $\pm$ 0.076 \\
 \hline
Our Adversarial & 0.06 & 78.64 & 0.62 $\pm$ 0.088 \\
 \hline
\end{tabular}
\newline
\caption{Results for evaluating our method on ImageNet validation images using two metrics. Note that higher localization error and lower histogram intersection is better.}
\label{table:PGD}
\end{table*}
}







\section{Conclusion}
We presented novel adversarial attack algorithms that go beyond fooling the prediction by hiding the cause of the mistake from our common interpretation tools to result in a stronger attack. We show that our attack tuned for Grad-CAM can transfer to other visualization algorithms and we also show that we can create universal patches that can generalize fooling across images. Our work shows that there is a need for more robust deep learning tools that reveal the correct cause of network's predictions.\\ \\

{\bf Acknowledgement:} This work was performed under the following financial assistance award: 60NANB18D279 from U.S. Department of Commerce, National Institute of Standards and Technology, and also funding from SAP SE.









\ignore{
\begin{figure*}[!h]
\centering
Visualization for category: Microphone\\
  \begin{tabular}{c c c}
    \includegraphics[width=.20\textwidth, height=.20\textwidth]{figures/pgd/microphone/orig_ILSVRC2012_val_00003465.JPEG}&
    \includegraphics[width=.20\textwidth, height=.20\textwidth]{figures/pgd/microphone/pgd_ILSVRC2012_val_00003465.JPEG}&
    \includegraphics[width=.20\textwidth, height=.20\textwidth]{figures/pgd/microphone/adv_ILSVRC2012_val_00003465.JPEG}\\
    (a) Orig Image & (b) PGD Adv & (c) Grad-CAM Adv \\
    Pred: Microphone & Pred: Screw & Pred: Joystick\\
    \includegraphics[width=.20\textwidth, height=.20\textwidth]{figures/pgd/microphone/orig_mask_ILSVRC2012_val_00003465_0650.JPEG}&
    \includegraphics[width=.20\textwidth, height=.20\textwidth]{figures/pgd/microphone/pgd_mask_ILSVRC2012_val_00003465_mask.JPEG}&
    \includegraphics[width=.20\textwidth, height=.20\textwidth]{figures/pgd/microphone/adv_mask_ILSVRC2012_val_00003465_1_0083_0650.JPEG}\\
    (d) Grad-CAM Orig  & (e) Grad-CAM PGD  & (f) Grad-CAM Ours
  \end{tabular}
  \caption{The adversarial images generated by our method is able to fool the Grad-CAM visualization result (f) compared to the Grad-CAM visualization of the adversarial image generated by PGD (e).}
\label{figPGD_1}

\end{figure*}
}





\bibliographystyle{unsrt}
\bibliography{main}


















\ignore{
\newpage

\begin{figure*}[h!]
  \begin{center}
  \begin{tabular}{| c c c c|}
  \hline

  & Grad-CAM & CAM & Occlusion\\
  \begin{sideways} Original Image \end{sideways}
\includegraphics[width=.17\textwidth]{figures/squeezenet/img_orig_ILSVRC2012_val_00005576.JPEG}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/mask_orig_ILSVRC2012_val_00005576_orig_0020_pred_0360.JPEG}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/img_orig_ILSVRC2012_val_00005576_CAM.png}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/img_orig_ILSVRC2012_val_00005576_OP.png}\\
 Pred: Water Ouzel &&&\\
 \begin{sideways} Our Adv. Image \end{sideways}
\includegraphics[width=.17\textwidth]{figures/squeezenet/img_ours_ILSVRC2012_val_00005576.JPEG}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/mask_ours_ILSVRC2012_val_00005576_orig_0020_pred_0360.JPEG}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/ILSVRC2012_val_00005576_adv_image_CAM.png}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/ILSVRC2012_val_00005576_all_masks_adv_OP.png}\\
 Pred: Otter &  &  &   \\
\hline
\hline
&&&\\
\begin{sideways} Original Image \end{sideways}
\includegraphics[width=.17\textwidth]{figures/squeezenet/img_orig_ILSVRC2012_val_00006174.JPEG}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/mask_orig_ILSVRC2012_val_00006174_orig_0270_pred_0265.JPEG}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/img_orig_ILSVRC2012_val_00006174_CAM.png}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/img_orig_ILSVRC2012_val_00006174_OP.png}\\
 Pred: White Wolf &&&\\
 \begin{sideways} Our Adv. Image \end{sideways}
\includegraphics[width=.17\textwidth]{figures/squeezenet/img_ours_ILSVRC2012_val_00006174.JPEG}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/mask_ours_ILSVRC2012_val_00006174_orig_0270_pred_0265.JPEG}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/ILSVRC2012_val_00006174_our_adv_image_CAM.png}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/ILSVRC2012_val_00006174_all_masks_adv_OP.png}\\
 Pred: Toy Poodle&&&  \\
\hline
\hline
&&&\\
\begin{sideways} Original Image \end{sideways}
\includegraphics[width=.17\textwidth]{figures/squeezenet/img_orig_ILSVRC2012_val_00009705_clean.JPEG}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/mask_orig_ILSVRC2012_val_00009705_orig_0497_pred_0873.JPEG}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/img_orig_ILSVRC2012_val_00009705_CAM.png}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/img_orig_ILSVRC2012_val_00009705_OP.png}\\
 Pred: Church &&&\\
 \begin{sideways} Our Adv. Image \end{sideways}
\includegraphics[width=.17\textwidth]{figures/squeezenet/ILSVRC2012_val_00009705.JPEG}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/mask_ours_ILSVRC2012_val_00009705_orig_0497_pred_0873.JPEG}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/ILSVRC2012_val_00009705_our_adv_image_CAM.png}&
\includegraphics[width=.17\textwidth]{figures/squeezenet/ILSVRC2012_val_00009705_all_masks_adv_OP.png}\\
 Pred: Triumphal arch  &&& \\
\hline

\end{tabular}
  \caption{We use Grad-CAM, CAM \cite{zhou2016learning}, and occluding patch \cite{zhou2014object} methods to asses the transferability of our attack across different methods. All visualizations are for the predicted category of the original image on the top-left of each panel. We use SqueezeNet for these experiments since CAM is not supported in networks without Global Average Pooling layer (GAP) \cite{zhou2016learning}. On the second row, our adversarial attack can fool all three visualization algorithms.}
  \end{center}
\label{figCAM_1}
\end{figure*}
}



\end{document}




