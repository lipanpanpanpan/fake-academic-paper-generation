\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\renewcommand{\labelitemii}{$\star$}
\newcommand{\red}[1]{\textcolor{red}{\textbf{#1}}}
\newcommand{\rcite}[1]{\textcolor{red}{\cite{}}}
\usepackage{caption}
\usepackage{stackengine}
\usepackage[caption=false]{subfig}
\usepackage{comment}
\usepackage{booktabs} % for \toprule, \midrule, and \bottomrule
\usepackage{boldline,multirow}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

 \cvprfinalcopy % *** Uncomment this line for the final submission

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\setlength\extrarowheight{3pt}
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

	\title{Priming Deep Neural Networks with Synthetic Faces \\for Enhanced Performance}
	\author{Adam Kortylewski \;\; Andreas Schneider \;\; Thomas Gerig \;\; Clemens Blumer\\ \;\; Bernhard Egger \;\; Corius Reyneke\;\; Andreas Morel-Forster\;\;Thomas Vetter\vspace{10pt}\\ 
		Department of Mathematics and Computer Science  \;\; \\
		University of Basel}

\maketitle

	\begin{abstract}
	Today's most successful facial image analysis systems are based on deep neural networks. However, a major limitation of such deep learning approaches is that their performance depends strongly on the availability of large annotated datasets. In this work, we prime deep neural networks by pre-training them with synthetic face images for specific facial analysis tasks. We demonstrate that this approach enhances both the generalization performance as well as the dataset efficiency of deep neural networks. Using a 3D morphable face model, we generate arbitrary amounts of annotated data with full control over image characteristics such as facial shape and color, pose, illumination, and background. With a series of experiments, we extensively test the effect of priming deep neural networks with synthetic face examples for two popular facial image analysis tasks: face recognition and facial landmark detection. We observed the following positive effects for \underline{both} tasks: 1) Priming with synthetic face images improves the generalization performance consistently across all benchmark datasets. 2) The amount of real-world data needed to achieve competitive performance is reduced by 75\% for face recognition and by 50\% for facial landmark detection. 3) Priming with synthetic faces is consistently superior at enhancing the performance of deep neural networks than data augmentation and transfer learning techniques. Furthermore, our experiments provide evidence that priming with synthetic faces is able to enhance performance because it reduces the negative effects of biases present in real-world training data. The proposed synthetic face image generator, as well as the software used for our experiments, have been made publicly available \footnote{https://github.com/unibas-gravis/parametric-face-image-generator}.
   
	\end{abstract}

	\section{Introduction}
    Facial image analysis tasks such as face recognition and facial landmark detection have gained a lot of attention from the computer vision community. In recent years, advances in deep learning \cite{krizhevsky2012imagenet} and the availability of large-scale datasets have enabled a large increase in the performance of facial image analysis systems \cite{taigman2014deepface,schroff2015facenet,ranjan2017hyperface}. 
    However, relying on large-scale datasets to enable successful facial image analysis is a limiting factor for several reasons: 
    1) Current large-scale datasets were mostly collected from the web and therefore have biases regarding several facial properties such as e.g. the head pose or the illumination. It is well known that such biases have a strong negative effect on the generalization performance of machine learning systems \cite{torralba2011unbiased,kortylewski2017empirically,tommasi2017deeper}. 2) The manual annotation of additional data, which prevents unbiasing of available datasets as well as the collection of data for novel facial image analysis tasks is expensive. 3) Large datasets of faces often cannot be released publicly due to privacy issues \cite{schroff2015facenet}.
        
	A promising approach for overcoming the data collection problem in computer vision is to use simulated datasets \cite{qiu2016unrealcv,jaderberg2014synthetic,butler2012naturalistic,chen2015deepdriving,gaidon2016virtual,park2015articulated}. Data synthesis allows massive amount of training samples with fine-grained annotation to be generated. In a recent special issue of the IJCV \cite{gaidon2018reasonable} several works demonstrated the "reasonable effectiveness of synthetic visual data" e.g. for autonomous driving \cite{muller2018sim4cv}, indoor scene understanding \cite{jiang2018configurable} or 3D object detection \cite{loing2018virtual}. Although the approach of using synthetic data for training has been widely applied, it has not received much attention in the context of facial image analysis.
	Only Kim et al. \cite{kim2017inversefacenet} have collected the first evidence that synthetic data is useful for facial image analysis in the context of 3D face reconstruction, where no real-world ground truth data is available for training.	In this work, we address the question: Can we use synthetic face images to enhance the performance of deep neural networks when performing facial image analysis tasks?
	
	We present an approach for generating synthetic face images with detailed annotations. We evaluate the effects of priming deep neural networks with synthetic faces for a specific facial image analysis task, before training them with real-world data. Our experiments are carried out on two popular facial analysis tasks: face recognition and facial landmark detection.
	
	For the synthesis of face images we use a 3D Morphable Face Model (3DMM) \cite{blanz1999morphable,gerig2018morphable}. The 3DMM is a statistical model of 3D faces which has been widely applied to facial image analysis in an analysis-by-synthesis setting.  The generation of novel synthetic face images is a basic capability of the 3DMM, which allows us to create possibly infinite datasets with any desired type of annotation with arbitrary depth (number of samples per identity) and width (number of identities). Additionally, synthetic face images can be released publicly for research purposes. The advantage of using 3DMMs for data synthesis over related generative face models such as e.g. GANs \cite{berthelot2017began,goodfellow2014generative} is that the 3DMM provides full control over disentangled parameters that change the facial identity in the terms of shape and albedo texture as well as pose, illumination and facial expression. 

	In our experiments, we pre-train deep neural networks with synthetic face images and subsequently fine-tune them with varying amounts of real-world data. In our analysis we observed the following positive effects when performing face recognition \textit{and} facial landmark detection tasks:
    \begin{enumerate}
		\item \textbf{Enhanced data-efficiency.} The amount of real-world data needed to achieve competitive performance is reduced considerably.
		\item \textbf{Enhanced generalization performance.} The generalization performance of deep neural networks is enhanced consistently across all benchmark datasets.
        \item \textbf{Superiority when compared to data augmentation and transfer learning approaches.} Priming deep neural networks with synthetic faces has a considerably more positive effect on performance when compared to general techniques such as data augmentation and transfer learning from classification tasks. 
    \end{enumerate}
    
	Finally, we demonstrate that deep learning systems for facial image analysis strongly benefit from priming with synthetic face images in terms of generalization performance and real-world data efficiency. Since our approach proves to be effective for two very different facial image analysis tasks, we conclude that other facial analysis tasks might also benefit from priming. Curiously, despite the success of 3D Morphable Face Models at facial image analysis, we are not aware of any previous work that uses this effective and easily accessible approach for training face recognition or facial landmark detection systems.
	
      
    
    A particularly successful network architecture was proposed by Schroff et al. \cite{schroff2015facenet}, who introduced an innovative triplet-loss to their FaceNet model. After training with a large-scale private dataset, they achieved a considerable increase in state-of-the-art face recognition performance. For our face recognition experiments we use a publicly available implementation of this FaceNet network \footnote{https://cmusatyalab.github.io/openface/}.
    
    \textbf{Deep facial landmark detection.} 
	The detection of fiducial  points is one of the most important tasks in facial image analysis. Common approaches include regression-based \cite{xiong2013supervised,cao2014face,kazemi2014one} and model-based \cite{cootes1995active,matthews2004active} methods. The former learns local updates given an initial alignment and the latter is based on adapting a holistic face model to the target image. In recent years, facial landmark detection approaches based on deep learning have achieved remarkable levels of performance \cite{sun2013deep}. Trigeorigis et al. \cite{trigeorgis2016mnemonic} combined a CNN with an RNN for cascaded regression and thereby jointly learned a feature representation for each fiducial point as well as the local update procedure. Multi-task learning approaches have gained recent popularity \cite{zhang2014facial,ranjan2017all}. They train a single deep network for the prediction of multiple facial image analysis tasks, thus enforcing the development of a holistic face representation which improves robustness and generalization across all individual tasks.	In this work, we investigate whether deep learning approaches to facial landmark detection benefit from training with synthetic data. For our facial landmark detection experiments we use a publicly available implementation of the multi-task HyperFace network \footnote{https://github.com/takiyu/hyperface}.
    Patel et al. \cite{patel2011illumination} performed face image relighting in order to make illumination changes between images of the same person more robust. Hu et al. \cite{hu2018frankenstein} composed novel face images by blending parts of different donor face images into a novel image.
    Masi et al. \cite{masi2016we} used domain-specific data augmentation by augmenting the Casia dataset \cite{casia} in 3D. They aligned a 3D face shape to landmarks in each training image and subsequently modified the face with regard to shape deformation, 3D pose, and expression neutralization. This artificial augmentation process does not follow any statistical distribution and thus can lead to unrealistic training data. Furthermore, they did not change the illumination of the images, thus relying on the illumination distribution of the Casia dataset. In this work, we follow a different approach by studying the effect of using fully synthetic face images which were generated with 3DMMs, a statistical illumination model, a camera model and computer graphics.
    
    \textbf{Deep learning with fully synthetic data.} 
    Fully synthetic datasets which are generated with computer graphics have been widely used for the evaluation and training of computer vision tasks such as optical flow \cite{butler2012naturalistic}, autonomous driving systems \cite{chen2015deepdriving}, object detection \cite{gupta2014learning}, pose estimation \cite{shotton2011real,park2015articulated,ionescu2014human3} and text detection \cite{gupta2016synthetic}. Recently, Qiu and Yuille \cite{qiu2017unrealcv} developed UnrealCV, a computer graphics engine for the assessment of computer vision algorithms for scene analysis. Following this approach, Kortylewski et al. \cite{kortylewski2017empirically} recently proposed to use synthetically generated face images for analyzing the generalization performance of different neural network architectures when performing face recognition in the virtual domain. Gaidon et al. presented Virtual KITTI \cite{gaidon2016virtual}, where they used synthetically generated data to pre-train a deep convolutional neural network for the tasks of object detection, tracking and scene segmentation in the context of automated driving systems. They showed that Deep Learning systems behave similarly when trained in the synthetic domain and evaluated in the real domain and vice-versa. In the context of facial analysis, Abbasnejad et al. \cite{abbasnejad2017using} trained a deep convolutional neural network for expression analysis on synthetic data and achieved state-of-the-art results in action unit classification on real data. Kim et al. \cite{kim2017inversefacenet} trained an AlexNet architecture for the regression of 3D morphable model parameters and achieved competitive results when compared to approaches that learn from real-world data. Curiously, despite the widespread use of synthetic data for deep learning, we are not aware of any work that studied how synthetic data can be leveraged to enhance face recognition and facial landmark detection in real-world images.

    In this work, we use a statistical model of 3D face shape, texture, expression and illumination to generate synthetic face images, which we use to train deep neural networks for face recognition and facial landmark detection.
	\begin{figure}
    	\centering
    	\stackunder[2pt]{\includegraphics[width=.15\textwidth]{0_49.jpg}}{}
    	\stackunder[2pt]{\includegraphics[width=.15\textwidth]{138_38.jpg}}{}\\
    	\stackunder[5pt]{\includegraphics[width=.15\textwidth]{19_47.jpg}}{}
    	\stackunder[5pt]{\includegraphics[width=.15\textwidth]{265_91.jpg}}{}
    	\caption{Example renderings generated with our parametric face image generator. The facial appearance in the images varies in terms of identity, head pose, expression and illumination. We simulate changes in the background by overlaying the generated face on random textures.}
    	\label{fig:exampleSynth}
    \end{figure}%%%%%%%%%%%%%%%%%%%%%%%%%%% FACE IMAGE GENERATOR%%%%%%%%%%%%%%%%%%%%%%%%%%\section{Face Image Generator}\label{sec:generator}
	We synthesize face images by sampling from a statistical 3D Morphable Model \cite{blanz1999morphable} of face shape, color and expression. For data synthesis, we use the generator proposed by Kortylewski et al. \cite{kortylewski2017empirically} and extend it with facial expressions as well as a natural illumination prior. In the following, we describe the most important parameters of the model and their influence on the facial appearance in the image.
	
	\textbf{Facial identity.} In order to simulate different facial identities we use the Basel Face Model 2017 \cite{bfm17} (BFM). The BFM describes a statistical distribution of face shape, color and expression which was learned from 200 neutral high-resolution 3D face scans. The parameters follow a Gaussian distribution. By drawing random samples from this distribution we generate novel 3D face meshes with unique color, shape and facial expression.
	
	\textbf{Illumination.} We assume the Lambertian reflectance model and approximate the environment map with 27 spherical harmonics coefficients (9 for each color channel). In order to generate natural illuminations, the spherical harmonics illumination parameters are sampled  from the Basel Illumination Prior (BIP) \cite{illuprior}. The BIP describes an empirical distribution of spherical harmonics coefficients estimated from 14'348 real-world face images.
	
	\textbf{Pose and camera}. The simulated faces are viewed with a fixed pinhole camera. The facial orientation with respect to the camera is controlled by six pose parameters (yaw, pitch, roll, and 3D translation). Throughout our experiments, we vary the head position and pose angles, while normalizing the head position to the center of the image frame.\\
	
	\textbf{Background}. We simulate changes in the background with a non-parametric background model by sampling randomly from a set of background images of the describable texture database \cite{textures}. The purpose of these random structured background changes is to aid deep learning systems in discovering the irrelevance of background structures for facial image analysis tasks.

	
	The synthesized images (Fig. \ref{fig:exampleSynth}) are fully specified by the aforementioned parameter distributions. Note that our data generation process follows a statistical distribution of face shapes and textures which is in contrast to, for example, the data augmentation in \cite{masi2016we}, where shape deformation between a few fixed 3D shapes is performed. The major benefit of the generator is that by sampling from its parameters we are able to synthesize an arbitrary amount of face images with any desired number of identities, with expressions, in different head poses and natural illumination conditions.
	
    \textbf{Software}. The synthetic face image generator is publicly available


    In this section, we empirically analyze the effect of priming deep neural networks with synthetic face images. After introducing our experimental setup (Section \ref{sec:exp-setup}), we study the performance of deep networks when performing face recognition and facial landmark detection trained using a combination of real and synthetic data (Section \ref{sec:exp-gapclose}). We compare our approach to traditional data augmentation methods and to transfer learning from a pre-trained network in Section \ref{sec:exp-face-specific}. Finally, we study how changing the pose distribution and the number of identities in the synthetic data affects the generalization performance of the primed models (Section \ref{sec:exp-changingSynthData}). 

    
	The real-world training data for face recognition is sampled from the cleaned Casia WebFace dataset \cite{casia}, which comprises 455,594 images of 10,575 different identities. From this dataset, we remove the 27 identities which overlap with the IJB-A dataset. For testing the generalization performance we use: 1) \textbf{CMU-Multi-PIE}\cite{multipie}, which was recorded under controlled illumination and background conditions. We use the neutral identities from session one with the frontal illumination setting. Images from the two overhead cameras are excluded.  2) \textbf{LFW}\cite{lfw} has been the de facto standard face recognition benchmark for many years. Face images in this dataset are subject to a complex illumination, partial occlusion, and background clutter. 3) \textbf{IJB-A}\cite{ijba} was proposed to further push the frontiers of face recognition. The conditions regarding pose, illumination and partial occlusion are more complex than LFW. In addition, subjects are possibly described by multiple gallery images, these image sets are commonly referred to as \textit{templates}. 
	We evaluate face recognition networks at the task of face verification. We measure the distance between two face images as the cosine distance between their 128-dimensional feature embeddings from the last layer of the FaceNet model.
    \begin{equation}
           s(a,b)=\frac{a \cdot b}{\lVert a\rVert_2 \lVert b\rVert_2} \hspace{0.2cm}.
    \end{equation}%FR-EVALUATION
    For comparing the templates in the IJB-A dataset, we perform softmax averaging of the similarity scores between each image pair as proposed by Masi et al. \cite{masi2016we}. We do not perform any dataset adaptation, thus we test the most challenging face recognition protocol with only \textit{unrestricted, labeled outside data}. For LFW and IJB-A, the pairwise comparisons are provided by their respective protocols. For the Multi-PIE dataset, we follow the LFW protocol. Thus, we perform 10 fold cross validation with 300 pairs of positive and negative samples.
    
	
	The real-world data used for training is sampled from the AFLW \cite{koestinger11a} dataset. We randomly select $1K$ images from the AFLW set for testing and use the rest for training. For benchmarking we use three facial landmarking datasets, all of which contain large variations in pose, illumination, facial occlusion and expression: 1) Our test split from the \textbf{AFLW} set. 2) The test set of the \textbf{LFPW}\cite{belhumeur2013localizing} dataset.  3) The \textbf{300-W}\cite{sagonas2013300} dataset which has been introduced as part of a challenge for ''in the wild'' facial landmark detection and is currently one of the most challenging datasets available for facial landmark detection. We combine all 600 images from the indoor and outdoor scenes into our test set.

	In our experiments, we compare the 21 AFLW-landmarks available in AFLW and 300-W. For LFPW we use the 20 landmarks which overlap with the AFLW set. We compute the detection accuracy as the mean euclidean distance between the prediction and the ground truth annotation, normalized according to the face size. 
    \begin{table}
    	\centering
    	\begin{tabular}{l|cV{2.5}cV{2.5}c}
    		\toprule
    		\multicolumn{4}{c}{\textbf{Face Recognition}} \\
    		\hline
    		Datasets & \textbf{Multi-PIE} & \textbf{LFW}  & \textbf{IJB-A} \\
    		\hline  
    		Metric              & Accuracy  & Accuracy  & TAR \\ 
    		\hline  
    		SYN-only            & 88.9     	& 80.1     & 62.5 \\
    		\clineB{1-4}{2.5}
    		Real-100\%          & 91.2     	& 94.1     & 86.8 \\ 
    		\hline  
    		+ Primed      	& \textbf{93.3}     	& \textbf{95.8}     & \textbf{90.6} \\ 			
    		\clineB{1-4}{2.5}
    		Real-50\%           	& 86.0     	& 92.7   	& 81.9 \\ 
    		\hline
    		+ Primed  		& \textbf{93.0}    		& \textbf{94.8}     & \textbf{88.2} \\ 
    		\clineB{1-4}{2.5}
    		Real-25\%           	& 83.6     	& 89.1     & 71.3 \\ 
    		\hline 
    		+ Primed  		& \textbf{91.3}    		& \textbf{93.6}     & \textbf{85.0} \\			
    		\clineB{1-4}{2.5}
    		Real-10\%            	& 81.7     	& 85.1     & 66.2 \\ 
    		\hline
    		+ Primed   		& \textbf{91.3}      	& \textbf{91.8}    	& \textbf{83.4} \\			
    		\clineB{1-4}{2.5}
    	\end{tabular}
    	\caption{Face recognition performance on the CMU-Multi-PIE, LFW and IJB-A benchmarks. We compare models trained on synthetic face images (SYN-only) to models trained on different sized subsets of the Casia dataset (Real-$\{10\%,25\%,50\%,100\%\}$). We denote primed models that were fine-tuned on real-world data by ``+ Primed" below the corresponding real-world data only result. We measure performance in terms of recognition accuracy and the true acceptance rate ($TAR$) at false acceptance rate $FAR=0.1$. Priming with synthetic faces improves the face recognition performance considerably.}
    	\label{tab:facerec}
    \end{table}\begin{table}
    	\centering
    	\begin{tabular}{c|ccV{2.5}ccV{2.5}cc}
    		\toprule
    		\multicolumn{7}{c}{\textbf{Facial Landmark Detection}} \\
    		\hline
    		Datasets & \multicolumn{2}{cV{2.5}}{\textbf{AFLW}} 	&		\multicolumn{2}{cV{2.5}}{\textbf{LFPW}} &		\multicolumn{2}{c}{\textbf{300-W}}\\				
    		\hline
    		Accuracy      		& 3\%     			& 5\%     			& 3\% & 5\%		  & 3\%      	& 5\%\\
    		\hline
    		SYN only           	& 22.9     			& 67.2      		& 36.9 & 81.6   & 5.1         & 52.5\\
    		\clineB{1-7}{2.5}
    		Real-100\%       	& 47.4     			& 88.8     			& 63.1 & 95.1     & 5.3      	& 78.2\\ 			
    		\hline  
    		+ Primed   				& \textbf{50.9}     & \textbf{89.1}     & \textbf{70.5} & \textbf{95.5}     & \textbf{14.7}      	& \textbf{87.8}\\ 			
    		\clineB{1-7}{2.5}
    		Real-50\%        	& 43.8     			&  83.5   			& 57.5 & 93.6     & 7.6     	& 75.6\\ 
    		\hline
    		+ Primed    				& \textbf{47.8} 	& \textbf{88.2}     & \textbf{68.5} & \textbf{94.6}     & \textbf{8.5}      	& \textbf{82.2}\\ 
    		\clineB{1-7}{2.5}
    		Real-25\%        	& 35.7     			& 80.8     			& 46.8 & 92.2     & 5.1      	& 70.1\\ 
    		\hline 
    		+ Primed    				& \textbf{42.1}    	& \textbf{84.8}     & \textbf{62.4} & 92.2     & \textbf{6.2}      	& \textbf{77.2}\\			
    		\clineB{1-7}{2.5}
    		Real-10\%        	& 26.5     			& 73.5     			& 29.8 & 85.1     & 5.3    		& 60.7\\ 
    		\hline
    		+ Primed    				& \textbf{37.8}     & \textbf{80.5}    	& \textbf{58.1} & \textbf{91.5}     & 5.3      	& \textbf{69.8}\\			
    		\clineB{1-7}{2.5}
    	\end{tabular}
    	\caption{Facial landmark detection performance on the AFLW, LFPW and 300-W benchmarks. We compare models trained on synthetic face images (SYN-only) to models trained on different sized subsets of the AFLW dataset (Real-$\{10\%, 25\%, 50\%, 100\%\}$).  We denote primed models that were fine-tuned on real-world data by ``+ Primed" below the corresponding real-world data only result. We measure detection error at $3\%$ and $5\%$ of the face size (diagonal of the face box). Priming with synthetic faces improves the facial landmark detection performance considerably in all but two experiments.}
    	\label{tab:faceloc}
    \end{table}\textbf{Synthetic face image generation.} The synthetic face images used for training are generated by randomly sampling the parameters of our face image generator (Section \ref{sec:generator}). The shape, color and expression parameters are sampled according to the Gaussian distribution defined by the BFM \cite{bfm17}, while the parameters for the illumination are sampled from the empirical BIP \cite{illuprior}. The head pose is sampled according to a uniform pose distribution on the yaw, pitch and roll angles in the respective ranges $r_{yaw}=[ -90^\circ,90^\circ ]$, $r_{pitch}=[ -30^\circ,30^\circ ]$ and $r_{roll}=[ -15^\circ,15^\circ ]$. Background clutter is simulated by randomly sampling a background texture as explained in Section \ref{sec:generator}. For face recognition, we generate one million face images with $10K$ different identities and $100$ example images per identity. For landmark detection we use $50K$ training images with $10K$ identities and five sample per identity. The number of synthetic images per task were determined empirically. In Section \ref{sec:exp-changingSynthData} we present an additional experimental analysis of the effect of changing these dataset characteristics. 	

\subsection{Effectiveness of Priming with Synthetic Faces}\label{sec:exp-gapclose}    
	In the following experiments, we study how priming deep neural networks with synthetic faces affects the generalization performance and data efficiency.
	
	\textbf{Learning from synthetic data only.} When training with synthetic data only (SYN-only), we observe that for the CMU-Multi-PIE benchmark the performance is similar to that of a network trained with real-world data (Real-100\%). This suggests that our synthetic face image generator can sufficiently model the facial appearance in constrained visual environments. However, on the benchmarks of LFW and IJB-A the SYN-only model performs worse when compared to a network trained with the full real-world dataset. Hence, a prominent real-to-virtual performance gap can be observed when testing with ``in the wild" images. 
	
	A similarly prominent real-to-virtual performance gap can be observed for the task of facial landmark detection (Table \ref{tab:faceloc}). The SYN-only model performs worse when compared to the one trained with real-world data (Real-100\%) across all benchmarks.

	These measurements complement a number of previous works which studied the effectiveness of using synthetic data for training computer vision models. These works show that for image analysis tasks such as e.g. object detection \cite{gupta2014learning} and optical flow \cite{butler2012naturalistic} synthetic data could very well replace real-world data, whereas, for example, for semantic segmentation \cite{mccormac2017scenenet} a real-to-virtual performance gaps exists, that can be closed by fine-tuning with real-world data. 
	
	For the context of facial image analysis, our findings demonstrate that so far it is not possible to fully replace real-world data with synthetic face images. A main reason for the persisting performance gap between models trained with real-world and synthetic face images is that some important characteristics of faces are currently not modeled with the parametric model. These include e.g. facial hair, partial occlusion, the mouth interior and detailed skin textures.	

\textbf{Fine-tuning with real-world data.} In order to bridge the real-to-virtual performance gap we fine-tune the SYN-only model with different subsets (10\%, 25\%, 50\%, 100\%) of the real-world training data. In this way, the synthetic data will prime the model towards the target facial image analysis task, enabling the model to leverage the information in the real-world data more efficiently during the fine-tuning process. The performance of the primed models is denoted as ``+ Primed" in Tables \ref{tab:facerec}\&\ref{tab:faceloc}.	The primed models considerably outperform the unprimed models at \textit{both} tasks: face recognition as well as facial landmark detection. Interestingly, even when fine-tuning with the full real-world datasets the models still have an enhanced performance compared to the unprimed models. Importantly, when less real-world data is available the overall increase in generalization performance becomes even more apparent.
       
       \begin{table}
       	\centering
      	\small       	
       	\begin{tabular}{c|ccV{2.5}ccV{2.5}cc}
       		\toprule
       		\multicolumn{7}{c}{\textbf{Face Recognition}} \\
       		\hline
       		Datasets & \multicolumn{2}{cV{2.5}}{\textbf{Multi-PIE}} 	&		\multicolumn{2}{cV{2.5}}{\textbf{LFW}} &		\multicolumn{2}{c}{\textbf{IJB-A}}\\				
       		\hline
       		\# real data 		& 50\%     			& 100\%     		& 50\% & 100\%		  	& 50\% & 100\%\\
       		\hline
       		Random           	& 86.0     			& 91.2      		& 92.7 & 94.1   		& 81.9  & 86.8\\
       		\clineB{1-7}{2.5}
       		Augmented     	    & 87.7     			& 91.3     			& 91.3 & 94.6     	 	& 82.8 	& 86.7\\ 			
       		\clineB{1-7}{2.5}
       		Primed        			& \textbf{93.0} 				& \textbf{93.3} 				& \textbf{94.8} & \textbf{95.8}     		& \textbf{88.2} 	& \textbf{90.6}\\ 
       		\clineB{1-7}{2.5}
       	\end{tabular}
       	\caption{Comparison of the face recognition performance when initializing the weights of our model randomly to training with augmented data, and to priming with synthetic face images. We measure the performance when using $50\%$ and $100\%$ of the cleaned Casia dataset for training. We test using the CMU Multi-PIE, LFW and IJB-A benchmarks. While the effect of data augmentation is marginal, priming with synthetic face images leads to a consistently enhanced generalization performance.}
       	\label{tab:facerec-imp}
       \end{table}\begin{table}
    	\centering
    	\small	    
    	\begin{tabular}{c|ccV{2.5}ccV{2.5}cc}
    		\toprule
    		\multicolumn{7}{c}{\textbf{Facial Landmark Detection}} \\
    		\hline
    		Datasets & \multicolumn{2}{cV{2.5}}{\textbf{AFLW}} 	&		\multicolumn{2}{cV{2.5}}{\textbf{LFPW}} &		\multicolumn{2}{c}{\textbf{300-W}}\\				
    		\hline
    		\# real data 		& 50\%     			& 100\%     		& 50\% & 100\%		  	& 50\% & 100\%\\
    		\hline
    		Random           	& 43.8     			& 47.4      		& 57.5 & 63.1   		& 75.6  & 78.2\\
    		\clineB{1-7}{2.5}
    		Augmented     	    & 44.9     			& 49.7     			& 61.2 & 68.8     		& 74.3 	& 84.3\\ 			
    		\hline  
    		ImageNet  			& 46.3    			& 47.6    			& 65.2 & 65.3     		& 79.8  & 85.1\\ 			
    		\clineB{1-7}{2.5}
    		Primed        			& \textbf{47.8} 	& \textbf{50.9} 	& \textbf{68.5} & \textbf{70.5}     		& \textbf{82.2} 	& \textbf{87.8}\\ 
    		\clineB{1-7}{2.5}
    	\end{tabular}
    	\caption{Comparison of the facial landmark detection performance when initializing the weights of our model randomly to training with augmented data, transfer learning from the ImageNet classification dataset, and to priming with synthetic face images. We measure the performance when using $50\%$ and $100\%$ of the AFLW training data. For the AFLW and LFPW datasets we show the detection accuracy at $3\%$ error and for 300-W at 5\% error. Priming with synthetic data leads to a consistent superior performance.}
    	\label{tab:faceloc-imp}
    \end{table}% FR fine tuning
	At the task of face recognition, the real-to-virtual gap can almost be closed with $25\%$ of the real data, already  outperforming the Real-100\% model with $50\%$ of the real-world dataset. Remarkably, priming with synthetic data leads to a performance increase across \textit{all} benchmarks, even though the individual datasets have very different imaging characteristics. Note that we do not perform any dataset adaptation in our experiments.
   
    For facial landmark detection we observe similar effects when priming with synthetic data. The real-to-virtual gap can mostly be closed with $50\%$ of the real data. Large performance gains are achieved at small error rates of $3\%$ compared to the unprimed models. Similarly, to the task of face recognition we observe a strong generalization performance \textit{across} datasets without performing dataset adaptation. This can be observed best from the results on the challenging 300-W benchmark. Note that there is a domain gap between the AFLW training data and the 300-W test data, which manifests in the worse performance of the Real-100\% model compared to the results of the LFPW benchmark. Using our approach this performance gap is constantly improved by about $7-9\%$ at an error rate of $5\%$ for all dataset sizes.
    
    In summary, our results in Tables \ref{tab:facerec}\&\ref{tab:faceloc} show that priming with synthetic faces has the following positive effects for \textit{both} face recognition and facial landmark detection tasks:
    \begin{enumerate} 	    
    	\item A compelling increase in the generalization performance is achieved when priming with synthetic data before training with $100\%$ of the real data.
     	\item After priming, only $25\%-50\%$ of the real-world data is needed to achieve the same performance as is obtained when training an unprimed model with the full real-world dataset.
    	\item Priming with synthetic data enhances the generalization performance consistently across all benchmark datasets.
    \end{enumerate}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% FACE-specific data advantage%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%			\subsection{Comparison to data augmentation and transfer learning}\label{sec:exp-face-specific}
   
   In this Section, we compare our priming approach to related techniques for enhancing the generalization ability of deep neural networks when performing facial image analysis. A popular approach to increase the training data (and thus the performance) is data augmentation. Thereby, the original training data is manipulated with general image transformations such as color changes, 2D rotations and mirroring \cite{chatfield2014return,yang2015mirror,wolf2011effective,chen2012dictionary}. Another widely applied tool for performance increase is transfer learning from image classification, where a model is pre-trained on a general object classification task and subsequently fine-tuned on the desired facial image analysis task. In this way, the model can re-use the knowledge acquired for discriminating objects and transfer it to facial image analysis.
   
      \begin{table}
      	\centering
      	\small
      	\begin{tabular}{l|cV{2.5}cV{2.5}c}
      		\toprule
      		\multicolumn{4}{c}{\textbf{Face Recognition}} \\
      		\hline
      		Datasets & \textbf{Multi-PIE} & \textbf{LFW}  & \textbf{IJB-A} \\
      		\hline  
      		Metric          & Accuracy  & Accuracy  & TAR \\ 
      		\hline  
      		Real-100\%      & 91.2     & 94.1     & 86.8 \\ 
      		\hline  
      		+ Primed  			& 93.3     & 95.8     & 90.6 \\
      		\hline     	    		 
      		+ Primed-Frontal   & 91.1     & 93.1     & 82.7 \\ 
      		\hline  
      		+ Primed-Double    & 95.4     & 96.0     & 92.4 \\
      		\clineB{1-4}{2.5}
      	\end{tabular}   	    		
     	
      	\caption{Effect of changing the pose distribution and number of facial identities in the synthetic data on the face recognition performance. We measure the recognition accuracy and $TAR$ at $FAR=0.1$ and compare to training with real-world data only (Real-100\%) as well as to priming with the original synthetic data (Primed). The introduction of a strong bias towards frontal face poses of $[ -35^\circ,35^\circ ]$ (Primed-Frontal) reduces the performance. Doubling the amount of facial identities (and thus training data) in the synthetic dataset to $20K$ (Primed-Double) increases to generalization performance.}
      	\label{tab:character-face}
      \end{table}\begin{table}
  	\centering
  	\small
  	\begin{tabular}{l|ccV{2.5}ccV{2.5}cc}
  		\toprule
  		\multicolumn{7}{c}{\textbf{Facial Landmark detection}} \\
  		\hline
  		Datasets & \multicolumn{2}{cV{2.5}}{\textbf{AFLW}} 	&		\multicolumn{2}{cV{2.5}}{\textbf{LFPW}} &		\multicolumn{2}{c}{\textbf{300-W}}\\				
  		\hline
  		Accuracy      		& 3\%     & 5\%      & 3\% & 5\%	   & 3\%  & 5\%\\
  		\hline
  		Real-100\%       	& 47.4    & 88.8     & 63.1 & 95.1     & 5.3  & 78.2\\ 			
  		\hline  
  		+Primed  			& 50.9    & 89.1     & 70.5 & 95.5     & 14.7 & 87.8\\ 			
  		\hline  
  		+Primed-Frontal		& 48.2    & 88.6     & 65.3 & 94.9     & 11.4 & 83.6\\	
  		\hline  
  		+Primed-Double  	& 51.8    & 89.4     & 70.8 & 95.9     & 15.1 & 88.2\\	
  		\clineB{1-7}{2.5}
  	\end{tabular}
  	\caption{Effect of changing the pose distribution and number of facial identities in the synthetic data on the facial landmark detection performance. We compare to training with real-world data only (Real-100\%) as well as to priming with the original synthetic data (Primed). The introduction of a strong bias towards frontal face poses of $[ -35^\circ,35^\circ ]$ (Primed-Frontal) reduces the performance compared to when priming with the unbiased dataset (Primed). Doubling the amount of facial identities (and thus training data) in the synthetic dataset to $20K$ (Primed-Double) increases the generalization performance marginally.}
  	\label{tab:character-loc}
  \end{table}        
        
   In our experimental setup, we augment training data by mirroring across the vertical axis and by rotating a face image twice around the center of the face randomly in a range of $[-30^\circ,30^\circ]$. In this way, we augment the head pose distribution in the training data. For pre-training, we use an AlexNet \cite{krizhevsky2012imagenet,jia2014caffe} that was trained for image classification on the ImageNet dataset~\cite{russakovsky2015imagenet}. We subsequently fine-tune this model for facial landmark detection. We could not test this setup for face recognition, as there are no pre-trained FaceNet architectures available. As a general performance baseline, we use a model with randomly initialized weights.
   
   In Tables \ref{tab:facerec-imp}\&\ref{tab:faceloc-imp}, we summarize the performance results for all tested approaches when using $50\%$ of the real-world training data and when using all of the available data for fine-tuning. We observe that data augmentation with 2D image transformations has only a marginal effect on face recognition. Apparently, the large-scale face recognition dataset already offers enough data to generalize well across the different head poses that were simulated during data augmentation. For facial landmark detection we observe a positive effect, due to the smaller scale of the training dataset. At this task, pre-training seems to have a more positive effect than data augmentation, when only $50\%$ of the training data is used.
   
	Our approach of priming the models with synthetic face images is superior to data augmentation with 2D image transformations and transfer learning from a classification task. In our results the performance of face recognition and facial landmark detection is enhanced with any amount of available real-world training data. The major advantage of our approach is that it offers \textit{face-specific} knowledge that goes beyond the information offered by general techniques such as data augmentation and transfer learning. It is also noteworthy that all three approaches are to some extent complementary, and can thus be combined.  
   

	In this section, we change	the pose-distribution and the number of identities in the synthetic dataset and study how this affects the generalization performance when performing face recognition and facial landmark detection tasks. Note that we change one dataset characteristic at a time while keeping all other parameters fixed to the original values of our previous experiments. The experimental results are summarized in Tables \ref{tab:character-face}\&\ref{tab:character-loc}.     
  
    \textbf{Bias to frontal pose.} We bias the yaw pose in the synthetic data to a range of $r_{yaw}=[ -35^\circ,35^\circ ]$, while keeping the amount of data the same as in the original SYN dataset. In our experiments, we prime the models with the biased dataset and subsequently fine-tune with the full real world datasets (``primed-frontal" in Tables \ref{tab:character-face}\&\ref{tab:character-loc} ). From the results, we can observe a performance decrease at both facial image analysis tasks compared to priming with the original unbiased dataset (``primed"). This negative effect is particularly prominent for the most challenging datasets, IJB-A and 300-W. This decrease in the generalization performance demonstrates that deep neural networks do benefit from the large pose variation in the synthetic data, suggesting that conceptual knowledge about 3D head pose variation is learned and transferred to the real-world data. 

\textbf{Increasing the number of training identities.} We double the number of identities in the synthetic data to $20K$, thus generating double the amount of training images for each facial image analysis task. It is important to note that such an increase in the training data would require a great effort for real-world data, whereas for synthetic data it is essentially cost-free. After priming with the new dataset and subsequently fine-tuning on the full real-world datasets, we observe an additional increase in the generalization performance when performing either facial image analysis task (``primed-double" in Tables \ref{tab:character-face}\&\ref{tab:character-loc} ). The performance increase is particularly prominent for face recognition, whereas for facial landmark detection it is marginal, suggesting that more complex facial image analysis tasks benefit more from additional synthetic data. Nevertheless, these results confirm our observations that priming with synthetic face images increases the generalization performance of facial image analysis tasks. Additionally, the results suggest that the generalization performance could be increased even more when additional synthetic training data is used.
    
    In summary, the experiments in this section demonstrate that the additional variability in the synthetic data, in terms of pose and facial identities, is an important factor for the overall increase in the generalization performance of real-world data.
    
	


	In this work, we demonstrated the positive effects of priming deep neural networks with synthetic face images for face recognition and facial landmark detection. In our experiments, we observed the following phenomena for \textit{both} facial image analysis tasks:
	
	\textbf{Enhanced generalization performance.} Priming with synthetic data followed by fine-tuning with real-world data enhances the generalization performance consistently across all benchmark datasets compared to training with real-world data only. Our experiments in Section \ref{sec:exp-changingSynthData} show that the additional variability in the synthetic data, in terms of head pose and facial identity, is a crucial factor for the measured increase in generalization performance. Thus, we provide evidence that the negative effects of biases in the real-world data are alleviated when priming with synthetic data that is unbiased in these variables.
	
	\textbf{Enhanced data efficiency.} The improvement in generalization performance becomes more prominent, as we reduce the amount of real-world data available at training time. More importantly, the smaller the amount of real-world training data, the larger is the improvement in performance, compared to models that were not primed. This result is intuitive since with increasing number of real-world data, any algorithm will eventually reach the optimal accuracy which is possible under the model class. Hence, the role of priming with synthetic face images is to provide knowledge about those facial appearance variations that are simulatable by our data generator. Only relatively few real-world data are then needed to learn the image variation which currently cannot be simulated well. Using our priming approach the number of real-world data needed to achieve competitive performance was reduced by 75\% for face recognition, and by 50\% for facial landmark detection.
	
	\textbf{Superiority to data augmentation and transfer learning from classification tasks.} Common ways of improving the sample efficiency and generalization performance of deep neural networks are data augmentation and transfer learning from large-scale object classification tasks. Our experiments show that the proposed method of priming with synthetic face images has a considerably more positive effect on the performance of deep neural networks than data augmentation and transfer learning from a classification task. This observation is intuitive because our approach provides additional information that is specific to the target facial image analysis tasks. This kind of knowledge cannot be learned from a general object classification task nor when the training data is augmented with basic image transformations.

	Notably, our approach of priming with synthetic face images is beneficial for two very different facial image analysis tasks: face recognition and facial landmark detection. We conclude from this observation that other facial image analysis tasks, such as face segmentation or pose estimation, could also potentially benefit from priming. In order to support future research in this direction our synthetic data generator has been made publicly available. 
	
	
	

	 

	\section*{Acknowledgment} Adam Kortylewski is supported by a Novartis University of Basel Excellence Scholarship for Life Sciences. We gratefully acknowledge the support of NVIDIA with the donation of a Titan Xp and a Quadro P5000.

	

	
	\bibliographystyle{spmpsci}
	\bibliography{egbib}  

\end{document}


