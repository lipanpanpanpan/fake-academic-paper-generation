\documentclass{article}
\usepackage[final]{nips_2017}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{soul}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{color}
\usepackage{lipsum}
\usepackage{enumitem}
\usepackage{microtype}
\usepackage{marginnote}
\usepackage{caption}
\newcommand{\R}{\mathbb {R}}
\newcommand{\GNAT}{GLO}
\newcommand{\note}[1]{\marginnote{{\color{red} #1}}}

\title{Optimizing the Latent Space of Generative Networks}

\author{Piotr Bojanowski, Armand Joulin, David Lopez-Paz, Arthur Szlam\\
Facebook AI Research\\
\texttt{\{bojanowski, ajoulin, dlp, aszlam\}@fb.com}}

\begin{document}

\maketitle

\begin{abstract}
Generative Adversarial Networks (GANs) have been shown to be able to sample impressively realistic images.
  GAN training consists of a saddle point optimization problem that can be thought of as an adversarial game between a generator
which produces the images, and a discriminator, which judges if the images are real. Both the generator and the discriminator are commonly parametrized as
 deep convolutional neural networks.
The goal of this paper is to disentangle the contribution of the optimization procedure and the network parametrization to the success of GANs.
To this end we
introduce and study \emph{Generative Latent Optimization}~(\GNAT{}), a framework to
train a generator without the need
to learn a discriminator, thus avoiding challenging adversarial
optimization problems.  We show experimentally that
\GNAT{} enjoys many of the desirable properties of GANs: learning from large data,
synthesizing visually-appealing samples, interpolating meaningfully between
samples, and performing linear arithmetic with noise vectors.
\end{abstract}

\section{Introduction}\label{sec:introduction}
Generative Adversarial Networks~ (GANs)~\citep{gan} are a powerful framework to
learn generative models of natural images.  GANs learn these generative models
by setting up an adversarial game between two learning machines.  First, a
\emph{generator} plays to transform \emph{noise vectors} into \emph{fake
samples}, which resemble \emph{real samples} drawn from a distribution of
natural images.  Second, a \emph{discriminator} plays to distinguish between
real and fake samples.  During training, the generator and the discriminator
learn in turns.  First, the discriminator learns to assign high scores to real
samples, and low scores to fake samples.  Second, the generator learns to
increase the scores of fake samples, as to fool the discriminator. After proper
training, the generator is able to produce visually-appealing samples from
noise vectors.

Recently, GANs have been used to produce samples of remarkable quality from
distributions of natural images, such as handwritten digits, human faces, and
house interiors~\citep{dcgan}.  Furthermore, GANs exhibit three strong signs of
generalization. First, the generator translates~\emph{linear interpolations in
the noise space} into~\emph{semantic interpolations in the image space}. In
other words, a linear interpolation in the noise space will generate a smooth
interpolation of visually-appealing images.  Second, the generator allows
\emph{linear arithmetic in the noise space}.  Similarly to word embeddings
\citep{mikolov2013efficient}, linear arithmetic indicates that the generator
organizes the noise space to disentangle the nonlinear factors of variation of
natural images into linear statistics.
Third, the generator is able to to synthesize new visually-appealing samples.
This allows for applications such as image inpainting
\citep{IizukaSIGGRAPH2017} and super-resolution~\citep{ledig2016photo}.

Despite their successes, training and evaluating GANs is notoriously difficult.
The adversarial optimization problem implemented by GANs is sensitive to random
initialization, architectural choices, and hyper-parameter settings.  In many
cases, a fair amount of human care is necessary to find the correct
configuration to train a GAN in a particular dataset. It is common to observe
generators with similar architectures and hyper-parameters to exhibit
dramatically different behaviors.  Even when properly trained, the resulting
generator may synthesize samples that resemble only a few localized regions (or
modes) of the data distribution~\citep{gan_tutorial}. While several advances
have been made to stabilize the training of GANs~\citep{salimans2016improved},
this task remains more art than science.

The difficulty of training GANs is aggravated by the challenges in their evaluation:
since the likelihood of a GAN with respect to the data is intractable, the
current gold standard to evaluate the quality of a GAN model is to eyeball the
samples produced by the generator.~The evaluation of the discriminator is also
not simple, since the visual features learned by the discriminator
do not always transfer well to supervised
tasks~\citep{donahue2016adversarial,dumoulin2016adversarially}. Finally, the
application of GANs to non-image data has been relatively limited.

\paragraph{Research question}
To model natural images with GANs, the generator and the discriminator are
often parametrized as deep Convolutional
Networks~(convnets)~\citep{lecun1998gradient}. Therefore, it is reasonable to
hypothesize that the reasons for the success of GANs in modeling natural images
come from two complementary sources:
\begin{enumerate}[label=(A\arabic*),start=1]
  \item Leveraging the powerful inductive bias of deep convnets.
  \item The adversarial training protocol.
\end{enumerate}
This work attempts to disentangle the factors of sucess (A1) and (A2) in GAN
models.  Specifically, we build an algorithm that relies on (A1), avoids (A2),
and obtains competitive results when compared to a GAN.

\paragraph{Contribution.}

We investigate the importance of (A1) and propose a drastic simplification
of GAN training (Section~\ref{sec:gnat}).
Our approach, called \emph{Generative Latent Optimization}~(\GNAT{}), maps one learnable noise vector
to each of the images in our dataset by minimizing a simple reconstruction
loss.
Since we are predicting~\emph{images from learnable noise},~\GNAT{} borrows
inspiration from recent methods to predict~\emph{learnable noise from
images}~\citep{nat}.
Alternatively, one can understand~\GNAT{} as an auto-encoder where the latent
representation is not produced by a parametric encoder, but learned freely.
In contrast to GANs,  we track of the correspondence between each learned noise
vector and the image that it represents.  Hence, the goal of \GNAT{} is to
find a meaningful organization of the noise vectors, such that they can be
mapped to their target images. To turn \GNAT{} into a generative model, we
observe that it suffices to draw vectors from the span of the learned noise vectors.

In our experiments (Section~\ref{sec:experiments}), we show
that \GNAT{} inherits many of the appealing features of GANs, while enjoying much
simpler training. In particular, we study the efficacy of \GNAT{} to compress and
decompress a dataset of images~(Section~\ref{sec:experiments:compression}),
generate new samples~(Section~\ref{sec:experiments:generation}), perform linear
interpolations and extrapolations in the noise
space~(Section~\ref{sec:experiments:interpolation}), perform linear
arithmetics~(Section~\ref{sec:experiments:arithmetic}) and
super-resolution~(Section~\ref{sec:experiments:conditional}).
Our experiments provide
quantitative and qualitative comparisons to Principal Component Analysis (PCA),
Deep Convolutional Auto-Encoders (DCAE), and GANs. We focus on the MNIST,
CIFAR-10, STL-10, CelebA, and LSUN-Bedroom datasets.
We conclude our exposition in Section~\ref{sec:discussion}.

\section{The Generative Latent Optimization (\GNAT{}) model}\label{sec:gnat}

First, we consider a large set of images~$\{ x_1, \ldots, x_N \}$, where each
image~$x_i \in \mathcal{X}$ has dimensions~$3 \times w \times h$.  Second, we
initialize a set of~$d$-dimensional random vectors~$\{ z_1, \ldots, z_N \}$,
where $z_i \in \mathcal{Z} \subseteq \mathbb{R}^d$ for all~$i =1, \ldots N$.
Third, we pair the dataset of images with the random vectors, obtaining the
dataset~$\{(z_1, x_1), \ldots, (z_N, x_N) \}$.  Finally, we jointly learn the
parameters~$\theta$ in~$\Theta$ of a generator~$g_\theta : \mathcal{Z} \to
\mathcal{X}$ and the optimal noise vector~$z_i$ for each image~$x_i$, by
solving:
\begin{equation}
  \label{eq:nai}
  \min_{\theta\in\Theta}~~\frac{1}{N}\sum_{i=1}^N~\left[~\min_{z_i \in\mathcal{Z}}~~\ell\left(g_\theta(z_i),x_i\right)\right],
\end{equation}
In the previous,~$\ell : \mathcal{X} \times \mathcal{X}$ is a loss function
measuring the reconstruction error from~$g(z_i)$ to~$x_i$.  We call this model
Generative Latent Optimization~(\GNAT{}). Next, let us describe the most
distinctive features of~\GNAT{}.

\paragraph{Learnable~$z_i$.} One of the distinctive features of~\GNAT{} is the joint
optimization of the random inputs~$z_1, \ldots, z_N$ and the model parameter~$\theta$.
This is in contrast to autoencoders \citep{boulard_ae}, which assume a parametric model~$f :
\mathcal{X} \to \mathcal{Z}$, usually referred to as \emph{encoder}, to compute
the vector~$z$ from samples~$x$, and minimize the reconstruction loss~$\ell(g(f(x)),x)$.
Since in~\GNAT{} the vector~$z$ is a free
parameter, our model can recover all the solutions to be found by an
autoencoder, and reach some others. In a nutshell,~\GNAT{} can be
viewed as an~``encoder-less'' autoencoder, or as a~``discriminator-less''~GAN.

\paragraph{Choice of~$\mathcal{Z}$.}
The representation space~$\mathcal{Z}$ should encapsulate all of
our~\emph{prior~knowledge} about the data~$\{x_1, \ldots, x_N\}$.  For
instance, in the sparse coding literature, it is common practice to set the
representation space to the closed unit ball~$\mathcal{B}(1, d,
p)$~\citep{bach2011convex}, where
\begin{equation*}
    \mathcal{Z} = \mathcal{B}(r,d,p) = \left\lbrace z \in \mathbb{R}^d : \| z \|_p
    \leq r
    \right\rbrace.
\end{equation*}
Since we are interested in matching the the properties of GANs, we make similar
choices to them when it comes to the representation space~$\mathcal{Z}$. The
most common choices of the representation space for GANs are either the uniform
distribution on the hypercube~$[-1, +1]^d$, or the Normal distribution
on~$\mathbb{R}^d$. Since Gaussian distributions lead to more stable GAN
training~\citep{dcgan}, we will use this distribution to design our
representation space. Since random vectors~$z$ drawn from the~$d$-dimensional
Normal distribution are very unlikely to land far outside the
ball~$\mathcal{B}(\sqrt{d}, d, 2)$, we rescale random vectors drawn from
the~$d$-dimensional Normal distribution, and set our representation space
to~$\mathcal{B}(1, d, 2)$.

\paragraph{Choice of loss function.}
On the one hand, the squared-loss function~$\ell_2(x,x') = \| x - x' \|_2^2$ is
a simple choice, but leads to blurry reconstructions of natural images.
On the other hand,~GANs use a~convnet~(the discriminator) as loss function. Since
the early layers of~convnets focus on edges, the samples from a~GAN are sharper.
Therefore, our experiments provide quantitative and qualitative comparisons
between the~$\ell_2$ loss and the Laplacian pyramid~$\text{Lap}_1$ loss~\citep{lapl1cost}.
The~$\text{Lap}_1$ loss corresponds to a small~convnet that estimates the Earth
mover's distance in the pixel space, using an Euclidean ground metric. Mathematically,
\begin{equation*}
    \label{eq:laploss}
	\text{Lap}_1(x,x') = \sum_{j} 2^{-2j} |L^j(x) - L^j(x')|_1
\end{equation*}
where~$L^j(x)$ is the~$j$-th level of the Laplacian pyramid representation of~$x$.
Therefore, the~$\text{Lap}_1$ loss weights the details
at multiple scales equally, and in particular, does not penalize small shifts in the locations of edges as strongly as the Euclidean metric.

\paragraph{Optimization.}
For any choice of differentiable generator, the objective \eqref{eq:nai} is
differentiable with respect to~$z$, and~$\theta$.
Therefore, we will learn~$z$ and~$\theta$ by Stochastic Gradient Descent (SGD).
The gradient of \eqref{eq:nai} with respect to~$z$ can be
obtained by backpropagating the gradients through the generator
function~\citep{compressed}. We project each~$z$ back to the representation
space~$\mathcal{Z}$ after each update. In the case of~$\mathcal{Z} =
\mathcal{B}_d^2$, projecting~$z$ after each update is a division by~$\max(\|z\|_2, 1)$.

\subsection{Prior work}

\paragraph{Generative Adversarial Networks.}
GANs were introduced by \citet{gan}, and
refined in multiple recent
works~\citep{denton2015deep,dcgan,zhao2016energy,salimans2016improved}.
As described in Section~\ref{sec:introduction}, GANs construct a generative
model of a probability distribution $P$ by setting up an
adversarial game between a generator $g$ and a discriminator $d$:
\begin{equation*}
  \label{eq:gan}
  \min_G \max_D \mathbb{E}_{x\sim P}\, \log d(x) + \mathbb{E}_{z \sim Q}\,
  (1-\log d(g(z))).
\end{equation*}
In practice, most of the applications of GANs concern modeling
distributions of natural images. In these cases, both the generator $g$ and
the discriminator $d$ are parametrized as deep convnets
\citep{lecun1998gradient}. Among the multiple architectural variations
explored in the literature, the most prominent is the Deep Convolutional
Generative Adversarial Network (DCGAN) \citep{dcgan}. Therefore, in this paper
we will use the specification of the generator function of the DCGAN to
construct the generator of \GNAT{} across all of our experiments.

\paragraph{Autoencoders.}
In their simplest form, an Auto-Encoder (AE) is a pair of neural networks, formed by
an encoder $f : \mathcal{X} \to \mathcal{Z}$ and a decoder $g : \mathcal{Z} \to
\mathcal{X}$. The role of an autoencoder is the compress the data $\{x_1,
\ldots, x_N\}$ into the representation $\{z_1, \ldots, z_N\}$ using the encoder
$f(x_i)$, and decompress it using the decoder $g(f(x_i))$. Therefore,
autoencoders minimize
$\mathbb{E}_{x \sim P}\,\ell(g(f(x)), x)$,
where $\ell : \mathcal{X} \times \mathcal{X}$ is a simple loss function, such
as the mean squared error.  There is a vast literature on autoencoders,
spanning three decades from their conception \citep{boulard_ae,
baldi1989neural}, renaissance \citep{hinton_science}, and
recent
probabilistic extensions~\citep{vincent2008extracting,kingma2013auto}.
One of the main messages of this paper is that Deep Convolutional Auto-Encoders~(DCAE),
that is, autoencoders where both the encoder and decoder are deep convnets,
exhibit many of the attractive features of GANs but, like \GNAT{}, allow a much
simpler training.

Several works have combined GANs with AEs. For instance, \citet{zhao2016energy}
replace the discriminator of a GAN by an AE, and \citet{adversarial_generator}
replace the decoder of an AE by a generator of a GAN.  Similar to \GNAT{},
these works suggest that the combination of standard pipelines can lead to good
generative models. In this work we attempt one step further, to
explore if learning a generator alone is possible.

\paragraph{Inverting generators.}
Several works attempt at recovering the latent representation of an
image with respect to a generator.
In particular, \citet{precise_recovery, zhu2016generative}
show that it is possible to recover $z$ from a generated sample.
Similarly, \citet{inverting_generator} show that it is possible to learn the
inverse transformation of a generator.
These works are similar to \citep{visualizing_cnn}, where the gradients of a
particular feature of a convnet are back-propagated to the pixel space in order to
visualize what that feature stands for. From a theoretical perspective,
\citet{bruna2013signal} explore the theoretical conditions for a network to be
invertible.
All of these inverting efforts are instances of the \emph{pre-image problem},
\citep{kwok2004pre}.

\citet{compressed} have recently showed that
it is possible to recover from a trained generator
with compressed sensing. Similiar to our work, they use a $\ell_2$ loss and backpropagate
the gradient to the low rank distribution. However, they do not train the generator
simulatenously. Jointly learning the representation and training the generator
allows us to extend their findings. \citet{santurkar2017generative} also use generative models
to compress images.

Several works have used an optimization of a latent representation for the express purpose of generating
realistic images, e.g. \citep{,Portilla2000,nguyen2016ppgn}.  In these works, the total loss
function optimized to generate is trained separately from the optimization of the latent representation (in the
former, the loss is based on a complex wavelet transform, and in the latter, on separately trained autoe-encoders
and classification convolutional networks).   In this work we train the latent representations and the generator together from scratch;
and show that at test time we may sample new latent $z$ either with simple parametric distributions or by interpolation in the latent space.

\paragraph{Learning representations.}
Arguably, the problem of learning representations from data in an unsupervised
manner is one of the long-standing problems in machine learning
\citep{bengio2013representation, lecun2015deep}. One of the earliest algorithms
used to achieve is goal is Principal Component Analysis, or PCA
\citep{pca_original, pca}. For instance, PCA has been used to learn
low-dimensional representations of human faces \citep{eigenfaces}, or to
produce a hierarchy of features \citep{pcanet}. The nonlinear extension of PCA
is an autoencoder \citep{baldi1989neural}, which is in turn one of the most
extended algorithms to learn low-dimensional representations from data.
Similar algorithms learn low-dimensional representations of data with certain
structure. For instance, in sparse coding \citep{aharon2006rm,
mairal2008sparse}, the representation of one image is the linear combination of
a very few elements from a dictionary of features.
More recently, \citet{understanding} realized the capability of deep neural
networks to map large collections of images to noise vectors, and \citet{nat}
exploited a similar procedure to learn visual features unsupervisedly.
Similarly to us, \citet{nat} allow the noise vectors $z$ to move in
order to better learn the mapping from images to noise vectors. The proposed \GNAT{} is the
analogous to these works, in the opposite direction: learn a map \emph{from} noise vectors
\emph{to} images.
Finally, the idea of mapping between images and noise to learn generative models
is a well known technique \citep{gaussianization, gauss_laparra,
nonequilibrium, infusion}.

\section{Experiments}\label{sec:experiments}

We organized our experiments as follows. First,
Section~\ref{sec:experiments:baselines} describes the generative models that we
compare, along with their implementation details. Section~\ref{sec:experiments:datasets} reviews
the image datasets used in our experiments.
Section~\ref{sec:experiments:results} discusses the results of our experiments,
including the compression of datasets
(Section~\ref{sec:experiments:compression}), the generation of new samples
(Section~\ref{sec:experiments:generation}), the interpolation of samples
(Section~\ref{sec:experiments:interpolation}), and the arithmetic of noise
vectors (Section~\ref{sec:experiments:arithmetic}).

\begin{figure}[t]
    \begin{center}
      \begin{tabular}{lr}
        \includegraphics[width=0.48\textwidth]{lsun_original.jpg}&
        \includegraphics[width=0.48\textwidth]{face_original.jpg}
        \\
        \includegraphics[width=0.48\textwidth]{lsun_rec.jpg}&
        \includegraphics[width=0.48\textwidth]{face_rec.jpg}
        \\
        \includegraphics[width=0.48\textwidth]{lsun_rec_hq.jpg}&
        \includegraphics[width=0.48\textwidth]{face_rec_hq.jpg}
      \end{tabular}
    \end{center}
    \caption{Reconstruction results for~\GNAT{} on LSUN-Bedroom and celebA. From
    top to bottom, the original images, the reconstruction with a noise
    representation with $d=100$ dimensions and $d=512$ dimensions.}
    \label{fig:qua}
\end{figure}

\subsection{Methods}\label{sec:experiments:baselines}
    Throughout our experiments, we compare with four different models.
    The representation space is $100$-dimensional for all models.
    \begin{itemize}
        \item \textbf{PCA} \citep{pca_original}, equivalent to a linear
        autoencoder \citep{baldi1989neural}, where we retain the top $100$
        principal components.
        \item \textbf{DCAE} \citep{cae}, a deep convolutional autoencoder
        minimizing mean-squared error. The encoder $f$ follows the same
        architecture as the DCGAN discriminator of~\citet{dcgan}, after
        removing the Tanh$()$ layer, and letting the last layer return $100$
        feature maps instead of $3$. The decoder $g$ follows the same
        architecture as the DCGAN generator. We do not impose any constraint on
        the representation space $\mathcal{Z}$, that is, the output of the
        encoder. We use the Adam optimizer \citep{adam} with default
        parameters.
        \item \textbf{DCGAN} \citep{dcgan}. Since GANs do not come with a
        mechanism (inverse generator) to retrieve the random vector $g^{-1}(x)$
        associated with an image $x$, we
        estimate this random vector by 1) instantiating a random vector $z_0$, and 2)
        computing updates $z_{i+1} = z_{i+1} - \alpha \frac{\partial
        \ell(g(z_i), x)}{\partial z_i}$ by backpropagation until convergence.
        Obviously, our experiments measuring reconstruction error are
        disadvantageous to GANs, since these are models that are \emph{not}
        trained to minimize this metric. We use the Adam optimizer \citep{adam}
        with the parameters suggested by \citep{dcgan}.
        \item \textbf{\GNAT{} (proposed model)}. We will train a \GNAT{} model
        where the generator follows the same architecture as the generator in
        DCGAN. We use Stochastic Gradient Descent (SGD) to optimize both $\theta$
        and $z$, setting the learning rate for $\theta$ at $1$ and the
        learning rate of $z$ at $10$. We set the representation space
        $\mathcal{Z} = \mathcal{B}(1,d,2)$. To
        ease optimization, we initialize the noise vectors to the solution
        provided by a PCA.
    \end{itemize}


\begin{figure}[t]
  \includegraphics[width=0.19\textwidth]{spectrum_mnist.pdf}\hfill
  \includegraphics[width=0.19\textwidth]{spectrum_cifar.pdf}\hfill
  \includegraphics[width=0.19\textwidth]{spectrum_celeb.pdf}\hfill
  \includegraphics[width=0.19\textwidth]{spectrum_stl.pdf}\hfill
  \includegraphics[width=0.19\textwidth]{spectrum_lsun.pdf}
  \caption{Percent of the representation space explained as a function of the number of singular vectors used to reconstruct it for PCA, AE and \GNAT{}.
  The more flat the curve, the more evenly distributed the energy across the spectrum.  Models with flat curves use more of their allotted space to store information.}
  \label{fig:eigenspectrum}
\end{figure}
\begin{table}[t]
  \begin{center}
  \begin{tabular}{@{}llccccc@{}}
    \toprule
    MSE && MNIST  & CIFAR-10 & STL-10 & CelebA & LSUN\\
    \midrule
    PCA && 0.011 & 0.016 & 0.039 & 0.024 & 0.036 \\
    DCAE && 0.0002 & 0.006 & 0.023 & 0.015 & 0.027 \\
    GAN  && 0.016 & 0.022 & 0.042 & 0.030 & 0.034  \\
    \midrule
    \textbf{\GNAT{} (L2)}  && 0.0001 & 0.005 & 0.019 & 0.010 & 0.021 \\
    \textbf{\GNAT{} (LapL1)}  && 0.0001 & 0.007 & 0.023 & 0.012 & 0.025 \\
    \bottomrule
  \end{tabular}
  \end{center}
  \caption{
    Reconstruction errors in $\ell_2$ %and $\text{Lap}_1$
    loss for all methods
    and datasets.
  }
  \label{table:errors}
\end{table}

\subsection{Datasets}\label{sec:experiments:datasets}

We evaluate all models on five datasets of natural images. Unless specified
otherwise, we use the prescribed training splits to train our generative
models. All the images are rescaled to have three channels, $64$ pixels on
their short side, center-cropped, and normalized to pixel values in $[-1,+1]$.
\begin{itemize}
  \item \textbf{MNIST}~\citep{mnist} is a set of handwritten digits.  We use the
  original training set, composed of $60,000$ grayscale images of size
  $28\times 28$ pixels.
  \item \textbf{CIFAR-10}~\citep{krizhevsky2009learning} is set of images
  belonging to ten different object categories.
  We use the original training set, composed of $50,000$ color
  images of size $32 \times 32$ pixels.
  \item \textbf{STL-10}~\citep{coates2011analysis}, where we use the set of
  $100,000$ unlabeled images of size $96 \times 96$ pixels. Some of these
  images contain thick black borders.
  \item \textbf{CelebA}~\citep{liu2015faceattributes} is a set of $202,599$
  portraits of celebrities. We use the \emph{aligned and cropped} version,
  which preprocesses each image to a size of $64\times 64$ pixels.
  \item \textbf{LSUN}~\citep{xiao2010sun} is a set of millions of images of
  scenes belonging to different scene categories. Following the tradition in
  GAN papers~\citep{dcgan}, we use the $3,033,042$ images belonging
  to the \emph{bedroom} category.
\end{itemize}

\begin{figure}[t]
  \begin{center}
  \includegraphics[width=0.49\textwidth]{gnat-stl-2-sample-000001.jpg}
  \includegraphics[width=0.49\textwidth]{gnat-celeba-2-sample-000001.jpg}
  \includegraphics[width=0.49\textwidth]{gaussian-STL-DLP.jpg}
  \includegraphics[width=0.49\textwidth]{gaussian-Faces-DLP.jpg}
  \end{center}
  \caption{
    Samples generated using GAN \textbf{(top)} and \GNAT{} \textbf{(bottom)} trained on STL \textbf{(left)} and CelebA
    \textbf{(right)}.  We generate samples by fitting a single
    Gaussian to the optimal points $z$.
  }
  \label{fig:samples}
\end{figure}

\newpage
\subsection{Results}\label{sec:experiments:results}
In the following, we compare the methods described on
Section~\ref{sec:experiments:baselines} when applied to all the datasets
described on Section~\ref{sec:experiments:datasets}. In particular, we evaluate
the performance of the methods in the tasks of compressing a dataset,
generating new samples, performing sample interpolation, and doing sample
arithmetic.

\subsubsection{Dataset compression}\label{sec:experiments:compression}

We start by measuring the reconstruction error in terms of the mean-squared
loss $\ell_2(x,x') = \| x - x' \|_2^2$ and the $\text{Lap}_1$ loss
\eqref{eq:laploss}
Table~\ref{table:errors} shows the reconstruction error of all models and
datasets for the $\ell_2$ and $\text{Lap}_1$ losses.
Despite being much simpler than AEs or GANs, \GNAT{} obtains better reconstruction error, even when using a $\text{Lap}_1$ loss.
Figure~\ref{fig:qua} shows a few reconstruction examples obtained with different sizes of latent space in \GNAT{}.

Figure~\ref{fig:eigenspectrum} show the quantity of the representation space explained as a function of the number of eigenvectors
used to reconstruct it. Our approach use more efficiently the representation space to spread the information around while PCA
concentrates the information in a few directions.

\subsubsection{Generation of new samples}\label{sec:experiments:generation}

Figure~\ref{fig:samples} shows samples from the DCAE, DCGAN, and \GNAT{} models for all
datasets. In the case of \GNAT{}, we fit a Gaussian distribution with full
covariance to the representation space, and sample from such distribution to
generate new samples.  We can see that the samples are quite good, even with this simple model of the $Z$ space; we leave more careful modeling
of the $Z$ space for future work.
\subsubsection{Sample interpolation}\label{sec:experiments:interpolation}
\begin{figure}[t]
    \begin{center}
        \includegraphics[width=0.49\textwidth]{l2_path.jpg}\        \includegraphics[width=0.49\textwidth]{lapl1_512_path.jpg}
    \end{center}
    \caption{Comparison between \GNAT{} trained with a $\ell_2$ and Laplacian $\ell_1$ loss on interpolation. We use $512$ dimensons for $z$.}
    \label{fig:comp22}
\end{figure}

Figure~\ref{fig:int} shows a few examples of interpolations between different images form the CelebA dataset and
LSUN bedroom.
We compare interpolation in the $z$-space where we linearly interpolate the $z$ and forward them to the model to
linear interpolation in the image space.
The interpolation in the $z$ are very different from the one in the image space, showing that our generator learns non-linear mapping
between the noise and the image. For example, the faces are slowly rotating. The difference between the two type of interpolations is
clearer when we are at the middle of the interpolation, that is the images on the  $4$-th and $5$-th columns.
The same non-linear transformation is observed on the bedrooms.

Finally, Figure~\ref{fig:comp22} show the difference between the $\ell_2$ loss and the Laplacian $\ell_1$ loss on high quality model, i.e. a $z$ space of $512$ dimensions.
The use of sparse loss creates sharper interpolations than the sqaure loss.

\begin{figure}
  \begin{center}
  \includegraphics[width=0.49\textwidth]{faces_gnat_inter_2.jpg} \  \includegraphics[width=0.49\textwidth]{bedroom_gnat_inter_3.jpg} \\
  \includegraphics[width=0.49\textwidth]{faces_linear_inter_2.jpg} \  \includegraphics[width=0.49\textwidth]{bedroom_linear_inter_3.jpg}
  \end{center}
  \caption{
    Illustration of image interpolations obtained with \GNAT{} for CelebA \textbf{(left)} and LSUN \textbf{(right)}.
    On top we show the reconstruction that we obtain for interpolated $z$ and compare to a linear mixture (bottom).
  }\label{fig:int}
\end{figure}

\begin{figure}
  \begin{minipage}[c]{0.5\textwidth}
  \includegraphics[width=\textwidth]{arith.pdf}
\end{minipage}~~~~~~
\begin{minipage}[c]{0.45\textwidth}
\caption{
    Illustration of feature arithmetic in a \GNAT{} model on CelebA.
    We show that by taking the average hidden representation of row \textbf{a}, substracting the one of row \textbf{b} and adding the one of row \textbf{c}, we obtain a coherent image.
  \label{fig:arit}}
\end{minipage}
\end{figure}

\subsubsection{Noise vector arithmetic}\label{sec:experiments:arithmetic}

In the spirit of~\cite{dcgan}, we showcase the effect of simple arithmetic operations in the noise space of a \GNAT{} model.
More precisely, we average the noise vector of three images of men wearing
sunglasses, remove the average noise vector of three images of men not wearing
sunglasses, and add the average noise vector of three images of women not
wearing sunglasses. The resulting image resembles a woman wearing sunglasses
glasses, as shown in Figure~\ref{fig:arit}.

\subsubsection{Conditional generation}\label{sec:experiments:conditional}
Our model can also be used for conditional generation.  To do this, we take a
fully trained model, and measure the loss only on the conditioned variables.
For example, to hallucinate $8\times$ upsamplings of an $8\times 8$ image (i.e.
condition on the output of an $8\times 8$ pooling operation), when
backpropagating to find the $z$ variable, we only measure error on the output
of the pooling operation applied to the generated image.   That is, if $x$ is
the small image to condition on, and $P$ is the pooling operator that takes a
large image and maps it down to the small image, we minimize $||P(g(z))-x||^2$
instead of  $||g(z)-x||^2$.  In a similar way, we can condition on the right
side pixels matching the target but the left side being free, etc.  Because the
optimization over $z$ does not usually lead to $0$ error, the conditioning is
not exact; but the error can often be made quite small.

\begin{figure}
  \begin{center}
  \includegraphics[width=0.49\textwidth]{face064128192_unproj.jpg} \  \includegraphics[width=0.49\textwidth]{face064128192_proj.jpg} \\
  \end{center}
  \caption{
  Conditional generation in a  \GNAT{} model.  Each image on the left maps via
  an $8\times 8$ pooling operator to the corresponding image on the right.  The
  model is trained as normal, but at sampling time, each $z$ variable is
  intialized independently, and we measure the loss on the targets after the
  pooling.  Note the variety of hallucinations generated.   Although the
  conditioning is not perfect (there are slight variations on the right side
  image), it is quite good; the average square error of each projected
  generated image from its projected target is about $.0002$ of the square norm of the projected target.
  \label{fig:cond}}
\end{figure}


\section{Discussion}\label{sec:discussion}

The experimental results presented in this work suggests that, in the image
domain, we can recover many of the properties of generative models trained with
GANs with convnets trained with reconstruction losses. While this does not invalidate the promise
of GANs as generic models of uncertainty or as methods for building generative
models, it does suggest that in order to more fully test the adversarial
construction, researchers will need to move beyond images and convnets.  On the
other hand, practitioners who care only about generating images for a
particular application, and find that the parameterized discriminator does
improve their results can use reconstruction losses in their model searches,
alleviating some of the instability of GAN training.

While the results are promising, they are not yet to the level of the latest results obtained
by recent works that generate images larger than the $64\times 64$ images generated here. There is much work to be done here to fill that gap. In particular, exploring other
loss functions and model architectures, adding more structure to the Z space, and using more sophisticated
sampling methods after training the model should all improve the results obtained by our method.

\paragraph{Acknowledgments.} Thanks to Mark Tygert and Yann LeCun for planting the seeds that grew into this work.

\bibliographystyle{abbrvnat}
\bibliography{paper}

\end{document}


