\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithmic}
\usepackage{float}
\usepackage{subcaption}
\usepackage{color}
\usepackage{xcolor}
\newcommand\myworries[1]{\textcolor{red}{#1}}
\usepackage{algorithm}
\usepackage[square,numbers,sort]{natbib}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\floatstyle{plain}
\newfloat{myalgo}{tbhp}{mya}
\newenvironment{Algorithm}[1][th]%
{\begin{myalgo}[#1]
\centering
\begin{minipage}{8cm}
\begin{algorithm}[H]}%
{\end{algorithm}
\end{minipage}
\end{myalgo}}

\def\sharedaffiliation{%
\end{tabular}
\begin{tabular}{c}}

\def\mA{\mathcal{A}}\def\mB{\mathcal{B}}\def\mC{\mathcal{C}}\def\mD{\mathcal{D}}\def\mE{\mathcal{E}}\def\mF{\mathcal{F}}\def\mG{\mathcal{G}}\def\mH{\mathcal{H}}\def\mI{\mathcal{I}}\def\mJ{\mathcal{J}}\def\mK{\mathcal{K}}\def\mL{\mathcal{L}}\def\mM{\mathcal{M}}\def\mN{\mathcal{N}}\def\mO{\mathcal{O}}\def\mP{\mathcal{P}}\def\mQ{\mathcal{Q}}\def\mR{\mathcal{R}}\def\mS{\mathcal{S}}\def\mT{\mathcal{T}}\def\mU{\mathcal{U}}\def\mV{\mathcal{V}}\def\mW{\mathcal{W}}\def\mX{\mathcal{X}}\def\mY{\mathcal{Y}}\def\mZ{\mathcal{Z}}\def\1n{\mathbf{1}_n}\def\0{\mathbf{0}}\def\1{\mathbf{1}}\def\A{{\bf A}}\def\B{{\bf B}}\def\C{{\bf C}}\def\D{{\bf D}}\def\E{{\bf E}}\def\F{{\bf F}}\def\G{{\bf G}}\def\H{{\bf H}}\def\I{{\bf I}}\def\J{{\bf J}}\def\K{{\bf K}}\def\L{{\bf L}}\def\M{{\bf M}}\def\N{{\bf N}}\def\O{{\bf O}}\def\P{{\bf P}}\def\Q{{\bf Q}}\def\R{{\bf R}}\def\S{{\bf S}}\def\T{{\bf T}}\def\U{{\bf U}}\def\V{{\bf V}}\def\W{{\bf W}}\def\X{{\bf X}}\def\Y{{\bf Y}}\def\Z{{\bf Z}}\def\a{{\bf a}}\def\b{{\bf b}}\def\c{{\bf c}}\def\d{{\bf d}}\def\e{{\bf e}}\def\f{{\bf f}}\def\g{{\bf g}}\def\h{{\bf h}}\def\i{{\bf i}}\def\j{{\bf j}}\def\k{{\bf k}}\def\l{{\bf l}}\def\m{{\bf m}}\def\n{{\bf n}}\def\o{{\bf o}}\def\p{{\bf p}}\def\q{{\bf q}}\def\r{{\bf r}}\def\s{{\bf s}}\def\t{{\bf t}}\def\u{{\bf u}}\def\v{{\bf v}}\def\w{{\bf w}}\def\x{{\bf x}}\def\y{{\bf y}}\def\z{{\bf z}}\def\balpha{\mbox{\boldmath{$\alpha$}}}\def\bbeta{\mbox{\boldmath{$\beta$}}}\def\bdelta{\mbox{\boldmath{$\delta$}}}\def\bgamma{\mbox{\boldmath{$\gamma$}}}\def\blambda{\mbox{\boldmath{$\lambda$}}}\def\bsigma{\mbox{\boldmath{$\sigma$}}}\def\btheta{\mbox{\boldmath{$\theta$}}}\def\bTheta{\mbox{\boldmath{$\Theta$}}}\def\bomega{\mbox{\boldmath{$\omega$}}}\def\bxi{\mbox{\boldmath{$\xi$}}}\def\bmu{\mbox{\boldmath{$\mu$}}}\def\bepsilon{\mbox{\boldmath{$\epsilon$}}}\def\bDelta{\mbox{\boldmath{$\Delta$}}}\def\bOmega{\mbox{\boldmath{$\Omega$}}}\def\bPhi{\mbox{\boldmath{$\Phi$}}}\def\bLambda{\mbox{\boldmath{$\Lambda$}}}\def\bSigma{\mbox{\boldmath{$\Sigma$}}}\def\bGamma{\mbox{\boldmath{$\Gamma$}}}\def\tt{\mbox{\tiny $T$}}\newcommand{\mymin}[1]{\mathop{\textrm{minimize}}_{#1}}\newcommand{\mymax}[1]{\mathop{\textrm{maximize}}_{#1}}\newcommand{\mymins}[1]{\mathop{\textrm{min}}_{#1}}\newcommand{\mymaxs}[1]{\mathop{\textrm{max}}_{#1}}\newcommand{\myargmin}[1]{\mathop{\textrm{argmin}}_{#1}}\newcommand{\myargmax}[1]{\mathop{\textrm{argmax}}_{#1}}\newcommand{\myst}{\textrm{s.t. }}\newcommand{\denselist}{\itemsep -1pt}\newcommand{\sparselist}{\itemsep 1pt}%\newcommand{\denselist}{\itemsep -3pt}\DeclareMathOperator{\vect}{vec}\def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}\let\endchangemargin=\endlist%%%%% Optimization -Satya %%%%%%%%%%%%% Optimization\newcommand{\objective}[2]{\ensuremath{\underset{#1}\min & & #2\\[5pt]}}\newcommand{\objectivemax}[2]{\ensuremath{\underset{#1}\max & & #2\\[5pt]}}\newcommand{\st}{\mathrm{s.t.}}\newcommand{\constraint}[2][]{& & #2 & #1}\newcommand{\argmin}[1]{\ensuremath{\underset{#1}{\mathrm{arg\,min\,\,}}}\xspace}\newcommand{\mytodo}[1]{\textcolor{red}{TODO: #1}}\newcommand{\minisection}[1]{\noindent \textbf{#1}}%\def\posemachine{\emph{Pose Machine}}\def\posemachine{pose machine}\newenvironment{optimize}{\begin{equation}
\begin{array}{rlll}}{\end{array}
\end{equation}}% Example%\begin{optimize}%\objective{x \in \mathbb{R}^n}{\mathbb{E}_{\omega} [f(x,\omega)]}%\st \constraint{g(x,\omega) \leq 0}{}\\%\constraint{x_l \leq x \leq x_u,}{}%\label{eq:gen_problem}%\end{optimize}

\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}


\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{1074} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\renewcommand\baselinestretch{.97}
\setlength{\belowcaptionskip}{-10pt}
\ifcvprfinal\pagestyle{empty}\fi
\renewcommand{\baselinestretch}{0.95}
\begin{document}
\title{Convolutional Pose Machines}


\author{
Shih-En Wei \\ \texttt{\small shihenw@cmu.edu}
\and
Varun Ramakrishna \\ \texttt{\small vramakri@cs.cmu.edu}
\and
Takeo Kanade \\ \texttt{\small Takeo.Kanade@cs.cmu.edu}
\and
Yaser Sheikh \\ \texttt{\small yaser@cs.cmu.edu}
\sharedaffiliation
 The Robotics Institute \\
 Carnegie Mellon University
}


\maketitle
\begin{abstract}

Pose Machines provide a 
sequential prediction framework 
for 
learning 
rich implicit spatial models.
In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation.
The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation. We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference.
Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.


\end{abstract}%Pose Machines provide a powerful modular framework for articulated pose estimation but  currently rely on manually designed features for representing image and spatial context information.  The contribution of this paper is a systematic approach to composing convolutional networks with large receptive fields for pose estimation tasks in the architecture of a pose machine. The sequential prediction framework of a pose machine which addresses the vanishing gradients problem by enforcing intermediate supervision. Convolutional pose machines inherit the benefits of the original pose machines framework, including the tight integration of multi-scale, multi-part cues and the ability to learn dependencies between image-based appearance and spatial context. In addition, the differentiability of the network allows joint training of all stages and levels through back propagation. We demonstrate that Convolutional Pose Machines outperform all state-of-the-art approaches in human pose detection on the standard LSP and FLIC benchmarks.

\section{Introduction}%Improvements in pose estimation of articulated objects have been mainly due to two reasons. The first is due to advances in developing feature representations which make the maximal use of image evidence \cite{jain2014learning,pishchulin13iccv} and the second is due to methods that are capable of sophisticated spatial modeling \cite{tompson2014joint,Ramakrishna2014posemachines,pishchulin2013poselet}.%Recent work \cite{Ramakrishna2014posemachines} introduced the modular \emph{\posemachine} framework which allows for the implicit learning  of complicated dependencies between part appearance and location, but relies on classical hand-crafted features for representing both image and spatial context information. In this work, we combine the rich spatial modeling afforded by the \posemachine~framework with recent advances in deep convolutional architectures to design a \emph{Convolutional Pose Machine}. Convolutional Pose Machines learn the image and spatial context representations directly from data, and the differentiability of the network allows joint training of all stages and levels through back propagation

We introduce \emph{Convolutional Pose Machines (CPMs)} for the task of articulated pose estimation. CPMs inherit the benefits of the \emph{\posemachine}\cite{Ramakrishna2014posemachines} architecture---the implicit learning of long-range dependencies between image and multi-part cues, tight integration between learning and inference, a modular sequential design---and combine them with the advantages afforded by convolutional architectures: the ability to learn feature representations for both image and spatial context directly from data; a differentiable architecture that allows for globally joint training with backpropagation; and the ability to efficiently handle large training datasets. 


CPMs consist of a sequence of convolutional networks that repeatedly produce 2D belief maps~\footnote{We use the term \emph{belief} in a slightly loose sense, however the belief maps described are closely related to beliefs produced in message passing inference in graphical models. The overall architecture can be viewed as an unrolled mean-field message passing inference algorithm~\cite{ross2011} that is learned end-to-end using backpropagation.} 
for the location of each part. At each stage in a CPM, image features and the belief maps produced by the previous stage are used as input. The belief maps provide the subsequent stage an expressive non-parametric encoding of the spatial uncertainty of location for each part, allowing the CPM to learn rich image-dependent spatial models of the relationships between parts. Instead of explicitly parsing such belief maps either using graphical models~\cite{tompson2014joint,tompson2015cvpr,pishchulin2015deepcut} or specialized post-processing steps~\cite{toshev2013deeppose,tompson2015cvpr}, we learn convolutional networks that directly operate on intermediate belief maps and learn implicit image-dependent spatial models of the relationships between parts. The overall proposed multi-stage architecture is fully differentiable and therefore can be trained in an end-to-end fashion using backpropagation.

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\columnwidth]{figures/teaser_half-crop}
    \caption{A \textbf{Convolutional Pose Machine} consists of a sequence of predictors trained to make dense predictions at each image location. Here we show the increasingly refined estimates for the location of the \emph{right elbow} in each stage of the sequence.
(a) Predicting from local evidence often causes confusion. (b) Multi-part context helps resolve ambiguity. (c) Additional iterations help converge to a certain solution.}
    \label{fig:teaser}
\end{figure}

At a particular stage in the CPM, the spatial context of part beliefs provide strong disambiguating cues to a subsequent stage. As a result, each stage of a CPM produces belief maps with increasingly refined estimates for the locations of each part (see Figure~\ref{fig:teaser}). 
In order to capture long-range interactions between parts, the design of the network in each stage of our sequential prediction framework is motivated by the goal of achieving a large receptive field on both the image and the belief maps. We find, through experiments, that large receptive fields on the belief maps are crucial for learning long range spatial relationships and result in improved accuracy.
Composing multiple convolutional networks in a CPM results in an overall network with many layers that is at risk of the problem of
\emph{vanishing gradients}~\cite{hochreiter2001gradient,bradley2010learning,glorot2010understanding,bengio1994learning} during learning. 
This problem can occur because back-propagated gradients diminish in strength as they are propagated through the many layers of the network. 
While there exists recent work~\footnote{New results have shown that using skip connections with identity mappings~\cite{he15arxiv} in so-called residual units also aids in addressing vanishing gradients in ``very deep'' networks. We view this method as complementary and it can be noted that our modular architecture easily allows us to replace each stage with the appropriate residual network equivalent.} which shows that supervising very deep networks at intermediate layers aids in learning~\cite{lee2014deeply,szegedy2014going}, they have mostly been restricted to classification problems.
In this work, we show how for a structured prediction problem such as pose estimation, CPMs naturally suggest a systematic framework that replenishes gradients and guides the network to produce increasingly accurate belief maps  by enforcing intermediate supervision periodically through the network. We also discuss different training schemes of such a sequential prediction architecture. 
Our main contributions are (a) learning implicit spatial models via a sequential composition of convolutional architectures and (b) a systematic approach to designing and training such an architecture  to learn both image features and image-dependent spatial models for structured prediction tasks, without the need for any graphical model style inference. We achieve state-of-the-art results on standard benchmarks including the MPII, LSP, and FLIC datasets, and analyze the effects of jointly training a multi-staged architecture with repeated intermediate supervision.


The classical approach to articulated pose estimation is the \textbf{pictorial structures} model \cite{fh2005pictorial,ramanan2005strike,Andriluka2010,Andriluka2009,pishchulin13iccv,pishchulin2013poselet,yang2011articulated,johnson2010clustered} in which spatial correlations between parts of the body are expressed as a tree-structured graphical model with kinematic priors that couple connected limbs. These methods have been successful on images where all the limbs of the person are visible, but are prone to characteristic errors such as double-counting image evidence, which occur because of correlations between variables that are not captured by a tree-structured model. The work of Kiefel et al. \cite{kiefel2014human} is based on the pictorial structures model but differs in the underlying graph representation. \textbf{Hierarchical models}\cite{tian2012exploring,sun2011articulated} represent the relationships between parts at different scales and sizes in a hierarchical tree structure. The underlying assumption of these models is that larger parts (that correspond to full limbs instead of joints) can often have discriminative image structure that can be easier to detect and consequently help reason about the location of smaller, harder-to-detect parts. 
\textbf{Non-tree models}\cite{wang2008multiple,sigal2006measure,lan2005beyond,karlinsky2012using,Dantone2013} incorporate interactions that introduce loops to augment the tree structure with additional edges that capture symmetry, occlusion and long-range relationships. These methods usually have to rely on approximate inference during both learning and at test time, and therefore have to trade off accurate modeling of spatial relationships with models that allow efficient inference, often with a simple parametric form to allow for fast inference. In contrast, methods based on a \textbf{sequential prediction} framework \cite{Ramakrishna2014posemachines} learn an \emph{implicit} spatial model with potentially complex interactions between variables by directly training an inference procedure, as in \cite{munoz2010,ross2011,tu2010PAMI,Pinheiro14recurrentconvolutional}. 

There has been a recent surge of interest in models that employ \textbf{convolutional architectures} for the task of articulated pose estimation \cite{ouyang2014multi,tompson2015cvpr,tompson2014joint,Chen_NIPS14,
carreia2015human,pishchulin2015deepcut,pfister2015flowing}. 
Toshev et al. \cite{toshev2013deeppose} take the approach of directly regressing the Cartesian coordinates using a standard convolutional architecture \cite{krizhevsky2012imagenet}.
Recent work regresses image to confidence maps, and resort to graphical models, which require hand-designed energy functions or heuristic initialization of spatial probability priors, to remove outliers on the regressed confidence maps. Some of them also utilize a dedicated network module for precision refinement \cite{tompson2015cvpr,pishchulin2015deepcut}.
In this work, we show the regressed confidence maps are suitable to be inputted to further convolutional networks with large receptive fields to learn implicit spatial dependencies without the use of hand designed priors, and achieve state-of-the-art performance over all precision region without careful initialization and dedicated precision refinement. Pfister et al. \cite{pfister2015flowing} also used a network module with large receptive field to capture implicit spatial models.
Due to the differentiable nature of convolutions, our model can be globally trained, where Tompson et al. \cite{tompson2014joint} and Steward et al. \cite{steward2015endtoend} also discussed the benefit of joint training.


Carreira et al.~\cite{carreia2015human} train a deep network that iteratively improves part detections using error feedback but use a cartesian representation as in~\cite{toshev2013deeppose} which does not preserve spatial uncertainty and results in lower accuracy in the high-precision regime.
In this work, we show how the sequential prediction framework takes advantage of the preserved uncertainty in the confidence maps to encode the rich spatial context, with enforcing the intermediate local supervisions to address the problem of vanishing gradients.

\section{Method}\begin{figure*}[t!]
    \centering
    \includegraphics[width=1\textwidth]{figures/effective_receptive_field-crop.pdf}
    \caption{\textbf{Architecture and receptive fields of CPMs.} We show a convolutional architecture and receptive fields across layers for a CPM with any $T$ stages. The \posemachine~\cite{Ramakrishna2014posemachines} is shown in insets (a) and (b), and the corresponding convolutional networks are shown in insets (c) and (d). Insets (a) and (c) show the architecture that operates only on image evidence in the first stage. Insets (b) and (d) shows the architecture for subsequent stages, which operate both on image evidence as well as belief maps from preceding stages. The architectures in (b) and (d) are repeated for all subsequent stages (2 to $T$). The network is locally supervised after each stage using an intermediate loss layer that prevents vanishing gradients during training. Below in inset (e) we show the effective receptive field on an image (centered at left knee) of the architecture, where the large receptive field enables the model to capture long-range spatial dependencies such as those between head and knees. (Best viewed in color.)}
    \label{fig:arch_half}
\end{figure*}\subsection{Pose Machines}
We denote the pixel location of the $p$-th anatomical landmark (which we refer to as a part),
Our goal is to predict the image locations 
for all $P$ parts. 
A \posemachine~\cite{Ramakrishna2014posemachines} (see Figure \ref{fig:arch_half}a and \ref{fig:arch_half}b) consists of a sequence of multi-class predictors, 
that are trained to predict the location of each part in each level of the hierarchy. In each \emph{stage}$t \in \{ 1 \ldots T\}$, the classifiers 
predict beliefs for assigning a location to each part $Y_p = z, ~\forall z \in \mathcal{Z},$ based on features extracted from the image at the location $z$ denoted by $\mathbf{x}_z \in \mathbb{R}^d$ and contextual information from the preceding classifier in the neighborhood around each $Y_p$ in stage $t$. 
A classifier 
in the first stage $t = 1$, therefore produces the following belief values:
g_{1}(\mathbf{x}_z) \rightarrow \left\{ b_1^p( Y_p = z)\right\}_{p \in \{0 \ldots P\}},
\end{equation}\noindent where 
is the score predicted by the classifier 
for assigning the $p^{\mathrm{th}}$ part 
in the first stage at image location $z$. We represent all the beliefs of part $p$%of level $l$% 
evaluated at every location $z=(u,v)^T$ in the image as 
where $w$ and $h$ are the width and height of the image, respectively. That is,
\mathbf{b}^p_t[u,v] = b^{p}_t(Y_p = z).
\end{equation}
For convenience, we denote the collection of belief maps for all the parts 
as 

In subsequent stages, the classifier predicts a belief for assigning a location to each part $Y_p = z, ~\forall z \in \mathcal{Z},$ based on (1) features of the image data $\mathbf{x}^t_z \in \mathbb{R}^d$ again, and (2) contextual information from the preceeding classifier in the neighborhood around each $Y_p$:
    g_t \left(\mathbf{x}'_z, \psi_t(z, \mathbf{b}_{t-1}) \right) \rightarrow \left\{ b_t^p( Y_p = z)\right\}_{p \in \{0 \ldots P+1\}},
    \label{eqn:predoutput}
\end{equation}\noindent where 
The \posemachine~proposed in~\cite{Ramakrishna2014posemachines} used boosted random forests for prediction 
($\{g_t\}$),
fixed hand-crafted image features across all stages ($\mathbf{x}' = \mathbf{x}$), and fixed hand-crafted context feature maps ($ \psi_t(\cdot)$) to capture spatial context across all stages.

We show how the prediction and image feature computation modules of a pose machine can be replaced by a deep convolutional architecture allowing for both image and contextual feature representations to be learned directly from data. Convolutional architectures also have the advantage of being completely differentiable, thereby enabling end-to-end joint training of all stages of a CPM. 
We 
describe our design for a CPM that combines the advantages of deep convolutional architectures with the implicit spatial modeling afforded by the pose machine framework.

The first stage of a convolutional pose machine predicts part beliefs from only local image evidence. Figure \ref{fig:arch_half}c shows the network structure used for part detection from local image evidence using a deep convolutional network. The evidence is \emph{local} because the receptive field of the first stage of the network is constrained to a 
small patch around the output pixel location.
We use a network structure composed of five convolutional layers followed by two $1 \times 1$ convolutional layers which results in a fully convolutional architecture \cite{long_shelhamer_fcn}.
In practice, to achieve certain precision, we normalize input cropped images to size $368\times 368$ (see Section~\ref{subset:Quantitative} for details), and the receptive field of the network shown above is $160 \times 160$ pixels. The network can effectively be viewed as sliding a deep network across an image and regressing from the local image evidence in each $160\times 160$ image patch to a $P + 1$ sized output vector that represents a score for each part at that image location.

\centering
\includegraphics[width=\linewidth]{figures/context-crop}
\caption{\textbf{Spatial context from belief maps} of easier-to-detect parts can provide strong cues for localizing difficult-to-detect parts. The spatial contexts from shoulder, neck and head can help eliminate wrong (red) and strengthen correct (green) estimations on the belief map of \emph{right elbow} in the subsequent stages.}
\label{fig:contextfeaturesgood}
\end{figure}%\vspace{-5pt}\subsubsection{Sequential Prediction with Learned Spatial\\ Context Features}%\vspace{-5pt}
While the detection rate on landmarks with consistent appearance, such as the head and shoulders, can be favorable, the accuracies are often much lower for landmarks lower down the kinematic chain of the human skeleton due to their large variance in configuration and appearance. The landscape of the belief maps around a part location, albeit noisy, can, however, be very informative. 
Illustrated in Figure \ref{fig:contextfeaturesgood}, when detecting challenging parts such as right elbow, the belief map for right shoulder with a sharp peak can be used as a strong cue. 
A predictor in subsequent stages ($g_{t>1}$) can use the spatial context ($\psi_{t>1}(\cdot)$) of the noisy belief maps in a region around the image location $z$ and improve its predictions by leveraging the fact that parts occur in consistent geometric configurations.  In the second stage of a \posemachine, the classifier 
accepts as input the image features 
and features computed on the beliefs via the feature function $\psi$ for each of the parts in the previous stage. The feature function $\psi$ serves to encode the landscape of the belief maps from the previous stage in a spatial region around the location $z$ of the different parts.
For a convolutional pose machine, we do not have an explicit function that computes context features. Instead, we define $\psi$ as being the receptive field of the predictor on the beliefs from the previous stage.


The design of the network is guided by achieving a receptive field at the output layer of the second stage network that is large enough to allow the learning of potentially complex and long-range correlations between parts. By simply supplying features on the outputs of the previous stage (as opposed to specifying potential functions in a graphical model), the convolutional layers in the subsequent stage allow the classifier to freely combine contextual information by picking the most predictive features. 
The belief maps from the first stage are generated from a network that examined the image locally with a small receptive field.
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.49\columnwidth]{figures/pdf/result_curves/FLIC/FLIC_wri_kernel_acc_exp41}
    \includegraphics[width=0.49\columnwidth]{figures/pdf/result_curves/FLIC/FLIC_elb_kernel_acc_exp41}
    \caption{\textbf{Large receptive fields for spatial context.} We show that networks with large receptive fields are effective at modeling long-range spatial interactions between parts. Note that these experiments are operated with smaller normalized images than our best setting.}
    \label{fig:flic_kernels}
\end{figure}%with a receptive field of size $160\times 160$. 
In the second stage, we design a network that drastically increases the equivalent receptive field. 
Large receptive fields can be achieved either by pooling at the expense of precision, increasing the kernel size of the convolutional filters at the expense of increasing the number of parameters, or by increasing the number of convolutional layers at the risk of encountering vanishing gradients during training. 
Our network design and corresponding receptive field for the subsequent stages ($t \geq 2$) is shown in Figure \ref{fig:arch_half}d.
We choose to use multiple convolutional layers to achieve large receptive field on the $8\times$ downscaled heatmaps, as it allows us to be parsimonious with respect to the number of parameters of the model. We found that our stride-$8$ network performs as well as a stride-$4$ one even at high precision region, while it makes us easier to achieve larger receptive fields.
We also repeat similar structure for image feature maps to make the spatial context be image-dependent and allow error correction, following the structure of \posemachine.

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=1\linewidth]{figures/pdf/gradient-crop}
    \caption{\textbf{Intermediate supervision addresses vanishing gradients.} We track the change in magnitude of gradients in layers at different depths in the network, across training epochs, for models with and without intermediate supervision. We observe that for layers closer to the output, the distribution has a large variance for both with and without intermediate supervision; however as we move from the output layer towards the input, the gradient magnitude distribution peaks tightly around zero with low variance (the gradients \emph{vanish}) for the model without intermediate supervision. For the model with intermediate supervision the distribution has a moderately large variance throughout the network. At later training epochs, the variances decrease for all layers for the model with intermediate supervision and remain tightly peaked around zero for the model without intermediate supervision. (Best viewed in color)}
    \label{fig:gradient_change}
\end{figure*}
We find that accuracy improves with the size of the receptive field. In Figure \ref{fig:flic_kernels} we show the improvement in accuracy on the FLIC dataset \cite{sappmodec} as the size of the receptive field on the original image is varied by varying the architecture without significantly changing the number of parameters,  through a series of experimental trials on input images normalized to a size of $304 \times 304$. We see that the accuracy improves as the effective receptive field increases, and starts to saturate around $250$ pixels, which also happens to be roughly the size of the normalized object. This improvement in accuracy with receptive field size suggests that the network does indeed encode long range interactions between parts and that doing so is beneficial.
In our best performing setting in Figure~\ref{fig:arch_half}, we normalize cropped images into a larger size of $368 \times 368$ pixels for better precision, and the receptive field of the second stage output on the belief maps of the first stage is set to $31\times 31$, which is equivalently $400\times 400$ pixels on the original image, where the radius can usually cover any pair of the parts. With more stages, the effective receptive field is even larger. In the following section we show our results from up to $6$ stages.
    \begin{subfigure}{0.33\textwidth}
   	     \centering
         \includegraphics[width=\textwidth]{figures/pdf/compare_with_pm/pck-total-lsp-PC.pdf}
         \caption{}
         \label{fig:comp_with_pm}
    \end{subfigure}
    \begin{subfigure}{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/pdf/result_curves/LEEDS_PC/self_comparison/pck-total-lsp-PC}
        \caption{}
        \label{fig:compare_training}
    \end{subfigure}
    \begin{subfigure}{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/pdf/result_curves/LEEDS_PC/stagewise_comparison/pck-total-lsp-PC}
        \caption{}
        \label{fig:compare_stages}
    \end{subfigure}
    \caption{\textbf{Comparisons on 3-stage architectures on the LSP dataset (PC)}: (a) Improvements over Pose Machine. (b) Comparisons between the different training methods. (c) Comparisons across each number of stages using joint training from scratch with intermediate supervision.} 
\end{figure*}\subsection{Learning in Convolutional Pose Machines}
The design described above for a \posemachine~results in a deep architecture that can have a large number of layers.  Training such a network with many layers can be prone to the problem of \emph{vanishing gradients}\cite{bradley2010learning,glorot2010understanding,bengio1994learning} where, as observed 
by Bradley \cite{bradley2010learning} and Bengio et al. \cite{glorot2010understanding}, the magnitude of back-propagated gradients decreases in strength with the number of intermediate layers between the output layer and the input layer.


Fortunately, the sequential prediction framework of the \posemachine~provides a natural approach to training our deep architecture that addresses this problem. Each stage of the \posemachine~is trained to repeatedly produce the belief maps for the locations of each of the parts. We encourage the network to repeatedly arrive at such a representation by defining a loss function at the output of each stage $t$%and hierarchy level $l$ 
that minimizes the $l_2$ distance between the predicted and ideal belief maps for each part. The ideal belief map for a part $p$ is written as 
which are created by putting Gaussian peaks at ground truth locations of each body part $p$.
The cost function we aim to minimize at the output of each stage at each level is therefore given by:
    f_t = \sum_{p = 1}^{P+1} \sum_{z\in \mathcal{Z}}  \| b^{p}_{t}(z) -  b^{p}_{*}(z)\|^{2}_{2}.
    \label{eqn:localobjective}
 \end{equation}

The overall objective for the full architecture is obtained by adding the losses at each stage and is given by:
    \mathcal{F} = \sum_{t = 1}^{T} f_t .
\label{eqn:fullobjective}
\end{equation}
We use standard stochastic gradient descend to jointly train all the $T$ stages in the network. To share the image feature $\mathbf{x}'$ across all subsequent stages, we share the weights of corresponding convolutional layers (see Figure~\ref{fig:arch_half}) across stages $t \geq 2$.

\section{Evaluation}\subsection{Analysis}\minisection{Addressing vanishing gradients.}
The objective in Equation \ref{eqn:fullobjective} describes a decomposable loss function that operates on different parts of the network (see Figure \ref{fig:arch_half}). Specifically, each term in the summation is applied to the network after each stage $t$ effectively enforcing supervision in intermediate stages through the network. Intermediate supervision has the advantage that, even though the full architecture can have many layers, it does not fall prey to the \emph{vanishing gradient} problem as the intermediate loss functions replenish the gradients at each stage.

We verify this claim by observing histograms of gradient magnitude (see Figure \ref{fig:gradient_change}) at different depths in the architecture across training epochs for models with and without intermediate supervision. In early epochs, as we move from the output layer to the input layer, we observe on the model \emph{without intermediate supervision}, the gradient distribution is tightly peaked around zero because of vanishing gradients. The model \emph{with intermediate supervision} has a much larger variance across all  layers, suggesting that learning is indeed occurring in all the layers thanks to intermediate supervision. We also notice that as training progresses, the variance in the gradient magnitude distributions decreases pointing to model convergence.

\minisection{Benefit of end-to-end learning.} 
We see in Figure \ref{fig:comp_with_pm} that replacing the modules of a pose machine with the appropriately designed convolutional architecture provides a large boost of $42.4$ percentage points over the previous approach of \cite{Ramakrishna2014posemachines} in the high precision regime (PCK@0.1) and  $30.9$ percentage points in the low precision regime (PCK@0.2).

\minisection{Comparison on training schemes.}\label{sec:compare_learning} We compare different variants of training the network in Figure \ref{fig:compare_training} on the LSP dataset with person-centric (PC) annotations. To demonstrate the benefit of intermediate supervision with joint training across stages, we train the model in four ways:
(i) training from scratch using a global loss function that enforces intermediate supervision
(ii) stage-wise; where each stage is trained in a feed-forward fashion and stacked
(iii) as same as (i) but initialized with weights from (ii), and
(iv) as same as (i) but with no intermediate supervision. We find that network (i) outperforms all other training methods, showing that intermediate supervision and joint training across stage is indeed crucial in achieving good performance. The stagewise training in (ii) saturate at sub-optimal, and the jointly fine-tuning in (iii) improves from this sub-optimal to the accuracy level closed to (i), however with effectively longer training iterations.

    \centering
    \includegraphics[width=1\linewidth]{figures/leedsqual}
    \vspace{-20pt}
    \caption{\textbf{Comparison of belief maps across stages} for the elbow and wrist joints on the LSP dataset for a 3-stage CPM.} 
    \label{fig:leeds_qual}
\end{figure*}\begin{figure*}[ht!]
        \centering
        \includegraphics[width=0.965\textwidth]{figures/pdf/result_curves/MPI/MPI_all.pdf}
        \vspace{-10pt}
        \caption{\textbf{Quantitative results on the MPII dataset} using the PCKh metric. We achieve state of the art performance and outperform significantly on difficult parts such as the ankle.}
        \label{fig:mpi_results}
\end{figure*}\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.965\linewidth]{figures/pdf/result_curves/LEEDS_PC/pck-all-lsp-PC}
    \vspace{-10pt}
    \caption{\textbf{Quantitative results on the LSP dataset} using the PCK metric. Our method again achieves state of the art performance and has a significant advantage on challenging parts.}
    \label{fig:leeds_oc_quant}
\end{figure*}\minisection{Performance across stages.}
We show a comparison of performance across each stage on the LSP dataset (PC) in Figure \ref{fig:compare_stages}. We show that the performance increases monotonically until 5 stages, as the predictors in subsequent stages make use of contextual information in a large receptive field on the previous stage beliefs maps to resolve confusions between parts and background. We see diminishing returns at the 6th stage, which is the number we choose for reporting our best results in this paper for LSP and MPII datasets.


In this section we present our numerical results in various standard benchmarks including the MPII, LSP, and FLIC datasets. To have normalized input samples of $368 \times 368$ for training, we first resize the images to roughly make the samples into the same scale, and then
crop or pad the image according to the center positions and rough scale estimations provided in the datasets if available. In datasets such as LSP without these information, we estimate them according to joint positions or image sizes. For testing, we perform similar resizing and cropping (or padding), but estimate center position and scale only from image sizes when necessary. In addition, we merge the belief maps from different scales (perturbed around the given one) for final predictions, to handle the inaccuracy of the given scale estimation.

We define and implement our model using the \emph{Caffe}\cite{jia2014caffe} libraries for deep learning. We publicly release the source code and details on the architecture, learning parameters, design decisions and data augmentation to ensure full reproducibility.\footnote{\url{https://github.com/CMU-Perceptual-Computing-Lab/convolutional-pose-machines-release}}\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.98\linewidth]{figures/pdf/quantitative/Quantitative.pdf}
    \vspace{-5pt}
    \caption{\textbf{Qualitative results} of our method on the MPII, LSP and FLIC datasets respectively.  We see that the method is able to handle non-standard poses and resolve ambiguities between symmetric parts for a variety of different relative camera views.}
    \label{fig:leeds_qualitative}
\end{figure*}\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.965\linewidth]{figures/viewpoint-crop}
    \vspace{-10pt}
    \caption{\textbf{Comparing PCKh-$0.5$ across various viewpoints in the MPII dataset.} Our method is significantly better in all the viewpoints.}
    \label{fig:mpi_vp}
\end{figure}\minisection{MPII Human Pose Dataset}. We show in Figure \ref{fig:mpi_results} our results on the MPII Human Pose dataset \cite{andriluka14cvpr} which consists more than 28000 training samples. 
We choose to randomly augment the data with rotation degrees in $[-40^{\circ},40^{\circ}]$, scaling with factors in $[0.7,1.3]$, and horizonal flipping.
The evaluation is based on PCKh metric \cite{andriluka14cvpr} where the error tolerance is normalized with respect to head size of the target. Because there often are multiple people in the proximity of the interested person (rough center position is given in the dataset), we made two sets of ideal belief maps for training: 
one includes all the peaks for every person appearing in the proximity of the primary subject and the second type where we only place peaks for the primary subject.  We supply the first set of belief maps to the loss layers in the first stage as the initial stage only relies on local image evidence to make predictions. We supply the second type of belief maps to the loss layers of all subsequent stages.
We also find that supplying to all subsequent stages an additional heat-map with a Gaussian peak indicating center of the primary subject is beneficial.

Our total PCKh-$0.5$ score achieves state of the art at $87.95\%$ ($88.52\%$ when adding LSP training data), which is $6.11\%$ higher than the closest competitor, and it is noteworthy that on the ankle (the most challenging part), our PCKh-$0.5$ score is $78.28\%$ ($79.41\%$ when adding LSP training data), which is $10.76\%$ higher than the closest competitor. This result shows the capability of our model to capture long distance context given ankles are the farthest parts from head and other more recognizable parts. Figure~\ref{fig:mpi_vp} shows our accuracy is also consistently significantly higher than other methods across various view angles defined in~\cite{andriluka14cvpr}, especially in those challenging non-frontal views.
In summary, our method improves the accuracy in all parts, over all precisions, across all view angles, and is the first one achieving such high accuracy without any pre-training from other data, or post-inference parsing with hand-design priors or initialization of such a structured prediction task as in \cite{tompson2014joint,pishchulin2015deepcut}. Our methods also does not need another module dedicated to location refinement as in \cite{tompson2015cvpr} to achieve great high-precision accuracy with a stride-8 network.

We trained on person-centric (PC) annotations and evaluate our method using the Percentage Correct Keypoints (PCK) metric \cite{yang2013articulated}.
Using the same augmentation scheme as for the MPI dataset, our model again achieves state of the art at $84.32\%$ ($90.5\%$ when adding MPII training data). Note that adding MPII data here significantly boosts our performance, due to its labeling quality being much better than LSP. Because of the noisy label in the LSP dataset, Pishchulin et al. \cite{pishchulin2015deepcut} reproduced the dataset with original high resolution images and better labeling quality.
We report accuracy as per the metric introduced in Sapp et al. \cite{sappmodec} for the elbow and wrist joints in Figure \ref{fig:flic_quant}. Again, we outperform all prior art at PCK@0.2 with $97.59\%$ on elbows and $95.03\%$ on wrists. In higher precision region our advantage is even more significant: $14.8$ percentage points on wrists and $12.7$ percentage points on elbows at PCK@0.05, and $8.9$ percentage points on wrists and $9.3$ percentage points on elbows at PCK@0.1.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=\columnwidth]{figures/pdf/result_curves/FLIC/comparison/pck-elbow-wrist.pdf}
    \vspace{-20pt}
    \caption{\textbf{Quantitative results on the FLIC dataset} for the elbow and wrist joints with a 4-stage  CPM. We outperform all competing methods.}
    \label{fig:flic_quant}
\end{figure}

\vspace{-10pt}\section{Discussion}\vspace{-5pt}%Convolutional pose machines show how a structured prediction task such as human pose estimation is handled by a sequential prediction framework with convolutional networks. Each stage of the network fully exploits the image feature and the preserved non-parametric spatial context to refine predictions.
Convolutional pose machines provide an end-to-end architecture for tackling structured prediction problems in computer vision without the need for graphical-model style inference. We showed that a sequential architecture composed of convolutional networks is capable of implicitly learning a spatial models for pose by communicating increasingly refined uncertainty-preserving beliefs between stages. Problems with spatial dependencies between variables arise in multiple domains of computer vision such as semantic image labeling, single image depth prediction and object detection and future work will involve extending our architecture to these problems.
Our approach achieves state of the art accuracy on all primary benchmarks, however we do observe failure cases mainly when multiple people are in close proximity. Handling multiple people in a single end-to-end architecture is also a challenging problem and an interesting avenue for future work.

{\small
\bibliographystyle{ieee}
\bibliography{posebib}
}


\end{document}


