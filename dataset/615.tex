\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}


\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\cvprfinalcopy % *** Uncomment this line for the final submission

\usepackage[colorlinks]{hyperref}
\usepackage{multirow}
\usepackage{booktabs}
\graphicspath{{figs/}}

\newcommand{\phantomtwo}{\phantom{aa}}
\newcommand{\phantomone}{\phantom{a}}

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

\title{ExpandNets: Exploiting Linear Redundancy to Train Small Networks}

\author{Shuxuan Guo \thanks{This work was supported in part by the Swiss National Science Foundation and by the Chinese Scholarship Council.} \\ 
CVLab, EPFL\\
Lausanne, Switzerland\\
{\tt\small shuxuan.guo@epfl.ch}
\and
Jose M. Alvarez\\
NVIDIA\\
Santa Clara, USA\\
{\tt\small josea@nvidia.com}
\and
Mathieu Salzmann\\
CVLab, EPFL\\
Lausanne, Switzerland\\
{\tt\small mathieu.salzmann@epfl.ch}
}

\maketitle
While very deep networks can achieve great performance, they are ill-suited to applications in resource-constrained environments. Knowledge transfer, which leverages a deep teacher network to train a given small network, has emerged as one of the most popular strategies to address this problem. In this paper, we introduce an alternative approach to training a given small network, based on the intuition that parameter redundancy facilitates learning. We propose to expand each linear layer of a small network into multiple linear layers, without adding any nonlinearity. As such, the resulting expanded network can be compressed back to the small one algebraically, but, as evidenced by our experiments, consistently outperforms training the small network from scratch. This strategy is orthogonal to knowledge transfer. We therefore further show on several standard benchmarks that, for any knowledge transfer technique, using our expanded network as student systematically improves over using the small network.
\end{abstract}

With the growing availability of large-scale datasets and advanced computational resources, deep learning has achieved tremendous success in a variety of computer vision tasks, such as image classification~\cite{NIPS2012_4824,he2016deep}, object detection~\cite{ren2015faster} and semantic segmentation~\cite{long2015fully}. Over the past few years, ``Wider and deeper are better" has become the rule of thumb to design network architectures~\cite{simonyan2014very,szegedy2015going,he2016deep,huang2017densely}. This trend, however, raises memory- and computation-related challenges, especially in the context of constrained environments, such as embedded platforms.

As a consequence, designing strategies to train a given small network, both shallow and thin, has emerged as an increasingly popular research direction. While deep and wide networks are well-known to be overparametrized, small networks are notoriously hard to train from scratch. Currently, one of the most popular strategies to overcome this consists of transferring the knowledge of a deep teacher network to the small one of interest~\cite{hinton2015distilling,romero2014fitnets,8100237,Zagoruyko2017AT,pkt_eccv}. 

In this paper, we introduce an alternative approach to training small neural networks. In particular, we build upon the intuition that redundancy is key to facilitating neural network training. However, instead of incorporating redundancy via extra layers separated by nonlinearities, we propose to exploit purely \emph{linear} ones. Specifically, we propose to expand each linear layer of a small network into a succession of multiple linear layers, without any nonlinearity in between. Since consecutive linear layers are known to be equivalent to a single one~\cite{saxe2014}, such an expanded network, or ExpandNet, can be algebraically compressed back to the original small network without any loss of information.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.9\columnwidth]{introduction.pdf}
	\caption{{\bf ExpandNets.} We study three strategies to linearly expand a small network so as to introduce parameter redundancy. The resulting expanded network can be compressed back to the small one algebraically, and consistently outperforms training the small network from scratch.}
	\label{fig:framework}
\end{figure}

As illustrated by Fig.~\ref{fig:framework}, we study three ways to expand a small networks: (i) replacing a fully-connected layer with multiple ones; (ii) replacing a $k\times k$ convolution by three convolutional layers with kernel size $1\times 1$, $k \times k$ and $1\times 1$, respectively; and (iii) replacing a $k\times k$ convolution with $k>3$ by multiple $3\times 3$ convolutions. Our experiments demonstrate that training the resulting ExpandNets from scratch consistently outperforms directly training the equivalent small networks. 

In the context of deep networks, it has been shown that initialization could have a crucial impact on the results~\cite{Mishkin2015AllYN, He:2015}. Initializing small networks, however, is an unexplored research area, with no clear solution other than using standard initialization methods~\cite{XavierInit,He:2015}. 
By contrast, an ExpandNet comes with a natural initialization strategy: it has a nonlinear counterpart, with an identical number of parameters, obtained by adding a nonlinear activation function between every two consecutive linear layers. The parameters of these layers, once trained, can directly be transferred to the ExpandNet for initialization purposes. We show that this simple strategy yields a further accuracy boost to an ExpandNet's results. 

In essence, our approach is orthogonal to knowledge transfer; that is, knowledge transfer can also be applied to our expanded networks. Our experiments then further demonstrate that, by doing so, and independently of the chosen knowledge transfer method, the use of our ExpandNets consistently outperforms that of the original small networks. This is true when using either a very deep teacher, as commonly done in practice, or the nonlinear counterpart of our ExpandNets as teacher, which is less prone to overfitting in the presence of limited data.


Our contributions therefore are (i) a simple strategy to introduce redundancy in small networks, thus facilitating their training while providing an algebraic way to retrieve the original small network; and (ii) an effective initialization scheme for these redundant networks. Our experiments on several standard knowledge transfer benchmark datasets demonstrate the effectiveness of our ExpandNets without and with knowledge transfer. Ultimately, in all settings, one always benefits from expanding.
We will make our code publicly available upon acceptance of the paper.

Very deep neural networks currently constitute the state of the art for many computer vision tasks. These networks, however, are known to be heavily over-parametrized, and making them smaller would facilitate their use in resource-constrained environments, such as embedded platforms. As a consequence, much research has recently been devoted to developing more compact architectures.

{\it Network compression} constitutes one of the most popular trends in this area. In essence, it aims at reducing the size of a pre-trained very deep network while losing as little accuracy as possible, or even none at all. In this context, existing approaches can be roughly grouped into two categories: (i) parameter pruning and sharing~\cite{optBrainDamage,han2015deep, courbariaux1602binarynet,Kohli2016,Pavlo2017,SoftWeightSharing17,Carreira2018}, which aim to remove the least informative parameters; and (ii) low-rank factorization~\cite{denil2013predicting, sainath2013low,CPDecICLR2015}, which uses decomposition techniques to reduce the size of the parameter matrix/tensor in each layer. While compression is typically performed as a post-processing step, it has been shown that incorporating it during training could be beneficial~\cite{Alvarez16,Alvarez17,wen2016learning,Wen2017CoordinatingFF}. In any event, even though network compression indeed produces networks that are smaller than the original ones, they do not provide one with the flexibility of designing a network with a specific architecture and of a specific size.

In a parallel line of research, several works have proposed design strategies to reduce a network's number of parameters~\cite{wu2016squeezedet,inceptionv4,howard2017mobilenets,ErFNet2018}. Again, while more compact networks can indeed be developed with these mechanisms, they impose constraints on the network architecture, and thus do not allow one to simply train a given small network. 

This is the task that we tackle here. That is, we seek to train a given small network with an arbitrary architecture. In essence, this task is also addressed by {\it knowledge transfer} approaches. To achieve this, existing methods leverage the availability of a pre-trained very deep teacher network. In~\cite{hinton2015distilling}, this is done by defining soft labels from the logits of the teacher; in~\cite{romero2014fitnets},~\cite{YimJBK17} and~\cite{Zagoruyko2017AT} by transferring intermediate representations, attention maps and Gram matrices, respectively, from the teacher to the small network; in~\cite{pkt_eccv} by aligning the feature distributions of the deep and small networks.

In this paper, we introduce an alternative strategy to train small networks. We propose to exploit parameter redundancy by expanding each linear layer in a given small network into a succession of multiple linear layers. We show that such expanded networks (i) are easier to train than the original small ones; (ii) can be used in conjunction with any knowledge transfer strategy to obtain better results than directly applying knowledge transfer to the small network. 

Note that deep linear networks (DLNs) have been studied both in the early neural network days~\cite{Baldi1989} and more recently~\cite{saxe2014,Kawaguchi2016,Laurent2018,YiZhou2018,AroraCH18}. The main motivation behind these works was to study the convergence properties of DLNs and their critical points. 
In particular, the recent work of~\cite{AroraCH18} showed that increasing the depth of a network by adding linear layers may accelerate optimization. Here, by contrast, we evidence that expanding a small network with linear redundancy converges to a better solution. 


	\centering
	\includegraphics[width=1.9\columnwidth]{losses.pdf}
	\caption{{\bf Motivating example.} We show the loss value (a) for the one-parameter case (in black); and (b) for the two-parameter one. The blue dots in (a) depict the optimization when using a single parameter, and the red ones when using two parameters. Note that the latter reaches a better minimum, despite the fact that both strategies were initialized to start at the same point ($w=-0.25$).}
	\label{fig:toy_example}
\end{figure*}
We now introduce our method to training small networks by expanding their linear layers. Our approach is built on the premise that adding redundancy, even a linear one, will facilitate training. We therefore first illustrate this with a motivating example. We then describe in detail our ExpandNet strategy and finally introduce our approach to initializing the ExpandNet parameters.

\subsection{Motivation}
To illustrate the benefits of linear redundancy, we first consider a simple toy example. To this end, we build a network consisting of 4 fully-connected layers, each having 10 input and output channels and relying on a ReLU activation function, followed by one classification layer mapping to two class probabilities via a logistic function. We make use of the cross-entropy over 100 training samples as loss function. The network weights and the input vector values are set uniformly randomly in the interval $[-1,1]$, and the labels assigned randomly. 

Let us now consider the scenario where all the network weights are fixed, except for one, the first one in the first layer, that is learnable. In Fig.~\ref{fig:toy_example}(a), we show the complete loss curve, in black, obtained by varying this weight $w \in [-0.75, 0.75]$ by steps of 0.01. To mimic linear redundancy, we then expand $w$ into the product of two parameters $w_{1} \times w_{2}$. In Fig.~\ref{fig:toy_example}(b), we plot the loss as a function of these two parameters. Note that this expansion does not affect the network capacity, since any solution for $w_1$ and $w_2$ has an equivalent solution for $w$. However, it provides additional freedom to move in the objective function landscape.

To evidence the benefits of parameter redundancy, we performed 200 runs of gradient-based optimization of either just $w$, or both $w_1$ and $w_2$. Each run started from a random initialization $w\in[-0.75, -0.25]$, which corresponds to the region around the bad local minimum. For the two-parameter case, we computed equivalent values by randomly setting $w_1\in[-0.01, 0.01]$ and $w_2=w/w_1$. We found that
the two-parameter formulation reached the good (global) minimum in  $51.5\%$ of the cases, versus none for the single-parameter one, thus showing the benefits of linear redundancy to better explore the solution space. We illustrate the behavior of one of these runs with the blue (one parameter) and red (two parameters) dots in Fig.~\ref{fig:toy_example}(a). 

\subsection{ExpandNets}

The results of the previous toy experiment confirm that adding linear redundancy in a network facilitates parameter optimization. We now extend this idea to expanding the layers of small networks into multiple consecutive linear layers. Because all the operations expansion relies on are linear, the resulting ExpandNet is equivalent to the original small network and can be compressed back to the original structure algebraically. Below, we describe three different expansion strategies, starting with the simplest case of fully-connected layers followed by the convolutional scenario.

\begin{figure*}[t]
	\centering
	\includegraphics[width=\textwidth]{blocks.pdf}
	\caption{{\bf Expanding convolutions.} We illustrate our two strategies to expand a $(k \times k \times M \times N)$ convolution ($k\times k$ filters, $M$ input channels and $N$ output ones), shown at the top. {\bf (Left)} The first one consists of expanding it into three layers of the form $(1 \times 1 \times M \times P)$, $(k \times k \times P \times Q)$ and $(1 \times 1 \times Q \times N)$, respectively. 
	{\bf (Right)} The second strategy applies only when $k>3$. In this case, we can expand the convolution into $L = (k-1)/2$ layers of the form $(3 \times 3 \times P_{i-1} \times P_i)$, where $P_0 = M$ and $P_L = N$. 
	}
	\label{fig:blocks}
\end{figure*}\vspace{0.2cm}\noindent\textbf{Expanding fully-connected layers.}
The weights of a fully-connected layer can easily be represented in matrix form. Therefore, expanding such layers can be done in a straightforward manner by relying on matrix product. Specifically, let $W_{N\times M}$ be the parameter matrix of a fully-connected layer with $M$ input channels and $N$ output ones. That is, given an input vector $x\in \mathbb{R}^M$, the output $y\in \mathbb{R}^N$ can be obtained as
\begin{equation}
    y = W_{N\times M} x\;.
\end{equation}
Note that, here, we ignore the bias, which can be taken into account by incorporating an additional channel with value 1 to $x$. Expanding such a fully-connected layer with an arbitrary number of linear layers can simply be achieved by observing that its parameter matrix can be equivalently written as
\begin{equation}
    \centering
    W_{N \times M} = W_{N \times P_{1}} \times W_{P_{1} \times P_{2}} \times \cdots \times W_{P_{L} \times M}\;.
\end{equation}
As in our toy example, while this brings parameter redundancy, expanding a fully-connected layer in this manner does not affect the inherent network capacity.
More importantly, note that this allows us to increase not only the number of layers, but also the number of channels by setting $P_i>N,M$ for all $i$. To define these parameters, we rely on the notion of {\it expansion rate}. Specifically, for an expansion rate of $r$, we define $P_1 = rM$ and $P_i=rN$, $\forall i \neq 1$. Note that other strategies are possible, e.g., $P_i = r^iM$, but ours has the advantage of preventing the number of parameters from becoming overly large.
In practice, considering the computational complexity of fully-connected layers, we advocate expanding each layer into only two or three layers with a small expansion rate.

\vspace{0.2cm}\noindent\textbf{Expanding convolutional layers.} 
The operation performed by a convolutional layer can also be expressed in matrix form, by vectorizing the input tensor and defining a highly structured matrix whose elements are obtained from the vectorized convolutional filters. While this representation could therefore allow us to use the same strategy as with fully-connected layers, using arbitrary intermediate matrices would ignore the convolution structure, and thus alter the original operation performed by the layer.
For a similar reason, one cannot simply expand a convolutional matrix with kernel size $k\times k$ with a series of $k \times k$ convolutions, since, unless $k=1$, the resulting receptive field size would differ from the original one.

To overcome this, we note that $1 \times 1$ convolutions retain the computational benefits of convolutional layers while not modifying the receptive field size. As illustrated on the left of Fig.~\ref{fig:blocks}, we therefore propose to expand a $k \times k$ convolutional layer into 3 consecutive convolutional layers: a $1 \times 1$ convolution; a $k\times k$ one; and another $1 \times 1$ one. This strategy still allows us to increase the number of channels in the intermediate layer. Specifically, for an original layer with $M$ input channels and $N$ output ones, given an expansion rate $r$, we define the number of output channels of the first $1 \times 1$ layer as $P=rM$ and the number of output channels of the intermediate $k\times k$ layer as $Q=rN$.

Compressing an expanded convolutional layer into the original one can still be done algebraically. To this end, one can reason with the matrix representation of convolutional layers. Specifically, for an input tensor of size $(S\times T \times M)$, the matrix representation of the original layer can be recovered as
\begin{equation}
    \centering
    W_{MV \times NV} = W_{MV \times PV} \times W_{PV \times QV} \times W_{QV \times NV}\;,
\end{equation}
where $V = S\cdot T$ and each intermediate matrix has a structure encoding a convolution. It can be verified that the resulting matrix will also have a convolution structure, with filter size $k\times k$.

\vspace{0.2cm}\noindent\textbf{Expanding convolutional kernels.} 
While $3 \times 3$ kernels have become increasingly popular in very deep architectures~\cite{he2016deep}, larger kernel sizes are often exploited in small networks, so as to increase their expressiveness and their receptive fields. Interestingly, as illustrated on the right of Fig.~\ref{fig:blocks}, $k\times k$ kernels with $k>3$ can be equivalently represented with a series of $L$$3 \times 3$ convolutions, where
\begin{equation}
         L = \frac{k-1}{2}\;.
\end{equation}
As before, the number of channels in the intermediate $3 \times 3$ layers can be larger than that in the original $k \times k$ one, thus allowing us to incorporate linear redundancy in the model. Similarly to the fully-connected case, for an expansion rate $r$, we set the number of output channels of the first $3 \times 3$ layer to $P_1 = rM$ and that of the subsequent layers to $P_i = rN$.
The same matrix-based strategy as before can then be used to algebraically compress the expanded kernels into the original $k \times k$ convolutional layer.

Overall, the three strategies discussed above allow us to expand an arbitrary small network into an equivalent deeper and wider one. Note that these strategies can be used independently or together. In any event, once the resulting ExpandNet is trained, it can be compressed back to the original small architecture in a purely algebraic manner, that is, at absolutely no loss of information.

\subsection{Initializing ExpandNets}
As will be demonstrated by our experiments, training an ExpandNet from scratch yields consistently better results than training the original small network. However, it has been shown in the context of deep networks that initialization could have an important effect on the final results~\cite{Mishkin2015AllYN, He:2015}. While designing an initialization strategy specifically for small networks is an unexplored and challenging research direction, our ExpandNets can be initialized in a very natural manner.

To this end, we exploit the fact that an ExpandNet has a natural nonlinear counterpart, which can be obtained by incorporating a nonlinear activation function between each pair of linear layers. We therefore propose to initialize the parameters of an ExpandNet by simply training its nonlinear counterpart and transferring the resulting parameters to the ExpandNet. The initialized ExpandNet is then trained in the standard manner. As evidenced by our experiments below, this simple strategy yields an additional accuracy boost to our approach.


	\centering
	\includegraphics[width=0.8\columnwidth]{snet_to_enet.pdf}
	\caption{{\bf Small network and corresponding ExpandNet-CL+FC.} This ExpandNet was obtained by expanding the convolutional and fully-connected layers with an expansion rate $r=4$.}
	\label{fig: snet_to_enet}
\end{figure}\section{Experiments}\label{sec: exps}
In this section, we first demonstrate the benefits of our ExpandNets over training a small network from scratch. We then turn to evaluating their use in the context of knowledge transfer. All the experiments reported below were performed using PyTorch. Each of them was run $5$ times, and we report the mean and standard deviation over the runs. We will make our code publicly available upon acceptance of the paper.

\subsection{ExpandNets vs Small Networks}\label{sec:expnet}
Let us first study how our different expansion strategies affect the results of the corresponding small networks. To this end, we make use of the standard CIFAR-10 and CIFAR-100 datasets. These datasets contain 50,000 training images and 10,000 test images of 10 and 100 classes, respectively. The images are of size $32 \times 32$.

\vspace{0.2cm}\noindent\textbf{Experimental setup.} For both datasets, we use the same small network as in~\cite{pkt_eccv}. It is composed of 3 convolutional layers with $3 \times 3$ kernels. These 3 layers have 8, 16 and 32 output channels, respectively. Each of them is followed by a batch normalization layer, a ReLU layer and a $2 \times 2$ max pooling layer. The output of the last layer is passed through a fully-connected layer with 64 units, followed by a logit layer with either 10 or 100 units, so as to match the number of classes in the dataset of interest. To further evaluate our kernel expansion strategy, we also report results on using a similar network where the $3\times 3$ kernels were replaced by $7\times 7$ ones. All networks, including our ExpandNets, were trained for $100$ epochs using a batch size of $128$. We used Adam with an initial learning rate of $0.001$, which was divided by $10$ at epoch $50$. 

Below, we refer to the expansion of convolutional layers as {\it ExpandNet-CL}, and the expansion of convolutional kernels as {\it ExpandNet-CK}. When combined with the expansion of the fully-connected layer, we denote the models as {\it ExpandNet-CL+FC} and {\it ExpandNet-CK+FC}, and add another suffix {\it +Init} to indicate that we used our initialization strategy. In this set of experiments, the expand rate $r$ was always set to $4$. We will evaluate the influence of this parameter in the ablation study below.
The small network and corresponding ExpandNet-CL+FC are shown in Fig.~\ref{fig: snet_to_enet}.

\begin{table}[!t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{r c c }
    \toprule
    Model &  mAP & Top-1 Accuracy \\ 
    \midrule
    SmallNet        & $39.37 \pm 0.31$ & $73.32 \pm 0.20$  \\ 
    \midrule
    ExpandNet-CL      & $40.60 \pm 0.34$ & $73.96 \pm 0.30$  \\ 
    ExpandNet-CL+FC   & $42.30 \pm 0.78$ & $74.45 \pm 0.29$  \\ 
    ExpandNet-CL+FC+Init & $\textbf{42.70} \pm \textbf{0.39}$ & $\textbf{75.16} \pm \textbf{0.23}$ \\ 
    \bottomrule
    \end{tabular}
    }
    \vspace{0.01cm}
    \caption{\textbf{Small network with $3\times 3$ kernels vs ExpandNets on CIFAR-10.} Our approach yields consistently better results than the small network, particularly when expanding all layers and using our initialization strategy. }
  \label{table:cifar10_baseline}
\end{table}\vspace{0.2cm}\noindent\textbf{Results.}
We first report results obtained with the original small network used in~\cite{pkt_eccv}, with kernel size $3\times 3$.
In Table~\ref{table:cifar10_baseline}, we compare these results for different versions of our ExpandNets and for the small network. We report the mean Average Precision (mAP) metric used in~\cite{pkt_eccv} and the top-1 accuracy. Note that expanding the convolutional layers yields higher accuracy than the small network. This is further improved by also expanding the fully-connected layer, and even more so when using our initialization strategy. This, we believe, already confirms the benefits of (i) exploiting linear redundancy; and (ii) paying attention to initialization.

In Tables~\ref{table:cifar10_ks_7} and~\ref{table:cifar100_ks_7}, we show similar results on CIFAR-10 and CIFAR-100 for the case where we replaced the $3\times 3$ kernels with $7\times 7$ ones.
Again, expanding the convolutional layers improves over the small network, and is further boosted by expanding the fully-connected one and by using our initialization scheme. The same conclusions can be drawn when expanding the kernels; on its own, it ourperforms the small network, and is further boosted by FC expansion and initialization. Altogether, ExpandNet-CL+FC+Init  and ExpandNet-CK+FC+Init yield the best results in terms of both mAP and accuracy on both datasets.

\begin{table}[!t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{r c c }
    \toprule
    Model &  mAP & Top-1 Accuracy \\ 
    \midrule
    SmallNet        & $46.99 \pm 0.88$ & $77.29 \pm 0.48$  \\ 
    \midrule
    ExpandNet-CL      & $47.58 \pm 0.69$ & $78.38 \pm 0.28$  \\ 
    ExpandNet-CL+FC   & $51.77 \pm 0.61$ & $78.75 \pm 0.26$  \\ 
    ExpandNet-CL+FC+Init &  $\textbf{54.21} \pm\textbf{ 0.97}$ & $79.18 \pm 0.29$\\ 
    \midrule
    ExpandNet-CK      &$47.24 \pm 0.83$ & $79.41 \pm 0.28$     \\ 
    ExpandNet-CK+FC   &$49.57 \pm 0.56$ & $79.65 \pm 0.42$  \\ 
    
    ExpandNet-CK+FC+Init & $51.28 \pm 0.64$ & $\textbf{79.73} \pm \textbf{0.52}$\\ 
    
    \bottomrule
    \end{tabular}
    }
    \vspace{0.01cm}
   \caption{\textbf{Small network with $7\times 7$ kernels vs ExpandNets on CIFAR-10.} Our approach yields consistently better results than the small network.}
    \label{table:cifar10_ks_7}
\end{table}\begin{table}[!t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{r c c }
    \toprule
    Model &  mAP & Top-1 Accuracy \\ 
    \midrule
    SmallNet        &   $12.28 \pm 0.07$ & $43.95 \pm 0.53$  \\ 
    \midrule
    ExpandNet-CL      & $12.52 \pm 0.17$ & $44.44 \pm 0.77$\\ 
    ExpandNet-CL+FC   & $12.67 \pm 0.14$ & $45.00 \pm 0.25$ \\ 
    ExpandNet-CL+FC+Init & $12.81 \pm 0.26$ & $46.42 \pm 0.58$ \\ 
    \midrule
    ExpandNet-CK      & $12.78 \pm 0.27$ & $45.90 \pm 0.58$   \\ 
    ExpandNet-CK+FC   & $13.00 \pm 0.23$ & $46.47 \pm 0.11$ \\ 
    ExpandNet-CK+FC+Init & $\textbf{13.31} \pm \textbf{0.32}$ & $\textbf{48.08} \pm \textbf{0.82}$ \\ 
    \bottomrule
    \end{tabular}
    }
    \vspace{0.01cm}
    \caption{\textbf{Small network with $7\times 7$ kernels vs ExpandNets on CIFAR-100.} Our approach yields consistently better results than the small network.}
    \label{table:cifar100_ks_7}
\end{table}\vspace{0.2cm}\noindent\textbf{Ablation study.}
We now evaluate the behavior of our different convolutional expansion strategies, CL and CK, separately, when varying the expansion rate $r\in \{2, 4, 8\}$ and the kernel size $k \in \{3,5,7,9\}$. Although we used the same network structure as before, the optimization strategy for this set of experiments was slightly different. Specifically, we used stochastic gradient descent with a momentum of $0.9$ and a weight decay of $0.0005$ for $150$ epochs. The initial learning rate was $0.01$, and was divided by $10$ at epoch $50$ and $100$. Furthermore, we used zero-padding to keep the size of the input and output feature maps of each convolutional layer unchanged.

The results of these experiments are provided in Tables~\ref{table: cifar10} and~\ref{table: cifar100} for CIFAR-10 and CIFAR-100, respectively, where, for reasons of space, we report the top-1 accuracy only. We observe that the different versions of our ExpandNets outperform the small network in almost all cases. In particular, for kernel sizes $k>3$, ExpandNet-CK yields consistently higher accuracies than the corresponding small network, independently of the expansion rate. For $k=3$, where ExpandNet-CK is not applicable, ExpandNet-CL comes as an effective alternative, also consistently outperforming the baseline.

In Fig.~\ref{fig:train_curve}, we plot the learning curves for the best ExpandNet models (both CL and CK) for each kernel size $k$. Note that, in general, CK models tend to initially learn faster than CL ones, which reflects their better performance in our ablation study. The jumps in the learning curves coincide with the epochs where the learning rate was changed.

\begin{figure*}[t]
	\centering
	\includegraphics[width=\textwidth]{train_curve_cifar100.pdf}
	\caption{{\bf Learning curves corresponding to our ablation study on CIFAR-100.} Note that the curves represent the mean over the 5 runs for each network. For each kernel size $k$, we show the best ExpandNet (both CK and CL) models. That is, (a) $k=3$, ExpandNet-CL with $r = 4$; (b) $k=5$, ExpandNet-CL with $r = 4$ and ExpandNet-CK with $r = 8$; (c) $k=7$, ExpandNet-CL with $r = 8$ and ExpandNet-CK with $r = 8$; (d) $k=9$,wo  ExpandNet-CL with $r = 8$, ExpandNet-CK with $r = 8$. The behavior is similar on CIFAR-10.}
	\label{fig:train_curve}
\end{figure*}\begin{table*}[!t]
	\centering
	\resizebox{0.75\linewidth}{!}{
	\setlength{\tabcolsep}{10pt}
	\begin{tabular}[]{r c c c @{}cccc}
		\toprule
		   Model & \phantomtwo &  $r$  & \phantomtwo & \multicolumn{4}{c}{Kernel size $k$}   \\
		   \cmidrule(r){5-8}
		    & & & & 3 & 5 & 7 & 9 \\
		      \midrule 
		      SmallNet & & & & $78.67 \pm 0.52$ & $79.70 \pm 0.21$ & $79.76 \pm 0.29$& $79.38 \pm 0.43$\\
		    \midrule 
		   \multirow{3}{*}{ExpandNet-CL}  
		   & & 2 & & $78.86 \pm 0.21$ & $79.87 \pm 0.31$ & $79.72 \pm 0.26$ & $79.55 \pm 0.49$ \\ 
		   & & 4 & & $\textbf{79.12} \pm \textbf{0.21}$ & $80.03 \pm 0.28$ & $79.93 \pm 0.31$ & $79.23 \pm 0.75$   \\
		   & & 8 & & $79.00 \pm 0.26$ & $79.71 \pm 0.47$ & $79.60 \pm 0.45\textbf{}$ & $79.60 \pm 0.16$ \\
		\midrule 
		 \multirow{3}{*}{ExpandNet-CK}  
		   & & 2 & & $N/A$ & $80.18 \pm 0.34 $ & $80.80 \pm 0.25$ & $80.35 \pm 0.39$ \\ 
		   & & 4 & & $N/A$ & $80.44 \pm 0.30$  & $81.14 \pm 0.43$ & $80.93 \pm 0.39$   \\
		   & & 8 & & $N/A$ & $\textbf{80.70} \pm \textbf{0.20}$ & $\textbf{81.26} \pm \textbf{0.25}$ & $\textbf{81.10} \pm \textbf{0.32}$ \\
		\bottomrule
	\end{tabular}
	}
	\vspace{0.1cm}
	  \caption{\textbf{ExpandNets vs small networks on CIFAR-10.} We report the top-1 accuracy for the original small networks and for different versions of our approach. Note that our ExpandNets yield higher accuracies than the small network in almost all cases.}
    \label{table: cifar10}
\end{table*}\begin{table*}[!t]
	\centering
	\resizebox{0.75\linewidth}{!}{
	\setlength{\tabcolsep}{10pt}
	\begin{tabular}[]{r c c c @{}cccc}
		\toprule
		   Model & \phantomtwo &  $r$  & \phantomtwo & \multicolumn{4}{c}{Kernel size $k$}   \\
		   \cmidrule(r){5-8}
		    & & & & 3 & 5 & 7 & 9 \\
		    \midrule 
		   SmallNet & & & & $47.03 \pm 0.45$ & $46.84 \pm 0.50$ & $47.49 \pm 0.38$ & $47.41 \pm 0.43$\\
		\midrule 
		   \multirow{3}{*}{ExpandNet-CL}  
		   & & 2 & & $47.17 \pm 0.79 $ & $47.91 \pm 0.53$ & $47.44 \pm 0.29$ & $47.86 \pm 0.60$ \\ 
		   & & 4 & & $47.07 \pm 0.25 $ & $48.04 \pm 0.39$ & $48.22 \pm 0.73$ & $47.87 \pm 0.78$   \\
		   & & 8 & & $47.12 \pm 0.54 $ & $47.89 \pm 0.39$ & $48.52 \pm 0.67$ & $48.23 \pm 0.64$ \\
		\midrule 
		 \multirow{3}{*}{ExpandNet-CK}  
		   & & 2 & & $N/A$ & $49.08 \pm 0.78$ & $48.81 \pm 0.86$ & $48.91 \pm 0.71$ \\ 
		   & & 4 & & $N/A$ & $49.19 \pm 0.51$ & $49.41 \pm 0.31$ & $49.64 \pm 0.50$  \\
		   & & 8 & & $N/A$ & $\textbf{49.73} \pm \textbf{0.53}$ & $\textbf{49.72} \pm \textbf{0.27}$ & $\textbf{50.09} \pm \textbf{0.85}$ \\
		\bottomrule
	\end{tabular}
	}
	\vspace{0.1cm}
	  \caption{\textbf{ExpandNets vs small networks on CIFAR-100.} We report the top-1 accuracy for the original small networks and for different versions of our approach. Note that our ExpandNets yield higher accuracies than the small network in almost all cases.}
    \label{table: cifar100}
\end{table*}\subsection{Knowledge Transfer with ExpandNets}
As mentioned above, our ExpandNet strategy is orthogonal to knowledge transfer; that is, we can apply any knowledge transfer method using our ExpandNet as student instead of the small network directly. To demonstrate the benefits of ExpandNets in this scenario, we therefore use them in conjunction with knowledge distillation (KD)~\cite{hinton2015distilling}, hint-based transfer (Hint)\cite{romero2014fitnets} and probabilistic knowledge transfer (PKT)~\cite{pkt_eccv}. Below, we first tackle image classification and then turn to face retrieval. 

\begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{r r c c }
    \toprule
    Network  & Transfer &   mAP & Top-1 Accuracy \\ 
    \midrule
    \multirow{3}{*}{SmallNet} 
    &  KD   & $40.33 \pm 0.39$ & $73.34 \pm 0.31$   \\  
    &  Hint & $22.37 \pm 2.77$ & $33.71 \pm 4.35$  \\   
    &  PKT  & $50.47 \pm 0.77$ & $68.36 \pm 0.35$  \\  
    \midrule
    \multirow{2}{*}{ExpandNet}
    &  KD   & $42.35 \pm 0.68$ & $74.52 \pm 0.37$  \\  
    &  Hint & $43.44 \pm 1.71$ & $52.46 \pm 2.43$  \\  
    \multirow{1}{*}{(CL+FC)}
    &  PKT  & $56.47 \pm 0.40$ & $70.97 \pm 0.70$   \\  
    \midrule
    \multirow{2}{*}{ExpandNet}
    &  KD   & $44.47 \pm 0.55$ & $\textbf{75.17} \pm \textbf{0.51}$  \\
    &  Hint & $45.70 \pm 2.43$ & $58.27 \pm 3.83$    \\   
    \multirow{1}{*}{(CL+FC+Init)}
    &  PKT  & $\textbf{56.69} \pm \textbf{0.48}$ & $71.65 \pm 0.41$ \\
    \bottomrule
    \end{tabular}
    }
    \vspace{0.02cm}
    \caption{{\bf Knowledge transfer from the ResNet18 on CIFAR-10.} Using ExpandNets as student networks yields consistently better results than directly using the small network.}
    \label{table:cifar10_KD_resnet18}
\end{table}\vspace{0.2cm}\noindent\textbf{Image classification.} For our first set of knowledge transfer experiments, we make use of the CIFAR-10 dataset, which is commonly used for this task. Recall that knowledge distillation relies on a deep teacher network to train the small one. Following~\cite{pkt_eccv}, we make use of the ResNet18 as teacher. Furthermore, we also use the same small network as in~\cite{pkt_eccv}, which was described in Section~\ref{sec:expnet}. In Table~\ref{table:cifar10_KD_resnet18}, we compare the results of different knowledge transfer strategies applied to the small network and to our ExpandNets, without and with initialization. Note that using our ExpandNets consistently outperforms using the small network. In particular, with KD, Hint and PKT, our approach yields a boost of more than 4, 23 and 6 mAP points, respectively. This is also reflected by large accuracy boosts. Note that our initialization still helps in all cases, and yields the highest mAP with PKT and highest accuracy with KD.

\begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{r r c c }
    \toprule
    Network  & Transfer &   mAP & Top-1 Accuracy \\
    \midrule
    Teacher &  & $64.73 \pm 0.88$ & $81.32 \pm 0.57$ \\
    \midrule
    \multirow{3}{*}{SmallNet} 
    &  KD   & $40.30 \pm 0.30$ & $72.82 \pm 0.42$  \\  
    &  Hint & $20.36 \pm 4.44$ & $29.55 \pm 7.94$  \\  
    &  PKT  & $45.03 \pm 0.57$ & $64.46 \pm 2.04$  \\         
    \midrule
    \multirow{2}{*}{ExpandNet}
    &  KD   & $44.02 \pm 0.64$ & $73.97 \pm 0.27$  \\   
    &  Hint & $46.23 \pm 3.73$ & $57.33 \pm 6.36$  \\  
    \multirow{1}{*}{(CL+FC)}
    &  PKT  & $48.31 \pm 0.51$ & $64.74 \pm 1.81$  \\
    \midrule
     \multirow{2}{*}{ExpandNet}
    &  KD   & $50.22 \pm 0.18$ & $\textbf{74.72} \pm \textbf{0.30}$  \\
    &  Hint & $47.26 \pm 3.06$ & $59.60 \pm 5.99$ \\
    \multirow{1}{*}{(CL+FC+Init)}
    &  PKT& $\textbf{51.99} \pm \textbf{0.57}$ & $69.98 \pm 1.77$ \\
    \bottomrule
    \end{tabular}
    }
    \vspace{0.02cm}
    \caption{{\bf Knowledge transfer from the nonlinear ExpandNet on CIFAR-10.} As in the ResNet18 case, transferring to ExpandNets is more effective than to the small network. 
    }
    \label{table:cifar10_KD_small_teacher}
\end{table}
As mentioned earlier, the nonlinear counterpart of our ExpandNet can also be used as a teacher, thus preventing one from having to train a very deep network, which can overfit. To demonstrate that the benefits of our approach generalize to using another teacher, we therefore replace the ResNet18 with our nonlinear ExpandNet. The results of this experiment are provided in Table~\ref{table:cifar10_KD_small_teacher}. As in the ResNet18 case, applying knowledge transfer to ExpandNets yields better results than using the small network. 

\vspace{0.2cm}\noindent\textbf{Face retrieval.} Finally, following~\cite{pkt_eccv}, we evaluate our approach on the large-scale YouTube Face dataset~\cite{conf/cvpr/WolfHM11}. In this experiment, the goal is to retrieve, for a given query image, the image of the same person in a library of face images.
This dataset contains $260,108$ frames extracted from videos, $200,000$ of which are used for training, and the rest for testing. For this experiment, we rely on the setup of~\cite{pkt_eccv} and use the 480-dimensional CS-LBP features of~\cite{conf/cvpr/WolfHM11} as teacher instead of a deep network. Because this experiment focuses on retrieval, and not classification, KD cannot be used. By contrast, Hint and PKT remain applicable. Note that, following~\cite{pkt_eccv}, we also report the results of S-PKT, which is a modified version of PKT that further incorporates hard-label supervision. 


For this experiment, the student network has the following architecture: conv($3\times3\times16$) - conv($3\times3\times32$) - max-pool($2\times 2$) - conv($3\times3\times64$) - conv($3\times3\times64$) - max-pool($2\times 2$) - conv($3\times3\times64$) - fc(64). Each convolutional layer is followed by a batch normalization and a ReLU is used after every layer. The network takes images resized to $64 \times 64$ as input. All models were trained using Adam  for 10 epochs with a learning rate of $0.0001$ and a batch size of $128$.

\begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{r r c c c c}
    \hline
    Network  & Model &   mAP & t-20 & t-50 & t-200 \\ 
    \toprule
    \multirow{1}{*}{Baseline} 
    & LBP   & ${46.38} \pm {0.88}$ & ${98.78}$ & ${95.66}$ & ${81.02}$  \\
  \midrule
    \multirow{3}{*}{SmallNet} 
    & Hint  & ${51.18} \pm {0.71}$ & ${97.60}$ & ${95.40}$ & ${84.88}$  \\
    & PKT   & ${54.68} \pm {0.82}$ & ${99.80}$ & ${98.86}$ & ${88.52}$  \\
    & S-PKT & ${69.59} \pm {1.05}$ & ${99.88}$ & ${99.28}$ & ${91.27}$  \\
    \midrule

    \multirow{3}{*}{ExpandNet-CL}
    &  Hint & ${53.77} \pm {0.99}$ & ${99.28} $ & ${98.04} $ & ${88.18} $  \\
    &  PKT & ${55.06} \pm {0.73}$ & ${99.85} $ & ${99.04} $ & ${88.91} $  \\
    & S-PKT & $\textbf{70.24} \pm \textbf{1.08}$ & ${99.90}$ & ${99.33}$ & ${91.61}$  \\
  \bottomrule
    \end{tabular}
    }
    \vspace{0.01cm}
    \caption{{\bf Knowledge transfer on the YouTube Face dataset.}}
    \label{table:yf}
\end{table}

The results of this experiment are reported in Table~\ref{table:yf}. Here, because the goal is retrieval, we also report the top-$k$ precision metric used in~\cite{pkt_eccv}.  Note that, since the kernel size is always $3\times 3$, ExpandNet-CK does not apply. Furthermore, since the input dimension of the fully-connected layer is $7744$, we do not expand this layer, so as to keep the computational cost manageable. As in the CIFAR-10 experiments, using ExpandNet as a student consistently yields better results than using the small network.


We have introduced an approach to training a given small network by exploiting parameter redundancy. Specifically, we have shown that incorporating {\it linear} redundancy facilitates the training of small networks, leading to better parameter values. Since the resulting networks are equivalent to the original small ones, they can be compressed back at no loss of information. Our technique is general and can also be used in conjunction with knowledge transfer. Our experiments have demonstrated that, both with and without transfer, ExpandNets consistently outperform the original small networks. In particular, when applicable, our CK expansion strategy tends to yield the best results. When it doesn't, that is, for a kernel size of 3, the CL ones nonetheless remains highly effective. Furthermore, our initialization scheme yields a consistent accuracy boost in all cases. This strategy, however, is not the only possible one.
In the future, we will therefore focus on developing other initialization techniques for our ExpandNets, and generally for small networks.
\thispagestyle{empty}



{\small
\input{top.bbl}
}


\end{document}


