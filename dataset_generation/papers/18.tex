\def\year{2019}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{aaai19}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\frenchspacing  %Required

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{xcolor}
\usepackage{tabularx}


\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
%PDF Info Is Required:
\pdfinfo{
	/Title (Pathological Evidence Exploration in Deep Retinal Image Diagnosis)
	/Author (Yuhao Niu, Lin Gu, Feng Lu, Feifan Lv, Zongji Wang, Imari Sato, Zijian Zhang, Yangyan Xiao, Xunzhang Dai, Tingting Cheng)
}
\setcounter{secnumdepth}{0}

\title{Pathological Evidence Exploration in Deep Retinal Image Diagnosis}
\author{
Yuhao Niu,\textsuperscript{1,2,}\thanks{These two authors contributed equally to the paper.}
Lin Gu,\textsuperscript{4,}$^*$
Feng Lu,\textsuperscript{1,2,3,}\thanks{Corresponding Author: Feng Lu (lufeng@buaa.edu.cn)}
Feifan Lv,\textsuperscript{1,3}
Zongji Wang,\textsuperscript{1}
Imari Sato,\textsuperscript{4}\\{\bf \Large
Zijian Zhang,\textsuperscript{5}
Yangyan Xiao,\textsuperscript{6}
Xunzhang Dai,\textsuperscript{5}
Tingting Cheng\textsuperscript{5}
} \\
\textsuperscript{1}State Key Laboratory of VR Technology and Systems, School of CSE, Beihang University, Beijing, China \\
\textsuperscript{2}Beijing Advanced Innovation Center for Big Data-Based Precision Medicine, Beihang University, Beijing, China\\
\textsuperscript{3}Peng Cheng Laboratory, Shenzhen, China \quad
\textsuperscript{4}National Institute of Informatics, Japan \\
\textsuperscript{5}Xiangya Hospital Central South University, China \quad
\textsuperscript{6}The Second Xiangya Hospital of Central South University, China
}

\begin{document}
	% The file aaai.sty is the style file for AAAI Press
	% proceedings, working notes, and technical reports.
	%

	\maketitle
	\begin{abstract}
		Though deep learning has shown successful performance in classifying the label and severity stage of certain disease, most of them give few evidence on how to make prediction. Here, we propose to exploit the interpretability of deep learning application in medical diagnosis. Inspired by Koch's Postulates, a well-known strategy in medical research to identify the property of pathogen, we define a pathological descriptor that can be extracted from the activated neurons of a diabetic retinopathy detector. To visualize the symptom and feature encoded in this descriptor, we propose a GAN based method to synthesize pathological retinal image given the descriptor and a binary vessel segmentation. Besides, with this descriptor, we can arbitrarily manipulate the position and quantity of lesions. As verified by a panel of 5 licensed  ophthalmologists, our synthesized images carry the symptoms that are directly related to diabetic retinopathy diagnosis. The panel survey also shows that our generated images is both qualitatively and quantitatively superior to existing methods.
	\end{abstract}

	\section{Introduction}

	Deep learning has become a popular methodology in analyzing medical images such as diabetic retinopathy detection~\cite{Gulshan2016jama}, classifying skin cancer~\cite{esteva2017nature}. Though these algorithms have proven quite accurate in classifying specific disease label and severity stage, most of them lack the ability to explain its decision, a common problem that haunts deep learning community. Lacking interpretability is especially imperative for medical image application, as physicians or doctors relies on medical evidence to determine whether to trust the machine prediction or not.

	In this paper, we propose a novel technique inspired by  Koch's Postulates to give some insights into how convolutional neural network (CNN) based pathology detector makes decision. In particular, we take the diabetic retinopathy detection network~\cite{oO2016detector} for example. Noted that not limited for ~\cite{oO2016detector}, this strategy could also be extended to interpret more general deep learning model.

    We at first apply~\cite{oO2016detector} on the  reference image (Fig~\ref{fig:koch}.(a)) and extract the pathological descriptor (Fig~\ref{fig:koch}.(b)) that encodes the neuron activation directly related to prediction. Picking thousand out of  millions of parameters in neuron network is like separating the potential pathogen. Koch's Postulates claims that the property of pathogen, though invisible for naked eye, could be determined by observing the arose symptom after injecting it into subject.  Similarly, we \emph{inject} the pathologic descriptor into the binary vessel segmentation (Fig~\ref{fig:koch}.(c)) to synthesize the retinal image. We achieve this with a GAN based network as illustrated in Fig~\ref{fig:pipeline}. Given pathologic descriptor and binary vessel segmentation, our generated image (Fig~\ref{fig:koch}.(d))  exhibits the expected symptom such as microaneurysms and hard exudates that appear in the target image. Since our descriptor is lesion-based and spatial independent, we could arbitrarily manipulate the position and number of lesions. Evaluated With a panel of 5  licensed  ophthalmologists, our generated retinal images are  qualitatively and quantitatively superior to existing methods.


    %We could visualize it into a new image (Fig~\ref{fig:koch}.(d)) which exhibits the symptom encoded in the descriptor.


    %	To intuitively illustrate the effect and meaning of this descriptor, we borrow the concept of Koch's Postulates, a strategy to identify potential pathogen. As illustrated in Fig~\ref{fig:koch}.(b), picking thousand out of  millions of parameters in neuron network into pathologic descriptor is like separating the potential pathogen from all sorts of materials. In medical research, the property of pathogen is then determined by observing the arose symptom after injecting it into body. Similarly, we \emph{inject} the pathologic descriptor into the binary vessel segmentation (Fig~\ref{fig:koch}.(c)) to synthesize the retinal image. We achieve this with a GAN based network as illustrated in Fig~\ref{fig:pipeline}. Given pathologic descriptor and binary vessel segmentation, our generated image (Fig~\ref{fig:koch}.(d))  exhibits the expected symptom such as microaneurysms and hard exudates that appear in the target image. Our generated retinal images are finally evaluated and verified by a group of licensed  ophthalmologists.




	\begin{figure*}[h!]
		\begin{center}
			\includegraphics[scale=0.52]{figures/koch.pdf}
		\end{center}
		\caption{Koch's Postulates are criteria in Evidence Based Medicine (EBM) to determine the pathogen for a certain disease. They state that the pathogen must be found in diseased subjects but not in healthy ones; the pathogen must be isolated and grown in pure culture; the cultured pathogen should cause disease after injected into healthy subject; the pathogen isolated again is the same as the injected one. The methodology of this paper is an analogy to Koch's Postulates. (a) Reference retinal image with disease. (b) Extract pathological descriptor from an image like separating pathogen. (c) Apply the descriptor on a binary vessel segmentation like injecting purified pathogen into the subject. (d) The synthesized image or subject with same symptom.}
		\label{fig:koch}
	\end{figure*}


   % The concept of this paper is to extract different pathological descriptors from a reference image, which are then applied on a vessel segmentation (with no diseases) to generate retinal fundus with the same lesions distributed in specific locations. This is similar with a medical experiment in which pathogen are separated from diseased animals and injected into healthy ones to cause disease.

	Specifically, we  encode a series of pathological descriptor as illustrated in Fig~\ref{fig:descriptor}. According to our analysis, the diabetic retinopathy detection network~\cite{oO2016detector} predicts the diabetic level through a few dimensions (6 among 1024, lighted with colors) of bottleneck feature in Fig~\ref{fig:descriptor}. We then identify the neurons that directly contribute to these 6 dimension bottleneck features in the activation net and record their position and activation value as the pathologic descriptor. Since neuron activation is spatially correlated with individual lesion, our descriptor is defined as lesion-based that allows us to manipulate its position and quantity.

	%To intuitively illustrate the effect and meaning of this descriptor, we borrow the concept of Koch's Postulates, a strategy to identify potential pathogen. As illustrated in Fig~\ref{fig:koch}.(b), picking thousand out of  millions of parameters in neuron network into pathologic descriptor is like separating the potential pathogen from all sorts of materials. In medical research, the property of pathogen is then determined by observing the arose symptom after injecting it into body. Similarly, we \emph{inject} the pathologic descriptor into the binary vessel segmentation (Fig~\ref{fig:koch}.(c)) to synthesize the retinal image. We achieve this with a GAN based network as illustrated in Fig~\ref{fig:pipeline}. Given pathologic descriptor and binary vessel segmentation, our generated image (Fig~\ref{fig:koch}.(d))  exhibits the expected symptom such as microaneurysms and hard exudates that appear in the target image. Our generated retinal images are finally evaluated and verified by a group of licensed  ophthalmologists.



	Our main contributions are mainly three-folds:
\begin{enumerate}
\item We define a pathological descriptor that encodes the key parameters of CNN which is directly related to disease prediction. This descriptor is associated with individual lesion.
\item Inspired by Koch's Postulates, we propose a novel interpretability strategy to visualize the pathological descriptor by synthesizing fully controllable pathological images. The synthesized images are  verified  by a group of licensed  ophthalmologists.
\item With our pathological descriptors, we could generate medical plausible pathology retinal image where the position and quantity of lesion could arbitrarily manipulated.
\end{enumerate}





%    1. We define a pathologic descriptor that encodes the key parameters of CNN which is directly related to disease prediction. 2. Inspired by Koch's Postulates, we propose a novel strategy to decode the pathologic descriptor by synthesizing images with symptom. 3. Our generated images are verified  by a group of licensed  ophthalmologists.



	%first method to synthesis vivid pathological symptom by decoding the pathological descriptor extracted from existing retinal diabetic algorithm. 2. We could synthesis retinal image under various condition from a segmentation.


	%is able to transfer the related symptom naturally by  retinal image with this descriptor and binary vessel segmentation. Our generated images were finally evaluated by licensed retinalogy doctor.




	%In practice, these methods mainly predict the pre-defined pathological label (such as diabetic level in Kaggle dataset ?) or identify the symptom region.
	%As illustrated in Fig?, we design a strategy to extract the pathological descriptor from existing deep learning based pathology detection algorithm such as ?.



	%Retinal fundus images have a very rich message and are very important for the detection and prevention of eye diseases. Taking diabetic patients as an example, it has been found that about one-third of patients will develop diabetic retinopathy. If timely detection and treatment are not available, it is very easy to deteriorate and cause Macular Edema, which may lead to blindness. Regular checkups are an effective way to reduce morbidity, but require a lot of medical resources.





	%Though these algorithms predict the pathological label quite accurately, they could hardly be called



	%The characteristics of the fundus image and its role in the medical field can be used to diagnose XXXX disease.
	%Deep learning is widely used and requires data.
	% Existing data sets, features, and collection methods.
	% Insufficient data, especially with various case characteristics.
	% The current synthesis method has xxx, simple analysis, what is the problem (cannot specify the type of lesion, etc.).
	% If you use style transfer, what is wrong with the current method.

	% The core idea of this method is xxx (extracting various case characteristics for guiding end-to-end synthesis). To this end, the method of this article includes the xxx step (brief introduction). Therefore, the method has the advantage of xxx effect.






	%Summary contribution point.

	\section{Related Works}
    \subsection{Diabetic Retinopathy  Detection}
    \label{sec:DR detector}
% With the fast development of deep learning, this technique has shown successful applications on several medical imaging challenges including skin cancer, lung node, breast cancer, \textit{etc.}
% (wzj)[in this sentence, deep learning can't show applications; and lung node is not a medical imaging challenge.] this technique has achieved success on several medical image analysis applications, such as computer-aided diagnosis of skin cancer, lung node, breast cancer, \textit{etc.}
With the fast development of deep learning, this technique has achieved success on several medical image analysis applications, such as computer-aided diagnosis of skin cancer, lung node, breast cancer, \textit{etc.} In the case of diabetic retinopathy (DR), automatic detection is particularly needed to reduce the workload of ophthalmologists, and slow down the progress of DR by performing early diagnose on diabetic patients~\cite{Gulshan2016jama}.
%while performing early diagnose on diabetic patients to slow down the  progress of DR.


     In 2015, a Kaggle competition~\cite{kaggle2016diabetic} was organized to automatically classify retinal images into five stages according to \textit{International Clinical Diabetic Retinopathy Disease Severity Scale}~\cite{aao2002drscale}. Not surprisingly, all of the top-ranking methods were based on deep learning. Then, another deep learning method~\cite{Gulshan2016jama}, trained on  128175 images, achieved a high sensitivity and specificity for detecting diabetic retinopathy. Noting that image-level grading lacks intuitive explanation, the recent methods~\cite{Yang17MICCAI,wang2017zoom} shifted the focus to locate the lesion position. However, these methods often relied on a large training set of lesion annotations from professional experts.

     In this paper, we propose a novel strategy to encode the descriptor from the DR detector's activated neurons directly related to the pathology. For the sake of generality, we select the o\_O\cite{oO2016detector}, a CNN based method within the top-3 entries on Kaggle's challenge. Even now, the performance of o\_O is still equivalent to the latest method~\cite{wang2017zoom}. This method is trained and tested on the image level DR severity stage.










  %  The early diganosis on the fundus image is the key to slowing down the disease as .


   % some work like~\cite{wang2017zoom} uses deep learning methods to detect diseases based on fundus images and has achieved amazing results.






    %Due to the uneven distribution, the small amount, and the large investment, medical resources often cannot fully meet social needs. For thousands of years, people have been eager for the machine to intelligently detect and diagnose diseases, thus alleviating the shortage of medical resources. With the development of technology, this issue is no longer out of reach.

	%With the development of deep learning, some work like~\cite{wang2017zoom} uses deep learning methods to detect diseases based on fundus images and has achieved amazing results. However, due to the particularity of medical diagnosis and the unexplained nature of deep learning, it is still open to question whether such methods can be accepted by doctors. On the other hand, as mentioned above, such methods is plagued by data shortages.

	%We have made a preliminary exploration of this dilemma, and proposed a solution that can explain how the prediction is made by deep learning methods, and visualize to prove our scheme by using the idea of Koch's postulates. At the same time, the synthetic realistic images can effectively alleviate data acquisition problems.

	\subsection{Generative Adversarial Networks}
Generative Adversarial Networks (GANs)~\cite{goodfellow2014generative} were first proposed in 2014, adopting the idea of zero-sum game. Subsequently, CGANs~\cite{mirza2014conditional} attempted to use additional information to make the GAN controllable. DCGAN~\cite{radford2015unsupervised} combined CNN with traditional GAN to achieve a shocking effect. Pix2pix~\cite{isola2017image} used the U-Net~\cite{ronneberger2015u} combined with adversarial training and achieved amazing results. CycleGAN~\cite{zhu2017unpaired} used two sets of GANs and added cycle loss to achieve style transfer on unpaired data.


    		\begin{figure*}[h!]
		\begin{center}
			\includegraphics[width=\textwidth]{figures/descriptor.pdf}
		\end{center}
		\caption{The process for extracting pathological descriptors. First, a pathological reference image is fed into the DR detection net. Next, the extracted features are mapped to the input pixel space through the activation net to get activation projections, which indicate the locations and appearance of most lesions. Finally, the features and related activation projections are cropped into small patches around the found lesions, which are recognized as pathological descriptors. }
		\label{fig:descriptor}
	\end{figure*}



	\subsection{Style Transfer}
%(wzj)['texture information' is only one aspect of 'style'
%Image style transfer has been studied for a long time, aiming at synthesizing a new picture by combining the content of one image and the style of another one. \textcolor{red}{With the great power brought by deep CNNs, neural style transfer becomes popular. A typical method was proposed in \cite{DBLP:conf/cvpr/GatysEB16}, in which they directly optimized the input pixels to get stylized image. CNNs were used to extract both content and style features, which constrained the training as two different losses. Later in \cite{DBLP:conf/cvpr/LuanPSB17}, the authors improved photo style transfer results by introducing semantic masks, which were downscaled to the feature space and used to filter specific styles in specific regions. Similar trick was adopted in \cite{DBLP:conf/cvpr/GatysEBHS17}, except for that they treated downscaled masks as new channels and concatenated them to the existing feature maps.
%	To speed up the transfer procedure, \cite{DBLP:conf/eccv/JohnsonAF16} added a network to synthesize image. Once trained by a given style reference image, it can finish style transfer by one feed-forward propagation.}
    Recently, neural style transfer using deep CNNs becomes popular. A typical method was proposed in \cite{DBLP:conf/cvpr/GatysEB16}, in which they directly optimized the input pixels to restrict both content and style features extracted by CNNs.
    Later in \cite{DBLP:conf/cvpr/LuanPSB17} and \cite{DBLP:conf/cvpr/GatysEBHS17}, semantic masks were introduced to improve image style transfer.
	To speed up the transfer procedure, \cite{DBLP:conf/eccv/JohnsonAF16} added a network to synthesize image. Once trained by a given style reference image, it can finish style transfer by one feed-forward propagation.
    %Similar generator-based style transfer also appears with GAN based methods, as mentioned in the previous section.


	\subsection{Synthesizing Biomedical Images}

%The synthesis of biomedical images is of great significance for the detection of diseases and auxiliary medicine. To alleviate the lack of annotations, there have been a  number of related researches.
	The traditional biomedical imaging synthesizing uses the medical and biological prior knowledge accumulated by humans, combined with complex simulation methods to produce realistic results. 	Probably most well-known efforts are the work~\cite{fiorini2014automatic}, the work~\cite{bonaldi2016automatic}, GENESIS~\cite{bower2015genesis}, NEURON~\cite{carnevale2006neuron}, L-Neuron~\cite{ascoli2000neuron} \textit{etc.}
	With the development of deep learning, some methods like~\cite{zhao2018synthesizing} began to synthesize realistic retinal and neuronal images in a data-driven way. Tub-sGAN, a variant of~\cite{zhao2018synthesizing}, synthesized image given a binary tubular annotation and a reference image. Although their generated images could show pleasant visual appearance, the diabetic retinopathy symptoms and retina physiological details are either lost or  incorrect as verified by the ophthalmologists. In this paper, we propose a pathologically controllable method that can generate realistic retinal image with medical plausible symptoms.


  %  a pattern of specified pathological features



  %  ,  transfers the image style such as color and brightness to



   % synthesizes lesion images similar to the reference lesion images, which is called Tub-sGAN in that paper.








   % Existing researches have confirmed that images generated by such methods are more realistic. Therefore, to visualize the symptom and feature encoded in the extracted descriptor, we along this line to synthesize realistic retinal phantoms in an end-to-end fashion with data-driven approach.

 %   \textcolor{red}{below are an old version of summary written by Lv Feifan, it may be redundant now}

  %  As mentioned above, the work~\cite{zhao2018synthesizing} can not only solve the problem of synthesizing retinal and neuronal images using small samples, but also synthesizing lesion images similar to the reference lesion images, which is called Tub-sGAN in that paper.

	%On the one hand, the precise and rigorous nature of medical images increases the difficulty of generating realistic medical images. In this regard, Tub-GAN still has a lot of room for improvement. For example, the optic disc is not obvious enough and the macula is slightly inferior in some results synthesized by Tub-GAN. On the other hand, when synthesizing images with specific lesions, the results of Tub-sGAN appear to mainly learn color features rather than specific pathological features.
%	Considering the above issue, we propose a pathologically controllable method that can generate a pattern of specified pathological features, and achieve complete controllability of position, quantity and type. A large number of experiments have proved that the image quality has also improved significantly.


\section{Pathological Descriptor}

In this section, we would describe how to extract lesion based pathological descriptor from the activated neurons of Diabetic Retinopathy (DR) detector~\cite{oO2016detector}.% a CNN based  Diabetic Retinopathy (DR) detecting method, that classifies the retinal image into 5 stage.


 % To make deep learning based medical imaging method more convincing, we aim to exploit the interpretability and medical basis of a network trained for detecting diabetic retinopathy from a retinal image. As discussed in the last section, for the sake of generality, we took o\_O~\cite{oO2016detector}, a CNN based  Diabetic Retinopathy (DR) detecting method, that classifies the retinal image into 5 stage.

%  In order to make CNNs more convincing and acceptable to be applied in medical scenes, as well as to improve the operability of them, we proposed a strategy to illustrate the interpretability and medical basis of a network trained for detecting and diagnosing fundus diseases.

%This is done by synthesizing images with medical diagnosis by the features we use for classification.

	%We took diabetic retinopathy for example in our work. In this section, we introduce a traditional DR detection network, and show how a CNN visualization method can be used to extract locations and pathological descriptors of retinal lesions. The whole pipeline is shown in Fig~\ref{fig:descriptor}.

	\subsection{Diabetic Retinopathy Detection}
  Here, we  briefly introduces  DR detector~\cite{oO2016detector} used in this paper. It takes retinal fundus image of shape $448 \times 448 \times 3$ as input  and outputs the 5 grades (0-4) diabetic retinopathy severity.
   As shown in the left part of Fig~\ref{fig:descriptor}, the DR detection network is stacked with several blocks, each of which consists of 2-3 convolutional layers and a pooling layer. As the number of layers increases, the network merges into a  $ 1\times 1 \times 1024$ bottleneck feature. To add nonlinearity to the net and to avoid neuronal death, a leaky ReLU~\cite{maas2013rectifier} with negative slope 0.01 is applied following each convolutional and dense layer.

   The bottleneck feature is fed into a dense layer (not shown in the figure) to predict the severity labels provided in the \textit{DR Detection Challenge} \cite{kaggle2016diabetic}. The network is trained with Nesterov momentum over 250 epochs. Data augmentation methods, such as dynamic data re-sampling, random stretching, rotation, flipping, and color augmentation, are all applied. We refered to ~\cite{oO2016detector} for details.


   %Moreover, as the large-scale network is hard to train, \textcolor{red}{one strategy is to train a smaller network first, and then use its weights to initialize the larger network}.





%   Considering that it is meaningful to treat severity as an continuous real number,



  % As the shape of feature map is reduced, the number of channels is gradually increased. When the feature map reduces to a shape of $ 1\times 1 $, the convolutional layer actually becomes a fully connected (dense) layer.


  %This method is with a configuration named c\_512\_4x4\_32 as the diabetic retinopathy detector \cite{oO2016detector}. The input is a retinal fundus image of shape $448 \times 448 \times 3$ and the output is the 5 grades (0-4) diabetic retinopathy severity of the patient.



	%Following the original method, we use fundus photographs and severity labels provided in the Kaggle diabetic retinopathy detection challenge \cite{kaggle2016diabetic} as the training dataset.  Considering that it is meaningful to treat severity as an continuous real number, the detector uses the mean squared error as loss function, rather than the cross entropy which is usually used in classification problems. The networks are trained with Nesterov momentum over 250 epochs, with the learning rate being adjusted according to the number of iterations (0.003 for epoch 0-150, 0.0003 for 150-220, and 0.00003 for 220-250). In order to improve the robustness of the network, \textcolor{red}{some data augmentation methods, such as dynamic data re-sampling, random stretching, rotation, flipping, and color augmentation, are applied at training phase}. Moreover, as the large-scale network is hard to train, \textcolor{red}{one strategy is to train a smaller network first, and then use its weights to initialize the larger network}. For details about the architecture and training process, please refer to the original work~\cite{oO2016detector}.
    %In order to improve the robustness of the network, dynamic data resampling and some data augmentation methods, such as random stretching, rotation, flipping as well as color augmentation, are applied at training phase. [wzj: data re-sampling is one of the data augmentation methods]

	%\textcolor{red}{(the tense of verbs should be consistent)}

    \subsection{Key Bottleneck Features}

   We further identify a few \emph{key features} (colored one in the middle of Fig~\ref{fig:descriptor}) from $1024$ dimension bottleneck features.  After the training stage of network, we are able to generate the bottleneck features for individual sample in the training set. Then we train a random forest~\cite{Dollar15PAMI} classifier to predict the severity label on these bottleneck features. Following~\cite{Gu17MICCAI}, we could identify the important features by counting the frequency of each feature that contributes to prediction. In the current setting, we find that, with random forest, only $6$  of  $1024$ bottleneck  features could deliver equivalent performance as o\_O~\cite{oO2016detector}.


    %\textcolor{blue} {After training, we fixed the weights in the DR detector and trained a random forest classifier with the 1024-dimension \textcolor{red}{feature in \#9 layer as input}, and severity level as output. We find that 6 dimensions (highlighted with colors in \#9 layer, Fig~\ref{fig:descriptor}) of the feature provide most part of the information for classifying.} So the later processing is based on the 6-dimension \emph{key bottleneck feature}.

	% \textcolor{red}{Please give a description on our method disease detection method including the architecture}

	\subsection{Activation Network}

    Among millions of neurons in the network, only thousands of neurons actually contribute to the bottleneck feature's activation and the final prediction. To explore the activity of these neurons, we introduce an activation network~\cite{zeiler2014visualizing}. We perform a back-propagation-liked procedure from the $6$ dimension key bottleneck features to get \emph{activation projections} for detector feature layers.

    As shown in the right part of Fig~\ref{fig:descriptor}, our activation net is a reverse version of the DR detector. For individual layer in detector, there is a corresponding reverse layer with the same configuration of strides and kernel size. For a convolutional layer, the corresponding layer performs transposed convolution, which shares same weights, except that the kernel is flipped vertically and horizontally. For each max pooling layer, there is an unpooling layer that conducts a partially inverse operation, where the max elements are located through a skip connection and non-maximum elements are filled with zeros. For a ReLU function, there is also a ReLU in the activation net, which drops out the negative projection. We treat the fully connection as $1\times 1$ convolution. In the implementation we use auto-differentiation provided in Tensorflow~\cite{abadi2016tensorflow} to reverse each layer. %In a more thorough way, this process is similar to the back propagation of gradients, except that no negative values are mapped back for the ReLU restriction.

	\begin{figure}[t]
		\begin{center}
			\includegraphics[width=\columnwidth]{figures/detected_lesion.pdf}
		\end{center}
		\caption{We input fundus with different lesions into the pipeline in Fig~\ref{fig:descriptor} and extract their related activation projections in layer \#0. Some results are cropped and shown here. }
		\label{fig:detected_lesion}
	\end{figure}

	\begin{figure}[t]
		\begin{center}
			\includegraphics[width=\columnwidth]{figures/feat_proj.pdf}
		\end{center}
		\caption{Feature maps and related activation projections. In each layer, only one channel is shown. }
		\label{fig:feat_proj}
	\end{figure}

	%Intuitively, when features flow over the DR detector and flow back along the activation network (\textcolor{red}{'flow back' might be confusing}), some redundant information is filtered out, and the information related to the key bottleneck feature is kept as a result to the detection.
    Here, we demonstrate some neuron activation examples in Fig~\ref{fig:detected_lesion}\&\ref{fig:feat_proj}. It shows that, though our DR detector is trained with the image-level labels,
    its neuronal activity is sensitive to and threfore can locate a variety of DR lesions such as microaneurysms, soft exudates and hard exudates.

    %, and presents some information for locations, colors, and so on. This visualization method can be used to locate the lesions in a retinal fundus.

	\subsection{Retinal Pathological Descriptor}

    %With the activation network, we could locate the neuron activation that  directly related to pathology prediction.
    Now, we define a \emph{pathological descriptor} to encode lesion feature and activation, which should serve as the evidence when doctors make diagnose. As the neuron activation is spatially correlated with the retinal lesions, we could associate the descriptor with individual lesion.  Fig~\ref{fig:feat_proj} shows how the lesion feature changes across different layers. A descriptor contains patches cropped from these maps.

   % we can obtain the position information of lesions according to the activation projection. Based on this, we define a \emph{pathological descriptor}, which contains the appearance feature of the lesion and can be revealed by the GAN  to be described in the next section.

	When a retinal fundus $ x_s $ is fed into the pipeline in Fig~\ref{fig:descriptor}, the features and activation projections in layer $ l $ are denoted as $ F_l $ and $ A_l $, respectively. In order to indicate position and boundary of an individual lesion, the last layer activation $ A_0 $ is thresholded into a binary mask $ M_0 $. To describe a lesion area, we define a rectangle region $r$ that covers a connected conponent (a lesion) in $ M_0 $. Thus different lesions could be denoted as different $r$. We then use the multi-layer information to construct our retinal pathological descriptor for each lesion. In particular, we first down-sample $M_0$ into each layer $l$ to generate the binary lesion mask $M_l$. With $r$, we cropped feature patch $F_{lr}$ from $F_l$, activation patch $A_{lr}$ from $A_l$ and mask patch $M_{lr}$ from $M_l$. The pathological descriptor for lesion $r$ consists of the information from multiple layers, written as $d = \{d_{l}|l\in \Lambda\}$, where $ d_{l} = \left<M_{lr}, A_{lr}, F_{lr}\right> $.


%     \textcolor{green}{By finding connected components in $ M_0 $, different lesions are taken apart from each other. We use a patch $ r \in R $ to contain one component, obtaining its own local mask $ M_{0r} $, downscaled from which the mask $M_{lr}$ of a layer $ l $ could be generated. Finally, we extract the corresponding patches of feature $ F_{lr} $ and activation $ A_{lr} $ in each concerned layer $ l \in \Lambda $, to get the pathological descriptor $ d_{rl} = \left<M_{lr}, A_{lr}, F_{lr}\right> $. Note that the pathological descriptor for a lesion contains multi-layer information from both features and activations.}


	% \textcolor{red}{Please explain which which parameters are extracted from the detection network and briefly explain its data structure}

	\begin{figure*}[h!]
		\begin{center}
			\includegraphics[width=0.9\textwidth]{figures/pipeline.png}
		\end{center}
		\caption{The architecture and data flow of our symptom transfer GAN, which contains four nets in training phase. After training, the generator itself is able to synthesize retinal fundus with lesions on specific locations.}
		\label{fig:pipeline}
	\end{figure*}

	\section{Visualizing Pathological Descriptor}

    According to Koch's Postulates, though pathogen is invisible (at least for naked eye), its property could be observed on the subject after injecting the purified pathogen. Similarly, we evaluate and visualize the interpretative medical meaning of this descriptor by using a GAN based method to generate fully controllable DR fundus images.  Our goal is to synthesize the diabetic retinopathy fundus images(Fig~\ref{fig:koch}.(d)) that carry the lesions that appear on a pathological reference one (Fig~\ref{fig:koch}.(a)). Since our descriptor is lesion based, we could even arbitrarily manipulate the number and position of symptom. As shown in Fig~\ref{fig:pipeline}, with given descriptors $ \mathcal{D} $, we design a novel conditional GAN to achieve this.

    Our whole network structure consists of four sub-nets: the generator net, the discriminator net, the retina detail net, and the DR detection net. Given the vessel segmentation image and a noise code as input, the generator tries to synthesize a tubular structured phantom. The discriminator net tries to distinguish the synthesized images from the real ones. To further enhance the physiological details during generation, we use the retina detail net to constrain the detail reconstruction. The DR detection net is the \emph{key part} of our proposed architecture, which constrains the synthesized images with the user-specified pathological descriptors in feature level. After training, one can easily obtain synthesized fundus from vessel segmentations using the generator net.

% 	As mentioned above, in order to prove that our extracted descriptors have corresponding interpretative medical prior knowledge, we use the GANs to generate fully controllable diabetic retinopathy fundus images. Our goal is to synthesize diabetic retinopathy fundus images controlled with the same lesion characteristics based on a style reference one. These contents will be explained in detail below.

% 	\subsection{Method Overview}

% 	Fig~\ref{fig:pipeline} displays the whole pipeline of synthesizing the controllable diabetic retinopathy fundus images. With given descriptors $ \mathcal{D} $ and locations $ \mathcal{R} $ where to redistribute lesions, we train a generator to synthesize fundus. The entire training procedure consists of four networks: generator network, discriminator network, trained detector network and trained VGG-19 network~\cite{DBLP:journals/corr/SimonyanZ14a} (\textcolor{red}{may be better to write like 'content network'}). The training losses consists of three parts: adversarial loss, content loss, and mask-based style loss. After training, the generator itself can synthesize fundus from vessel segmentations.


	% overview: for given style reference image, train a generator that synthesize fundus with disease from a vessel segmentation image

	% 4 losses in training: adversarial loss, content loss, mask-based style loss, style smooth loss

	\subsection{Generator and Discriminator}
%	With reference to the success of Tub-GAN,

    We use a U-Net~\cite{ronneberger2015u} network structure for our generator network.% and discriminator network.
	Taking a segmentation image $y \in \{ 0,1 \}^{W \times H}$ with a noise code $z \in \mathbb{R}^Z $ as input, the network outputs a synthesized diabetic retinopathy fundus RGB image $\hat{x} \in \mathbb{R}^{W \times H \times 3}$.
	The entire image synthesis process can be expressed as $G_{\theta}$ : $ ( y,z ) \mapsto \hat{x}$. Similarly, we can also define discriminant function $D_{\gamma}$ : $ ( X , y) \mapsto p \in [0,1] $. When $X$ is the real image $x$, $p$ should tend to 1 and when $X$ is the composite image $\hat{x}$, $p$ should tend to 0.
	We follow the GAN's strategy and solve the  following optimization problem that characterizes the interplay between $ G $ and $ D $:
	\begin{align}
	\label{align_allloss} \textstyle
	\max_{\theta} & \textstyle\min_{\gamma} L(G_{\theta}, D_{\gamma}) = \nonumber \\
	&\mathbb{E}_i [L_\mathrm{adv}(i, \theta, \gamma) + L_\mathrm{retina}(i, \theta)
	  + L_\mathrm{patho} (i, \theta)],
	\end{align}
	where $
	L_\mathrm{adv} = \log D_\gamma(x_i, y_i) +
	 \log(1 - D_\gamma(G_\theta(y_i,z_i),y_i)),
	$ is the adversarial loss, with $ L_\mathrm{retina} $ and $ L_\mathrm{patho} $ being retina detail loss and pathological loss.
	To be more specific, learning the discriminator $ D $ amounts to maximizing $ -L_D = L_\mathrm{adv} $
	and the generator $ G $ is learned by minimizing a loss $ L_G = \tilde L_\mathrm{adv} + L_\mathrm{retina} + L_\mathrm{patho} $ with a simpler adversarial
	\begin{align}
	\label{align_Gloss} \textstyle
	\tilde L_\mathrm{adv} = -\log D_\gamma (G_\theta(y_i, z_i),y_i) .
	\end{align}

	%In an intuitive manner, content loss guarantees that the synthesized fundus images are more realistic, while pathological loss promotes synthesized fundus images with specific pathologies. These two losses will be described in detail below.

	% \textcolor{red}{In this equation, $\lambda$ is the trade-off constant. The last item $Loss$ is introduced to ensure the he synthesized image will not deviate significantly from the real image while having the selected pathological features. }
	% $\hat{x} \in \mathbb{R}^{W \times H \times 3} = G_{\theta} ( y\in \{ 0,1 \}^{W \times H},z \in \mathbb{R}^Z)$.

	% input/output specification

	% definition of adversarial loss

	% L1 content loss

	% \subsection{Pathology Descriptor by DR detector}

	% DR detection network introduction(I/O)

	% generate activation pattern by CNN visualization methods

	\subsection{Retina Detail Loss}
    \label{sec:retina_detail}

    Though a  L1 loss (or MAE) between synthetic image could deliver a satisfactory result for style transfer application on common images, it fails to preserve the physiological details in the fundus. We will elaborate this in the experiment section. Therefore, we define \emph{retina detail loss} as:
	\begin{align}
	\label{align_retina_loss}
	L_\mathrm{retina} = w_\mathrm{dd} L_\mathrm{dd} + w_\mathrm{tv} L_\mathrm{tv},
	\end{align}
	where diverge of details $L_\mathrm{dd}$ is meant to preserve physiological details in the fundus, while total variance loss $L_\mathrm{tv}$ is the global smoothing term.

		We choose to measure diverge of details in VGG-19~\cite{DBLP:journals/corr/SimonyanZ14a} feature space. For specific layer $ \lambda $ and VGG feature extraction function $ F_V^\lambda $,
	\begin{align}
	\label{align_dd_loss}
	L_\mathrm{dd} =  {\lVert F_V^\lambda(x_i) - F_V^\lambda(\hat x_i) \lVert}.
	\end{align}

	In addition, to ensure the overall smoothness, we also regulate image gradients to encourage spatial smoothness $L_\mathrm{tv}$:
	\begin{align}
	\label{align_tv_loss} \textstyle
	\sum_{w,h}{\lVert \hat{x}_i^{(w,h+1)} - \hat{x}_i^{(w,h)} \lVert}
	+ {\lVert \hat{x}_i^{(w+1,h)} - \hat{x}_i^{(w,h)} \lVert}.
	\end{align}

	\subsection{Pathological Loss}

   To constrain the synthesized image to carry pathological features, we enforce it to have the similar detector neuron activation on lesion regions to the reference image. In this way, the synthesized image should be equivalent to the reference from DR detector point of view. We regulate the neuron activation to be close to the ones recorded in pathological descriptors $ \mathcal{D} $ with a  \emph{pathological loss} :
    \begin{align}
	\label{align_patho_loss}
	L_\mathrm{patho} = w_\mathrm{dp} L_\mathrm{dp} + w_\mathrm{mv} L_\mathrm{mv},
	\end{align}
	where the pathological diverge $L_\mathrm{dp}$ is the differences on features summed over lesion regions and layers, while masked variance loss $L_\mathrm{mv}$ represents the local smoothing term.

    As shown in  Fig~\ref{fig:pipeline}, we input the synthesized image $ \hat x_i $ into the pre-trained DR detector.
    To ensure the extracted feature pathes $ \mathcal{F}_{l\rho} $ in fixed pre-specified regions $ \rho(d) $ are close to those $ F_{lr} $ in given descriptors $ d \in \mathcal{D} $ across all layers $ l \in \Lambda $, we define $ L_\mathrm{dp}$ as ($ \otimes $ means elementwise multiply)
    \begin{align}
    \label{align_dp_loss} \textstyle
    L_\mathrm{dp} = \frac{1}{|\mathcal{D}|} \sum_{d\in \mathcal{D}} &\textstyle \frac{1}{|\Lambda|} \sum_{l \in \Lambda}\nonumber \\ \textstyle \frac{w_\mathrm{gram}}{W_\rho H_\rho} \cdot
    || \mathbb{G}(\mathcal{M}_{ld}& \otimes  \mathcal{F}_{l\rho}(\hat x_i))
    - \mathbb{G}(\mathcal{M}_{ld} \otimes F_{lr}(d)) ||,
    \end{align}
	where $ \mathcal{M}_{ld} = M_{lr} \otimes \mathrm{normalize}(|A_{lr}|) $ is computed with $ M_{lr}, A_{lr}$, the elements of $ d_l $, and used as a feature mask. The binary mask $ M_{lr} $ restricts loss in pixel-level, and $ A_{lr} $ stresses the lesion region as a soft mask.
	In this definition, we measure the diverge by the symmetric Gram matrix $ \mathbb{G}_{K\times K} $, which represents the covariance of different channels in feature maps $ F \in \mathbb{R}^{W\times H\times K}$:
	\begin{align}
	\label{align_gram} \textstyle
	\mathbb{G}(F)_{i, j} = \sum_{w,h} F_{whi}F_{whj}.
	\end{align}

	At the same time, our network would integrate the synthetic lesion features into the current background. First of all, a lesion redistribution mask $ M_\mathrm{rd} $, which covers lesion regions in the synthesized image, is computed based on all $ \rho $ and $ M_{0r} $. Then the mask is dilated to $ M_\mathrm{grd} $ by a gauss kernel to softly expand the boundary. With masked synthetic image $ \tilde{x}_i = M_\mathrm{grd} \hat{x}_i $, we define masked variance loss $L_\mathrm{mv}$ as
	\begin{align}
	\label{align_mtv_loss} \textstyle
	\sum_{w,h}{\lVert \tilde{x}_i^{(w,h+1)} - \tilde{x}_i^{(w,h)} \lVert}
	+ {\lVert \tilde{x}_i^{(w+1,h)} - \tilde{x}_i^{(w,h)} \lVert}.
	\end{align}
	% where $ \tilde{x}_i = M_\mathrm{grd} \hat{x}_i $ is the masked synthetic image.


	%mask-based style loss
	%
	%mask-based L1 content loss \& smooth loss





	\subsection{Implementation Details}

	%All implementation details of our method (such as datasets, network architecture, hardware configuration, etc.) can be found in this section.

	%\subsubsection{Dataset and Preparation}
	%In this paper we chose three datasets: DRIVE~\cite{staal:2004-855}, STARE~\cite{hoover2000locating} and Kaggle~\cite{kaggle2016diabetic}.
	%DRIVE contains 20 training images and 20 test images, with each of size 584$\times$565$\times$3. STARE contains 402  images and the size is 700$\times$605$\times$3. The Kaggle dataset contains 53576 training images and 35118 test images, however the image size is not fixed. We train the Generator using the DRIVE train dataset and train the detector with the Kaggle train dataset. Select images from STARE and Kaggle as the style reference images. In the experiment, we used image size of 512 $\times$ 512$\times$3. The detailed cutting method can refer to the work~\cite{zhao2018synthesizing} and the work~\cite{oO2016detector}. For any of the above-mentioned fundus image datasets, the final results are obtained by applying its prescribed circular-shaped mask, so only inside pixels are retained as foreground.


	%\subsubsection{Network Architecture and internal parameters}

	%^We refer to the mature network structure of Tub-GAN~\cite{zhao2018synthesizing} and the detailed structure can be found in that paper.

	% \textcolor{green}{The chosen norm in above equations is L1. There is only one layer, the first convolutional layer, in the layer set $ \Lambda $. When we get activation projections, a normalization is applied on $ A_{lr} $. Weights for different losses are $ w_\mathrm{dd} = 1, w_\mathrm{tv} = 100, w_\mathrm{dp} = 10, w_\mathrm{mv} = 5 w_\mathrm{tv}, w_\mathrm{gram} = 10^6 $. }
	The chosen norm in above equations is L1.  Weights for different losses are $ w_\mathrm{dd} = 1, w_\mathrm{tv} = 100, w_\mathrm{dp} = 10, w_\mathrm{mv} = 5 w_\mathrm{tv}, w_\mathrm{gram} = 10^6 $. Based on experience, we set $ \lambda $ to be the second convolutional layer in the fourth block of VGG.

	The batch size is set to 1. Before each training step, the input image values are scaled to $[-1, 1]$, and a random rotation is performed on the input.
	The training is done using the ADAM optimizer~\cite{kingma2014adam} and the learning rate is set to 0.0002 for the generator and 0.0001 for the discriminator. In order to ensure that generator and discriminator are adapted, we update generator twice then update discriminator once. During training, the noise code is sampled element-wise from zero-mean Gaussian with standard deviation 0.001; At testing run, it is sampled in the same manner but with a different standard deviation of 0.1.
	The training finishes after 20000 mini-batches. In addition, we find the result more robust if the whole model is initialized to a trained one with no pathological loss.




	\section{Experiment Results}

	\subsection{Dataset and preparation}
	In this paper, we select three datasets: DRIVE~\cite{staal:2004-855}, STARE~\cite{hoover2000locating} and Kaggle~\cite{kaggle2016diabetic}.
	DRIVE contains 20 training images and 20 test images, with each of size 584$\times$565$\times$3. STARE contains 40  images and the size is 700$\times$605$\times$3. The Kaggle dataset contains 53576 training images and 35118 test images of various size. The DR detector is trained on Kaggle dataset following ~\cite{oO2016detector}. When training generator, we uses binary image and its corresponding retinal image in the DRIVE dataset. Before processing, we change all of the image into size of 512 $\times$ 512$\times$3 following ~\cite{zhao2018synthesizing}.% For any of the above-mentioned fundus image datasets, the final results are obtained by applying its prescribed circular-shaped mask, so only inside pixels are retained as foreground.

     %Select images from STARE and Kaggle as the reference images.

    \subsection{Visualization of Pathological Descriptor}

    We use images in STARE and Kaggle as pathological references to extract the pathological descriptor. The position of individual lesion is randomly chosen.  We have organized a group of 5 ophthalmologists to evaluate our results.
    After training the generator, we test it on binary vessel images from DRIVE test set. The exemplary result in Fig~\ref{fig:results_show} shows that our pathological descriptors contain appearance and color features of lesions in different types. Microaneurysms in (a) and (b) looks very plausible.  The laser scars in (c) are consistent with the fundus of treated RD patients. However, our generated hard exudates in (d) are of some artifacts.

	\begin{figure}[t]
		\begin{center}
			\includegraphics[width=\columnwidth]{figures/results_show.pdf}
		\end{center}
		\caption{Results of our experiment. We use reference images in row 1 and vessel segmentations in row 2, to generate synthesized retinal fundus, as shown in row 3-4.}
		\label{fig:results_show}
	\end{figure}

    In Fig~\ref{fig:compare_tubsgan}, we compare Tub-sGAN's results with ours. Our method generates images with realistic lesion details where Tub-sGAN fails. For example, in images synthesized by our method (row 2), we can clearly spot the microaneurysms appeared at the far end of vessels. However, the lesions of Tub-sGAN (row 1) could not be classified into any known symptom. In addition, our method could output images with clear vessels, and the optic disc is better with concave appearance.

	\begin{figure}[t]
		\begin{center}
			\includegraphics[width=\columnwidth]{figures/compare_tubsgan.pdf}
		\end{center}
		\caption{Pathological details comparison between Tub-sGAN and ours. }
		\label{fig:compare_tubsgan}
	\end{figure}

	\subsection{Quantitative Comparison}

	To further strengthen our method, we organized a peer review by a board of 5 professional ophthalmologists. The ophthalmologists were asked to double-blindly evaluate the randomly shuffled fundus images synthesized by our method and Fila-sGAN. For each image, they gave three scores (ranged 1-10, higher indicates better): 1. realness of the fundus image, 2. realness of the lesions, 3. severity of the DR. Finally, we collected valid scores on 560 images, of which the average scores are show in Table \ref{tab:quantitative_comparison}. The p value of T-test is 9.80e-10 and 7.95e-5 for fundus and lesion realness respectively that our mean score is higher than Fila-sGAN.

	\begin{table}[h!]
		\begin{center}
			\begin{tabularx}{0.8\columnwidth}{Xccc}
				\hline
				& Score 1 & Score 2 & Score 3 \\ \hline
				Tub-sGAN & 2.91 & 2.53 & 2.53 \\
				Ours & 4.21 & 3.37 & 3.08 \\ \hline
			\end{tabularx}
		\end{center}
		\caption{Average scores from the ophthalmologists.}
		\label{tab:quantitative_comparison}
	\end{table}

    \subsection{Lesion Manipulation}

	As mentioned above, our method is lesion-based, which makes lesion-wise manipulation possible. As shown in Fig~\ref{fig:redistribute}, we trained two generators from the same reference image (row 1, col 1) but distribute the lesion to the upper region (row1, col 2\&4) and lower region(row1, col 3\&5) respectively. On the other hand, we can also control the number of lesions. For example, drop out some of descriptors to generate less lesions, or clone some of descriptors to get more lesions. Row 2 in Fig~\ref{fig:redistribute} shows a series of synthesized pictures with lesion number increasing from zero to 3 times of that in the reference image, and the varying severity of the DR symptom could be observed in the fundus images.



	\begin{figure}[h!]
		\begin{center}
			\includegraphics[width=\columnwidth]{figures/redistribute.pdf}
		\end{center}
		\caption{Results of lesion manipulation.}
		\label{fig:redistribute}
        \end{figure}


  We also evaluate the above synthesized images on the Diabetic Retinopathy Detector~\cite{oO2016detector}.  According to~\cite{aao2002drscale}, number of microaneurysms is an important criteria for the severity diagnose.  Here we manipulate the number of microaneurysms and count its severity prediction. For each lesion number, we synthesize 10 images on each segmentation in DRIVE test set which has 20 testing image. Thus, we count the predicted severity for 200 images over each number of lesions. As reported in Fig~\ref{fig:number_severity}, by manipulating the lesion number, we receive consistency result from DR detector.

  	\begin{figure}[t]
		\begin{center}
			\includegraphics[width=\columnwidth]{figures/number_severity.pdf}
		\end{center}
		\caption{The severity score with increasing lesions number.}
		\label{fig:number_severity}
	\end{figure}

    \subsection{Detail Preservation}
    \label{sec:exp_detail}
	%The adoption of VGG in retina detail loss is a trade off between computational efficiency and medical significance.

    As we discussed above, it is intuitive to regulate the difference between real and synthesized images on L1 loss as most of style transfer application~\cite{DBLP:conf/cvpr/GatysEB16}. However, this kind of metrics such as MAE, MSE, and SSIM only focus on low-level information in the image. In our practice on retinal image, we find the images generated with L1 loss rather than our retina detail loss fail to preserve some important physiological details such as optic disc boundary and choroid. Here we compare the images generated by Tub-sGAN~\cite{zhao2018synthesizing}, method with L1  loss and our current application in Fig~\ref{fig:optic_disc}.   We can see our method with a retina detail loss is appropriate for synthesizing photorealistic fundus images.

	\begin{figure}[h!]
		\begin{center}
			\includegraphics[width=\columnwidth]{figures/optic_disc.pdf}
		\end{center}
		\caption{The bright area of optic disc. Row 1 are two real fundus reference images with a clear boundary around each optic disc. Below are fundus synthesized by different methods, among which ours is best in realism. }
		\label{fig:optic_disc}
	\end{figure}

	\subsection{Ablation study}

    We evaluate the effects of individual components by adjusting their weights.

	%We name each loss function by its affect on the result, which are explained through experiments for ablation study.

    \subsubsection{Retina Detail Loss}

	The retina detail loss serves to preserve physiological detail. When reducing the weight of retina detail loss, the synthesized optic disc blurs, and noises in the image increase, as shown in Fig~\ref{fig:ablation_retina}.

	%Similarly, we use DR detector to predict the severity of different weights. When reducing the weight, average severity prediction does not change a lot, but the variance increases.
	\begin{figure}[t]
		\begin{center}
			\includegraphics[width=\columnwidth]{figures/ablation_retina.pdf}
		\end{center}
		\caption{The generated image with increasing weights of retina detail loss.}
		\label{fig:ablation_retina}
	\end{figure}



    \subsubsection{Pathological Loss}

    The pathological loss controls the synthesized lesions. When reducing the weights of pathological loss, we observe that lesions become weaker and weaker before they disappear. To further confirm this point, we evaluate the severity score of the generated images by DR detector. As Fig~\ref{fig:ablation_patho} shows, the severity increases with the weight of pathological loss. It worth pointing out that the severity score is $0$ when the constraints of pathological loss is absent.

    %This is confirmed by severity score evaluated by DR detector, shown in Fig~\ref{fig:ablation_patho}.

	\begin{figure}[h!]
		\begin{center}
			\includegraphics[width=\columnwidth]{figures/ablation_patho.pdf}
		\end{center}
		\caption{The severity score with increasing weights of pathological loss.}
		\label{fig:ablation_patho}
	\end{figure}



        \subsection{Color Transfer}

        Unlike traditional style transfer,  our method focuses on transferring pathological feature rather than color or brightness. Here, we transfer the appearance by applying Deep Photo Style Transfer~\cite{DBLP:conf/cvpr/LuanPSB17} (DPST) after our synthesized image. As shown in Fig~\ref{fig:results_show_DPT}, compared to the direct synthesized image at row 2, the image after DPST possess a higher color consistency while preserving the  pathological lesions such as microneurysms.



        %our  synthesized image at rows 2 are able to present sharp boundary around optic dis while clearly render the corresponding lesion details.


        %On the contrary, the images synthesized by Tus-sGAN (row 4) neither preserve physiological detail (boundary of optic disk), nor transfer the microneurysms.

%	As shown above, our method can synthesize lesions from pathological descriptors, but doesn't transfer the colors in the original method. We believe that with a color transfer, the synthesized photo can be more realistic.


  %  So we simply apply deep-photo-transfer~\cite{DBLP:conf/cvpr/LuanPSB17} after our synthesized image as a post processing. The results are shown in Fig~\ref{fig:results_show_DPT}. As we can see, the synthesized fundus images of our method (row 2 in Fig~\ref{fig:results_show_DPT}) are able to present sharp boundary around the bright area of optic disc, and the corresponding lesion details are clearly rendered. Note that although the images synthesized by Tus-sGAN (row 4) possess higher color consistency with the reference ones (row 1), they are not good at reconstructing the details (e.g., the shape of bright area of optic disc, \textcolor{red}{small dots around vessel}, and so on). After using a deep-photo-transfer (row 3), our method could generate realistic fundus images both in color style and in pathological details.

   % \textcolor{blue}{TODO: describe \ref{fig:compare_tubsgan}}

	\begin{figure}[h!]
		\begin{center}
			\includegraphics[width=\columnwidth]{figures/results_show_DPT.pdf}
		\end{center}
		\caption{Color transfer results. Our method with a post process achieves a better visual effect. }
		\label{fig:results_show_DPT}
	\end{figure}

	\subsection{Computation Time}

	All the experiments are tested out on a sever with Intel Xeon E5-2643 CPU, 256GB memory and Titan-Xp GPU. Training time on DRIVE and testing time are shown in table~\ref{tab:computation_time}. Compared to Tub-sGAN~\cite{zhao2018synthesizing}, we are faster which benefits from a more streamlined feature extraction network and descriptor-based comparison.

	\begin{table}[h!]
		\begin{center}
		\begin{tabularx}{\columnwidth}{XrrX}
			\hline
			 & Training & Testing & Platform \\ \hline
			Tub-sGAN & 108/109 min & 0.45/0.12 s & Titan-X/Xp \\
			Ours & 90 min & 0.12 s & Titan-Xp \\ \hline
		\end{tabularx}
		\end{center}
		\caption{Computation time of the different methods. The Tub-sGAN time on Titan-X is reported in their paper, and its time on Titan-Xp is mesured by us.}
		\label{tab:computation_time}
	\end{table}

	%\subsubsection{ Evaluation on DR Detector}



	%The DR detector provide information based on real images. Ideally, the synthesized retinal fundus should work with DR detector, and follow the same property with real one. So, we train some generators, and get the severity of synthesized images, to find out whether the intuitive property with medical basis are still kept.

	%We generate some models by manipulate the number of lesions, and use every model to synthesize 10 retinal fundus with different noise code for each segmentation in DRIVE test set. We diagnose the synthetic image by DR detector, and plot the curve of severity and lesion quantity.
    % lesions to see how the synthesized severity changes. reference: 4531left, test: 10 epochs,



	\section{Conclusion}
        To exploit the network interpretability in medical imaging, we proposed a novel strategy to encode the descriptor from the activated neurons that directly related to the prediction. To visually illustrate the extracted pathologic descriptor, we followed the similar methodology of Koch's Postulates that aim to identify the unknown pathogen. In addition, we proposed a GAN based visualization method to visualize the pathological descriptor into a fully controllable pathology retinal image from an unseen binary vessel segmentation. The retinal images we generated have shown medical plausible symptoms  as the reference image. Since pathological descriptor is associated with individual lesion and spatial independent, we could arbitrarily manipulate the position and quantity of the symptom. We verified the generated images with  a group of  licensed ophthalmologists and our result is shown to be both qualitatively and quantitatively superior to state-of-the-art.
        The feedback of doctors shows our strategy  has strengthened their understanding on how deep learning makes prediction. Not limited in interpreting medical imaging, we will extend our strategy to more general interpretability problem.











        %Finally, We organise a group of  licensed ophthalmologists to evaluate and compare our result.

        \subsubsection{Acknowledgement.} This work was supported by National Natural Science Foundation of China (NSFC) under Grant 61602020.




	%\textcolor{blue}{A data-driven approach is proposed to synthesize retinal fundus and neuronal images. Based on the same tubular structured annotation, multiple realistic-looking phantoms could be generated. Moreover, the model is capable
% 		of learning from small training sets of as few as 10-20 examples. Experimental results suggest that it helps to improve image segmentation performance when our synthesized images are used as additional training images. For
% 		future work, we consider the exploration of possible performance gains in other downstream tasks such as image
% 		level recognition or grading of diabetic retinopathy. We
% 		also plan to investigate the combination of different styles
% 		in one training procedure instead of training dedicated
% 		models for individual styles. Finally, its applications in
% 		related medical images such as magnetic resonance angiography and x-ray angiography are also of interest \cite{kaggle2016diabetic}}



\bibliography{mybib}
\bibliographystyle{aaai}
\end{document}
