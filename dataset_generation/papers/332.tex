
%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE!
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[conference]{IEEEtran}
\usepackage{blindtext, graphicx}
\usepackage{float}
\usepackage[section]{placeins}
\usepackage{array}
\usepackage{tabularx, booktabs}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\usepackage{authblk}
\usepackage{cite}
\usepackage{lettrine}
\usepackage{fancyhdr}
\usepackage{caption}
\pagestyle{fancy}
\fancyhf{}

\lhead{Accepted and published in Arabic Script Analysis and Recognition (ASAR) 2017, IEEE Xplore}
% Add the compsoc option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/





% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.



%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\pagestyle{fancy}
\title{Deep Learning based Isolated Arabic Scene Character Recognition}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author[1,3]{Saad Bin Ahmed}
\author[2]{Saeeda Naz}
\author[1]{Muhammad Imran Razzak}
\author[3]{Rubiyah Yousaf}
\affil[1]{King Saud bin Abdulaziz University for Health Sciences, Riyadh, 11481, Saudi Arabia
\authorcr Email: {\{ahmedsa, razzaki\}@ksau-hs.edu.sa}}
\affil[2]{GGPGC No.1, Abbottabad, Higher Education Department, Khyber Pakhtunkhua (KPK), Pakistan
\authorcr Email: { \{saeeda292\}@gmail.com}}
\affil[3]{Malaysia Japan Institute of Information Technology (MJIIT), Universiti Teknologi Malaysia, KualaLumpur, Malaysia
\authorcr Email: {\{rubiyah.kl\}@utm.my}\vspace{1.5ex}}


% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
%
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3},
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle


\begin{abstract}
%\boldmath

The technological advancement and sophistication in cameras and gadgets prompt researchers to have focus on image analysis and text understanding. The deep learning techniques demonstrated well to assess the potential for classifying text from natural scene images as reported in recent years. There are variety of deep learning approaches that prospects the detection and recognition of text, effectively from images. In this work, we presented Arabic scene text recognition using Convolutional Neural Networks (ConvNets) as a deep learning classifier. As the scene text data is slanted and skewed, thus to deal with maximum variations, we employ five orientations with respect to single occurrence of a character.
The training is formulated by keeping filter size $3$ x $3$ and $5$ x $5$ with stride value as $1$ and $2$.
During text classification phase, we trained network with distinct learning rates.
Our approach reported encouraging results on recognition of Arabic characters from segmented Arabic scene images.


\end{abstract}
% IEEEtran.cls defaults to using nonbold math in the Abstract.
% This preserves the distinction between vectors and scalars. However,
% if the journal you are submitting to favors bold math in the abstract,
% then you can use LaTeX's standard command \boldmath at the very start
% of the abstract to achieve this. Many IEEE journals frown on math
% in the abstract anyway.

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Deep Learning, Convolutional, Scene Text
\end{IEEEkeywords}






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
\lettrine[lines=2]{\bf{T}}{he} content based image analysis has obtained popularity in recent years.
The most complex part of content based image analysis is scene text recognition which is categorized as a special problem in the field of  Optical Character Recognition (OCR).
In OCR, the techniques and methods that have applied on cleaned machine rendered and synthetic images produced desired results.
It is considered as a solved problem for most of the scripts.
However, due to infancy of scene text recognition, it is struggling towards accuracy particularly in cursive scripts~\cite{str1, str2, str3, str4, str5}.


The scene image having a text captured from camera has built-in complex noise associated to it.
The detection and recognition of text from scene text images is considered as subtle task because there may be non-text elements in an image which should be detected and removed before applying any classification technique on such intrinsic images.
We can not process scene text data or printed data in a same way.
The techniques and methods that have already been applied on printed and clean scanned data drastically failed on recognition of scene text data.
Because captured images do not have only textual information, instead we need to tackle non-text objects and the issue of text appearance in various colors, formats, and sizes that make it harder to apply automated tools to detect and eliminate such irrelevant data.

The probable applications of scene text recognition is to assist the visually impaired, number plate recognition, intelligent vehicle driving systems, machine language translation and may provide help in machine reading for robotics systems.
The image content depicts the intuitive information, each with a different challenge.
Among various challenges the most prominent is orientation and size of a text in a scene image.

The scene text recognition has been divided into three phases by most of the authors ~\cite{it,td,tr,tm,icdar2011,ah, es}.
These stages included text segmentation or localization, text extraction and text recognition.
In every phase, intense preprocessing is required to accomplish the task~\cite{it}.
In text segmentation or localization, we detect text area in presence of other objects in an image, while extraction means to segment the text carefully so that it may recognized in last stage by OCR technique.
It is obvious that OCR will not directly process the video image because as mentioned before the nature of OCR is more towards to process clean document images taken in standard resolution and in specific settings.
The video images often has color blending, blur, low resolution and complicated background in presence of different objects.
It is hereby assumed that scene text and video text shares same sort of problems and difficulties in the recognition.

Most of scene text recognition techniques have been witnessed on Latin or English text.
The  database plays a vital role in evaluation of state-of-the-art techniques.
Some scene text datasets are available for Latin script~\cite{it,tm}.
The cursive script is not thoroughly investigated by researchers yet.
The availability of benchmark or large size dataset is a fundamental requirement for training and testing the state-of-the-art classifiers in scene text recognition.
Therefore, the acquisition of scene text images, development of scene text based database, and its distribution to the researchers for comparison of different techniques and methods is one main focus of attention.
We have prepared and compiled  Arabic scene text data and consider its subset for evaluation on Convolutional Neural Network (ConvNet).

In this paper, we evaluate the potential of ConvNets on Arabic scene text recognition.
The Arabic scene was segmented from captured images.
The preprocessing was performed for uniform representation of segmented data before passed them to classifier.
We performed experiments on different parameters variations that reveals satisfactory results.

The rest of this paper includes related work as presented in Section II.
The proposed methodology including feature extraction technique and description about learning classifier and dataset is elaborated in Section III whereas in Section IV, we managed to explain about our experimental parameters and their settings.
This section further discuss about learning accuracy and influential parameters.
Section V summarized our work under conclusion.


\section{Related Work}
In scene text recognition, text detection and segmentation pose a great challenge.
Once text have been segmented correctly then there is a need to extract features from segmented text image and pass it to the classifier, this is how machine learning approaches work.
We have compiled few latest work presented, so that we may know about how much work has been done in this field by keeping in view the Arabic or cursive script.
The efficient scene text localization and recognition technique is proposed by~\cite{es}.
They used region based text  detection which refine text hypothesis with the assumption that all characters are spotted through connected component. Their proposed technique executed in real time and have been evaluated on ICDAR 2013 dataset. A complete system for text detection and localization in gray scale images is proposed by~\cite{td}.
The boosting framework integration feature in combination to the computational complexity approach named weak classifier is developed to the make efficient text detector.
They evaluate their proposed scheme on ICDAR 2003 robust reading and text localizing dataset.
Their proposed technique performed well on various font sizes, styles, and types exist in natural scenes. Another approach by [8] proposed text localization using conditional random fields. The preprocessing is performed by conversion of color image into grayscale and then make histogram of oriented gradients as a feature. The connected component analysis was performed after analysis of text and non-text regions by conditional random fields. Their proposed technique gives better results in comparison to ICDAR 2003 competition dataset.

The color based approach for text detection of Farsi text is proposed by~\cite{ah}.
The text images are then detected by fusion of color and edge information. The extracted text are verified by wavelet histogram and histogram of oriented gradient. They reported effective results on their large dataset.
The work on Arabic text extraction from video images is proposed by~\cite{at}.
They used synthetic text images taken from numerous news channels.
They localize and segment the Arabic text encrypted in video. The text and background pixels were determined through thresholding that produced binary image. They also maintained the temporal information of a video image for verification purpose. They reported their experimentation results on their own proposed dataset as robust.
In recognition phase of scene text images, OCR techniques applies for learning of a text and recognition purpose. The evaluation of cursive and non-cursive scripts using Recurrent Neural Network is proposed by~\cite{ec}. The cursive script's experiments were performed on large Arabic script synthetic dataset. They reported encouraging results on both scripts. Another effort to develop a standard handwritten Arabic Nastal'iq script is compiled by~\cite{ucom}.
They gathered handwritten text from 500 individuals which is evaluated by Bidirectional Long Short Term Memory network~\cite{hu}. However, we use Bidirectional Long Short Term  Memory (BLSTM) network as a classifier to learn the detected scene images.

There exist some algorithms using Scale-Invariant Feature Transform (SIFT).~\cite{sift} proposed a very interesting technique for scene text recognition using SIFT vector. They proposed novel approach for Scale based region growing algorithm.  They used SIFT keypoints to manipulate the local text region. The SIFT algorithm known as an efficient technique. By using it in their proposed work the keypoint  extraction time drops down exponentially in comparison to~\cite{at}. They evaluate their technique on two publicly available datasets i.e., MSRG and ICDAR. They reported good results on their proposed algorithm in comparison to~\cite{cvpr} and~\cite{icip} on same datasets.
The multi frame scene text recognition in video images is presented by~\cite{icme}.
They developed a framework on Scene Text Character (STC) recognition for predicting the character and conditional random field was used for word spotting. The STC features were taken from SIFT descriptors and Fisher vector.
They also collected the dataset from natural scene videos and extract text from it. They evaluated their algorithm on their own collected dataset and three bechmark datasets i.e., CHARS74K, ICDAR2003, ICDAR2011 as reported in their manuscript. The results were conducted on single frame and multiframe scene text and conclude that their approach performs much better on multiframe scene text.

All presented state-of-the-art techniques have been evaluated on Latin and Chinese script but Arabic script is not been addressed in more detail by these mentioned techniques. The availability of dataset is essential for the purpose to assess the performance of proposed algorithm.
By keeping this forefront, we presented state-of-the art based approach on Arabic scene text recognition.

\section{Methodology}
In this manuscript we proposed ConvNets for Arabic scene text recognition.
ConvNets is type of deep learning Neural Network that is based on the idea of multilayer perceptrons (MLPs).
It has been successfully applied on recognition of various objects in image.
Unlike Recurrent Neural Networks (RNNs), ConvNets is more focused on single instance learner rather a sequence learner.
The context is not important for ConvNets training.
Nowadays, ConvNets are considered as an important tool in machine learning applications~\cite{str3, str4, str5}.
The Arabic script is complex and cursive in nature.
Various authors have reported work on synthetic and scanned Arabic text but very few works are presented on Arabic scene text recognition till date.
\begin{figure*}[h]
\centering
\includegraphics[scale=0.7]{cnn-2}
\caption{\bf Proposed methodology based on ConvNets}
\label{methodology}
\end{figure*}
In Figure~\ref{methodology}, the input image of arbitrary size is preprocessed with respect to size ($50$ x $50$) and converted it into gray scale.
The image is saved with five various orientations.
With oriented images we are processing five images against one input image.
The convolution is performed and features were extracted from pooling.
The detail about feature extraction is mentioned in the following sub-section.
In the last stage fully connected layers classified the given image and compute the probability by keeping in view the current input image.


\subsection{ConvNets as a Feature Extractor}

Suppose, we have relatively big image in size and we want to extract and learn $70$ features from each image.
The architecture we used is fully connected feed forward network.
In this situation, the computation would be so complex and takes much time to process a single epoch.
Even in backpropagation the computation would be slower.

By keeping in mind the performance measure, In ConvNets, the solution is to limit the connections between hidden units and input units.
By this, hidden unit will connect only a subset of input units.
In particular, each hidden unit will connect to small group of contagiously located pixels in input unit.
The image volume $I_v$ is computed by width $w$, height $h$ and depth $d$.
\begin{equation}
    I_v = w+h+d
\end{equation}

Lets assume, number of filters as $k$,
the spatial extent as $f$, the stride as $s$, and amount of zero padding $p$.
Here the zero padding is relevant to linear output. The non linear output is represented as a negative values which is replaced by zero to get linear layer output.
At each location where filter process and moving as stride dictates, the $w$ and $h$ is computed for each kernel as follows,
where $W_i$ and $H_i$ are width and height of $ith$ kernel.
The number of kernels make the depth $d$.
\begin{equation}
W_i= (w_1-f+2p)/s+1
\end{equation}

\begin{equation}
H_i= (h_1-f+2p)/s+1
\end{equation}

\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{cnn-7}
\caption{\bf Feature extraction using ConvNets}
\label{feature}
\end{figure}

    As shown in Figure~\ref{feature}, the filter was sliding over the whole image.
    At each time when it stops (dictated by stride), it takes a maximum value as a feature from involved pixels and write at (1,1) of output layer.
    When stride value is $1$, it means the filter will move one pixel to the right and will perform the same operation as previously mentioned.
    After performing operation in one row, it will move one down and begin the entire process again until it process whole image.

\subsection{ConvNets as a Learning Classifier}
Although ConvNets is suitable for feature extraction but it can be used as a learning classifier.
In our proposed work we used ConvNets as our classification technique.
We used fully connected $3$ x $3$ and $5$ x $5$ spatial convolution kernels architecture with max pooling strategy as represented in equation,

\begin{equation}
F^{`}(x)= max_k f(x_{sj})
\end{equation}

The max pooling strategy takes maximum value $max_k$ from the filter which is been observed on pixel $x_{sj}$.
The Rectified Linear Unit (ReLU) is used as an activation function which removes the non-linearity of processed data.
The features that have been learned through training is compared with extracted features of testset data.
The difference is computed and accuracy is measured.
The output neurons in the proposed network are represented as activation of each class.
The most active neuron analogously predict the class for given input.
The softmax layer is used to interpret the prediction about activation value for each class.



\section{Results and Discussions}
The details about dataset and performed experiments are mentioned in the following sub-sections.

\subsection{Dataset}
We have extracted Arabic images from EAST (English-Arabic Scene Text) dataset.
The Arabic scene text sample is presented in Figure~\ref{sample}.

\begin{figure}[H]
\centering
\includegraphics[scale=0.3]{cnn-4}
\caption{\bf Sample Arabic scene text image}
\label{sample}
\end{figure}
In Arabic, it is cumbersome to disintegrate the word into individual characters because of different shape variations with respect to character's position and occurrence of two consecutive characters on a same level as presented in Figure~\ref{segmented}, makes a challenge for segmentation techniques to work perfectly on such complex text image.
In such scenario we require explicit segmentation that segments the characters.
We manually segmented characters from a segmented textline or words as shown in Figure~\ref{char}.
Through empirical methods it becomes impossible to correctly segment the characters from words.
The acquired images were taken in presence of different illumination which is impacted by surrounding environment.
\begin{figure}[H]
\centering
\includegraphics[scale=0.6]{cnn-5}
\caption{\bf Segmented Arabic text lines from natural image}
\label{segmented}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.9]{cnn-6}
\caption{\bf Character segmentation of a word}
\label{char}
\end{figure}

In such situation an impediment is been associated with captured images, such impediment may blur the visibility of a text, such images are represented in Figure~\ref{blur}.
\begin{figure}[H]
\centering
\includegraphics[scale=0.9]{cnn-8}
\caption{\bf Captured images with blur and other impediments}
\label{blur}
\end{figure}
For the purpose to recognize text correctly there is a need to correctly segment text image and remove noise so that classifier may correctly classify the features, learn and recognize the text.
We identified $27$ classes in Arabic script.
Every class is represented by $20$ images in trainset as depicted in Figure~\ref{samplei}.
We consider five various orientations of each character.
As summarized in Table~\ref{tab1}, we have identified $100$ characters representation for each class.
In testset, each class is represented by $5$ variant positions.
After having oriented images we identified $20$ samples for each class.

\begin{table}
  \centering
  \begin{tabular}{|P{2.5cm}|P{1.5cm}|}
    \hline

 \textbf{Number of characters}  & $2700$ \\
 \hline
\textbf{Classes}  & $27$ \\
\hline
\textbf{Sample per class with oriented images} & $100$\\
 \hline
 \textbf{Training set} & $2450$\\
 \hline
 \textbf{Test set} & $250$\\
 \hline
\end{tabular}
\newline\newline
\caption{Dataset Statistics}
\label{tab1}
\end{table}

The scene text image is manually segmented into different text lines for example we segmented scene text image into $6$ text lines as represented in Figure~\ref{segmented}.

\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{cnn-3}
\caption{\bf Various representations of character "aain" and "wao" with five orientations}
\label{samplei}
\end{figure}


We performed experiments on various parameters like changing filter size, and a learning rate.
We reported best accuracy when the filter size is $3$ x $3$ and learning rate is $0.005$.


\subsection{Experiments}
Experiments have been performed on limited number of dataset.
There is no publicly available dataset for Arabic scene text recognition.
So, we are preparing comprehensive Arabic scene text dataset, but currently, we performed experiments on subset of our collected data.
We conducted experiments according to the parameters mentioned in Table~\ref{tab2} .
\begin{table}
  \centering
  \begin{tabular}{|P{1.0cm}|P{1.0cm}|P{1.0cm}|P{1.0cm}|}
    \hline
    \textbf{Filter Size}  & \textbf{Stride}   &  \textbf{Learning Rate}  & \textbf{Error Rate (\%)} \\ \hline
    3 x 3 & 1 & 0.005 & 14.57\\ \hline
    3 x 3 & 1 & 0.5 & 20.93\\ \hline
    3 x 3 & 2 & 0.005 & 18.24\\ \hline
    3 x 3 & 2 & 0.5 & 25.59\\ \hline
    5 x 5 & 1 & 0.005 & 19.75\\ \hline
    5 x 5 & 1 & 0.5 & 29.01\\ \hline
    5 x 5 & 2 & 0.005 & 22.20\\\hline
    5 x 5 & 2 & 0.5 & 33.97\\\hline
  \end{tabular}
  \newline\newline
  \caption{Experimental parameters with error rates}\label{tab2}
\end{table}

Training and testing samples have distributed on the underlaying $27$ identified classes.
Every segmented character is rescaled ($50$ x $50$) and oriented into five different angles.
We performed training on $2450$ character images while trained network is evaluated on $250$ images.
The CovNets has been implemented with $2$ convolutional layers followed by $1$ fully connected layer.
Both convolutional layers uses $5$ x $5$ convolutions with stride value $2$.
The error rate was reported on $27.01$\%.
In another setting, we introduce max-pooling after each convolutional layer and add an extra fully connected layer with stride value $1$.
The filter size is $5$ x $5$ whereas, learning rate is empirically experimented.
In this way $19.57$\% error rate is measured.

The best accuracy is reported on $3$ x $3$ filter size instead of $5$ x $5$.
The reason to choose minimum filter size is to capture more details about the character image, as Arabic characters also appears with diacritics.
Moreover, we may have more details in pixels about the image.
As learning rate is empirically selected, $14.57$\% error rate is been delineated on $0.005$ learning rate.
The detail about our performed experiments with observed error rate have summarized in Table~\ref{tab2}

The ConvNets are suitable for instance learning tasks rather than sequence learning. We can not learn context from ConvNets rather may extract detailed features of a given pattern.
The feature's detail scrutinized the given pattern at pixel level by variant filter size.

\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{cnn-9}
\caption{\bf ConvNets performance comparison with $3$ x $3$ and $5$ x $5$ filter sizes by keeping learning rate as $0.5$ and $0.005$}
\label{graph}
\end{figure}

As mentioned before that we have evaluated ConvNets on small subset of Arabic scene text images and received encouraging results.
Although there is not any publicly available Arabic scene text dataset but we have investigated ConvNets on subset of our collected data which is been in a process of collection and preparation for Arabic scene text research tasks.
We extracted few variations of $27$ identified Arabic characters.
The best result were reported when filter size was $3$ x $3$ as can be observed in Figure~\ref{graph}.
It is believed and have been noticed from our performed experiments that if filter size is minimum then it may covers more feature which is suitable for languages represented in cursive scripts.


\subsection{Comparison with various feature extraction approaches}
The drawback of ConvNets is that it guaranteed higher accuracy on large dataset.
Most of reported work on cursive scene text recognition obtained good accuracy on huge data.
As Arabic scene text recognition passing through it's infancy stage, therefore state of the art techniques yet to apply.
But scene text work on other cursive scripts are available.
We have summarized recent work based on feature extraction approach on various cursive scene texts in Table~\ref{tab3}.

\begin{table}
  \centering
  \begin{tabular}{|P{1.0cm}|P{1.0cm}|P{1.0cm}|P{1.0cm}|}
    \hline
    \textbf{Study}  & \textbf{Script}   &  \textbf{Feature extraction approach}  & \textbf{Error Rate} \\ \hline
    Ren et al~\cite{c1} & Chinese & ConvNets-7 & 0.24\\ \hline
    Ren et al~\cite{c1} & Chinese & ConvNets-9 & 0.31\\ \hline
    Ǵomez et al~\cite{c2}&Multilingual&ConvNets and K-mens &   0.029\\ \hline
    Tounsi et al~\cite{c3}& Arabic & SIFT & 0.24\\ \hline
    Zheng et al~\cite{c4} & Chinese, Japanese, Korean & SIFT & 0.059\\ \hline
   \textbf{Proposed}  & \textbf{Arabic} & \textbf{ConvNets} & \textbf{0.15}\\ \hline

  \end{tabular}
  \newline\newline
  \caption{Performance Comparison of cursive scripts scene data with our proposed method}\label{tab3}
\end{table}

As observed from Table~\ref{tab3}, other than our proposed work one more work on Arabic script is proposed in recent years.
They used scale invariant feature extraction technique.
Our experiments represented good result in comparison to ~\cite{c3}.
We assumed here that ConvNets extracts more detailed features through its strong layers mechanism whereas, scale invariant feature considered robust but not handling features through layers by which we may get more precise detail of the image in question.







% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex,
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation Results}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command, the
% \label for the overall figure must come after \caption.
% \hfil must be used as a separator to get equal spacing.
% The subfigure.sty package works much the same way, except \subfigure is
% used instead of \subfloat.
%
%\begin{figure*}[!t]
%\centerline{\subfloat[Case I]\includegraphics[width=2.5in]{subfigcase1}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{subfigcase2}%
%\label{fig_second_case}}}
%\caption{Simulation results}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals use top floats exclusively.
% Note that, LaTeX2e, unlike IEEE journals, places footnotes above bottom
% floats. This can be corrected via the \fnbelowfloat command of the
% stfloats package.



\section{Conclusion}

The ConvNets is suitable to learn patterns of visual images.
The ability to learn without considering the context make it as instance learner.
The potential of investigating the image at pixels level and pool them together on the basis of maximum value make ConvNets a unique deep learning approach.
Such approach is more appropriate in cursive scripts where to extract features is a real challenge.
As the Arabic script has numerous challenges associated like variant shape of characters with respect to positions.
There is no space in two words which make it harder to segment them with automated tools.
Therefore, by keeping in view these limitations in Arabic script, we used explicit segmentation and feature extraction approaches that may guide us to desired accuracy.
We evaluated the ConvNets deep learning approach on intrinsic Arabic script and report invigorating results.
The experimental results indicates that the ConvNets can improve accuracy on large and variant dataset hence to get better performance on captured Arabic scene text pattern.

\section*{Acknowledgment}
The authors would like to thank Ministry of Education Malaysia and Universiti Teknologi Malaysia for funding this research project through a research Grant (4F801).

% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


% use section* for acknowledgement



% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}

%\bibitem{text-L}


\bibitem{it}
Jacqueline Feild, "Improving Text Recognition in Images of Natural Scenes", Doctoral Dissertations 2014, University of Massachusetts Amherst, United States. http://scholarworks.umass.edu/dissertations\_2/37
\bibitem{td}
S. M. Hanif and L. Prevost, "Text Detection and Localization in Complex Scene Images using Constrained AdaBoost Algorithm”, ICDAR, pp: 15, 2009
 \bibitem{tr}
Goekhan Yildirim and Radhakrishna Achanta and  Sabine Suesstrunk, Text Recognition in Natural Images using Multiclass Hough Forests", Proceedings of the International Conference on Computer Vision Theory and Applications, Volume 1, Barcelona, Spain, pp: 737741, February, 2013
\bibitem{tm}
Andrej Ikica, Text detection methods in images of natural scenes”, PhD Thesis, Computer and Information Science, University of Ljubljana, Slovenia  October 15, 2013, url: http://eprints.fri.uni-lj.si/2236/1/1Ikica.pdf
\bibitem{icdar2011}
Shahab Asif, Shafait Faisal, Dengel Andreas, "ICDAR 2011 Robust Reading Competition Challenge 2: Reading Text in Scene Images", International Conference on Document Analysis and Recognition (ICDAR), 2011 , pp.1491-1496, 18-21 Sept. 2011
\bibitem{ah}
Maryam Darab and Mohammad Rahmati, "A Hybrid Approach to Localize Farsi Text in Natural Scene Images",  Procedia Computer Science, Vol: 13, pages 171-184, Elsevier, 2012
\bibitem{es}
Luk Neumann and Ji Matas, "Efficient Scene Text Localization and Recognition with Local Character Refinement", Computer Science - Computer Vision and Pattern Recognition, volume = "abs/1504.03522", April 2015
\bibitem{at}
 M. Ben Halima and H. Karray and A. M. Alimi, "Arabic Text Recognition in Video Sequences", International Journal of Computational Linguistics Research. Computer Vision and Pattern Recognition  August 2013, url: http://arxiv.org/abs/1308.3243
\bibitem{ec}
Saad Bin Ahmed, Saeeda Naz, Muhammad Imran Razzak, Sheikh Faisal Rashid, Zeeshan Afzal, Thomas Breuel, "Evaluation of Cursive and non-cursive scripts using recurrent neural networks." Neural Computing and Applications (NCA), Volume: 27, No. 03, pp: 603-613, April 2015
\bibitem{ucom}
Ahmed SB, Naz S, Swati S, Razzak MI, Khan AA, Umar AI, "UCOM offline dataset: a Urdu handwritten dataset generation". Int Arab Journal of Information Technology Volume 14, No. 02, url:  http://ccis2k.org/iajit/PDF/Vol\%2014,\%20No.\%202/8721.pdf,	2015
\bibitem{hu}
Saad Bin Ahmed, Saeeda Naz, Salahuddin, Muhammad Imran Razzak, "Handwritten Urdu text recognition using 1-D LSTM classifier", In press in Neural Computing and Applications (NCA), 2016
\bibitem{sift}
Morteza Zahedi and Saeideh Eslami, "Farsi/Arabic optical font recognition using {SIFT} features", Elsevier, WCIT, Vol:3, pp 1055--1059, Procedia Computer Science, 2011, url: http://www.sciencedirect.com/science/journal/18770509/3
\bibitem{cvpr}
Epshtein, E. Ofek, and Y. Wexler. "Detecting text in natural scenes with stroke width transform". In Proc. CVPR, IEEE Computer Society, pages 2963–2970, 2010
\bibitem{icip}
H. Chen, S. S. Tsai, G. Schroth, D. M. Chen, R. Grzeszczuk, and B. Girod. "Robust text detection in natural images with edge-enhanced maximally stable extremal regions". In Proc. ICIP pages 2609–2612, 2011
\bibitem{icme}
Xuejian Rong, Chucai Yi, Xiaodong Yang and Yingli Tian, "Scene text recognition in multiple frames based on text tracking",  IEEE Computer Society, pp 1-6, ICME, url: http://dblp.uni-trier.de/db/conf/icmcs/icme2014.html\#RongYYT14, 2014
\bibitem{str1}
Cunzhao Shi and Chunheng Wang and Baihua Xiao and Yang Zhang and Song Gao and Zhong Zhang, "Scene Text Recognition Using Part-Based Tree-Structured Character Detection", CVPR 2013, IEEE Computer Society, ISBN "978-0-7695-4989-7, pp 2961-2968
\bibitem{str2}
Cunzhao Shi and Chunheng Wang and Baihua Xiao and Song Gao and Jinlong Hu,"Scene Text Recognition Using Structure-Guided	Character Detection and Linguistic Knowledge", IEEE Trans.no.7, Vol 24, pp 1235-1250, 2014
\bibitem{str3}
Guo Qiang and Tu Dan and Li Guohui and Lei Jun, "Memory Matters: Convolutional Recurrent Neural Network for Scene Text Recognition", "Computer Science - Computer Vision and Pattern Recognition (CVPR), 2016
\bibitem{str4}
Ruobing Wu and Baoyuan Wang and Wenping Wang and Yizhou Yu, "Harvesting Discriminative Meta Objects with Deep {CNN} Features for Scene Classification", Computer Science - Computer Vision and Pattern Recognition- 2015
\bibitem{str5}
Xiaohang Ren and Kai Chen and Jun Sun, "A Novel Scene Text Detection Algorithm Based On Convolutional Neural Network", Computer Science - Computer Vision and Pattern Recognition, IWPR, 2016
\bibitem{c1}
Xiaohang Ren and Kai Chen and Jun Sun, "A {CNN} Based Scene Chinese Text Recognition Algorithm With Synthetic Data Engine", DAS 2016, url: http://arxiv.org/abs/1604.01891
\bibitem{c2}
L. G. i Bigorda, D. Karatzas, "A fine-grained approach to scene text script identification", CoRR 2016 Volume: abs/1602.07475, url: http://arxiv.org/abs/1602.07475
\bibitem{c3}
M. Tounsi, I. Moalla, A. M. Alimi, F. Lebourgeois, "Arabic characters recognition in natural sciences using sparse coding for feature representations", in: ICDAR, IEEE, 2015, pp. 1036–1040.
\bibitem{c4}
Qi Zheng, Kai Chen, Yi Zhou, Congcong Gu, Haibing Guan, "Text localization and recognition in complex scenes using local features", ACCV (3), Vol. 6494 of Lecture Notes in Computer Science, Springer, 2010, pp. 121-132, url: http://dx.doi.org/10.1007/978-3-642-19318-7
\end{thebibliography}

% biography section
%
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{biography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{picture}}]{John Doe}
\blindtext
\end{IEEEbiography}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}




% that's all folks
\end{document}
