\documentclass[9pt,technote,compsoc]{./sty/IEEEtran}

%--add--
\bibliographystyle{IEEEtran}

% *** MISC UTILITY PACKAGES ***
%
\usepackage{ifpdf}


% *** CITATION PACKAGES ***
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi

% *** GRAPHICS RELATED PACKAGES ***
%
\usepackage{color}
%\usepackage[dvipdfmx]{graphicx}

% *** MATH PACKAGES ***
%
\usepackage{amsmath}
\interdisplaylinepenalty=2500

% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}

% *** ALIGNMENT PACKAGES ***
\usepackage{array}

% *** SUBFIGURE PACKAGES ***
\ifCLASSOPTIONcompsoc
 \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
\else
  \usepackage[caption=false,font=footnotesize]{subfig}
\fi


% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}

% add
\usepackage{slashbox}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{comment}
\usepackage{amsopn}
\usepackage[export]{adjustbox} 

\graphicspath{{fig/}}
\newcommand{\Tref}[1]{Table~\ref{#1}}
\newcommand{\Eref}[1]{Eq.~\ref{#1}}
\newcommand{\Fref}[1]{Fig.~\ref{#1}}
\newcommand{\Aref}[1]{Alg.~\ref{#1}}
\newcommand{\Sref}[1]{Section \ref{#1}}
\newcommand{\argmax}{\mathop{\rm arg~max}\limits}
\newcommand{\argmin}{\mathop{\rm arg~min}\limits}
\newcommand{\etal}{et al.~}
\hyphenation{Image-Net}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}

\title{Significance of Softmax-based Features in Comparison to Distance Metric Learning-based Features}

%\author{Shota~Horiguchi,~\IEEEmembership{Member,~IEEE},
%Daiki~Ikami,~\IEEEmembership{Student member,~IEEE} and~Kiyoharu~Aizawa,~\IEEEmembership{Fellow,~IEEE}% <-this % stops a space
\author{Shota~Horiguchi,~Daiki~Ikami, ~Kiyoharu~Aizawa
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem S.Horiguchi was and D.Ikami, K.Aizawa are with the Department
of Information and Communication Engineering, The University of Tokyo, Bunkyo, Tokyo, Japan
133-8656.\protect\\
% note need leading \protect in front of \\ to get a newline within \thanks as
% \\ is fragile and will error, could use \hfil\break instead.
E-mail: see http://www.hal.t.u-tokyo.ac.jp/}
%\IEEEcompsocthanksitem J. Doe and J. Doe are with Anonymous University.}% <-this % stops an unwanted space
%\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}
}

% note the % following the last \IEEEmembership and also \thanks -
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
%
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!

% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, October~2017}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Computer Society Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
%
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.


\IEEEtitleabstractindextext{%
\begin{abstract}

End-to-end distance metric learning (DML) has been applied to obtain features useful in many CV tasks. However, these DML studies have not provided equitable comparisons between features extracted from DML-based networks and softmax-based networks. In this paper, we present objective comparisons between these two approaches under the same network architecture.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
deep learning, distance metric learning, classification, retrieval
\end{IEEEkeywords}}



% make the title area
\maketitle


\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

\vspace{1cm}

\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}

Recent developments in deep convolutional neural networks have made it possible to classify many classes of images with high accuracy.
It has also been shown that such classification networks work well as feature extractors.
Features extracted from classification networks show excellent performance in image classification \cite{donahue2014decaf}, detection, and retrieval \cite{razavian2014cnn}\cite{liu2015deepindex}, even when they have been trained to classify 1000 classes of the ImageNet dataset \cite{russakovsky2015imagenet}.
It has also been shown that fine-tuning for target domains further improves the features' performance \cite{wan2014deep}\cite{babenko2014neural}.



%\begin{figure*}[t!]
\begin{figure*}[h!]
	\centering
	\hfill
	\subfloat[Siamese ($dim=2$)]{
		\includegraphics[height=0.22\linewidth, bb=0 0 450 450]{fig/fig1/siamese_2.jpg}
		\label{fig1:siamese}
	}
	\hfill
	\subfloat[Softmax ($dim=2$)]{
		\includegraphics[height=0.22\linewidth, bb=0 0 450 450]{./fig/fig1/softmax_2.jpg}
		\label{fig1:softmax2}
	}\hfill
	\subfloat[Softmax ($dim=3$) + L2 normalization]{
%		\includegraphics[height=0.22\linewidth, bb=0 0 2500 1250]{./fig/fig1/softmax_3.jpg}
		\includegraphics[height=0.22\linewidth, bb=0 0 450 225]{./fig/fig1/softmax_3-2.jpg}
		\label{fig1:softmax3}
	}
	\hfill
	\subfloat{
		\raisebox{2.5mm}{\includegraphics[height=0.18\linewidth, bb=0 0 83 496]{./fig/fig1/legend.jpg}}
		\label{c}
	}
	\caption{Depiction of MNIST dataset. (a) Two-dimensional features obtained by siamese network. (b) Two-dimensional features extracted from softmax-based classifier; these features are well separated by angle but not by Euclidean norm. (c) Three-dimensional features extracted from softmax-based classifier; we normalized these to have unit L2 norm and depict them in an azimuth--elevation coordinate system. The three-dimensional features are well separated by their classes.}
	\label{fig:fig1}
\end{figure*}


On the other hand, distance metric learning (DML) approaches have recently attracted considerable attention.
These obtain a feature space in which distance corresponds to class similarity; it is not a byproduct of the classification network.
End-to-end distance metric learning is a typical approach to constructing a feature extractor using convolutional neural networks and has been the focus of numerous studies \cite{bell2015productnet,schroff2015facenet,song2016deep,sohn2016improved,song2017learnable}.

However, there have been no experiments comparing softmax-based features with DML-based features under the same network architecture or with adequate fine-tuning.
An analysis providing a true comparison of DML features and softmax-based features is long overdue.

Fig. \ref{fig:fig1} depicts the feature vectors extracted from a softmax-based classification network and a metric learning-based network.
We used LeNet architecture for both networks, and trained on the MNIST dataset \cite{lecun1998gradient}.
For DML, we used the contrastive loss function \cite{hadsell2006dimensionality} to map images in two-dimensional space.
For softmax-based classification, we added a two- or three-dimensional fully connected layer before the output layer for visualization.
DML succeeds in learning feature embedding (Fig. \ref{fig1:siamese}).
Softmax-based classification networks can also achieve a result very similar to that obtained by DML--- Images are located near one another if they belong to the same class and far apart otherwise (Fig. \ref{fig1:softmax2}, Fig. \ref{fig1:softmax3}).


Our contributions in this paper are as follows:
\begin{itemize}
	\item We show methods to exploit the ability of deep features extracted from softmax-based networks, such as normalization and proper dimensionality reduction. They are technically not novel, but they must be used for fair comparison between the image representations.
	\item We demonstrate that deep features extracted from softmax-based classification networks show competitive, or better results on clustering and retrieval tasks comparing to those from state-of-the-art DML-based networks \cite{song2016deep,sohn2016improved,song2017learnable} in the Caltech UCSD Birds 200-2011 dataset and the Stanford Cars 196 dataset.
	\item We show how the clustering and retrieval performances of softmax-based features and DML features change according to the size of the dataset. DML features show competitive or better
performance in the stanford Online Product dataset which consists of very small number of samples
per class.
\end{itemize}

In order to align the condition of the network architecture, we restrict the network architecture to GoogLeNet \cite{szegedy2015going} which has been used in state-of-the-art of DML studies \cite{song2016deep,sohn2016improved,song2017learnable}.



%----------------------------------------------------------------
\section{Background}
\subsection{Previous Work}
\subsubsection{Softmax-Based Classification and Repurposing of the Classifier as a Feature Extractor}
Convolutional neural networks have demonstrated great potential for highly accurate image recognition \cite{krizhevsky2012imagenet}\cite{simonyan2015very}\cite{szegedy2015going}\cite{he2016deep}.
It has been shown that features extracted from classification networks can be repurposed as a good feature representation for novel tasks \cite{donahue2014decaf}\cite{razavian2014cnn}\cite{qian2015fine} even if the network was trained on ImageNet \cite{russakovsky2015imagenet}.
For obtaining better feature representations, fine-tuning is also effective \cite{babenko2014neural}.

\subsubsection{Deep Distance Metric Learning}
Distance metric learning (DML), which learns a distance metric, has been widely studied
\cite{bromley1994signature}\cite{chopra2005learning}\cite{chechik2010large}\cite{qian2015fine}.
Recent studies have focused on end-to-end deep distance metric learning \cite{bell2015productnet}\cite{schroff2015facenet}\cite{song2016deep}\cite{sohn2016improved}\cite{song2017learnable}.
However, in most studies comparisons of end-to-end DML with features extracted from classification networks have not been performed using architectures and conditions suited to enable a true comparison of performance.

Bell and Bala\cite{bell2015productnet} compared classification
networks and siamese networks, but they used coarse class labels
for classification networks and fine labels for siamese networks;
thus, it was left unclear whether siamese networks are
better for feature-embedding learning than classification networks.
Schroff et al.\cite{schroff2015facenet} used triplet loss for deep metric
learning in their FaceNet, which showed performance that
was state-of-the-art at the time, but their network was deeper
than that of the previous method (Taigman et al.\cite{taigman2014deepface}); thus,
triplet loss might not have been the only reason for the performance
improvement, and the contribution from adopting triplet
loss remains uncertain. Song et al.\cite{song2016deep} used lifted structured
feature embedding; however, they only compared their
method with a softmax-based classification network pretrained
on ImageNet (Russakovsky et al.,\cite{russakovsky2015imagenet}) and
did not compare it with a fine-tuned network.  Sohn\cite{sohn2016improved},
and Song et al.\cite{song2017learnable}
also compared their methods to lifted
structured feature embedding, thus the comparisons with softmax-based
features have not been shown.

%-------------------------------------------------------------------------
\subsection{Differences Between Softmax-based Classification and Metric Learning}
\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth, bb=0 0 1200 460]{fig/fig2/fig2.jpg}
	\caption{Illustration of learning processes for softmax-based classification network and siamese-based DML network. For softmax, the gradient is defined by the distance between a sample and a fixed one-hot vector; for siamese by the distance between samples.}
	\label{fig:softmaxAndDML}
\end{figure}



For classification, the softmax function (\Eref{eq:softmax}) is typically used:
\begin{equation}
\label{eq:softmax}
p_c=\frac{\exp(u_c)}{\sum_{i=1}^{C} \exp(u_i)},
\end{equation}
where $p_c$ denotes the probability that the vector $\mathbf{u}$ belongs to the class $c$.
The loss of the softmax function is defined by the cross-entropy
\begin{equation}
E=-\sum_{c=1}^{C} q_c\log p_c,
\end{equation}
where $\mathbf{q}$ is a one-hot encoding of the correct class of $\mathbf{u}$.
To minimize the cross-entropy loss, networks are trained to make the output vector $\mathbf{u}$ close to its corresponding one-hot vector.
It is important to note that the target vectors (the correct outputs of the network) are fixed during the entire training (\Fref{fig:softmaxAndDML}).

On the other hand, DML methods use distance between samples.
They do not use the values of the labels; rather, they ascertain whether the labels are the same between target samples.
For example, contrastive loss \cite{hadsell2006dimensionality} considers the distance between a pair of samples.
Recent studies \cite{schroff2015facenet}\cite{song2016deep}\cite{song2017learnable}\cite{sohn2016improved} use pairwise distances between three or more images at the same time for fast convergence and efficient calculation.
However, these methods have some drawbacks.
For DML, in contrast to optimization of the softmax cross-entropy loss, the optimization targets are not always consistent during training even if all possible distances within the mini-batch are considered.
Thus, the DML optimization converges slowly and is not stable.



\begin{figure}[t]
	\centering
	\subfloat[GoogLeNet (dimensionality is reduced to n by PCA)]{
%		\includegraphics[width=0.86\linewidth, bb=0 0 2933 617]{fig/dimension/googlenet.jpg}
		\includegraphics[width=0.9\linewidth, bb=0 0 2933 617]{fig/dimension/googlenet.jpg}
		\label{fig:googlenet}
	}
	\\
	\subfloat[GoogLeNet with dimensionality reduction by a fully connected layer just before the output layer (FCR1)]{
%		\includegraphics[width=0.95\linewidth, bb=0 0 3400 617]{./fig/dimension/googlenet_fc1.jpg}
		\includegraphics[width=\linewidth, bb=0 0 3400 617]{./fig/dimension/googlenet_fc1.jpg}
		\label{fig:googlenet_fc1}
	}
	\\
	\subfloat[GoogLeNet with dimensionality reduction by a fully connected layer followed by a dropout layer (FCR2)]{
%		\includegraphics[width=0.95\linewidth, bb=0 0 3400 617]{./fig/dimension/googlenet_fc2.jpg}
		\includegraphics[width=\linewidth, bb=0 0 3400 617]{./fig/dimension/googlenet_fc1.jpg}
		\label{fig:googlenet_fc2}
	}
	\caption{	GoogLeNet \cite{szegedy2015going} architecture we use in this paper. We extracted the features of the red-colored layers. For (a), we applied PCA to reduce the number of feature dimensions. For (b) and (c), the dimensionality is reduced by the fc\_reduction layer.}
\end{figure}

%\begin{table}[tbp]
\begin{table}[h]
	\centering
	\caption{Properties of datasets used in our experiments. Each cell shows the number of images (upper figure) and the number of classes (lower figure).}
	\label{tbl:clustering_dataset}
	\begin{tabular}{cccc}\toprule
		Dataset&Train&Test&Total\\\midrule
		\multirow{2}{*}{CUB \cite{wah2011caltech}}&5,864&5,924&11,788\\
		&100&100&200\\\midrule
		\multirow{2}{*}{CAR \cite{krause20133d}}&8,054&8,131&16,185\\
		&98&98&196\\\midrule
		\multirow{2}{*}{OP \cite{song2016deep}}&59,551&60,502&120,053\\
		&11,318&11,316&22,634\\\bottomrule
	\end{tabular}
\end{table}


\begin{table*}
	\centering
	\caption{CUB: NMI (clustering) and Recall@K (retrieval) scores for the test set of the Caltech UCSD Birds 200-2011 (CUB) dataset.}
	\label{tbl:CUB}
	\begin{tabular}{lcccccc}\toprule
		&&(clustering)&\multicolumn{4}{c}{Recall@K (retrieval)}\\\cmidrule(lr){4-7}
		&dim&NMI&K=1&K=2&K=4&K=8\\\midrule
		Lifted struct \cite{song2016deep}&64&56.5&43.6&56.6&68.6&79.6\\
                                                          &64&(56.0)&(42.7)&(55.0)&(67.2)&(78.1)\\
		N-pair loss \cite{sohn2016improved}&64&57.2&45.4&58.4&69.5&79.5\\
		Clustering loss \cite{song2017learnable}&64&59.2&48.2&61.4&71.8&81.9\\
		PCA + L2&64&\textbf{60.8}&\textbf{51.1}&\textbf{64.0}&\textbf{75.3}&\textbf{84.0}\\
		FCR1 + L2&64&59.1&49.0&61.1&72.7&82.3\\
		FCR2 + L2&64&57.4&48.0&60.3&72.2&81.6\\\bottomrule
	\end{tabular}
\end{table*}
\begin{table*}
	\centering
	\caption{CAR: NMI (clustering) and Recall@K (retrieval) scores for the test set of the Stanford Cars 196 (CAR) dataset.}
	\label{tbl:CAR}
	\begin{tabular}{lcccccc}\toprule
		&&(clustering)&\multicolumn{4}{c}{Recall@K (retrieval)}\\\cmidrule(lr){4-7}
		&dim&NMI&K=1&K=2&K=4&K=8\\\midrule
		Lifted struct \cite{song2016deep}&64&56.9&53.0&65.7&76.0&84.0\\
		                                             &64&(57.1)&(50.5)&(63.6)&(74.9)&(83.6)\\
		N-pair loss \cite{sohn2016improved}&64&57.8&53.9&66.8&77.8&86.4\\
		Clustering loss \cite{song2017learnable}&64&59.0&58.1&70.6&80.3&87.8\\
		PCA + L2&64&58.3&\textbf{69.4}&\textbf{80.0}&\textbf{87.2}&\textbf{92.4}\\
		FCR1 + L2&64&58.7&66.7&77.7&85.2&90.8\\
		FCR2 + L2&64&\textbf{60.4}&67.9&78.4&86.1&91.3\\\bottomrule
	\end{tabular}
\end{table*}
\begin{table*}
	\centering
	\caption{OP: NMI (clustering) and Recall@K (retrieval) scores for the test set of the Online Product (OP) dataset.}
	\label{tbl:OP}
	\begin{tabular}{lccccc}\toprule
		&&(clustering)&\multicolumn{3}{c}{Recall@K (retrieval)}\\\cmidrule(lr){4-6}
		&dim&NMI&K=1&K=10&K=100\\\midrule
		Lifted struct \cite{song2016deep}&64&88.7&62.5&80.8&91.9\\
		                                             &64&(87.7)&(61.0)&(79.9)&(91.5)\\
		N-pair loss \cite{sohn2016improved}&64&89.4&66.4&83.2&93.0\\
		Clustering loss \cite{song2017learnable}&64&\textbf{89.5}&\textbf{67.0}&\textbf{83.7}&\textbf{93.2}\\
		PCA + L2&64&87.5&62.4&78.9&89.7\\
		FCR1 + L2&64&87.7&61.3&78.6&90.1\\
		FCR2 + L2&64&87.9&62.5&79.8&90.8\\\bottomrule
	\end{tabular}
\end{table*}

%-------------------------------------------------------------------------
\section{Methods}

\subsection{Dimensionality Reduction Layer}
\label{sec:dimensionaliry_reduction_layer}

One of DML's strength in using fine-tuning is the flexibility of its output dimensionality by a final fully connected layer.
When using features of a mid-layer of a softmax classification network, on the other hand, the dimensionality of the features is fixed.
Some existing methods \cite{babenko2014neural} use PCA or discriminative dimensionality reduction to reduce the number of feature dimensions.
In our experiment, we evaluated three methods for changing the feature dimensionality.
Following conventional PCA approaches, we extracted features from a 1024-dimensional pool5 layer of GoogLeNet \cite{szegedy2015going} (\Fref{fig:googlenet}) and applied PCA to reduce the dimensionality.
In a contrasting approach, we made use of a fully connected layer---we added a fully connected layer having the required
number of neurons just before the output layer (FCR1, \Fref{fig:googlenet_fc1}).
We also investigated a third approach in which a fully connected layer is added followed by a dropout layer (FCR2, \Fref{fig:googlenet_fc2}).


\subsection{Normalization}
In this study, all the features extracted from the classification networks are from the last layer before the last output layer.
The outputs are normalized by the softmax function and then evaluated by the cross-entropy loss function in the networks.
Assume that the output vector is $\mathbf{p}=\left(p_i\right)$ where $\sum_{i}p_i=1$.
For arbitrary positive constant $\alpha$, $\mathbf{y}=\left(\log{\alpha p_i}\right)$ returns the same vector $\mathbf{p}$ after the softmax function is applied.
The features $\mathbf{x}$ we extract from the networks are given as $\mathbf{x}=W^{-1}\mathbf{y}$, where $W$ denotes the linear projection matrix from the layer before the output layer to the output layer.
The vector $\mathbf{y}$ has an ambiguity in its scale, thus its linear transformed vector $\mathbf{x}$ also has an ambiguity in the scale---therefore $\mathbf{x}$ should be normalized.
As \Fref{fig1:softmax2} clearly indicates, the distance between features extracted from a softmax-based classifier should be evaluated by cosine similarity, not by the Euclidean distance.

Some studies used L2 normalization for deep features extracted from softmax-based classification networks \cite{taigman2014deepface}\cite{babenko2014neural}, whereas many recent studies have used the features without any normalization \cite{krizhevsky2012imagenet}\cite{song2016deep,wei2016dense}.
In this study, we also validate the efficacy of normalizing deep features.


%***************************************************

\begin{figure*}[t!]
	\begin{minipage}{0.90\linewidth}
\hfill
		\begin{minipage}{0.45\hsize}
			\centering
			\includegraphics[width=\linewidth, bb=0 0 600 453]{fig/fig4-cub/differ_embed_NMI.jpg}
		\end{minipage}
		\hfill
		\begin{minipage}{0.45\hsize}
			\centering
			\includegraphics[width=\linewidth, bb=0 0 600 453]{fig/fig4-cub/differ_embed_Recall.jpg}
		\end{minipage}
		\caption{CUB: Comparisons between softmax-based features and lifted structured feature embedding \cite{song2016deep} on NMI (clustering), and Recall@K (retrieval) scores for the test set of the Caltech UCSD Birds 200-2011 (CUB) dataset. The dimension of the feature used in the retrieval experiments is 64.}
		\label{fig:CUB}
	\end{minipage}
	\\
	\begin{minipage}{0.90\linewidth}
\hfill
		\begin{minipage}{0.45\hsize}
			\centering
			\includegraphics[width=\linewidth, bb=0 0 600 453]{fig/fig5-car/differ_embed_NMI.jpg}
		\end{minipage}
		\hfill
		\begin{minipage}{0.45\hsize}
			\centering
			\includegraphics[width=\linewidth, bb=0 0 600 453]{fig/fig5-car/differ_embed_Recall.jpg}
		\end{minipage}
		\caption{CAR: Comparisons between softmax-based features and lifted structured feature embedding \cite{song2016deep} on NMI (clustering), and Recall@K (retrieval) scores for the test set of the Stanford Cars 196 (CAR) dataset. The dimension of the feature used in the retrieval experiments is 64.}
		\label{fig:CAR}
	\end{minipage}
%\end{figure*}

%\begin{figure*}
	\begin{minipage}{0.90\linewidth}
%		\begin{minipage}{0.32\hsize}
             \hfill
		\begin{minipage}{0.45\hsize}
			\centering
			\includegraphics[width=\linewidth, bb=0 0 600 453]{fig/fig6-OP/differ_embed_NMI.jpg}
		\end{minipage}
		\hfill
		\begin{minipage}{0.45\hsize}
			\centering
			\includegraphics[width=\linewidth, bb=0 0 600 453]{fig/fig6-OP/differ_embed_Recall.jpg}
		\end{minipage}
		\caption{OP: Comparisons between softmax-based features and lifted structured feature embedding \cite{song2016deep} on NMI (clustering), and Recall@K (retrieval) scores for the test set of the Online Products (OP) dataset.The dimension of the feature used in the retrieval experiments is 64.}
		\label{fig:OP}
	\end{minipage}
\end{figure*}



%----------------------------------------------------
\section{Experiments}

In this section, we compared the deep features extracted from classification networks to those from state-of-the-art DML-based networks \cite{song2016deep}\cite{sohn2016improved}\cite{song2017learnable}.
The GoogLeNet architecture \cite{szegedy2015going} was used for all the methods---thus, the numbers of parameters are the same between DML-based networks and softmax-based features.
All the networks were fine-tuned from the weights pretrained on ImageNet \cite{russakovsky2015imagenet}.
We used the Caffe \cite{jia2014caffe} framework for the implementation.

\subsection{Comparisons between softmax-based features and DML-based features}
\label{sec:clustering}

Here, we give our evaluation of clustering and retrieval scores for the state-of-the-art DML methods \cite{song2016deep}\cite{sohn2016improved}\cite{song2017learnable} and for the softmax classification networks.
We used the Caltech UCSD Birds 200-2011 (CUB) dataset \cite{wah2011caltech}, the Stanford Cars 196 (CAR) dataset \cite{krause20133d}, and the Stanford Online Products (OP) dataset \cite{song2016deep}.
For CUB and CAR, we used the first half of the dataset classes for training and the rest for testing.
For OP, we used the training--testing class split provided.
The dataset properties are shown in \Tref{tbl:clustering_dataset}.
We emphasize that the class sets used for training and testing were completely different.

For clustering evaluation, we applied k-means clustering 100 times and calculated
NMI (Normalized Mutual Information) \cite{manning2009introduction}; the value for $k$ was set to the number of classes in the test set.
For retrieval evaluation, we calculated Recall@K \cite{jegou2011product}.


In \Tref{tbl:CUB} and \Tref{tbl:CAR}, we show comparisons of the performance of clustering and retrieval using NMI and Recall@K scores, respectively, for CUB and CAR datasets. We compared the three softmax-based features, lifted structure\cite{song2016deep}, N-pair loss \cite{sohn2016improved} and the clustering loss \cite{song2017learnable}.
The results of the DML methods were quoted from the paper \cite{song2017learnable}.
Regarding  the lifted structure\cite{song2016deep}, the results in the parenthesis correspond to the scores
we obtained from running the publicly available code ourselves, which we confirmed were almost the same as those in \cite{song2017learnable}.
As we can see from \Tref{tbl:CUB} and \Tref{tbl:CAR}, softmax-based features outperformed DML features.
The softmax-based features all performed well in the two datasets.

In OP dataset shown in \Tref{tbl:OP}, contrasting to CUB and CAR datasets, DML features outperform softmax-based features. We will make detailed analysis in the subsequent section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}[t!]
	\begin{minipage}{0.90\linewidth}
\hfill
		\begin{minipage}{0.45\hsize}
			\centering
			\includegraphics[width=\linewidth, bb=0 0 600 453]{clustering_cub/differ_train_NMI.jpg}
		\end{minipage}
		\hfill
		\begin{minipage}{0.45\hsize}
			\centering
			\includegraphics[width=\linewidth, bb=0 0 600 453]{clustering_cub/differ_train_Recall.jpg}
		\end{minipage}
		\caption{CUB: NMI (clustering), and Recall@K (retrieval) scores for test set of the Caltech UCSD Birds 200-2011 dataset under different dataset sizes. The feature dimensionality is fixed at 1024.}
		\label{fig:CUB_scale}
	\end{minipage}
	\\
	\begin{minipage}{0.90\linewidth}
\hfill
		\begin{minipage}{0.45\hsize}
			\centering
			\includegraphics[width=\linewidth, bb=0 0 600 453]{clustering_car/differ_train_NMI.jpg}
		\end{minipage}
		\hfill
		\begin{minipage}{0.45\hsize}
			\centering
			\includegraphics[width=\linewidth, bb=0 0 600 453]{clustering_car/differ_train_Recall.jpg}
		\end{minipage}
		\caption{CAR: NMI (clustering), and Recall@K (retrieval) scores for test set of the Stanford Cars 196 dataset under different dataset sizes. The feature dimensionality is fixed at 256.}
		\label{fig:CAR_scale}
	\end{minipage}
\end{figure*}


\subsection{Detailed comparisons between softmax-based features and lifted structure embedding features}

We made detailed comparisons between softmax-based features and lifted structure embedding \cite{song2016deep} when changing dimensionalities and size of data. We conducted these experiments using the code available for lifted structure embedding \cite{song2016deep}.

Firstly, we show how the performance varies when changing the feature dimensionalities. We changed the dimensionalities of softmax-based features via PCA, FCR1 and FCR2, and investigated how the performance of clustering and retrieval varied. We compared them against those of lifted structure embedding of the same dimensionality.

For training, we multiplied the learning rates of the changed layers (output layers for all models and the fully connected layer added for FCR1 and FCR2) by 10.
The batch size was set to 128, and the maximum number of iterations for our training was set to 20,000, which was large enough for the three datasets to converge as mentioned in \cite{song2017learnable}.
These training strategies were exactly the same as those used in \cite{song2016deep}.

We show the results  for CUB and CAR datasets in \Fref{fig:CUB} and in \Fref{fig:CAR}, respectively, under varying dimensionalities. The deep features extracted from the softmax-based classification networks outperformed the lifted structured feature embedding in clustering (NMI) and retrieval (Recall@K).

For clustering performance measured by NMI, all of the softmax models (PCA, FCR1, and FCR2) showed better scores than the lifted structured feature embedding. Regarding normalization, softmax-based features with L2 normalization showed better performance than those without normalization.

%
The NMI scores of PCA, FCR1 and FCR2 monotonically increased as the feature dimensionality increased for the CUB dataset (\Fref{fig:CUB}).
On the other hand, in CAR dataset (\Fref{fig:CAR}), the NMI scores of FCR2 and the lifted structure embeddings decreased from 256 dimensions
and those of PCA and FCR1 were saturated above 256 dimensions.
This experimental result shows that 1024 dimensions is too large to represent the image classes of CAR dataset.
It also implies that the feature dimensionality should be carefully
considered in order to achieve best performance depending on the target data.

For retrieval performance measured by Recall@K metric, the softmax-based features also outperformed features of lifted structured feature embedding.
Regarding L2 normalization, features with normalization showed better score than without L2-normalization.

\Fref{fig:OP} shows the clustering and retrieval performance measured by NMI, and Recall@K, respectively, for the Online Products dataset.
Contrasting to CUB and CAR datasets, the softmax-based features with L2 normalization and the lifted structure embedding showed almost the same performance in the clustering and retrieval.
As shown in \Tref{tbl:clustering_dataset}, the OP dataset is very different from the CUB and CAR datasets in terms of the number of classes and the number of samples per class---the number of classes is 22k and the number of samples is 120k. The number of samples per class in the OP dataset is  5.3 on average, which is far smaller than the CUB and CAR dataset.

\subsection{The effect of the dataset scales}
From the results for these three datasets, we conjecture that the dataset size---that is the number of samples per class---has a considerable influence on softmax-based features.
Hence, we changed the size of datasets by sampling the images of CUB and CAR datasets for each class and ran the experiments again.
We constructed seven datasets of different sizes, containing 5, 10, 20, 40, 60, 80, and 100\% of the whole dataset, respectively.
Among them, 5\% corresponds to approximately 3 and 4 images per class in the CUB and the CAR dataset, respectively.
As shown in \Fref{fig:CUB_scale} and \Fref{fig:CAR_scale}, the differences between the scores for softmax and DML
were small if the size of the training dataset was small.
The gap between softmax and DML became larger as the dataset size increased.
The softmax-based classifier was largely influenced by the size of the dataset.


\section{Conclusion}
Because there was no equitable comparison in previous studies, we conducted comparisons of the softmax-based features and the state-of-the-art DML features using a design that would enable these methods to objectively demonstrate their true performance capabilities.
Our results showed that the features extracted from softmax-based classifiers performed better than those from state-of-the-art DML methods \cite{song2016deep}\cite{sohn2016improved}\cite{song2017learnable} on fine-grained classification, clustering, and retrieval tasks when the size of the training dataset (samples per class) is large.
The results also showed that the size of the dataset largely influenced the performace of softmax-based features.
When the size of the dataset was small, DML showed better or competitive performance.
DML methods have advantages when the number of classes is very large and the softmax-based classifier is no longer applicable.
In DML studies, softmax-based feature have rarely been compared fairly with DML-based feature under the same network architecture or with adequate fine-tuning. This paper revealed that the softmax-based features are still strong baselines. The results suggest that fine-tuned softmax-based features should be taken into account when evaluating the performance of deep features.



\begin{thebibliography}{1}

\bibitem{donahue2014decaf}
J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, T. Darrell,
DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition,
ICML, pp.647--655, 2014

\bibitem{razavian2014cnn}
A. S. Razavian, H. Azizpour, J. Sullivan, S. Carlsson,
CNN Features Off-the-Shelf: An Astounding Baseline for Recognition,
CVPR Workshops, pp.512--519, 2014

\bibitem{liu2015deepindex}
Y. Liu, Y. Guo, S. Wu, Song, M. S. Lew,
DeepIndex for Accurate and Efficient Image Retrieval,
ICMR, pp. 43--50, 2015

\bibitem{russakovsky2015imagenet}
O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,  Z. Huang, A. Karpathy, A. Khosla,
M. Bernstein, A. C. Berg, Fei-Fei Li,
ImageNet Large Scale Visual Recognition Challenge,
IJCV, Vol. 115, No. 3, pp.211-252, 2015

\bibitem{wan2014deep}
J. Wan, D. Wang, S. H. Hoi, P. Wu, J. Zhu, Y. Zhang, J. Li,
Deep Learning for Content-Based Image Retrieval: A Comprehensive Study,
ACM Multimedia, pp.157--166, 2014

\bibitem{babenko2014neural}
A. Babenko, Artem A. Slesarev, A. Chigorin, V. Lempitsky,
Neural Codes for Image Retrieval,
ECCV, pp.584--599, 2014

\bibitem{bell2015productnet}
S. Bell, K. Bala,
Learning Visual Similarity for Product Design with Convolutional Neural Networks,
SIGGRAPH, 2015

\bibitem{schroff2015facenet}
F. Schroff, Florian,  D. Kalenichenko, J. Philbin,
FaceNet: A Unified Embedding for Face Recognition and Clustering,
CVPR, pp.815--823, 2015

\bibitem{song2016deep}
H. O. Song,  Y. Xiang,  S. Jegelka, S. Savarese,
Deep Metric Learning via Lifted Structured Feature Embedding,
CVPR, pp.4004--4012, 2016

\bibitem{sohn2016improved}
K. Sohn, Improved Deep Metric Learning with Multi-class N-pair Loss Objective,
NIPS, pp.1857--1865, 2016

\bibitem{song2017learnable}
Hyun Oh Song, S. Jegelka, V. Rathod, K. Murphy, Deep Metric Learning via Facility Location
CVPR, pp.2206--2214, 2017

\bibitem{lecun1998gradient}
Y. LeCun, L. Bottou and Y. Bengio, P. Haffner,
Gradient-based learning applied to document recognition,
Proceedings of the IEEE, Vol. 86, No.11 pp.2278-2324, 1998

\bibitem{hadsell2006dimensionality}
R. Hadsell, S. Chopra, Y. LeCun,
Dimensionality reduction by learning an invariant mapping,
CVPR, pp.1735--1742, 2006

\bibitem{szegedy2015going}
C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, A. Rabinovich,
Going Deeper with Convolutions, CVPR, pp.1--9, 2015

\bibitem{krizhevsky2012imagenet}
A. Krizhevsky, I. Sutskever, G. E. Hinton
ImageNet Classification with Deep Convolutional Neural Networks,
NIPS, 1097--1105, 2012

\bibitem{simonyan2015very}
K. Simonyan, A. Zisserman,
Very Deep Convolutional Networks for Large-Scale Image Recognition,
ICLR, 2015

\bibitem{he2016deep}
K. He, X. Zhang, S. Ren, J. Sun,
Deep Residual Learning for Image Recognition,
CVPR, pp. 770--778, 2016

\bibitem{qian2015fine}
Q. Qian, R. Jing, S. Zhu, Y. Lin,
Fine-grained visual categorization via multi-stage metric learning,
CVPR, pp.3716--3724, 2015

\bibitem{bromley1994signature}
J. Bromley,  I. Guyon, Y. LeCun, S. Eduard, R. Shah,
Signature Verification using a "Siamese" Time Delay Neural Network,
NIPS, pp. 737--744, 1994

\bibitem{chopra2005learning}
S. Chopra, R. Hadsell, Y. LeCun,
Learning a similarity metric discriminatively, with application to face verification,
CVPR, pp. 539--546, 2005

\bibitem{chechik2010large}
Large scale online learning of image similarity through ranking,
G. Chechik, V. Sharma, U. Shalit, S. Bengio,
JMLR, pp.1109--1135, 2010

\bibitem{taigman2014deepface}
Y. Taigman, M. Yang, M. Ranzato, L. Wolf,
DeepFace: Closing the Gap to Human-Level Performance in Face Verification,
CVPR, pp. 1701-1708, 2014

\bibitem{wah2011caltech}
C. Wah, S. Branson, P. Welinder, P. Perona, S. Belongie,
The Caltech-UCSD Birds-200-2011 Dataset,
California Institute of Technology, CNS-TR-2011-001, 2011

\bibitem{krause20133d}
J. Krause, M. Stark, J. Deng, Fei-Fei Li,
3D Object Representations for Fine-Grained Categorization,
4th International IEEE Workshop on 3D Representation and Recognition,
pp.554-561, 2013

\bibitem{wei2016dense}
L. Wei, Q. Huang, D. Ceylan, E. Vouga, H. Li,
Dense Human Body Correspondences Using Convolutional Networks,
CVPR, pp.1544--1553, 2016

\bibitem{jia2014caffe}
Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, T. Darrell,
Caffe: Convolutional Architecture for Fast Feature Embedding
arXiv preprint arXiv:1408.5093, 2014


\bibitem{manning2009introduction}
C. D. Manning, P. Raghavan, H. Sch\"{u}tze,
Introduction to Information Retrieval,	Cambridge University Press, 2008

\bibitem{jegou2011product}
H. Jegou, M. Douze, C. Schmid,
Product Quantization for Nearest Neighbor Search,
IEEE Trans. PAMI, Vol. 33, No.1 pp.117-128, 2011
\end{thebibliography}

\end{document}
