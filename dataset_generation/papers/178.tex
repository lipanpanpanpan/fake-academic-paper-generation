\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

%\usepackage{nips_2017}


% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath,graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{authblk}
\graphicspath{{./images/}}
\usepackage[colorinlistoftodos]{todonotes}


%\usepackage[pdfborder={0 0 0}]{hyperref}  % hyperlinks
%\usepackage[inline]{enumitem}
%\usepackage{graphicx}
%\usepackage{subfig}
%\usepackage{float}
% this is to add comments in pdf
%\usepackage[colorinlistoftodos]{todonotes}

\title{Brain MRI super-resolution using 3D generative adversarial networks}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.



\author{
  Irina S\'{a}nchez , Ver\'{o}nica Vilaplana \\
  Universitat Polit\`{e}cnica de Catalunya - BarcelonaTech\\
  Department of Signal Theory and Communications\\
  Barcelona, Spain \\
  \texttt{veronica.vilaplana@upc.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
In this work we propose an adversarial learning approach to generate high resolution MRI scans from low resolution images. The architecture, based on the SRGAN model, adopts 3D convolutions to exploit volumetric information. For the discriminator, the adversarial loss uses least squares in order to stabilize the training. For the generator, the loss function is a combination of a least squares adversarial loss and a content term based on mean square error and image gradients in order to improve the quality of the generated images. We explore different solutions for the upsampling phase. We present promising results that improve classical interpolation, showing the potential of the approach for 3D medical imaging super-resolution. Our code is publicly available \footnote{Source code available at https://github.com/imatge-upc/3D-GAN-superresolution}.

\end{abstract}

\section{Introduction}
\label{sec:intro}

In many medical applications, high resolution images are required to facilitate early and accurate diagnosis. However, due to economical, technological or physical limitations, it may not be easy to obtain images at the desired resolution. Super-resolution techniques solve this problem by creating a High Resolution (HR) image from a Low Resolution one (LR).
In the past decade a variety of super-resolution methods have been successfully applied to magnetic resonance imaging (MRI) data to increase the spatial resolution of scans after acquisition has been performed. Approaches can be broadly categorized into reconstruction-based and learning-based methods \cite{Reeth}.
Within the second group, solutions based on deep learning are currently being investigated motivated by the success of deep learning models in many computer vision tasks \cite{Oktay,Pham}. In particular, Generative Adversarial Networks (GANs) \cite{goodf:gan} are a very promising approach for image generation, and have been also used for super-resolution \cite{ledig:srgan}. Recently, different architectures and loss functions that try to improve the quality of the images generated using GANs have been presented \cite{metz:dcgan,arj:wass,mao:lsgan}. However, these approaches have been proposed for 2D data.

In this work we propose an architecture for MRI super-resolution that completely exploits the available volumetric information contained in MRI scans, using 3D convolutions to process the volumes and taking advantage of an adversarial framework, improving the realism of the generated volumes.
The model is based on the SRGAN network \cite{ledig:srgan}. The adversarial loss uses least squares to stabilize the training and the generator loss, in addition to the adversarial term contains a content term based on mean square error and image gradients in order to improve the quality of the generated images. We explore three different methods for the upsampling phase: an upsampling layer which uses nearest neighbors to replicate consecutive pixels followed by a convolutional layer to improve the approximation, sub-pixel convolution layers as proposed in \cite{shi:subpixel} and a modification of this method \cite{aitken:checkboardfree} that alleviates checkbock artifacts produced by sub-pixel convolution layers \cite{odena:deconv}.
%Furthermore, is this adversarial term in the loss function that allows the network to generate realistic images, as opposite to generate an image as the mean of all the possible solutions



\section{Method}
\label{sec:method}

\subsection{Generative Adversarial Networks}
\label{ssec:GANs}

GANs\cite{goodf:gan} are generative models that consist of a generator $G$ and a discriminator $D$ that compete in a two-player minimax game. The target of $G$ is to learn the distribution over data $x$ starting from sampling input variables from a uniform or Gaussian distribution  $p_z(z)$, while the discriminator $D$ is typically a binary classifier that tries to decide whether a sample is from the training data or has been generated by $G$.

The two players learn by means of an adversarial training, where G has to learn how to cheat D, making the images perceptually closer to the input data, while D has to recognize efficiently the real samples from the fake ones.
The process is formulated with the following minimax objective:
\begin{equation}\label{eq:minimax}
	\min_{G} \max_{D} V(D,G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)]+ \mathbb{E}_{z \sim p_{z}(z)}[\log (1-D(G(z))]
\end{equation}

%GANs propose a scenario with two different players, a Discriminant (D) and a Generator (G). G generates images from a uniform or Gaussian distribution G(z), while D tries to recognize if an image is from the input data or if it is generated by G. G will have to learn how to cheat D, making the images perceptually closer to the input data, while D will have to recognize efficiently the real samples from the generated ones.
In the case of image super-resolution, the goal is to generate a high resolution image $I_{SR}$ from a low resolution input image $I_{LR}$. The image $I_{LR}$ is a low-resolution version of a high-resolution image $I_{HR}$, obtained by applying a Gaussian filter and a downsampling operator with downsampling factor $r$. The high-resolution images $I_{HR}$ are only available during training.
The generator $G$ is a convolutional neural network that is trained to generate a high resolution counterpart from a low resolution input image. The discriminator $D$ is another neural network that tries to differentiate the generated $I_{SR}$ from the real $I_{HR}$.

%In the case of image super-resolution, the input of G is the low resolution image I\textsubscript{LR}. G will have to create a realistic high resolution image I\textsubscript{SR} in order to fool D, while D will be trained to differentiate I\textsubscript{SR} from the real image I\textsubscript{HR}. G and D are usually two different neural networks, which makes
Due to the adversarial formulation GANs may be difficult to train; it is necessary to provide a balance between both players, so neither of them can outperform the other.
For that reason, different methods and architectures have been proposed recently to make GANs more stable, and also to increase the quality of the images.
%There have been recent improvements in the GAN methodology to stabilize training and also to increase the quality of the generated images.
Examples of these methods, that we incorporate in our model, are the following:
the use of batch normalization in all layers (except in the G output and D input), one-side label smoothing to prevent extreme extrapolation behavior in the discriminator and reduce its confidence and the use of loss functions that avoid the vanishing gradient problem of the classical approach and helps to stabilize the training.
%loss functions that sustitutes the orginal sigmoid cross-entropy loss by least-squares function,

\begin{figure*}
  \includegraphics[width=\textwidth]{Figures/3D_SRGAN_D_}
  \caption{Architecture of the Discriminator network. For each convolutional layer: kernel size (3x3x3), number of filters, stride (s).}
  \label{fig:D}
\end{figure*}
\begin{figure*}
   \includegraphics[width=\textwidth]{Figures/3D_SRGAN_G_-Comparison}
   \caption{Architecture of the Generator network. For each convolutional layer: kernel size (3x3x3), number of filters, stride (s).}
	\label{fig:G}
\end{figure*}

\subsection{Architecture}
\label{ssec:arch}

Our network architecture (Fig.~\ref{fig:D} and Fig.~\ref{fig:G}) is based on the SRGAN model \cite{ledig:srgan}. The main difference is found in the convolutional filters; in our architecture, to be able to deal with volumetric information we use 3D convolutional layers. Furthermore, we introduce changes in the loss function, both to make the network more stable and to improve the quality of the generated images. These changes will be explained in section \ref{ssec:loss}.

The discriminator network is composed of eight convolutional layers with 3x3x3 kernels. The number of filters increases by a factor of 2 from 32 to 256. Strided convolutions are used to reduce the image resolution each time the number of features is doubled. After the convolutional layers, there are two dense layers and a final sigmoid activation that outputs a probability indicating whether the input image is real or fake.

For the generator, we use six residual blocks composed of a convolution with 32 filters of size 3x3x3, batch normalization, LeakyReLU activation and another convolution with the same parameters and batch normalization. As in the SRGAN network, there are connections between input and output in the residual blocks, and between the input of the first residual block and the output of the last residual block, allowing the network to access low level features and improve the quality of the generated images.
After the residual blocks, there are upsampling blocks that increase the resolution of the input image. Each block up-samples the image by a factor of 2, so blocks are replicated in order to obtain higher upsampling factors. We explore different configurations for the upsampling blocks that are explained in the following section.

\begin{figure*}
   \includegraphics[width=\textwidth]{Figures/Upsamplings}
   \caption{Architecture of the different upsampling methods.}
	\label{fig:Upsamplings}
\end{figure*}
% \begin{figure*}
%    \includegraphics[width=\textwidth]{EDSRGAN}
%    \caption{Architecture of the Enhance Deep Residual Network for Single Image Super-Resolution }
% 	\label{fig:EDSR}
% \end{figure*}

\subsection{Upsampling methods}
\label{ssec:upsampling}
%We explore three different methods for the upsampling blocks in the Generator network. When the downsampling factor is 4, these blocks are replicated to be able to upsample the input image 4 times its size.
Figure ~\ref{fig:Upsamplings} shows the configuration of the different methods used for the upsampling blocks in the generator network. In the following, we describe each low-resolution volume by a real valued tensor of size $H \times W \times D \times C$, and the high-resolution volume by $rH \times rW \times rD \times C$, where $r$ is the downsampling factor, $H$, $W$, $D$ and $C$ are the height, width, depth and number of channels, respectively.

\subsubsection{Resize convolution}
\label{sssec:nn}
The first method upscales the input feature maps using nearest neighbor interpolation, and then employs a standard convolutional layer with both input and output in the higher resolution space.

% A natural approach to solve the checkerboard artifacts in the resulting images when computing the previous sub-pixel convolution, is to perform a previous upsampling by a classical method such as nearest neighbor followed by a convolution layer \cite{odena:deconv}. By doing that, we are removing the effects of random initialization since neighbor pixels are equally distributed. As counterpart, the amount of trainable parameters is reduced, reducing also the modeling power of the network.


\subsubsection{Sub-pixel convolution}
\label{sssec:subpixel}
An efficient way to increase the resolution of a 2D image was proposed in \cite{shi:efficsubpixel}. In their work, the method to perform the upsampling operation consists of a convolutional layer followed by a pixel-shuffler that arranges the result of the convolution with shape $ H \times W \times Cr^{2} $ into a tensor of size $ rH \times rW \times C $. In our case, since we are dealing with 3D volumes, the size of the initial tensor is $ H \times W \times D \times Cr^{3} $ and is finally arranged into a tensor of size $ rH \times rW \times rD \times C $. This method is equivalent to a deconvolution operation with stride 2 as observed in \cite{shi:deconvsubpix}. Moreover, sub-pixel convolutions can be interpreted as a standard convolution in low-resolution space followed by a periodic shuffling operation.

Following this interpretation, we create a set of kernels that, rearranged, will build the HR image. Given a volume of size $ H \times W \times D \times C $, convolved by a set of kernels $(nf, C, k, k, k)$ (where $nf$ is the number of filters of size $k \times k \times k$) after computing the pixel-shuffling operation we have a volume of size $ rH \times rW \times rD \times \frac{nf}{r^{3}} $. Here, note that by rearranging pixels the feature maps (output channels after the convolution) are mixed, creating consecutive pixels.

The advantage of this method over resize convolution is that with the same computational complexity, it has more parameters, improving the modeling power of the network. However, it suffers from checkerboard artifacts since consecutive pixels depend on different feature maps that are independently randomly initialized.



\subsubsection{Convolution nearest neighbor resize}
\label{sssec:free_subpixel}
A modification of the sub-pixel convolution was proposed in \cite{aitken:checkboardfree} in order to remove the  checkerboard artifacts produced by random initialization while maintaining the number of trainable parameters. Resize convolution produces upsampled images free of checkerboard artifacts. For that reason, if the sub-pixel operation is made identical to a nearest neighbor resize after the initialization step, the advantages of the sub-pixel method will be preserved, while eliminating the undesired artifacts.

The method consists on performing first a deconvolution operation with stride 2 (equivalent to pixel-shuffling after convolution) with kernels initialized with size $(nf, \frac{nf}{r^{3}}, k, k, k)$. Then, an upsampling of this initialization is performed to finally compute the deconvolution layer with filters of size $(nf, \frac{nf}{r^{3}}, kr, kr, kr)$.

\subsection{Loss function}
\label{ssec:loss}
A very critical point when designing a GAN is the definition of the loss function.
Many methods have been proposed recently to stabilize the training and improve the quality of the synthesized images.

One of these works is \cite{mao:lsgan}, where the behavior of the sigmoid cross entropy loss function, commonly used in the classic GANs framework is studied; this loss function causes the problem of vanishing gradients for the samples that lie far away from the correct decision boundary. To overcome this problem a loss function based on least-squares is proposed (LSGAN), substituting the cross-entropy loss by a least square function with binary coding (1 for real, 0 for fake).

%When using a loss function based on least square error, the images that are far away from the decision boundary, even being well classified, will be penalized because they are perceptually far from the real images. Additionally, since the least square loss function is flat only at one point, LSGANs perform more stable during training.
Using this formulation, equation \ref{eq:minimax} changes to:
\begin{equation}\label{eq:min_D}
	\min_{D} V_{LSGAN}(D) = \frac{1}{2} \mathbb{E}_{x \sim p_{data}(x)}[D(x)-1^2]+ \frac{1}{2} \mathbb{E}_{z \sim p_{z}(z)}[D(G(z)^2]
\end{equation}
\begin{equation}\label{eq:min_G}
	\min_{G} V_{LSGAN}(G) = \frac{1}{2} \mathbb{E}_{z \sim p_{z}(z)}[(D(G(z)-1)^2]
\end{equation}

\paragraph{Discriminator loss:}
In our super-resolution model, the adversarial loss used for the discriminator is
\begin{equation}\label{eq:cost_DSR}
	l_{D}^{adv} = \frac{1}{2}[D(I\textsubscript{HR})-1]^{2} + \frac{1}{2}[D(G(I\textsubscript{SR})]^{2}
\end{equation}

\paragraph{Generator loss:}
The loss function used for the generator is a combination of an adversarial term and a content term, as proposed in \cite{ledig:srgan} (in our experiments we use $\alpha = 10^{-3}$):
\begin{equation}
l_{G} = \alpha*l_{G}^{adv} + l_{G}^{cont}
\end{equation}

The adversarial loss is based on least squares:
\begin{equation}\label{eq:cost_Adv}
	l_{G}^{adv} = \frac{1}{2}[D(G(I\textsubscript{SR})-1]^{2}
\end{equation}

while the content loss is a combination of two terms $l_{G}^{cont}= l_G^{cont/MSE} + l_G^{cont/gdl}$

The first term is the mean squared error (MSE) between the original high resolution image $I_{HR}$ and the super resolved image $I_{SR}$, calculated as
\begin{equation}\label{eq:loss_MSE}
	l_{G}^{cont/MSE} = \frac{1}{WHD} \sum_{x=1}^{rW} \sum_{y=1}^{rH}\sum_{z=1}^{rD}(I_{x,y,z}^{HR}-I_{x,y,z}^{SR})^2
\end{equation}

The pixel-wise MSE loss help to achieve high PSNR values, but tends to create blurry images. In order to improve the quality of the samples, an image gradient-based loss term is used as proposed in \cite{couprie:video}.
\begin{equation}\label{eq:loss_gdl}
l_{G}^{cont/gdl} = ||\nabla I\textsubscript{HR}_{x}| - |\nabla I\textsubscript{SR}_{x}||^2 + ||\nabla I\textsubscript{HR}_{y}| - |\nabla I\textsubscript{SR}_{y}||^2 + ||\nabla I\textsubscript{HR}_{z}| - |\nabla I\textsubscript{SR}_{z}||^2
\end{equation}
This Gradient Difference Loss (GDL) sharpens the image prediction by penalizing the differences of image gradient predictions.

% A very critical point is the definition of the cost function. In our method we have two terms in the loss function:

% \begin{itemize}
% \item An adversarial loss, that will be the responsible of allowing D to learn to classify images according to its origin (real or generated) and consequently making G better to be able to cheat D.
% \item A content loss, that will improve the quality of the generated images. This term will only be found in the Generator loss: $l_{G} = \alpha*l_{G}^{adv} + l_{G}^{content} $

% \end{itemize}
% \subsubsection{Adversarial Loss}
% \label{sssec:adv_loss}
% Many works have been proposed to improve the stability and quality of the images generated by GANs.
% One of these works is LSGAN \cite{mao:lsgan}, where the behavior of the sigmoid cross entropy loss function, commonly used in GANs frameworks, is studied; this loss function causes the problem of vanishing gradients for the samples that lie far away from the correct decision boundary. When using a loss function based on Least Square Error, the images that are far away from the decision boundary, even being well classified, will be penalized because they are perceptually far from the real images. Additionally, the least square loss function is flat only at one point, while the sigmoid cross entropy loss function saturates when x has large values. This property makes LSGANs more stable to train.
% \begin{itemize}
% \item Discriminative loss:
% \begin{equation}\label{eq:cost_DSR}
% 	l_{D} = \frac{1}{2}[D(I\textsubscript{HR})-1]^{2} + \frac{1}{2}[D(G(I\textsubscript{SR})]^{2}
% \end{equation}
% \item Generative loss:
% \begin{equation}\label{eq:cost_Adv}
% 	l_{G}^{adv} = \frac{1}{2}[D(G(I\textsubscript{SR})-1]^{2}
% \end{equation}
% \end{itemize}

% \subsubsection{Content Loss}
% \label{sssec:cont_loss}
% For the content loss we adopted two terms for its computation:
% \begin{itemize}
% \item Mean Squared Error (MSE) loss:
% \begin{equation}\label{eq:loss_MSE}
% 	l_{G}^{c/MSE} = \frac{1}{WHD} \sum_{x=1}^{rW} \sum_{y=1}^{rH}\sum_{z=1}^{rD}(I_{x,y,z}^{HR}-I_{x,y,z}^{SR})^2
% \end{equation}
% where W, H, D are width, height and depth, respectively, and r = 1/\textit{d}, for a downsampling factor \textit{d}.

% The pixel-wise MSE loss creates blurry images, due to its lack of high-frequency components.
% \item Image gradient difference loss: In order to deal with the blurry predictions obtained by the MSE loss and improve the quality of the samples, an image gradient-based loss term is added to the loss of G, as proposed in  \cite{couprie:video}.
% \begin{equation}\label{eq:loss_gdl}
% \begin{split}
% 	l_{G}^{c/gdl} = & ||\nabla I\textsubscript{HR}_{x}| - |\nabla I\textsubscript{SR}_{x}||^2 + ||\nabla I\textsubscript{HR}_{y}| - |\nabla I\textsubscript{SR}_{y}||^2   \\ &
%    + ||\nabla I\textsubscript{HR}_{z}| - |\nabla I\textsubscript{SR}_{z}||^2
% \end{split}
% \end{equation}

% \item Autoencoder loss: This loss is inspired by the VGG loss used in the SRGAN network. We trained an autoencoder, that creates highly informative features, in order to obtain a compact representation of the input image. Instead of computing a distance in the image space, these feature representations will be used to compute the distance between the real and the generated images.
%\begin{equation}\label{eq:loss_autoenc}
% 	l_{G}^{c/autoenc} = || \phi(I\textsubscript{HR})-\phi(I\textsubscript{SR})||_{2}^2
%\end{equation}
%Where $\phi$ (I) represents the feature representation of the image I.
% \end{itemize}


\section{Experiments and Results}
\label{sec:expresults}

We perform our experiments using a set of normal control T1-weighted images from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (see www.adni-info.org for details). Skull stripping is performed in all volumes and part of the background is removed. Final volumes have dimensions 224x224x152.

Due to memory constraints the training is patch-based; for each volume we extract patches of size 128x128x92, with a step of 112x112x76, so there are 8 patches per volume, with an overlap of 16x16x16. We have a total number of 589 volumes, 470 are used for training while 119 are used for testing. We use batches of two patches, thus for each volume we perform 4 iterations. We use Adam optimization for both the generator and the discriminator, while the learning rate for the generator is set to 1e-5 and for the discriminator 1e-4.

To evaluate the quality of the images synthesized by our model we make two sets of experiments, using the original MRI volumes as ground truth and training the network with downsampled versions of the images by factors 2 and 4, using the three upsampling strategies described in Section \ref{ssec:upsampling}. In the following, Subpixel stands for sub-pixel convolution and Subpixel-NN stands for convolution nearest neighbor resize. We have also compared the performance of our model with a classical cubic spline interpolation.



\begin{table}[h]
\centering
\begin{tabular}{ l@{}c c c c c c c c }
\hline
&\multicolumn{8}{c}{Upsample x2}\\
\hline
&\multicolumn{2}{c}{Cubic Int.} &
 \multicolumn{2}{c}{Resize Conv.} &
 \multicolumn{2}{c}{Subpixel} &
 \multicolumn{2}{c}{Subpixel-NN} \\
\hline
& PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM  \\
\hline
\textbf{Mean } & 38.06 & 0.9848 & 39.11 & \textbf{0.9913} & 39.09 & 0.9898 & \textbf{39.28} & 0.9849\\
\textbf{Std} & 1.2085 & 0.0020 & 1.0608 & 0.0014 & 1.0203 & 0.0016 & 1.0724 & 0.0028\\
\textbf{Min} & 34.65 & 0.9792 & 35.93 & 0.9868 & 36.61 & 0.9855 & 36.65 & 0.9781\\
\textbf{Max} & 41.45 & 0.9897 & 41.88 & 0.9940 & 42.39 & 0.9933 & 42.54 & 0.9907\\
\hline
&\multicolumn{8}{c}{Upsample x4}\\
\hline
&\multicolumn{2}{c}{Cubic Int.} &
 \multicolumn{2}{c}{Resize Conv.} &
 \multicolumn{2}{c}{Subpixel} &
 \multicolumn{2}{c}{Subpixel-NN} \\
\hline
& PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM  \\
\hline
 \textbf{Mean } & 31.76 & 0.9412 & 33.33 & \textbf{0.9688} & 32.86 & 0.9638 & \textbf{33.58} & 0.9582\\
 \textbf{Std} & 0.9948 & 0.0078 & 1.1813 & 0.0070 & 1.2241 & 0.0085 & 1.1456 & 0.0097 \\
 \textbf{Min} & 29.78 & 0.9312 & 30.54 & 0.9531 & 29.96 & 0.9462 & 31.01 & 0.9388\\
 \textbf{Max} & 33.74 & 0.9534 & 36.86 & 0.9816 & 36.51 & 0.9787 & 37.23 & 0.9770\\

\hline
\end{tabular}
\caption{Numerical Results}
\label{tab:results_performance}
\end{table}


% \begin{figure*}
% \begin{tabular}{ccc}
% {\includegraphics[height = 1.2in]{Figures/box_ds2.png}} &
% {\includegraphics[height = 1.2in]{Figures/box_ds4.png}} \\
% \end{tabular}
% \end{figure*}


\begin{figure*}
\begin{tabular}{ccc}
Original image & Cubic spline interpolation \\
{\includegraphics[width = 1.5in,height = 1.5in]{Figures/ds4cub/real.png}} &
{\includegraphics[width = 1.5in,height = 1.5in]{Figures/ds4cub/cubic.png}} \\
Resize convolution & Subpixel & Subpixel-NN \\
{\includegraphics[width = 1.5in,height = 1.5in]{Figures/ds4cub/lsgan-cnn.png}} &
{\includegraphics[width = 1.5in,height = 1.5in]{Figures/ds4cub/lsgan-subpix.png}} &
{\includegraphics[width = 1.5in,height = 1.5in]{Figures/ds4cub/lsgan-subpixnn.png}} \\
\end{tabular}
\centering
\caption{Illustration of SR results using a downsampling factor of 4. The first row shows the original high resolution image and the result of cubic spline interpolation. The next row presents the results of applying our GAN with the three proposed upsampling methods: resize convolution, sub-pixel and subpixel-NN.}
\label{fig:comparison-visual-1}
\centering
\end{figure*}




\begin{figure*}
\begin{tabular}{ccc}
(a)
{\includegraphics[width = 1.2in,height = 1.2in]{Figures/ds4/nogauss_real.png}} &
{\includegraphics[width = 1.2in,height = 1.2in]{Figures/ds4/nogauss_zoom_real.png}} \\
\end{tabular}
\\
\begin{tabular}{ccccc}
(b)
{\includegraphics[width = 1.2in,height = 1.2in]{Figures/ds2/nogauss_rc.png}} &
{\includegraphics[width = 1.2in,height = 1.2in]{Figures/ds2/nogauss_zoom_rc.png}} &
{\includegraphics[width = 1.2in,height = 1.2in]{Figures/ds4/nogauss_RC.png}} &
{\includegraphics[width = 1.2in,height = 1.2in]{Figures/ds4/nogauss_zoom_RC.png}}\\
(c)
{\includegraphics[width = 1.2in,height = 1.2in]{Figures/ds2/nogauss_subpixel.png}} &
{\includegraphics[width = 1.2in,height = 1.2in]{Figures/ds2/nogauss_zoom_subpixel.png}} &
{\includegraphics[width = 1.2in,height = 1.2in]{Figures/ds4/nogauss_subpixel.png}} &
{\includegraphics[width = 1.2in,height = 1.2in]{Figures/ds4/nogauss_zoom_subpixel.png}}\\

(d)
{\includegraphics[width = 1.2in,height = 1.2in]{Figures/ds2/nogauss_subpixelnn.png}} &
{\includegraphics[width = 1.2in,height = 1.2in]{Figures/ds2/nogauss_zoom_subpixelnn.png}} &
{\includegraphics[width = 1.2in,height = 1.2in]{Figures/ds4/nogauss_subpixelnn.png}} &
{\includegraphics[width = 1.2in,height = 1.2in]{Figures/ds4/nogauss_zoom_subpixelnn.png}}\\
\end{tabular}
\caption{Illustration of SR results (zoom-ins of the blue region) using downsampling factors 2 (left) and 4 (right). Original image (a), resize convolution (b), subpixel (c), subpixel-NN (d) upsampling}
\label{fig:comparison-visual}
\end{figure*}

% {\includegraphics[width = 1.3in,height = 1.3in]{Figures/ds2/nogauss_interp.png}} &
% {\includegraphics[width = 1.3in,height = 1.3in]{Figures/ds2/nogauss_zoom_interp.png}}&
% {\includegraphics[width = 1.3in,height = 1.3in]{Figures/ds4/nogauss_interp.png}} &
% {\includegraphics[width = 1.3in,height = 1.3in]{Figures/ds4/nogauss_zoom_interp.png}}\\


%MSE-based techniques are able to maximize the PSNR value, but generate poor results taking into account the human visual perception of images. On the other hand, SSIM is computed taking into account the image degradation and incorporates important perceptual phenomena, leading to a perception-based measure that is able to model better the human visual perception.
The generated volumes are compared to the ground-truth volumes in terms of peak signal-to-noise-ratio (PSNR) and structural similarity index measure (SSIM). Numerical results can be found in Table \ref{tab:results_performance}. We observe that the resize convolution upsampling method outperforms the others in terms of SSIM a metric that is closer to human visual perception than the PSNR, while sub-pixel nearest neighbor convolution is the best in terms of PSNR.

In Figures ~\ref{fig:comparison-visual-1} and \ref{fig:comparison-visual} we present some results of the different methods, for downsampling factors 2 and 4. If we take a closer look at the resulting images, we can see that the resize convolution method produces perceptually better results. While the changes proposed by the sub-pixel nearest neighbor convolution method improve the behavior of the original sub-pixel convolution, this upsampling method still produce some checkboard artifacts in our images.

\section{Conclusions}
\label{sec:foot}

%In this paper we introduce an approach for MRI super-resolution using a generative adversarial network. We use an adversarial term loss in the generator loss to create more realistic samples, and a content loss to reduce the differences between real and generated images.
%We present promising results that improve classical interpolation when the downsampling factor is high, showing the potential of the approach for 3D medical imaging super-resolution.
In this work a method for MRI super-resolution has been implemented within the generative adversarial framework.
We use an adversarial term loss in the generator loss to create more realistic samples, and a content loss to reduce the differences between real and generated images.
We present promising results that improve classical interpolation when the downsampling factor is high, showing the potential of the approach for 3D medical imaging super-resolution.
Possible future work involves the exploration of better architectures and the inclusion of other perceptual terms in the loss function in order to increase the quality of the generated volumes. Further experiments need to be done using other datasets and comparing the performance of this approach to other methods. Also a mean opinion score (MOS) test should be performed to evaluate the ability of the method to generate perceptually convincing images. This is an important issue since minor errors in the reconstruction might lead to big differences in clinical interpretation.

% \subsection{Footnotes}

% Footnotes should be used sparingly.  If you do require a footnote,
% indicate footnotes with a number\footnote{Sample of the first
%   footnote.} in the text. Place the footnotes at the bottom of the
% page on which they appear.  Precede the footnote with a horizontal
% rule of 2~inches (12~picas).

% Note that footnotes are properly typeset \emph{after} punctuation
% marks.\footnote{As in this example.}


% \subsection{Figures}
% \begin{figure}[h]
%   \centering
%   \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
%   \caption{Sample figure caption.}
% \end{figure}

% \subsection{Tables}

% All tables must be centered, neat, clean and legible.  The table
% number and title always appear before the table.  See
% Table~\ref{sample-table}.

% Place one line space before the table title, one line space after the
% table title, and one line space after the table. The table title must
% be lower case (except for first word and proper nouns); tables are
% numbered consecutively.

% Note that publication-quality tables \emph{do not contain vertical
%   rules.} We strongly suggest the use of the \verb+booktabs+ package,
% which allows for typesetting high-quality, professional tables:
% \begin{center}
%   \url{https://www.ctan.org/pkg/booktabs}
% \end{center}
% This package was used to typeset Table~\ref{sample-table}.

% \begin{table}[t]
%   \caption{Sample table title}
%   \label{sample-table}
%   \centering
%   \begin{tabular}{lll}
%     \toprule
%     \multicolumn{2}{c}{Part}                   \\
%     \cmidrule{1-2}
%     Name     & Description     & Size ($\mu$m) \\
%     \midrule
%     Dendrite & Input terminal  & $\sim$100     \\
%     Axon     & Output terminal & $\sim$10      \\
%     Soma     & Cell body       & up to $10^6$  \\
%     \bottomrule
%   \end{tabular}
% \end{table}




\subsubsection*{Acknowledgments}

This work has been partially supported by the project  MALEGRA TEC2016-75976-R financed by the Spanish Ministerio de Econom\'{i}a y Competitividad and the European Regional Development Fund (ERDF).


\medskip
%\newpage
%\small

\bibliographystyle{unsrt}
\bibliography{srganbib}
% * <veronica.vilaplana@upc.edu> 2018-04-06T16:00:22.949Z:
%
% ^.
\end{document}
