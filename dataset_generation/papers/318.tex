%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
%\documentclass[smallextended]{svjour3}      % onecolumn (second format)
\documentclass[onecolumn,10pt]{svjour3}
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
\usepackage{fixltx2e}
\usepackage[misc]{ifsym}
\usepackage{float}
\usepackage[multi-part-units=single]{siunitx}
\sisetup{separate-uncertainty,detect-all=true,detect-inline-weight = math}
\usepackage[font=small,labelfont=bf,tableposition=top]{caption}

\DeclareCaptionLabelFormat{andtable}{#1~#2  \&  \tablename~\thetable}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%
\begin{document}

\title{Plaque Classification in Coronary Arteries from IVOCT Images Using Convolutional Neural Networks and Transfer Learning}

%\subtitle{Do you have a subtitle?\\ If so, write it here}

\titlerunning{Plaque Classification in Coronary Arteries}        % if too long for running head

\author{Nils Gessert$^1$        \and
        Markus Heyder$^1$  \and
        Sarah Latus$^1$  \and
        Matthias Lutz$^2$ \and
        Alexander Schlaefer$^1$  %etc.
}

%\authorrunning{Short form of author list} % if too long for running head

\institute{\Letter \quad Nils Gessert, \email{nils.gessert@tuhh.de}, Tel.: +49 (0)40 42878 3389, https://orcid.org/0000-0001-6325-5092 \\ \\ $^1$ Hamburg University of Technology, Schwarzenbergstra\ss{}e 95, 21073 Hamburg \\ $^2$ Universittsklinikum Schleswig-Holstein, Arnold-Heller-Strae 3, 24105 Kiel}

\date{Preprint, submitted to CARS 2018, accepted for publication}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}

Advanced atherosclerosis in the coronary arteries is one of the leading causes of deaths worldwide while being preventable and treatable. In order to image atherosclerotic lesions (plaque), intravascular optical coherence tomography (IVOCT) can be used. The technique provides high-resolution images of arterial walls which allows for early plaque detection by experts. Due to the vast amount of IVOCT images acquired in clinical routines, automatic plaque detection has been addressed. For example, attenuation profiles in single A-Scans of IVOCT images are examined to detect plaque. We address automatic plaque classification from entire IVOCT images, the cross-sectional view of the artery, using deep feature learning. In this way, we take context between A-Scans into account and we directly learn relevant features from the image source without the need for handcrafting features.

\keywords{IVOCT \and Plaque Classifcation \and Deep Learning \and CNN}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}

\section{Methods}

\begin{figure*}
  \centering
  \includegraphics[width=1\textwidth]{ivoct.pdf}
% figure caption is below the figure
\caption{Left, a cartesian IVOCT image is shown. Right, the polar image is shown. Note, that there are various artifacts in the vessel which impede a clear view of the vessel wall. Lipid-rich plaque is visible in this image. *’ denotes the guide-wire artifact.}
\label{fig:ivoct}       % Give a unique label
\end{figure*}

We built a new database of IVOCT images using in-vivo patient data acquired with a St. Jude Medical Ilumien OPTIS. A trained expert with daily application experience with IVOCT provides the ground-truth labels for the images. Each B-Scan is assigned a binary label of plaque” or no plaque”. In total, the dataset contains 41 patients with 2868 labeled B-Scans. We split off a test set of 6 patients with 509 B-Scans. The dataset is slightly imbalanced with \SI{45}{\percent} of the B-Scans being labelled as no plaque”.
In contrast to previous approaches \cite{ughi2013automated,rico2016automatic}, we do not apply extensive pre-processing for lumen segmentation and flattening or guide-wire artifact and catheter removal. Therefore, we force our models to be robust towards all kinds of artifacts which will often appear in clinical practice. This allows our models to deal with any raw data without having to rely on an automatic segmentation procedure which might fail if the artery wall is not consistently visible, as shown in Figure\ref{fig:ivoct}.
We employ convolutional neural networks (CNNs) to directly learn relevant features for plaque classification from the B-Scans.  We make use of standard architectures for image classification and object detection in non-medical settings. The architectures are Resnet50, Resnet101, InceptionV3 and Inception-ResnetV2 \cite{litjens2017survey,Szegedy.2017}. Moreover, we apply transfer learning which can help with the adaptation to new problem domains where data is limited \cite{ravishankar2016understanding}. Therefore, we pretrain the models on the ImageNet dataset which contains 1.2 million natural images with 1000 classes. We remove the models’ last layer and add a layer with two outputs for binary classification. In their original design, the Inception-based models employ dropout before the output. For comparability, we employ dropout with a probability of p=0.5 before the output of every model.
The images that are fed into the model can be represented either in polar or cartesian form, see Figure~\ref{fig:ivoct}. In polar form, the acquired A-Scans are aligned next to each other in temporal acquisition order. Each A-Scan represents a single depth scan at a certain angle of the artery cross-section. The polar image can be transformed into cartesian space by mapping the A-Scans to their respective angle and applying interpolation in between. This representation provides a more intuitive cross-sectional view of the artery and is therefore used by practitioners. From an image processing point of view, both should capture the same amount of information. We investigate whether either representation is more advantageous for deep feature learning.
We resize the input images to a size of 300x300 pixels. For data augmentation, we apply random cropping with a patch size of 270x270 pixels during training. Furthermore, we apply random rotations to the cartesian images and we randomly flip polar images along the temporal axis. For evaluation, we use a single center crop of the training patch size without flipping or rotations.

\section{Results}

\begin{figure*}
  \centering
  \includegraphics[width=1\textwidth]{sens_spec.pdf}
% figure caption is below the figure
\caption{The sensitivity and 1-specificity of 16 different models for binary plaque classification is shown. Each mark represents one network architecture. *’ refers to ResNet50, x’ refers to ResNet101, o’ refers to InceptionV3 and +’ refers to Inception-ResnetV2.}
\label{fig:res}       % Give a unique label
\end{figure*}

The resulting prediction performance of our models on the test set is shown in Figure~\ref{fig:res}. The sensitivity and 1-specificity of each model for classification of an image as plaque” is shown. The four models were each trained on polar and cartesian images, both with and without transfer learning. Models in the upper left corner perform better as they have a higher sensitivity and specificity.
Overall, pretraining on ImageNet appears to improve performance significantly as the best model without pretraining shows an overall accuracy of \SI{78.9}{\percent} (Inception-ResnetV2 with polar images) with a sensitivity of \SI{77.9}{\percent} and a specificity of \SI{80.3}{\percent} while the best model with pretraining (ResNet101 with cartesian images) shows an overall accuracy of  \SI{88.0}{\percent} with a sensitivity of \SI{90.0}{\percent} and a specificity of \SI{85.5}{\percent}. It appears, that meaningful feature transfer from the natural image domain to the IVOCT image domain was achieved.
Also, using cartesian representations results in better classification performance. All models with pretraining achieve both a higher sensitivity and specificity when being trained on cartesian images. For example, the best model with polar images shows an accuracy of \SI{82.8}{\percent} compared to \SI{88.0}{\percent} for cartesian images (both Resnet101). This indicates that a cartesian real-world image representation helps CNN-based learning when employing transfer learning. Without pretraining, the difference is not as clear and some models perform better with polar representations.
The different models all perform similar with Resnet101 standing out slightly as it performs best for the two pretrained cases. All in all, the choice of image representation and transfer learning has a larger impact on performance than the model choice.

\section{Conclusions}

We perform plaque classification from IVOCT images using CNNs for deep feature learning. For this purpose, we build a database with in-vivo patient image data that is labelled by a trained expert. We allow images with various artifacts in our dataset und force our models to learn robustness. We employ various standard CNN models with additional pretraining on ImageNet for transfer learning. Our results show that pretraining significantly boosts performance. Moreover, using cartesian image representations appears to be beneficial for CNN learning. Overall, our best model achieves an accuracy of \SI{88.0}{\percent} for plaque classification.

%\begin{acknowledgements}
%If you'd like to thank anyone, place your comments here
%and remove the percent signs.
%\end{acknowledgements}

% BibTeX users please use one of
%\bibliographystyle{spbasic}      % basic style, author-year citations
\bibliographystyle{spmpsci}
%\bibliographystyle{ieeetr}     % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
\bibliography{egbib}   % name your BibTeX data base



\end{document}
% end of file template.tex
