\documentclass[sigconf]{acmart}

\usepackage{booktabs} % For formal tables
\usepackage{graphicx}
\usepackage{amsmath,amssymb} % define this before the line numbering.
%\usepackage{color}


%\usepackage{times}
\usepackage{epsfig}
%\usepackage{bm}
\usepackage{subfigure}
\usepackage{epstopdf}
\usepackage{multirow}
\usepackage{array}

% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
%\acmDOI{xx.xxx/xxx_x}
%
%% ISBN
%\acmISBN{xxx-xxxx-xx-xxx/xx/xx}
%
%%Conference
%\acmConference[MM '17]{ACM Multimedia conference}{October 23-27 2017}{Mountain View, CA USA}
%\acmYear{2017}
%\copyrightyear{2017}
%
%\acmPrice{15.00}


\begin{document}
\title{Online action recognition based on incremental learning of weighted covariance descriptors}
%\titlenote{Produces the permission block, and
% copyright information}
%\subtitle{Extended Abstract}
%\subtitlenote{The full version of the author's guide is available as
%  \texttt{acmart.pdf} document}

%\author{Anonymous ACM MM submission}
%
%\affiliation{%
%  \institution{Paper ID 76}
%}
\author{Chang Tang}
%\authornote{Dr.~Trovato insisted his name be first.}
%\orcid{1234-5678-9012}
\affiliation{%
  \institution{School of Computer Science, \\China University of Geosciences}
  %\streetaddress{P.O. Box 1212}
  \city{Wuhan}
  \state{Hubei, China}
  \postcode{430074}
}
\email{happytangchang@gmail.com}

\author{Pichao Wang}
\authornote{Corresponding author.}
\affiliation{%
  \institution{Advanced Multimedia Research Lab, University of Wollongong}
  %\streetaddress{P.O. Box 1212}
  \city{Wollongong}
  \state{NSW, Australia}
  \postcode{2500}
}
\email{pw212@uowmail.edu.au}

\author{Wanqing Li}
%\authornote{}
\affiliation{%
  \institution{Advanced Multimedia Research Lab, University of Wollongong}
  %\streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Wollongong}
  \state{NSW, Australia}
  \postcode{2500}
}
\email{wanqing@uow.edu.au}





\begin{abstract}
Different from traditional action recognition based on video segments, online action recognition aims to recognize actions from unsegmented streams of data in a continuous manner.  One way for online recognition is based on the evidence accumulation over time to make predictions from stream videos. This paper presents a fast yet effective method to recognize actions from stream of noisy skeleton data, and a novel weighted covariance descriptor is adopted to accumulate evidence. In particular, a fast incremental updating method for the weighted covariance descriptor is developed for accumulation of temporal information and online prediction. The weighted covariance descriptor takes the following principles into consideration:  past frames have less contribution for recognition and recent and informative frames such as key frames contribute more to the recognition. The online recognition is achieved using a simple nearest neighbor search against a set of offline trained action models. Experimental results on MSC-12 Kinect Gesture dataset and our newly constructed online action recognition dataset have demonstrated the efficacy of the proposed method.
\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
%
 \begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010178.10010224.10010225.10010228</concept_id>
<concept_desc>Computing methodologies~Activity recognition and understanding</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Activity recognition and understanding}



\keywords{Online Action Recognition, Weighted Covariance, Incremental Learning, Skeleton, Online RGB-D Dataset}


\maketitle

\section{Introduction}

Human action recognition is an active research topic in computer vision due to its wide range of potential applications, viz. surveillance, video games, video indexing and search, and human-robot interaction. In the last decade many approaches have been proposed to recognize actions from monocular or RGB video sequences~\cite{aggarwal2011human}. However, these methods face the difficulties posed by changes in illumination, variations in viewpoint, occlusion and cluttered background. Perhaps more importantly, these methods are somewhat impaired by the loss of 3D information in conventional video.

Since the release of low-cost RGB-D sensors such as Microsoft Kinect~\texttrademark sensors, many efforts and advances have been made on action recognition from depth maps. Compared with RGB data, depth maps have several advantages for action recognition, typically, being insensitive to illumination changes and reliable to estimate body silhouette and skeleton~\cite{shotton2013real}.

%\begin{figure}
%  \centering
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=0.5\textwidth]{Figures/FigCovRepreSkeleton.eps}\\
%  \caption{Test}\label{FigCovRepreSkeleton}
%\end{figure}

Many methods have been proposed for recognizing actions from depth or skeleton data~\cite{aggarwal2014human,li2010action}. These methods are often based on hand-crafted features, such as depth-map-based~\cite{yang2012recognizing}, skeleton joints~\cite{vemulapalli2014human,zanfir2013moving} or body parts~\cite{shotton2013real}, cloud points~\cite{wang2012robust,rahmani2014hopc}, local interest points~\cite{xia2013spatio},and surface normals~\cite{oreifej2013hon4d,yang2014super}. With the development of deep learning approach, several works have been proposed based on Convolutional Neural Networks (CNNs)~\cite{wang2015convnets,wang2016mm,wang2016action} and Recurrent Neural Networks (RNNs)~\cite{donahue2015long,du2015hierarchical,shahroudy2016ntu}. However, all of these methods only focus on classifying actions from segmented sequences of input data, with each segment corresponding to one single action. They assumed that all the instances, training or testing, are temporally segmented before recognition. This assumption is usually not valid when data are streamed in real-time and recognition has to be conducted online, where boundaries between different kinds of actions within the stream are unknown.

%In this paper, we propose to fast recognize actions from stream of skeleton data on the fly.
Generally speaking, there are two main approaches to online recognition based on the types of video representation: frame-level representation and sliding window based representation. In the former, the stream video is represented by a series of frame-level descriptors where each descriptor is extracted from one or several frames in a short temporal interval~\cite{yu2014discriminative,jiang2014online,miranda2014online,hu2016discriminative}. The advantage of this representation is its simplicity, without detecting the start and end frame of each action and online recognition is achieved by aggregating frame-based classification. However, this representation tends to neglect the temporal coherence, which is vital for recognizing complex actions, and frame-based classification is prone to error.
%Sliding window based representation is a simple and effective method to recognize human actions from stream data~\cite{kviatkovsky2014online,kulkarni2014continuous,zhu2016online}. %Sliding window based method is based on the idea of evidence accumulation to make predictions and takes

Sliding window based methods~\cite{kviatkovsky2014online,kulkarni2014continuous,zhu2016online} are a simple extension of segmented-based action recognition methods. They often consider the temporal coherence within the window for prediction and the window-based predictions are further fused to achieve online recognition. However, the performance of these methods are sensitive to the window size which depends on actions and is hard to set. Either too large or too small window size could lead to a significant drop in recognition. In addition, in previous sliding window based methods, all the frames in the sliding window are often considered equally important which are not justifiable. When the window is large and covers more than one actions, the past frames should contribute less to the recognition of recent frames. Moreover, frames that are discriminative should also contribute more to the recognition than non-discriminative frames. Motivated by the mathematical properties of covariance descriptors~\cite{hussein2013human,kviatkovsky2014online,sanin2013spatio} and its being able to incrementally updated,
%Inspired by the promising results achieved by covariance descriptor, which is a good representation for evidence accumulation~\cite{hussein2013human,kviatkovsky2014online,sanin2013spatio}, and taking the disadvantages of sliding window based method into account,
this paper proposes a fast online action recognition method from skeleton data based on weighted covariance descriptors. The method assumes that segmented and labeled action instances are available for offline training and recognition is to be performed in an online manner.
To facilitate the online recognition, an incremental learning of the weighted covariance descriptors is developed by taking into consideration the importance of frames with respect to their temporal order and discrimination. Such an incremental learning provides an effective mechanism to accumulate information over time for recognition.
%where the past frames have less contributions to the descriptor and current frames and informative frames such as key frames contributes more towards the descriptor. This weighting scheme  encodes the discriminative temporal information into the covariance descriptor, and the incremental learning process guarantees the fast recognition. To facilitate the research of online action recognition, a new online RGB-D dataset has been constructed to evaluate the proposed method.
Experimental results on MSRC-12 Kinect Gesture dataset~\cite{fothergill2012instructing} and on our newly collected online action recognition dataset have demonstrated the efficacy of the proposed method. The new dataset will be released to the public upon the acceptance of this paper.

The contributions of this paper are summarized as follows: 1) An effective evidence accumulation based fast online action recognition method is proposed based on weighted covariance descriptors, which is the first attempt to adopt weighted covariance descriptor for stream based action recognition; 2) a fast incremental learning of the covariance descriptors with two kinds of weights capturing both temporal order, i.e. past frames are gradually ``forgotten", and the discrimination of frames; 3) a new RGB-D online action recognition dataset was created and will be released to the public; 4) state-of-the-art results are achieved by the proposed method on the two datasets.

The remainder of this paper is organized  as follows. Section~\ref{related} presents the related work. Section~\ref{method} describes the proposed method. Experimental results on the two datasets are presented in Section~\ref{experimental}. Section~\ref{conclusion} concludes the paper with discussion on the future work.


\section{Related work}
\label{related}
%Recently, several online action recognition methods have been proposed. The types of video representation for these methods can be mainly categorized into to two classes: frame-level based representation and sliding window based representation.
Recently, several online action recognition methods have been proposed. These methods can be mainly categorized into to two classes: frame-level based  and sliding window based.

A typical frame-based method was introduced by Miranda et al.~\cite{miranda2014online} for real-time gesture recognition from noisy skeleton streams. In their method, key poses were identified by the descriptors composed of angular representation of the skeleton joints, and the gesture was labeled on-the-fly from the key pose sequence with a decision forest; Zhao et al.~\cite{zhao2014structured} proposed a new feature, structured streaming skeleton, for online human gesture recognition, which is constructed for each frame through dynamic matching; De Rosa et al.~\cite{DeRosa2014} proposed a novel algorithm for online nonparametric recognition built upon a recent nonparametric regression method. Their online action recognition system can be combined with any set of frame-by-frame feature descriptors; Yu et al.~\cite{yu2014discriminative} proposed a discriminative orderlet mining method for real-time recognition of human-object interaction by using both depth maps and skeleton joints; Hu et al.~\cite{hu2016discriminative} represented an approach for online human action recognition, where the videos were represented by frame-level descriptors. They proposed a method to discover an action states from frame-level descriptors. However, these frame based methods tend to neglect the temporal coherence, which is vital for recognizing complex actions.

Sliding window based methods, are a simple extension of segmented based action recognition in which the temporal coherence inside the window is well taken into consideration. For example, in~\cite{hoai2011joint}, action segmentation and recognition were jointly performed based on a discriminative temporal extension of the spatial bag-of-words model; Minhas et al.~\cite{minhas2012incremental} approximated the shape of a human by adaptively changing intensity histograms to extract pyramid histograms of oriented gradient features. They then examined incremental learning as an overlooked obstruction to the implementation of reliable real-time recognition; Vieira et al.~\cite{vieira2014improvement} constructed a new high dimensional feature vector, called Space-Time Occupancy Pattern (STOP) by dividing space and time axes into multiple segments. Online action recognition was performed by combining depth maps with skeletons; Kulkarni et al.~\cite{kulkarni2014continuous} built on the well known dynamic time warping (DTW) framework and devised a visual alignment technique, namely dynamic frame warping (DFW), which performed isolated recognition based on per-frame representation of videos; Hasan et al.~\cite{hasan2014continuous} proposed a continuous human activity learning framework from streaming videos by intricately tying together deep networks and active learning. In their work, given the segmented activities from streaming videos, they learned features in an unsupervised manner using deep networks and use active learning to reduce the amount of manual labeling of classes; In~\cite{zhu2016online}, the authors did not detect the start and end points of each human action explicitly, but segmented feature sequences online, and employed a variable-length MEMM method to recognize human actions based on the online model matching results of feature segments; Bloom et al.~\cite{bloom2016hierarchical} proposed a hierarchical transfer learning algorithm for online detection of compound actions for robust action recognition. Transfer learning was employed to allow the tasks of action segmentation and modelling, and model adaptation was used to improve performance on complex datasets.

It is worth noting that in online action recognition past frames from a stream may not necessarily contribute to the classification of current frames. This is especially true when the past frames are actually from an action different from the current action. In addition, frames in the period of a same action would have different discriminative power to the classification of the frames. Therefore, a mechanism is needed to address these two factors for online action recognition.

Covariance descriptor which was originally designed for object detection, has been successfully used for 2D and 3D action recognition~\cite{harandi2012kernel,hussein2013human,kviatkovsky2014online,sanin2013spatio}. In the work of Harandi et al.~\cite{harandi2012kernel}, the covariance descriptors were mapped into the Euclidean space by a Riemannian locality preserving projection (RLPP) technique and action classification was performed by standard classification methods. Sanin et al.~\cite{sanin2013spatio} used the RLPP with AdaBoost to learn a set of covariance descriptors and obtained impressive results in the recognition of a specific set of actions. Hussein et al.~\cite{hussein2013human} used a temporal hierarchy of covariance matrices on 3D joint locations over time as a discriminative descriptor for a sequence. They obtained good action recognition results by using linear SVM on the descriptors. Kviatkovsky et al.~\cite{kviatkovsky2014online} introduced an incremental updating rule for covariance matrices and used a sliding window to construct a covariance feature descriptor, the final frame by frame online action recognition was performed through nearest neighbor classification. This paper also uses the incremental covariance update rule but our work is different from what Kviatkovsky et al.~\cite{kviatkovsky2014online} reported. In their work, action class of a current frame was detected based on the covariance matrix constructed from previous $W-1$ frames and current frame; $W$ is the sliding window size. So if the current frame is the beginning of a new action, it is usually wrongly classified to the previous action. This situation arises because most of the information captured by the current covariance matrix is from previous action frames.

Another issue with their work is that they considered each frame in the sliding window equally important. Commonly,  current frames are representative for actions being performed, hence, they are more important, than past frames. Then key-frame based action recognition has demonstrated that frames within an instance of an action are not equally discriminative, for instance, frames of neutral poses often do not contribute to the classification.

This paper addresses the two drawbacks discussed above by proposing a weighted covariance descriptor and its incremental updating that take into consideration both the temporal order and discrimination of frames.
%These two drawbacks motivated the proposal of a new weighted covariance update rule that applies less temporal weight to past frames than more recent frames. Moreover, since the individual frames contribute differently to the recognition of actions (e.g. neutral pose making no contribution) we also propose frame-based weighting on different frames.


%-------------------------------------------------------------------------
\section{The Proposed Method}\label{method}
%\begin{figure*}
%  \centering
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=1.0\textwidth]{Figures/ProposedMethod.eps}\\
%  \caption{Overview of our proposed method.}\label{ProposedMethod}
%\end{figure*}
\label{Framework}
%\subsection{Problem Formulation}
%\label{ProblemDes}
Suppose there are $L$ possible action classes and $M$ segmented training action instances, and each training action instance corresponds to one of the $L$ action class. The action-label set can be denoted by $\mathcal{L} = \{ l\} _{l = 1}^L$. Let $\left\{ {M_n^l} \right\}_{n = 1}^{{N_l}}$ denote the set
of $N_l$ single-action instances of class $l$, where ${N_1} +  \cdots  + {N_L} = M$. Given a test video sequence $V$ with an unknown order and number
of actions $I$, its unknown label sequence is represented as $Z = ({z_1}, \cdots ,{z_i}, \cdots ,{z_I})$, $z_i \in \mathcal{L}$; the boundaries between two consecutive actions are also unknown. In practice, we cannot access the whole test video sequence at once; only one frame of the test streaming video is available at time $t$. The task of online action recognition is to decide what action class the human is performing at any time $t$ using sufficient information of the previous frames before $t$, and find a partition between two consecutive different actions. If sufficient information is accumulated at time $t$ to make a decision, we determine that the subject is performing an action $l$, otherwise continue accumulating information from time $t + 1$.

\subsection{The Covariance Descriptor}
\label{CovarianceDescriptor}
Let $\mathbf{S}=[{s_1},{s_2}, \cdots ,{s_n}]$ be the data matrix composed of $n$ feature vectors, each feature vector containing $d$ feature variables. The data matrix can be represented by the $d\times d$ sample covariance matrix $C$ as follows:
\begin{equation}\label{CovDef}
C = \frac{1}{{n - 1}}\sum\limits_{i = 1}^n {\left( {{s_i} - \bar s} \right)} {\left( {{s_i} - \bar s} \right)^T},
\end{equation}
where $\bar s$ is the mean of the feature vectors. The diagonal entries of the covariance matrix represent the variance of each individual feature variable, while the non-diagonal entries are their respective correlations. Our motivation of using covariance as a feature descriptor can be
summarized as follows. Firstly, the covariance matrix can be calculated in an incremental manner so as to accumulate evidence over time and it is simple and fast to compute, thus meeting the essential requirement of online recognition. Secondly, the covariance matrix captures information about the shape and distribution of the set of feature variables. The feature variables of different actions generally have different distributions, so covariance matrices can discriminate different actions.

%-------------------------------------------------------------------------
\subsection{Overview of the method}
\label{Overview}
In this section, the details of the proposed method are presented. There are two main phases in the proposed method: offline training and online recognition.

\subsubsection{Offline Training}
During the training phase, we use each of the labeled training instances to construct a covariance matrix for each action. In order to make the covariance matrices more discriminative, the symmetric positive definite (SPD) matrix dimensionality reduction method~\cite{harandi2014manifold} is adopted to learn a projection matrix. The original covariance matrices are then projected onto a low dimensional but more discriminative space. As described in~\cite{harandi2014manifold}, the projection matrix learning can be expressed as the following minimization problem
\begin{equation}\label{LearningProjectionMat}
\begin{array}{l}
{P^ * } = \mathop {\arg \min }\limits_{P \in {\mathbb{R}^{n \times m}}} \sum\limits_{i,j} {{A_{ij}}} {\delta ^2}\left( {{P^T}{X_i}P,{P^T}{X_j}P} \right)\;\\
\;s.t.\;{P^T}P = {I_m}
\end{array}
\end{equation}
where $X_i$ and $X_j$ are any two different covariance matrices in the training set. $A_{ij}$ is a real symmetric affinity matrix which encodes the structure of the original data. $n$ is the original covariance matrix dimension and $m$ is the projected covariance matrix dimension. $I_m$ is a $m \times m$ identity matrix. $\delta$ is the Stein metric function~\cite{srapositive} or AIRM metric function~\cite{pennec2006riemannian}. In our experiment, we use the Stein metric for training and testing. For any two SPD covariance matrices $X$ and $Y$, their Stein metric is defined as
\begin{equation}\label{SteinMetric}
{\delta ^2}\left( {X,Y} \right) = \ln \det \left( {\frac{{X + Y}}{2}} \right) - \frac{1}{2}\ln \det \left( {XY} \right).
\end{equation}
There are two advantages in using the projection method. First, it tends to render the covariance matrices more discriminative. Second, it projects the high dimensional matrix into a low dimensional matrix, so the subsequent estimation of the distance between the two matrices is computationally efficient.

\begin{figure}
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.5\textwidth]{DistanceCurve.png}\\
  \caption{Minimum distances between online covariance matrices and training covariance matrices of each action class.}\label{DistanceCurve}
\end{figure}

\subsubsection{Online Recognition}
During the online recognition phase, a continuous video stream consists of some actions with unknown order, and its labels are predicted frame by frame. At time $t$, we use the previous $t-1$ frames and current frame to construct a covariance matrix $C_t$. Then $C_t$ is projected to a low dimensional space using $P$ and  the distance between all the projected training covariance matrices and $C_t$ is compared. For each training class $l$, the $N_l$ distances are represented as $\left\{ {{d_{{C_t},{l_1}}}, \cdots ,{d_{{C_t},{l_{N_l}}}}} \right\}$. The final distance between $C_t$ and class $l$ is given by
\begin{equation}\label{MinDistance}
{d_{{C_t},l}} = \min \left\{ {{d_{{C_t},{l_1}}}, \cdots ,{d_{{C_t},{l_{N_l}}}}} \right\}.
\end{equation}
Thus, $L$ distances for each frame are generated as time progresses. At time $t$, the $L$ distances and their standard deviation are used to decide what action is being performed or determine whether there is a boundary between two consecutive actions. At the beginning, a covariance matrix using the first $t_0$ frames is initialized, and  an action label $l_{t_0}$ is given by the criterion
\begin{equation}\label{InitialDecision}
{l_{t_0}} = \arg \mathop {\min }\limits_l \left\{ {{d_{{C_{t_0}},l}}} \right\}_{l = 1}^L.
\end{equation}
From time $t_0+1$, if the standard deviation value of the $L$ distances is a local minima and a new action is detected by Eq.~\ref{InitialDecision}, action change is considered to take place. Otherwise, no action change happens. It is convenient to use the standard deviation of the $L$ distances to decide whether there exists a boundary between two actions. For instance, when the estimated distance at a given time is the minimum among others and the standard deviation is also large, this indicates that some specific action is being performed and there is no action change. In contrast, if a new action starts and the previous action ends, there exists a transitional stage, so all the estimated distances are similar and the standard deviation is relatively low. In~\cite{fanello2013keep} and~\cite{DeRosa2014}, a similar method is used on the SVM scores. Fig.~\ref{DistanceCurve} shows a segment of minimum distances between online covariance matrix and trained covariance matrices for each action class. Fig.~\ref{Std} shows the standard deviation of the distances in Fig.~\ref{DistanceCurve}. As can be seen, when it comes to an action change, the standard deviation value goes to a local minimal.
\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{std.png}\\
  \caption{The standard deviation of the covariance distances.}\label{Std}
\end{figure}

\subsection{Incremental leaning of weighted covariance matrices}
\label{IncrementalCovarianceLearning}
In previous methods, feature descriptors extracted from one video frame or video segments have been weighted equally. However, the contribution of each frame to action recognition varies as some frames are more discriminative than others. In this regard, the important frame should be weighted proportionally higher (frame-based weighting). Furthermore, during the online action recognition, recent frames nearer the current frame usually provide more information than past frames (temporal weighting). The proposed method incorporates these two weighting schemes in an efficient algorithm to update the covariance descriptor. Specifically, we assign two kinds of weights to each video frame - frame weight and temporal weight. There are a number of considerations in assigning the weights for each frame: 1) the frame weight should depend on the discriminative power of the frame with respect to the action and is independent of time; 2) the temporal weight of each frame should vary over time $t$ and recent frames should have higher time weights than past frames; 3) the weights should be computationally efficient and support incremental updating of the covariance matrix. We denote the frame weight of the $i_{th}$ frame as $\xi_i$. Similarly, at current time $t$, the temporal weight of the $i_{th}$ frame is denoted as $\omega _t^i$ is defined as follows:
\begin{equation}\label{TimeW}
\begin{array}{l}
\omega _t^i = g(t - i),\;\;\;i \in [1,t],\;\;\;\\
s.t.\;\\
%g(t - 1) = 0,\;\\
g(0) = 1,\\
%g(t - i) < g(t - i - 1),\\
g(t + 1 - i) = \eta g(t - i),\; \eta \in [0,1].
\end{array}
\end{equation}
Then the final weight of the $i_{th}$ frame can be obtained as ${\psi _i} = {\xi _i}\omega _t^i$. With this weight setting, when a new frame at time $t+1$ is available, the covariance can be incrementally updated. Wu et al.~\cite{wu2012real} used a similar weighting scheme for tracking based on covariance descriptor, but only temporal weight was considered in their work. Next, we provide details of an efficient algorithm to update the weighted covariance matrix.

Let the feature vector extracted from the $i_{th}$ frame be ${f_i}$. The weighted feature vectors up to current time $t$ can be written as
\begin{equation}\label{FVectorSetTimeT}
{F_t} = {\left\{ {{f_i},\;{\psi _i}} \right\}_{i = 1, \cdots ,t}}
\end{equation}

Let $C_t$ and $\mu_t$ be the weighted covariance and the weighted mean of the feature vectors up to time $t$. The
formulation of $C_t$ and $\mu_t$ are as follows~\cite{price1972extension}:
\begin{equation}\label{CovT}
{C_t} = \frac{1}{{1 - \tilde \omega _t^2}}\sum {_{i= 1}^t\frac{\psi_i}{{{{\hat \omega }_t}}}({f_i} - {\mu _t}){{({f_i} - {\mu _t})}^T}},
\end{equation}
and
\begin{equation}\label{MeanT}
{\mu _t} = \frac{1}{{{{\hat \omega }_t}}}\sum {_{i = 1}^t} \psi_i{f_i},
\end{equation}
where
\begin{equation*}{{\hat \omega }_t} = \sum {_{i = 1}^t} \psi_i \;\;\;\;\;\;\;\; \tilde \omega _t^2 = \sum {_{i = 1}^t} {\left( {\frac{{\psi_i}}{{{{\hat \omega }_t}}}} \right)^2}.
\end{equation*}

Our goal, therefore, is to efficiently compute the new covariance $C_{t+1}$ and mean $\mu_{t+1}$ using $C_t$ and $\mu_t$ when given $f_{t+1}$, ${\hat \omega }_{t}$, and $\tilde \omega _{t}^2$, without explicitly recomputing them from the data ${F_{t+1}}$. We give the incremental
covariance update rule in the following Theorem 1.
\newtheorem{CovUpdateRule}{Theorem}\label{CovUpdateRule}
\begin{CovUpdateRule}
Given $C_t$, $\mu_t$, ${\hat \omega }_{t}$, $\tilde \omega _{t}^2$, $f_{t+1}$,
$\xi_i$, $\omega _t^i = {g(t-i)}$, and $\eta$, the relation between
$C_t$ and $C_{t+1}$ can be written as
\begin{equation}\label{IncrementalCovRule}
\begin{array}{l}
{C_{t + 1}}{\rm{ = }}\frac{1}{{2\eta {{\hat \omega }_t}{\xi _{t + 1}} + {\eta ^2}{{\hat \omega }_t}^2\left( {1 - \tilde \omega _t^2} \right)}}\left\{ {\left[ {\eta {{\hat \omega }_t}\left( {1 - \tilde \omega _t^2} \right){C_t}} \right]\left( {{\xi _{t + 1}} + \eta {{\hat \omega }_t}} \right)} \right.\\
\;\;\;\;\;\;\;\;\; + \left. {\frac{{\eta {{\hat \omega }_t}\left( {{\xi _{t + 1}}^2 + \eta {{\hat \omega }_t}{\xi _{t + 1}}} \right)}}{{\eta {{\hat \omega }_t} + {\xi _{t + 1}}}}({f_{t + 1}} - {\mu _t}){{({f_{t + 1}} - {\mu _t})}^T}} \right\}
\end{array}
\end{equation}
and the relation between $\mu_t$ and $\mu_{t+1}$ is as follows
\begin{equation}\label{IncrementalMeanRule}
{\mu _{t{\rm{ + }}1}}{\rm{ = }}\frac{{\eta {{\hat \omega }_t}{\mu _t}{\rm{ + }}{\xi _{t + 1}}{f_{t + 1}}}}{{\eta {{\hat \omega }_t}{\rm{ + }}{\xi _{t + 1}}}},
\end{equation}
\end{CovUpdateRule}

In order to make the proof of Theorem 1 concise we first give some
lemmas. The proofs of all the lemmas appear in Appendix A.
%%%Lemmas********************************************************************************************
%\newtheorem{theorem}{Lemma}
%%% Lemma Sumw ******Lemma Sumw***********************************************************************
%\begin{theorem}
%\label{LemmaSum}
%If $\omega _t^i$ is given by Eq.\ref{TimeW}, then we have \\
%${{\hat \omega }_{t + 1}} = \eta {{\hat \omega }_t} + {\xi _{t + 1}}$ and $\tilde \omega _{t + 1}^2 = \frac{{{{\hat \omega }_t}^2{\eta ^2}\tilde \omega _t^2 + {\xi _{t + 1}}^2}}{{{{\left( {{\xi _{t + 1}} + \eta {{\hat \omega }_t}} \right)}^2}}}$.
%\end{theorem}

%\newtheorem{lemma}{\textbf{Lemma}}
\begin{lemma}
\label{LemmaSum}
If $\omega _t^i$ is given by Eq.~\ref{TimeW}, then we have
${{\hat \omega }_{t + 1}} = \eta {{\hat \omega }_t} + {\xi _{t + 1}}$ and $\tilde \omega _{t + 1}^2 = \frac{{{{\hat \omega }_t}^2{\eta ^2}\tilde \omega _t^2 + {\xi _{t + 1}}^2}}{{{{\left( {{\xi _{t + 1}} + \eta {{\hat \omega }_t}} \right)}^2}}}$.
\end{lemma}

%% Lemma Transpose ******Lemma Transpose***********************************************************************
\begin{lemma}
\label{Transpose}
$\sum {_{i = 1}^t{\xi _i}\omega _{t + 1}^i({f_t} - {\mu _t})}=0$,
and ${\sum {_{i = 1}^t{\xi _i}\omega _{t + 1}^i({f_i} - {\mu _t})} ^T} = 0$.
\end{lemma}
%% Lemma Multiply ******Lemma Transpose Multiply**********************************************************************
\begin{lemma}
\label{TransposeMul}
$\left( {{\mu _t} - {\mu _{t + 1}}} \right){\left( {{\mu _t} - {\mu _{t + 1}}} \right)^T} = \frac{{{\xi _{t + 1}}^2\left( {{\mu _t} - {f_{t + 1}}} \right)}}{{{{\left( {{\xi _{t + 1}} + \eta {{\hat \omega }_t}} \right)}^2}}}$
\end{lemma}
%% Lemma FeatureAndMuTransposeMul ******Lemma FeatureAndMuTransposeMul*************************************
\begin{lemma}
\label{FeatureAndMuTransposeMul}
$\sum {_{i = 1}^t{\xi _i}\omega _{t + 1}^i({f_i} - {\mu _{t + 1}}){{({f_i} - {\mu _{t + 1}})}^T}}\\ = \eta \left( {1 - \tilde \omega _t^2} \right){{\hat \omega }_t}{C_t} + \frac{{\eta {{\hat \omega }_t}{\xi _{t + 1}}^2\left( {{\mu _t} - {f_{t + 1}}} \right){{\left( {{\mu _t} - {f_{t + 1}}} \right)}^T}}}{{{{\left( {{\xi _{t + 1}} + \eta {{\hat \omega }_t}} \right)}^2}}}$
\end{lemma}
Now, we give the proofs of Eq.~\ref{IncrementalCovRule} and Eq.~\ref{IncrementalMeanRule}.
According to the definition by Eq.~\ref{CovT} and Eq.~\ref{MeanT}, we have
\begin{equation}\label{CovTplusOne}
{C_{t + 1}} = \frac{1}{{1 - \tilde \omega _{t + 1}^2}}\sum {_{i = 1}^{t + 1}\frac{{{\xi _i}\omega _{t + 1}^i}}{{{{\hat \omega }_{t + 1}}}}({f_i} - {\mu _{t+ 1}}){{({f_i} - {\mu _{t + 1}})}^T}},
\end{equation}
and
% \begin{equation}\label{MeanTplusOne}
% {\mu _{t + 1}} = \frac{1}{{{{\hat \omega} }_{t + 1}}}}\sum {_{i = 1}^{t + 1}}
% {\xi _i}\omega _{t + 1}^i{f_i}.
% \end{equation}
\begin{align}
\label{MeanTplusOne}
 \mu_{t+1}=\dfrac{1}{\hat{\omega}_{t=1}}\sum_{i=1}^{t+1}\xi_i\omega_{t+1}^{i}f_i
\end{align}

\textbf{\emph{Proof of Eq.~\ref{IncrementalMeanRule}:}}
\begin{equation*}
\begin{array}{l}
{\mu _{t + 1}} = \frac{1}{{{{\hat \omega }_{t + 1}}}}\sum {_{i = 1}^{t + 1}} {\xi _i}\omega _{t + 1}^i{f_i}\\
\;\;\;\;\;\;\; = \frac{1}{{{{\hat \omega }_{t + 1}}}}\left\{ {\sum {_{i = 1}^t} {\xi _i}\omega _{t + 1}^i{f_i} + {\xi _{t + 1}}\omega _{t + 1}^{t + 1}{f_{t + 1}}} \right\}\\
\;\;\;\;\;\;\; = \frac{1}{{{{\hat \omega }_{t + 1}}}}\left\{ {\eta \sum {_{i = 1}^t} {\xi _i}\omega _t^i{f_i} + {\xi _{t + 1}}{f_{t + 1}}} \right\}\\
\;\;\;\;\;\;\; = \frac{1}{{{{\hat \omega }_{t + 1}}}}\left\{ {\eta {\mu
_t}{{\hat \omega }_t} + {\xi _{t + 1}}{f_{t + 1}}}
\right\}\footnote{1}\\
\;\;\;\;\;\;\; = \frac{{\eta {\mu _t}{{\hat \omega }_t} + {\xi _{t + 1}}{f_{t +
1}}}}{{{\xi _{t + 1}} + \eta {{\hat \omega
}_t}}}\footnote{2}
\end{array}
\footnotetext[1]{Using definition by Eq.\ref{MeanT}.}
\footnotetext[2]{Using Lemma.\ref{LemmaSum}.}
\end{equation*}
\textbf{\emph{Proof of Eq.~\ref{IncrementalCovRule}:}}
From Eq.~\ref{CovTplusOne}, we have
% \footnote {This is what a footnote looks like.  It
% often distracts the reader from the main flow of the argument.}
\begin{equation*}
\begin{array}{l}
{{\hat \omega }_{t + 1}} \cdot (1 - \tilde \omega _{t + 1}^2){C_{t + 1}} = \sum {_{i = 1}^{t + 1}{\xi _i}\omega _{t + 1}^i({f_i} - {\mu _{t + 1}}){{({f_i} - {\mu _{t + 1}})}^T}} \\
\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; = \sum {_{i = 1}^t{\xi _i}\omega _{t + 1}^i({f_i} - {\mu _{t + 1}}){{({f_i} - {\mu _{t + 1}})}^T}} \\
\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; + {\xi _{t + 1}}\omega _{t + 1}^{t + 1}({f_{t + 1}} - {\mu _{t + 1}})({f_{t + 1}} - {\mu _{t + 1}})^T\\
\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; = \eta \left( {1 - \tilde \omega _t^2} \right){{\hat \omega }_t}{C_t}\\
 \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+ \frac{{\eta {{\hat \omega }_t}{\xi _{t + 1}}^2\left( {{\mu _t} - {f_{t + 1}}} \right){{\left( {{\mu _t} - {f_{t + 1}}} \right)}^T}}}{{{{\left( {{\xi _{t + 1}} + \eta {{\hat \omega }_t}} \right)}^2}}}\\
\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; + \frac{{{\eta ^2}{{\hat \omega }_t}^2{\xi _{t + 1}}\left( {{\mu _t} - {f_{t + 1}}} \right){{\left( {{\mu _t} - {f_{t + 1}}} \right)}^T}}}{{{{\left( {{\xi _{t + 1}} + \eta {{\hat \omega }_t}} \right)}^2}}}\footnote{3}
\end{array}
\footnotetext[3]{Using Lemma.\ref{FeatureAndMuTransposeMul} and Eq.\ref{IncrementalMeanRule}.}
\end{equation*}
therefore,
\begin{equation*}
\begin{array}{l}
{C_{t + 1}}{\rm{ = }}\frac{1}{{2\eta {{\hat \omega }_t}{\xi _{t + 1}} + {\eta ^2}{{\hat \omega }_t}^2\left( {1 - \tilde \omega _t^2} \right)}}\left\{ {\left[ {\eta {{\hat \omega }_t}\left( {1 - \tilde \omega _t^2} \right){C_t}} \right]\left( {{\xi _{t + 1}} + \eta {{\hat \omega }_t}} \right)} \right.\\
\;\;\;\;\;\;\;\;\; + \left. {\frac{{\eta {{\hat \omega }_t}\left( {{\xi _{t + 1}}^2 + \eta {{\hat \omega }_t}{\xi _{t + 1}}} \right)}}{{\eta {{\hat \omega }_t} + {\xi _{t + 1}}}}({f_{t + 1}} - {\mu _t}){{({f_{t + 1}} - {\mu _t})}^T}} \right\}\footnote{4}
\end{array}
\footnotetext[4]{Using Lemma.\ref{LemmaSum}.}
\end{equation*}

In Eq.~\ref{IncrementalCovRule} and Eq.~\ref{IncrementalMeanRule}, if all the
frame weights
and temporal weights are equal to 1, we obtain
\begin{equation}\label{IncrementalCovRuleWeight1}
{C_{t + 1}} = \frac{{t - 1}}{t}{C_t} + \frac{{\left( {{f_{t + 1}} - {\mu _t}} \right){{\left( {{f_{t + 1}} - {\mu _t}} \right)}^T}}}{{t + 1}}
\end{equation}
and
\begin{equation}\label{IncrementalMeanRuleWeight1}
{\mu _{t + 1}} = \frac{{t{\mu _t} + {f_{t + 1}}}}{{t + 1}},
\end{equation}
which is the most common formulation.
%\subsubsection{Online action recognition using incremental covariance updating}
%During the process of online covariance updating as introduced above, we computes $M$ distances between the current covariance matrix and all the training covariance matrices for each frame. For each training class $l\in \mathcal{L}$ which contains $X_l$ training instances, we compute the average distances of these $X_l$ distances. Finally, we can get $\mathcal{L}$ distances at time $t$. By using all these $\mathcal{L}$ distances, we can analyse the evolution of the action probabilities over time, and segment a continuous
%sequence of multiple actions by detecting the beginning and the end of an individual action. We use the standard deviation of the distances to give a action recognizing decision. At time $t$, if one of the $\mathcal{L}$ distances is clearly smaller than the others, and the standard deviation of the $\mathcal{L}$ distances high, this indicates that some specific action is being performed. In contrast, when all distances are similar and the standard deviation is low, this indicates the end of a past action and onset of a new action.


\section{Experimental Results and Discussions}\label{experimental}
In this section, we give the experimental results on MSRC-12 Kinect Gesture dataset~\cite{fothergill2012instructing} and our newly collected online action recognition dataset. Skeleton was the only data used for our experiment and we adopt latency, miss rate and error rate to evaluate the performance. Similar to~\cite{ellis2013exploring} and~\cite{kviatkovsky2014online},  latency was used as one of the criteria to evaluate our algorithm. If the interval between the frame a subject begins the action and the frame our algorithm classifies the action is $h$, and the whole length of an action is $H$, then the latency of this action is defined as $\frac{h}{H}$. The miss rate and error rate were used to measure audio diarization error~\cite{tranter2006overview}. We argue that our online action recognition is largely similar to the audio diarization process, so we also uses these two criteria. If an action appears $n$ times in a video sequence, our algorithm can detect $m$ times, then the miss rate is defined as $\frac{{n - m}}{n}$. If an action can be detected and the length of this action is $W$, but during these $W$ frames, there are $w$ frames detected as other actions, then the error rate is defined as $\frac{w}{W}$. The latency was used to evaluate the sensitivity of our method while the miss rate and error rate are adopted to evaluate the accuracy of our method.

For the purpose of demonstrating the efficacy of the temporal weight and frame weight, the results without the weights, represented as ``No $\omega^i$'' and ``No $\xi_i$'' are reported respectively in the following tables.

The algorithms are compared with other two methods~\cite{kviatkovsky2014online,hussein2013human}, and both of them are based on covariance descriptors and sliding window. In~\cite{kviatkovsky2014online}, the online action recognition is accomplished by nearest neighbor search based on a sliding window. Here, we set the sliding window size to 40 frames (most of the actions in the testing datasets are accomplished by 35--50 frames). In order to compare with~\cite{hussein2013human}, their work is modified to make a prediction by SVM based on a 40-frame sliding window.

\subsection{Skeleton data normalization}
\label{SkeletonNormalization}
Generally, human actions usually involve body movement such as displacement and angle change, and different people have different body sizes. So it is important to normalize the skeleton data and make it body size and view angle invariant. In our work, the normalized relative 3D coordinates are used instead of the original absolute coordinates. In the original skeleton video data, each joint $i$ has three coordinates which can be represented as ${p_i}\left( t \right) = \left( {{x_i}\left( t\right),{y_i}\left( t \right),{z_i}\left( t \right)} \right)$ at frame $t$. Here, the \emph{hip center} is set as our new origin, and the new coordinates of other joints can be obtained from the difference between them and the hip center. In order to remove the body size variant, each new coordinates are normalized by the distance between the shoulder center joint and spline joint calculated using the original coordinates. Then the new coordinates of each joints $i$ can be expressed as
${{p'}_i}\left( t \right) = \left(
{\frac{{{x_i}\left( t \right) - {x_{hip}}\left( t
\right)}}{d},\frac{{{y_i}\left( t \right) - {y_{hip}}\left( t
\right)}}{d},\frac{{{z_i}\left( t \right) - {z_{hip}}\left( t \right)}}{d}}
\right)$.

\subsection{Covariance descriptor}
\label{OurCovDes}
The update process could face numerical instability. Hence all the covariance matrices are forced to be positive definite because conventional covariance matrices distance metric methods such as Affine-Invariant distance metric~\cite{pennec2006riemannian}, Log-Euclidean metric~\cite{arsigny2007geometric} and S-Divergence root~\cite{srapositive} could be unstable. Here, the simplest skeleton 3D coordinates were adopted as our feature, which is a $(K-1) \times 3$ dimension vector. $K$ is the joints number of each frame and the coordinates of normalized origin is $(0,0,0)$, which was removed from our final feature representation. In order to avoid singularity of the covariance matrices, a small perturbation is added to the covariance matrices~\cite{wang2012covariance}.

\subsection{Temporal weight and frame weight}
As illustrated in section~\ref{IncrementalCovarianceLearning},
the temporal weight of each frame should vary over time $t$;
and the frames from the current time should have higher temporal weights than
previous frames. Intuitively and based on these two points, the
power function is applied to compute our time weight. At time $t$, our temporal weight for
the $t_{th}$ frame is defined as
\begin{equation}\label{SpecificTimeW}
\omega _t^i = {\eta ^{t - i}},\;\;\;i \in [1,t],\;\eta  \in [0,1].
\end{equation}
We plot the temporal weight as it varies over time $t$ when
$\eta$ is set to 0.9 in Fig.~\ref{TimeWeight}. With the progression of time,
the temporal weight of past frames is effectively attenuated and
the most current frame is always assigned temporal weight of 1. Note that
other functions that satisfy the conditions in Eq.~\ref{TimeW} can be also used.

\begin{figure}
  \centering
  \subfigure[Temporal weight varies with different time ($\eta=0.9$).]{
    \label{TimeWeight} %% label for first subfigure
    \includegraphics[width=0.45\textwidth]{TimeWeightCurve.eps}}
  \subfigure[Frame weight of different frames in an action instance.]{
    \label{FrameWeight} %% label for second subfigure
    \includegraphics[width=0.45\textwidth]{FrameWeightCurve.eps}}
  \caption{Temporal weight and frame weight.}
  \label{Weight} %% label for entire figure
\end{figure}

If a frame is important to a specific action, it is usually
quite different to the neutral pose (the position of joints where the bones that make up the joints are placed in the optimal position for maximal movement). Therefore, we use the average relative
distance of each joint between current frame and manually selected neutral pose to represent the frame weight. As all the actions on the two datasets have similar neutral pose, we only selected once for the neutral pose.
Fig.~\ref{FrameWeight} shows an example and it can be seen that the most initial
frames and the ending frames have relatively smaller frame weight because these
frames are similar to natural pose. They contribute little to recognize an
action.

\subsection{Experimental Results on MSRC-12 Kinect Gesture Dataset}

The MSRC-12 Gesture dataset~\cite{fothergill2012instructing}
contains 594 sequences, and there are more than 700000 frames
collected from 30 people performing 12 classes of gestures.
The frames were manually labeled into 6244 gesture instances. Twenty human body
skeleton joints ($K=20$) were captured by the Microsoft Kinect system. The body
poses were captured at a sample rate of 30Hz with an accuracy approximately two
centimeters in joint positions. The 12 actions are: lift outstretched arms, duck, push right, goggles, wind it up, shoot, bow, throw, had enough, change weapon, beat both.
%\begin{table}[!htb]
%\caption{\textcolor[RGB]{255,0,0}{Comparison of rectangle window based average results of all the actions in MSRC-12 Gesture Dataset (W=35 \& W=50 represent different window sizes).}}
%\label{LMRResultsRectMSRC-12}\centering
%\begin{tabular}{|p{2.0cm}|p{0.8cm}|p{0.8cm}|p{0.8cm}|p{0.8cm}|p{0.8cm}|p{0.8cm}|p{0.8cm}|p{0.8cm}|p{0.8cm}|}
%%\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
%\hline
%\multirow{2}{*}{Evaluation} & \multicolumn{3}{c|}{Latency (\%)} & \multicolumn{3}{c|}{Miss rate (\%)} & \multicolumn{3}{c|}{Error rate (\%)}\\
%\cline{2-10}
%& W=35 & W=50 & Ours & W=35 & W=50 & Ours & W=35 & W=50 & Ours\\
%&&&&&&&&&\\
%\hline
%Average score & 41.3   &  52  & \textbf{29.0} &  15.2  & 20.7   & \textbf{9.4} &  57.4   &  63.8 & \textbf{51.6} \\
%\hline
%\end{tabular}
%\end{table}

%\begin{table*}[!htp]
%\caption{\textcolor[RGB]{255,0,0}{Average F-scores of the 5 modalities in MSRC-12 dataset.}}
%\label{Fscore}\centering
%\begin{tabular}{|c||c|c|c|c|c|c|}
%\hline
%\multirow{2}{*}{Modality} & \multirow{2}{*}{Baseline} \cite{fothergill2012instructing} & \multicolumn{5}{|c|}{Ours}\\
%\cline{3-7}
%&& No $\omega^i$ & No $\xi_i$ & 35 frames Window & 50 frames Window & With $\omega^i$ \& $\xi_i$ \\
%\hline
%Text  & 0.479 & -- & 0.554 & 0.525 & 0.502 & 0.606\\
%\hline
%Images & 0.549 & -- & 0.672 & 0.589 &0.574 & 0.695\\
%\hline
%Video & 0.627 & -- & 0.713 & 0.654 &0.647 & 0.764\\
%\hline
%Images+Text & 0.563 & -- & 0.687 &0.609 & 0.647& 0.726\\
%\hline
%Video + Text & 0.679 & -- & 0.694 & 0.683 & 0.681 & 0.717\\
%\hline
%\end{tabular}
%\end{table*}

In this dataset, the participants were provided with three
instruction modalities or their combination to perform
gestures in order to research various methods of teaching human on
how to perform different gestures. The three instruction modalities are i) text
descriptions, ii) image sequences, and iii) video demos. The two combination of
the three modalities are images with text and video with text. So the whole
dataset can be divided into 5 parts. For each part, the ``leave-person-out'' protocol
is used. Each time the training instances are used to calculate training
covariance matrices, and use these matrices to learn a projection matrix $P$ as
introduced in Section~\ref{Overview}. The training covariance matrices and
online updated covariance matrix are projected to a lower dimensional space by
the projection matrix $P$ before measuring their distances. In the testing
phase, $\eta$ is set to 0.95 to obtain the best result; the initial frame number
is 30. The action instances of the test subject are randomly stitched together
by each time. For each action in each modality, its average latency is obtained.
We finally average the 3 evaluation criteria on the 5 different modalities and the results
are shown in Table~\ref{LMRResultsMSRC-12Weight}. The results without temporal weight (NO $\omega^i$) or frame weight (NO $\xi_i$) are also presented. From Table~\ref{LMRResultsMSRC-12Weight}, it can be observed
that most action latency and miss rate are very low with our weighting function. This verifies the effectiveness of our temporal weight function. At a time, the current frame is assigned the maximum weight, when a new action is available, it can be quickly detected because the influence of past frames are diminished. The
comparison results between our methods and other methods are shown in Table~\ref{LMRResultsMSRC12Compare}, as can be seen, our method can perform better than other sliding window based methods.
\begin{table*}[!htb]
\caption{Average latency, miss rate and error rate with/without weighting of all the actions on MSRC-12 gesture dataset.}
\label{LMRResultsMSRC-12Weight}\centering
\begin{tabular}{|p{3.0cm}<{\centering}|p{0.6cm}<{\centering}|p{0.6cm}<{\centering}|p{1.0cm}<{\centering}|p{0.6cm}<{\centering}|p{0.6cm}<{\centering}|p{1.0cm}<{\centering}|p{0.6cm}<{\centering}|p{0.6cm}<{\centering}|p{1.0cm}<{\centering}|}
%\begin{tabular}{|p{0.2cm}|p{0.25cm}|p{0.9cm}|p{0.9cm}|p{0.9cm}|p{0.9cm}|p{0.9cm}|p{0.9cm}|p{0.9cm}|p{0.9cm}|p{0.9cm}|p{0.9cm}|p{0.9cm}|p{0.9cm}|p{0.9cm}|p{0.9cm}|}
%\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Evaluation} & \multicolumn{3}{c|}{Latency (\%)} & \multicolumn{3}{c|}{Miss rate (\%)} & \multicolumn{3}{c|}{Error rate (\%)}\\
\cline{2-10}
& No $\omega^i$ & No $\xi_i$ & With $\omega^i$\&$\xi_i$ & No $\omega^i$ & No $\xi_i$ & With $\omega^i$\&$\xi_i$ & No $\omega^i$ & No $\xi_i$ & With $\omega^i$\&$\xi_i$ \\
\hline
\textbf{Average score} &  --   &  31.6    & \textbf{29.0 }& 92 &  13.8   &  \textbf{9.4} &  91.7    & 54.1 &     \textbf{51.6}\\
\hline
\end{tabular}
\end{table*}

\begin{table*}[!htbp]
\caption{Comparison of our online action recognition results with~\cite{kviatkovsky2014online} and~\cite{hussein2013human} on  MSRC-12 gesture dataset.}
\label{LMRResultsMSRC12Compare}\centering
\begin{tabular}{|p{1.6cm}<{\centering}|p{0.5cm}<{\centering}|p{0.5cm}<{\centering}|p{0.6cm}<{\centering}|p{0.5cm}<{\centering}|p{0.5cm}<{\centering}|p{0.6cm}<{\centering}|p{0.5cm}<{\centering}|p{0.5cm}<{\centering}|p{0.6cm}<{\centering}|}
%\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{3}{*}{Evaluation} & \multicolumn{3}{c|}{Latency (\%)} & \multicolumn{3}{c|}{Miss rate (\%)} & \multicolumn{3}{c|}{Error rate (\%)}\\
\cline{2-10}
& \cite{kviatkovsky2014online} & \cite{hussein2013human} & Ours & \cite{kviatkovsky2014online} & \cite{hussein2013human} & Ours & \cite{kviatkovsky2014online} & \cite{hussein2013human} & Ours \\
\hline
Average & 52 & 41.3 & \textbf{29.0}  & 20.7 & 15.2 &  \textbf{9.4} & 63.8 & 57.4 & \textbf{51.6}\\
\hline
\end{tabular}
\end{table*}
%If no temporal weight function is used, the miss rate and error rate are very high because nearly all the actions can't be correctly detected. So computing the latency makes no sense, we don't give the latency in this case.

%In order to compare with the results of \cite{fothergill2012instructing},
%we also obtain the F-scores for each modality. The final F-score of each
%modality is an average over the stitched test sequences. We show the average
%F-score of each modality in Table.\ref{Fscore}. As can be seen, our method
%performs better.

%\textcolor[RGB]{255,0,0}{We also use sliding
%window based method to have a test. Table.\ref{LMRResultsRectMSRC-12} and Table.\ref{Fscore} also show the Superiority of our weighting based method.}


\subsection{Experimental Results on the Newly Collected Online Action Recognition Dataset}
As far as we know, there is nearly no essentially benchmark dataset for online action recognition. %Even the MSRC-12 gesture dataset we used in the previous subsection is not entirely appropriate for online action recognition because subjects were instructed to make a pause between each repetition of the gesture. In practical, human usually perform different actions continuously without pause. Taking this into account,
Thus we collected a dataset (Online Action3D\footnote{The dataset will be released with this paper.}) that human subjects perform actions continuously and naturally using the Microsoft Kinect V2.0~\texttrademark(25 human body skeleton joints are captured, $K=25$). The original actions of MSR Action3D dataset are used here. Firstly, 20 participants performed all the actions 5 or 6 times, and these samples are for training. Then, each participant performed the 20 actions continuously 1 or 2 times in random order, and these continuous action sequences are for online testing. The actions were captured at a sample rate of 20Hz. The ground truth of action segments were marked manually. Table~\ref{LMRResultsOnlineAction3D} gives the results with/without our weight function. As can be seen, experimental results on our new dataset also verify the efficacy of our method. Table~\ref{LMRResultsOnlineAction3DCompare} presents the comparison with other methods, it also shows the superiority of our method. Our method can handle the transition between any two different actions well.

%%With weighting function
\begin{table*}[!htb]
\caption{Average latency, miss rate and error rate with/without weighting of all the actions on Online Action3D dataset.}
\label{LMRResultsOnlineAction3D}\centering
\begin{tabular}{|p{3.0cm}<{\centering}|p{0.6cm}<{\centering}|p{0.6cm}<{\centering}|p{1.0cm}<{\centering}|p{0.6cm}<{\centering}|p{0.6cm}<{\centering}|p{1.0cm}<{\centering}|p{0.6cm}<{\centering}|p{0.6cm}<{\centering}|p{1.0cm}<{\centering}|}
%\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Evaluation} & \multicolumn{3}{c|}{Latency (\%)} & \multicolumn{3}{c|}{Miss rate (\%)} & \multicolumn{3}{c|}{Error rate (\%)}\\
\cline{2-10}
& No $\omega^i$ & No $\xi_i$ & With $\omega^i$\&$\xi_i$ & No $\omega^i$ & No $\xi_i$ & With $\omega^i$\&$\xi_i$ & No $\omega^i$ & No $\xi_i$ & With $\omega^i$\&$\xi_i$ \\
&&&&&&&&&\\
\hline
Average score & --   &  36.1  &  32.5 & 91.2  & 30.1  &  28.6  & 93.8  & 55.8  & 57.2 \\
\hline
\end{tabular}
\end{table*}

%\begin{table}[!htb]
%\caption{\textcolor[RGB]{255,0,0}{Comparison of rectangle window based average results of all the actions on Online Action3D dataset (W=35 \& W=50 represent different window sizes).}}
%\label{LMRResultsRectOnlineAction3D}\centering
%\begin{tabular}{|p{2.0cm}|p{0.8cm}|p{0.8cm}|p{0.8cm}|p{0.8cm}|p{0.8cm}|p{0.8cm}|p{0.8cm}|p{0.8cm}|p{0.8cm}|}
%%\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
%\hline
%\multirow{2}{*}{Evaluation} & \multicolumn{3}{c|}{Latency (\%)} & \multicolumn{3}{c|}{Miss rate (\%)} & \multicolumn{3}{c|}{Error rate (\%)}\\
%\cline{2-10}
%& W=35 & W=50 & Ours & W=35 & W=50 & Ours & W=35 & W=50 & Ours\\
%&&&&&&&&&\\
%\hline
%Average score & 40.6   &  34.7  & \textbf{29.0} &  42.3  & 49.5   & \textbf{9.4} &  61.8   &  67.4 & \textbf{51.6} \\
%\hline
%\end{tabular}
%\end{table}

\begin{table*}[!htbp]
\caption{Comparison of our online action recognition results with~\cite{kviatkovsky2014online} and~\cite{hussein2013human} on  Online Action3D dataset.}
\label{LMRResultsOnlineAction3DCompare}\centering
\begin{tabular}{|p{1.6cm}<{\centering}|p{0.5cm}<{\centering}|p{0.5cm}<{\centering}|p{0.6cm}<{\centering}|p{0.5cm}<{\centering}|p{0.5cm}<{\centering}|p{0.6cm}<{\centering}|p{0.5cm}<{\centering}|p{0.5cm}<{\centering}|p{0.6cm}<{\centering}|}
%\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{3}{*}{Evaluation} & \multicolumn{3}{c|}{Latency (\%)} & \multicolumn{3}{c|}{Miss rate (\%)} & \multicolumn{3}{c|}{Error rate (\%)}\\
\cline{2-10}
& \cite{kviatkovsky2014online} & \cite{hussein2013human} & Ours & \cite{kviatkovsky2014online} & \cite{hussein2013human} & Ours & \cite{kviatkovsky2014online} & \cite{hussein2013human} & Ours \\
\hline
Average & 40.6 & 34.7 & \textbf{32.5}  & 49.5 & 42.3 &  \textbf{28.6} & 67.4 & 61.8 & \textbf{57.2}\\
\hline
\end{tabular}
\end{table*}


\subsection{Efficiency of the Weighted Covariance Learning}
Note that our method only uses skeleton data and operates in near real-time. Fig.~\ref{RunTime} shows the running time of each frame (initial frame number is 30). As can be seen, if all the covariance matrices are computed in batch mode, the running time will grow rapidly with video frames. When it comes to a very long video, processing one frame will take too much time. In contrast, our online incrementally updating manner can maintain a constant time for each frame, no matter how long the video.
%In Kviatkovsky et al.'s work \cite{kviatkovsky2014online}, the RGB data is used and, the training and test actions are very simple. Furthermore
%the subjects in their training and test data are the same. The average
%latency of all actions in their work is 37\% and the average latency of 12
%actions is about 29\%, as shown in Table.\ref{LMRResultsMSRC-12Weight}. This implies that our method can quickly detect the action change. Our method does not address
%human-object interaction. Yu et al.~\cite{yu2014discriminative} address the
%human-object interaction recognition problem by combinng the skeleton
%feature and local occupancy pattern (LOP) \cite{wang2012mining} to obtain the
%object shape information from the depth video. Our ongoing work is
%exploring how to incorporate human-object interaction in our method.
\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{RunTime.png}\\
  \caption{Running time of our online updating mode and batch mode for each frame.}\label{RunTime}
\end{figure}

%------------------------------------------------------------------------
\section{Conclusion}\label{conclusion}
In this paper, we presented a fast online action recognition method based on incremental learning of weighted covariance descriptors. The covariance descriptor is robust to noise and can be updated efficiently using frame based features. We used a temporal weight function to effectively accumulate information with the evolution of time. In our current work, we adopted the average relative distance of each joint between current frame and neutral pose to represent frame weight, which suppresses the effect of the neutral poses and improve the recognition performance. In this paper, the neutral pose is selected manually, how to develop a neutral pose model which can recover the corresponding neutral pose from any frame is one of the future works. Another issue is that when some high dimensional feature vectors are used, the dimension of covariance matrices is very high, leading to the distance between covariance matrices not stable, and this fact may affect the final recognition accuracy. How to get an accurate metric (such as kernel methods) between two covariance matrices is also our future task.

%\end{document}  % This is where a 'short' article might terminate



\appendix
%%Appendix A
%\section{Headings in Appendices}

\section{Proof of the Lemmas}
\subsection{Proof of Lemma~\ref{LemmaSum}:}
\begin{equation*}
\begin{array}{l}
{{\hat \omega }_{t + 1}} = \sum {_{i = 1}^{t + 1}} {\xi _i}\omega _{t + 1}^i = \sum {_{i = 1}^t} {\xi _i}\omega _{t + 1}^i + {\xi _{t + 1}}\omega _{t + 1}^{t + 1}\\
\;\;\;\;\;\;\; = \eta \sum {_{i = 1}^t} {\xi _i}\omega _t^i + {\xi _{t + 1}}\\
\;\;\;\;\;\;\; = \eta {{\hat \omega }_t} + {\xi _{t + 1}}
\end{array}
\end{equation*}
and
\begin{equation*}
\begin{array}{l}
\tilde \omega _{t + 1}^2 = \sum {_{i = 1}^{t + 1}} {\left( {\frac{{{\xi _i}\omega _{t + 1}^i}}{{{{\hat \omega }_{t + 1}}}}} \right)^2} = \sum {_{i = 1}^t} {\left( {\frac{{{\xi _i}\omega _{t + 1}^i}}{{{{\hat \omega }_{t + 1}}}}} \right)^2} + {\left( {\frac{{{\xi _{t + 1}}}}{{{{\hat \omega }_{t + 1}}}}} \right)^2}\\
\;\;\;\;\;\;\; = \frac{1}{{{{\hat \omega }_{t + 1}}^2}}\left\{ {\sum {_{i = 1}^t} {{\left( {{\xi _i}\omega _{t + 1}^i} \right)}^2} + {\xi _{t + 1}}^2} \right\}\\
\;\;\;\;\;\;\; = \frac{1}{{{{\hat \omega }_{t + 1}}^2}}\left\{ {{\eta ^2}\sum {_{i = 1}^t} {{\left( {{\xi _i}\omega _t^i} \right)}^2} + {\xi _{t + 1}}^2} \right\}\\
\;\;\;\;\;\;\; = \frac{1}{{{{\hat \omega }_{t + 1}}^2}}\left\{ {{\eta ^2}{{\hat \omega }_t}^2\sum {_{i = 1}^t} {{\left( {\frac{{{\xi _i}\omega _t^i}}{{{{\hat \omega }_t}}}} \right)}^2} + {\xi _{t + 1}}^2} \right\}\\
\;\;\;\;\;\;\; = \frac{1}{{{{\hat \omega }_{t + 1}}^2}}\left\{ {{\eta ^2}{{\hat \omega }_t}^2\tilde \omega _t^2 + {\xi _{t + 1}}^2} \right\}\\
\;\;\;\;\;\;\; = \frac{{{\eta ^2}{{\hat \omega }_t}^2\tilde \omega _t^2 + {\xi _{t + 1}}^2}}{{{{\left( {\eta {{\hat \omega }_t} + {\xi _{t + 1}}} \right)}^2}}}
\end{array}
\end{equation*}
\subsection{Proof of Lemma~\ref{Transpose}:}
\begin{equation*}
\begin{array}{l}
\sum {_{i = 1}^t{\xi _i}\omega _{t + 1}^i({f_i} - {\mu _t})}  = \sum {_{i = 1}^t{\xi _i}\omega _{t + 1}^i{f_i}}  - \sum {_{i = 1}^t{\xi _i}\omega _{t + 1}^i{\mu _t}} \\
\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; = \eta \sum {_{i = 1}^t{\xi _i}\omega _t^i{f_i} - \eta \sum {_{i = 1}^t{\xi _i}\omega _t^i{\mu _t}} } \\
\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; = \eta {\mu _t}{{\hat \omega }_t} - \eta {\mu _t}{{\hat \omega }_t}\\
\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; = 0,
\end{array}
\end{equation*}
and
\begin{equation*}{\sum {_{i = 1}^t{\xi _i}\omega _{t + 1}^i({f_i} - {\mu _t})} ^T} = {\left\{ {\sum {_{i = 1}^t{\xi _i}\omega _{t + 1}^i({f_i} - {\mu _t})} } \right\}^T} = 0.
\end{equation*}
\subsection{Proof of Lemma~\ref{TransposeMul}:}
\begin{equation*}
\begin{array}{l}
{\mu _t} - {\mu _{t + 1}} = {\mu _t} - \frac{{\eta {{\hat \omega }_t}{\mu
_t}{\rm{ + }}{\xi _{t + 1}}{f_{t + 1}}}}{{\eta {{\hat \omega }_t}{\rm{ + }}{\xi
_{t + 1}}}}\footnote{6}\\
\;\;\;\;\;\;\;\;\;\;\;\;\;\; = \frac{{{\xi _{t + 1}}\left( {{\mu _t} - {f_{t + 1}}} \right)}}{{\eta {{\hat \omega }_t}{\rm{ + }}{\xi _{t+ 1}}}}.
\end{array}
\footnotetext[6]{Using definition by Eq.~\ref{IncrementalMeanRule}.}
\end{equation*}
\subsection{Proof of Lemma~\ref{FeatureAndMuTransposeMul}:}
\begin{equation*}
\begin{array}{l}
\sum {_{i = 1}^t{\xi _i}\omega _{t + 1}^i({f_i} - {\mu _{t + 1}}){{({f_i} - {\mu _{t + 1}})}^T}}\\
= \sum {_{i = 1}^t{\xi _i}\omega _{t + 1}^i({f_i} - {\mu _t} + {\mu _t} - {\mu _{t + 1}}){{({f_i} - {\mu _t} + {\mu _t} - {\mu _{t + 1}})}^T}} \\
= \sum {_{i = 1}^t{\xi _i}\omega _{t + 1}^i({f_i} - {\mu _t}){{({f_i} - {\mu _t})}^T}} \\
\;\;\;\; + \left\{ {\sum {_{i = 1}^t{\xi _i}\omega _{t + 1}^i({f_i} - {\mu _t})} } \right\}{({\mu _t} - {\mu _{t + 1}})^T}\\
\;\;\;\;+ \left\{ {\sum {_{i = 1}^t{\xi _i}\omega _{t + 1}^i{{({f_i} - {\mu _t})}^T}} } \right\}({\mu _t} - {\mu _{t + 1}})\\
\;\;\;\; + \left\{ {({\mu _t} - {\mu _{t + 1}}){{({\mu _t} - {\mu _{t + 1}})}^T}} \right\}\sum {_{i = 1}^t{\xi _i}\omega _{t + 1}^i} \\
= \eta \sum {_{i = 1}^t{\xi _i}\omega _t^i({f_i} - {\mu _t}){{({f_i} - {\mu _t})}^T}} \;\;\;\\
\;\;\;\; + \frac{{\eta {{\hat \omega }_t}{\xi _{t + 1}}^2\left( {{\mu _t} -
{f_{t + 1}}} \right){{\left( {{\mu _t} - {f_{t + 1}}} \right)}^T}}}{{{{\left(
{\eta {{\hat \omega }_t}{\rm{ + }}{\xi _{t + 1}}}
\right)}^2}}}\footnote{7}\\
= \eta \left( {1 - \tilde \omega _t^2} \right){{\hat \omega }_t}{C_t} +
\frac{{\eta {{\hat \omega }_t}{\xi _{t + 1}}^2\left( {{\mu _t} - {f_{t + 1}}}
\right){{\left( {{\mu _t} - {f_{t + 1}}} \right)}^T}}}{{{{\left( {\eta {{\hat
\omega }_t}{\rm{ + }}{\xi _{t + 1}}} \right)}^2}}}\footnote{8}.
\end{array}
\footnotetext[7]{Using Lemma~\ref{Transpose} and Lemma~\ref{TransposeMul}.}
\footnotetext[8]{Using Eq.~\ref{CovT}.}
\end{equation*}

%\begin{acks}
%
%\end{acks}


%%% -*-BibTeX-*-
%%% Do NOT edit. File created by BibTeX with style
%%% ACM-Reference-Format-Journals [18-Jan-2012].

\begin{thebibliography}{00}

%%% ====================================================================
%%% NOTE TO THE USER: you can override these defaults by providing
%%% customized versions of any of these macros before the \bibliography
%%% command.  Each of them MUST provide its own final punctuation,
%%% except for \shownote{}, \showDOI{}, and \showURL{}.  The latter two
%%% do not use final punctuation, in order to avoid confusing it with
%%% the Web address.
%%%
%%% To suppress output of a particular field, define its macro to expand
%%% to an empty string, or better, \unskip, like this:
%%%
%%% \newcommand{\showDOI}[1]{\unskip}   % LaTeX syntax
%%%
%%% \def \showDOI #1{\unskip}           % plain TeX syntax
%%%
%%% ====================================================================

\ifx \showCODEN    \undefined \def \showCODEN     #1{\unskip}     \fi
\ifx \showDOI      \undefined \def \showDOI       #1{{\tt DOI:}\penalty0{#1}\ }
  \fi
\ifx \showISBNx    \undefined \def \showISBNx     #1{\unskip}     \fi
\ifx \showISBNxiii \undefined \def \showISBNxiii  #1{\unskip}     \fi
\ifx \showISSN     \undefined \def \showISSN      #1{\unskip}     \fi
\ifx \showLCCN     \undefined \def \showLCCN      #1{\unskip}     \fi
\ifx \shownote     \undefined \def \shownote      #1{#1}          \fi
\ifx \showarticletitle \undefined \def \showarticletitle #1{#1}   \fi
\ifx \showURL      \undefined \def \showURL       #1{#1}          \fi
% The following commands are used for tagged output and should be
% invisible to TeX
\providecommand\bibfield[2]{#2}
\providecommand\bibinfo[2]{#2}
\providecommand\natexlab[1]{#1}
\providecommand\showeprint[2][]{arXiv:#2}

\bibitem[\protect\citeauthoryear{Aggarwal and Xia}{Aggarwal and Xia}{2014}]%
        {aggarwal2014human}
\bibfield{author}{\bibinfo{person}{JK Aggarwal} {and} \bibinfo{person}{Lu
  Xia}.} \bibinfo{year}{2014}\natexlab{}.
\newblock \showarticletitle{Human activity recognition from 3d data: A review}.
\newblock \bibinfo{journal}{{\em Pattern Recognition Letters\/}}
  \bibinfo{volume}{48} (\bibinfo{year}{2014}), \bibinfo{pages}{70--80}.
\newblock


\bibitem[\protect\citeauthoryear{Aggarwal and Ryoo}{Aggarwal and Ryoo}{2011}]%
        {aggarwal2011human}
\bibfield{author}{\bibinfo{person}{Jake~K Aggarwal} {and}
  \bibinfo{person}{Michael~S Ryoo}.} \bibinfo{year}{2011}\natexlab{}.
\newblock \showarticletitle{Human activity analysis: A review}.
\newblock \bibinfo{journal}{{\em ACM Computing Surveys (CSUR)\/}}
  \bibinfo{volume}{43}, \bibinfo{number}{3} (\bibinfo{year}{2011}),
  \bibinfo{pages}{16}.
\newblock


\bibitem[\protect\citeauthoryear{Arsigny, Fillard, Pennec, and Ayache}{Arsigny
  et~al\mbox{.}}{2007}]%
        {arsigny2007geometric}
\bibfield{author}{\bibinfo{person}{Vincent Arsigny}, \bibinfo{person}{Pierre
  Fillard}, \bibinfo{person}{Xavier Pennec}, {and} \bibinfo{person}{Nicholas
  Ayache}.} \bibinfo{year}{2007}\natexlab{}.
\newblock \showarticletitle{Geometric means in a novel vector space structure
  on symmetric positive-definite matrices}.
\newblock \bibinfo{journal}{{\em SIAM journal on matrix analysis and
  applications\/}} \bibinfo{volume}{29}, \bibinfo{number}{1}
  (\bibinfo{year}{2007}), \bibinfo{pages}{328--347}.
\newblock


\bibitem[\protect\citeauthoryear{Bloom, Argyriou, and Makris}{Bloom
  et~al\mbox{.}}{2016}]%
        {bloom2016hierarchical}
\bibfield{author}{\bibinfo{person}{Victoria Bloom}, \bibinfo{person}{Vasileios
  Argyriou}, {and} \bibinfo{person}{Dimitrios Makris}.}
  \bibinfo{year}{2016}\natexlab{}.
\newblock \showarticletitle{Hierarchical transfer learning for online
  recognition of compound actions}.
\newblock \bibinfo{journal}{{\em Computer Vision and Image Understanding\/}}
  \bibinfo{volume}{144} (\bibinfo{year}{2016}), \bibinfo{pages}{62--72}.
\newblock


\bibitem[\protect\citeauthoryear{De~Rosa, Cesa-Bianchi, Gori, and
  Cuzzolin}{De~Rosa et~al\mbox{.}}{2014}]%
        {DeRosa2014}
\bibfield{author}{\bibinfo{person}{Rocco De~Rosa}, \bibinfo{person}{Nicol
  Cesa-Bianchi}, \bibinfo{person}{Ilaria Gori}, {and} \bibinfo{person}{Fabio
  Cuzzolin}.} \bibinfo{year}{2014}\natexlab{}.
\newblock \showarticletitle{Online Action Recognition via Nonparametric
  Incremental Learning}. In \bibinfo{booktitle}{{\em BMVC}}.
  \bibinfo{publisher}{BMVA Press}.
\newblock


\bibitem[\protect\citeauthoryear{Donahue, Anne~Hendricks, Guadarrama, Rohrbach,
  Venugopalan, Saenko, and Darrell}{Donahue et~al\mbox{.}}{2015}]%
        {donahue2015long}
\bibfield{author}{\bibinfo{person}{Jeffrey Donahue}, \bibinfo{person}{Lisa
  Anne~Hendricks}, \bibinfo{person}{Sergio Guadarrama}, \bibinfo{person}{Marcus
  Rohrbach}, \bibinfo{person}{Subhashini Venugopalan}, \bibinfo{person}{Kate
  Saenko}, {and} \bibinfo{person}{Trevor Darrell}.}
  \bibinfo{year}{2015}\natexlab{}.
\newblock \showarticletitle{Long-term recurrent convolutional networks for
  visual recognition and description}. In \bibinfo{booktitle}{{\em CVPR}}.
\newblock


\bibitem[\protect\citeauthoryear{Du, Wang, and Wang}{Du et~al\mbox{.}}{2015}]%
        {du2015hierarchical}
\bibfield{author}{\bibinfo{person}{Yong Du}, \bibinfo{person}{Wei Wang}, {and}
  \bibinfo{person}{Liang Wang}.} \bibinfo{year}{2015}\natexlab{}.
\newblock \showarticletitle{Hierarchical Recurrent Neural Network for Skeleton
  Based Action Recognition}. In \bibinfo{booktitle}{{\em CVPR}}.
\newblock


\bibitem[\protect\citeauthoryear{Ellis, Masood, Tappen, Laviola~Jr, and
  Sukthankar}{Ellis et~al\mbox{.}}{2013}]%
        {ellis2013exploring}
\bibfield{author}{\bibinfo{person}{Chris Ellis}, \bibinfo{person}{Syed~Zain
  Masood}, \bibinfo{person}{Marshall~F Tappen}, \bibinfo{person}{Joseph~J
  Laviola~Jr}, {and} \bibinfo{person}{Rahul Sukthankar}.}
  \bibinfo{year}{2013}\natexlab{}.
\newblock \showarticletitle{Exploring the trade-off between accuracy and
  observational latency in action recognition}.
\newblock \bibinfo{journal}{{\em International Journal of Computer Vision\/}}
  \bibinfo{volume}{101}, \bibinfo{number}{3} (\bibinfo{year}{2013}),
  \bibinfo{pages}{420--436}.
\newblock


\bibitem[\protect\citeauthoryear{Fanello, Gori, Metta, and Odone}{Fanello
  et~al\mbox{.}}{2013}]%
        {fanello2013keep}
\bibfield{author}{\bibinfo{person}{Sean~Ryan Fanello}, \bibinfo{person}{Ilaria
  Gori}, \bibinfo{person}{Giorgio Metta}, {and} \bibinfo{person}{Francesca
  Odone}.} \bibinfo{year}{2013}\natexlab{}.
\newblock \showarticletitle{Keep it simple and sparse: Real-time action
  recognition}.
\newblock \bibinfo{journal}{{\em The Journal of Machine Learning Research\/}}
  \bibinfo{volume}{14}, \bibinfo{number}{1} (\bibinfo{year}{2013}),
  \bibinfo{pages}{2617--2640}.
\newblock


\bibitem[\protect\citeauthoryear{Fothergill, Mentis, Kohli, and
  Nowozin}{Fothergill et~al\mbox{.}}{2012}]%
        {fothergill2012instructing}
\bibfield{author}{\bibinfo{person}{Simon Fothergill}, \bibinfo{person}{Helena
  Mentis}, \bibinfo{person}{Pushmeet Kohli}, {and} \bibinfo{person}{Sebastian
  Nowozin}.} \bibinfo{year}{2012}\natexlab{}.
\newblock \showarticletitle{Instructing people for training gestural
  interactive systems}. In \bibinfo{booktitle}{{\em SIGCHI}}. ACM,
  \bibinfo{pages}{1737--1746}.
\newblock


\bibitem[\protect\citeauthoryear{Harandi, Salzmann, and Hartley}{Harandi
  et~al\mbox{.}}{2014}]%
        {harandi2014manifold}
\bibfield{author}{\bibinfo{person}{Mehrtash~T Harandi},
  \bibinfo{person}{Mathieu Salzmann}, {and} \bibinfo{person}{Richard Hartley}.}
  \bibinfo{year}{2014}\natexlab{}.
\newblock \showarticletitle{From manifold to manifold: geometry-aware
  dimensionality reduction for SPD matrices}.
\newblock In \bibinfo{booktitle}{{\em ECCV}}. \bibinfo{publisher}{Springer},
  \bibinfo{pages}{17--32}.
\newblock


\bibitem[\protect\citeauthoryear{Harandi, Sanderson, Wiliem, and
  Lovell}{Harandi et~al\mbox{.}}{2012}]%
        {harandi2012kernel}
\bibfield{author}{\bibinfo{person}{Mehrtash~Tafazzoli Harandi},
  \bibinfo{person}{Conrad Sanderson}, \bibinfo{person}{Arnold Wiliem}, {and}
  \bibinfo{person}{Brian~C Lovell}.} \bibinfo{year}{2012}\natexlab{}.
\newblock \showarticletitle{Kernel analysis over Riemannian manifolds for
  visual recognition of actions, pedestrians and textures}. In
  \bibinfo{booktitle}{{\em WACV}}. IEEE, \bibinfo{pages}{433--439}.
\newblock


\bibitem[\protect\citeauthoryear{Hasan and Roy-Chowdhury}{Hasan and
  Roy-Chowdhury}{2014}]%
        {hasan2014continuous}
\bibfield{author}{\bibinfo{person}{Mahmudul Hasan} {and}
  \bibinfo{person}{Amit~K Roy-Chowdhury}.} \bibinfo{year}{2014}\natexlab{}.
\newblock \showarticletitle{Continuous Learning of Human Activity Models using
  Deep Nets}.
\newblock In \bibinfo{booktitle}{{\em ECCV}}. \bibinfo{publisher}{Springer},
  \bibinfo{pages}{705--720}.
\newblock


\bibitem[\protect\citeauthoryear{Hoai, Lan, and De~la Torre}{Hoai
  et~al\mbox{.}}{2011}]%
        {hoai2011joint}
\bibfield{author}{\bibinfo{person}{Minh Hoai}, \bibinfo{person}{Zhen-Zhong
  Lan}, {and} \bibinfo{person}{Fernando De~la Torre}.}
  \bibinfo{year}{2011}\natexlab{}.
\newblock \showarticletitle{Joint segmentation and classification of human
  actions in video}. In \bibinfo{booktitle}{{\em CVPR}}.
\newblock


\bibitem[\protect\citeauthoryear{Hu, Yuan, and Wu}{Hu et~al\mbox{.}}{2016}]%
        {hu2016discriminative}
\bibfield{author}{\bibinfo{person}{Bo Hu}, \bibinfo{person}{Junsong Yuan},
  {and} \bibinfo{person}{Yuwei Wu}.} \bibinfo{year}{2016}\natexlab{}.
\newblock \showarticletitle{Discriminative Action States Discovery for Online
  Action Recognition}.
\newblock \bibinfo{journal}{{\em IEEE Signal Processing Letters\/}}
  \bibinfo{volume}{23}, \bibinfo{number}{10} (\bibinfo{year}{2016}),
  \bibinfo{pages}{1374--1378}.
\newblock


\bibitem[\protect\citeauthoryear{Hussein, Torki, Gowayyed, and
  El-Saban}{Hussein et~al\mbox{.}}{2013}]%
        {hussein2013human}
\bibfield{author}{\bibinfo{person}{Mohamed~E Hussein}, \bibinfo{person}{Marwan
  Torki}, \bibinfo{person}{Mohammad~Abdelaziz Gowayyed}, {and}
  \bibinfo{person}{Motaz El-Saban}.} \bibinfo{year}{2013}\natexlab{}.
\newblock \showarticletitle{Human Action Recognition Using a Temporal Hierarchy
  of Covariance Descriptors on 3D Joint Locations}. In \bibinfo{booktitle}{{\em
  IJCAI}}. AAAI press, \bibinfo{pages}{2466--2472}.
\newblock


\bibitem[\protect\citeauthoryear{Jiang, Zhong, Peng, and Qin}{Jiang
  et~al\mbox{.}}{2014}]%
        {jiang2014online}
\bibfield{author}{\bibinfo{person}{Xinbo Jiang}, \bibinfo{person}{Fan Zhong},
  \bibinfo{person}{Qunsheng Peng}, {and} \bibinfo{person}{Xueying Qin}.}
  \bibinfo{year}{2014}\natexlab{}.
\newblock \showarticletitle{Online robust action recognition based on a
  hierarchical model}.
\newblock \bibinfo{journal}{{\em The Visual Computer\/}} \bibinfo{volume}{30},
  \bibinfo{number}{9} (\bibinfo{year}{2014}), \bibinfo{pages}{1021--1033}.
\newblock


\bibitem[\protect\citeauthoryear{Kulkarni, Evangelidis, Cech, and
  Horaud}{Kulkarni et~al\mbox{.}}{2014}]%
        {kulkarni2014continuous}
\bibfield{author}{\bibinfo{person}{Kaustubh Kulkarni},
  \bibinfo{person}{Georgios Evangelidis}, \bibinfo{person}{Jan Cech}, {and}
  \bibinfo{person}{Radu Horaud}.} \bibinfo{year}{2014}\natexlab{}.
\newblock \showarticletitle{Continuous action recognition based on sequence
  alignment}.
\newblock \bibinfo{journal}{{\em International Journal of Computer Vision\/}}
  (\bibinfo{year}{2014}), \bibinfo{pages}{1--25}.
\newblock


\bibitem[\protect\citeauthoryear{Kviatkovsky, Rivlin, and
  Shimshoni}{Kviatkovsky et~al\mbox{.}}{2014}]%
        {kviatkovsky2014online}
\bibfield{author}{\bibinfo{person}{Igor Kviatkovsky}, \bibinfo{person}{Ehud
  Rivlin}, {and} \bibinfo{person}{Ilan Shimshoni}.}
  \bibinfo{year}{2014}\natexlab{}.
\newblock \showarticletitle{Online action recognition using covariance of shape
  and motion}.
\newblock \bibinfo{journal}{{\em Computer Vision and Image Understanding\/}}
  \bibinfo{volume}{129} (\bibinfo{year}{2014}), \bibinfo{pages}{15--26}.
\newblock


\bibitem[\protect\citeauthoryear{Li, Zhang, and Liu}{Li et~al\mbox{.}}{2010}]%
        {li2010action}
\bibfield{author}{\bibinfo{person}{Wanqing Li}, \bibinfo{person}{Zhengyou
  Zhang}, {and} \bibinfo{person}{Zicheng Liu}.}
  \bibinfo{year}{2010}\natexlab{}.
\newblock \showarticletitle{Action recognition based on a bag of 3d points}. In
  \bibinfo{booktitle}{{\em CVPR Workshop}}. IEEE, \bibinfo{pages}{9--14}.
\newblock


\bibitem[\protect\citeauthoryear{Minhas, Mohammed, and Wu}{Minhas
  et~al\mbox{.}}{2012}]%
        {minhas2012incremental}
\bibfield{author}{\bibinfo{person}{Rashid Minhas}, \bibinfo{person}{Abdul~Adeel
  Mohammed}, {and} \bibinfo{person}{QM~Jonathan Wu}.}
  \bibinfo{year}{2012}\natexlab{}.
\newblock \showarticletitle{Incremental learning in human action recognition
  based on snippets}.
\newblock \bibinfo{journal}{{\em Circuits and Systems for Video Technology,
  IEEE Transactions on\/}} \bibinfo{volume}{22}, \bibinfo{number}{11}
  (\bibinfo{year}{2012}), \bibinfo{pages}{1529--1541}.
\newblock


\bibitem[\protect\citeauthoryear{Miranda, Vieira, Mart{\'\i}nez, Lewiner,
  Vieira, and Campos}{Miranda et~al\mbox{.}}{2014}]%
        {miranda2014online}
\bibfield{author}{\bibinfo{person}{Leandro Miranda}, \bibinfo{person}{Thales
  Vieira}, \bibinfo{person}{Dimas Mart{\'\i}nez}, \bibinfo{person}{Thomas
  Lewiner}, \bibinfo{person}{Antonio~W Vieira}, {and} \bibinfo{person}{Mario~FM
  Campos}.} \bibinfo{year}{2014}\natexlab{}.
\newblock \showarticletitle{Online gesture recognition from pose kernel
  learning and decision forests}.
\newblock \bibinfo{journal}{{\em Pattern Recognition Letters\/}}
  \bibinfo{volume}{39} (\bibinfo{year}{2014}), \bibinfo{pages}{65--73}.
\newblock


\bibitem[\protect\citeauthoryear{Oreifej and Liu}{Oreifej and Liu}{2013}]%
        {oreifej2013hon4d}
\bibfield{author}{\bibinfo{person}{Omar Oreifej} {and} \bibinfo{person}{Zicheng
  Liu}.} \bibinfo{year}{2013}\natexlab{}.
\newblock \showarticletitle{Hon4d: Histogram of oriented 4d normals for
  activity recognition from depth sequences}. In \bibinfo{booktitle}{{\em
  CVPR}}. IEEE, \bibinfo{pages}{716--723}.
\newblock


\bibitem[\protect\citeauthoryear{Pennec, Fillard, and Ayache}{Pennec
  et~al\mbox{.}}{2006}]%
        {pennec2006riemannian}
\bibfield{author}{\bibinfo{person}{Xavier Pennec}, \bibinfo{person}{Pierre
  Fillard}, {and} \bibinfo{person}{Nicholas Ayache}.}
  \bibinfo{year}{2006}\natexlab{}.
\newblock \showarticletitle{A Riemannian framework for tensor computing}.
\newblock \bibinfo{journal}{{\em International Journal of Computer Vision\/}}
  \bibinfo{volume}{66}, \bibinfo{number}{1} (\bibinfo{year}{2006}),
  \bibinfo{pages}{41--66}.
\newblock


\bibitem[\protect\citeauthoryear{Price}{Price}{1972}]%
        {price1972extension}
\bibfield{author}{\bibinfo{person}{George~R Price}.}
  \bibinfo{year}{1972}\natexlab{}.
\newblock \showarticletitle{Extension of covariance selection mathematics}.
\newblock \bibinfo{journal}{{\em Annals of human genetics\/}}
  \bibinfo{volume}{35}, \bibinfo{number}{4} (\bibinfo{year}{1972}),
  \bibinfo{pages}{485--490}.
\newblock


\bibitem[\protect\citeauthoryear{Rahmani, Mahmood, Huynh, and Mian}{Rahmani
  et~al\mbox{.}}{2014}]%
        {rahmani2014hopc}
\bibfield{author}{\bibinfo{person}{Hossein Rahmani}, \bibinfo{person}{Arif
  Mahmood}, \bibinfo{person}{Du~Q Huynh}, {and} \bibinfo{person}{Ajmal Mian}.}
  \bibinfo{year}{2014}\natexlab{}.
\newblock \showarticletitle{HOPC: Histogram of Oriented Principal Components of
  3D Pointclouds for Action Recognition}.
\newblock In \bibinfo{booktitle}{{\em ECCV}}. \bibinfo{publisher}{Springer},
  \bibinfo{pages}{742--757}.
\newblock


\bibitem[\protect\citeauthoryear{Sanin, Sanderson, Harandi, and Lovell}{Sanin
  et~al\mbox{.}}{2013}]%
        {sanin2013spatio}
\bibfield{author}{\bibinfo{person}{Andres Sanin}, \bibinfo{person}{Conrad
  Sanderson}, \bibinfo{person}{Mehrtash~Tafazzoli Harandi}, {and}
  \bibinfo{person}{Brian~C Lovell}.} \bibinfo{year}{2013}\natexlab{}.
\newblock \showarticletitle{Spatio-temporal covariance descriptors for action
  and gesture recognition}. In \bibinfo{booktitle}{{\em WACV}}. IEEE,
  \bibinfo{pages}{103--110}.
\newblock


\bibitem[\protect\citeauthoryear{Shahroudy, Liu, Ng, and Wang}{Shahroudy
  et~al\mbox{.}}{2016}]%
        {shahroudy2016ntu}
\bibfield{author}{\bibinfo{person}{Amir Shahroudy}, \bibinfo{person}{Jun Liu},
  \bibinfo{person}{Tian-Tsong Ng}, {and} \bibinfo{person}{Gang Wang}.}
  \bibinfo{year}{2016}\natexlab{}.
\newblock \showarticletitle{{NTU RGB+ D}: A Large Scale Dataset for {3D} Human
  Activity Analysis}. In \bibinfo{booktitle}{{\em CVPR}}.
\newblock


\bibitem[\protect\citeauthoryear{Shotton, Sharp, Kipman, Fitzgibbon, Finocchio,
  Blake, Cook, and Moore}{Shotton et~al\mbox{.}}{2013}]%
        {shotton2013real}
\bibfield{author}{\bibinfo{person}{Jamie Shotton}, \bibinfo{person}{Toby
  Sharp}, \bibinfo{person}{Alex Kipman}, \bibinfo{person}{Andrew Fitzgibbon},
  \bibinfo{person}{Mark Finocchio}, \bibinfo{person}{Andrew Blake},
  \bibinfo{person}{Mat Cook}, {and} \bibinfo{person}{Richard Moore}.}
  \bibinfo{year}{2013}\natexlab{}.
\newblock \showarticletitle{Real-time human pose recognition in parts from
  single depth images}.
\newblock \bibinfo{journal}{{\it Commun. ACM}} \bibinfo{volume}{56},
  \bibinfo{number}{1} (\bibinfo{year}{2013}), \bibinfo{pages}{116--124}.
\newblock


\bibitem[\protect\citeauthoryear{Sra}{Sra}{2011}]%
        {srapositive}
\bibfield{author}{\bibinfo{person}{Suvrit Sra}.}
  \bibinfo{year}{2011}\natexlab{}.
\newblock \showarticletitle{Positive definite matrices and the symmetric Stein
  divergence}.
\newblock \bibinfo{journal}{{\em preprint\/}} (\bibinfo{year}{2011}).
\newblock


\bibitem[\protect\citeauthoryear{Tranter and Reynolds}{Tranter and
  Reynolds}{2006}]%
        {tranter2006overview}
\bibfield{author}{\bibinfo{person}{Sue~E Tranter} {and}
  \bibinfo{person}{Douglas~A Reynolds}.} \bibinfo{year}{2006}\natexlab{}.
\newblock \showarticletitle{An overview of automatic speaker diarization
  systems}.
\newblock \bibinfo{journal}{{\em Audio, Speech, and Language Processing, IEEE
  Transactions on\/}} \bibinfo{volume}{14}, \bibinfo{number}{5}
  (\bibinfo{year}{2006}), \bibinfo{pages}{1557--1565}.
\newblock


\bibitem[\protect\citeauthoryear{Vemulapalli, Arrate, and
  Chellappa}{Vemulapalli et~al\mbox{.}}{2014}]%
        {vemulapalli2014human}
\bibfield{author}{\bibinfo{person}{Raviteja Vemulapalli},
  \bibinfo{person}{Felipe Arrate}, {and} \bibinfo{person}{Rama Chellappa}.}
  \bibinfo{year}{2014}\natexlab{}.
\newblock \showarticletitle{Human action recognition by representing 3d
  skeletons as points in a lie group}. In \bibinfo{booktitle}{{\em CVPR}}.
  IEEE, \bibinfo{pages}{588--595}.
\newblock


\bibitem[\protect\citeauthoryear{Vieira, Nascimento, Oliveira, Liu, and
  Campos}{Vieira et~al\mbox{.}}{2014}]%
        {vieira2014improvement}
\bibfield{author}{\bibinfo{person}{Antonio~W Vieira},
  \bibinfo{person}{Erickson~R Nascimento}, \bibinfo{person}{Gabriel~L
  Oliveira}, \bibinfo{person}{Zicheng Liu}, {and} \bibinfo{person}{Mario~FM
  Campos}.} \bibinfo{year}{2014}\natexlab{}.
\newblock \showarticletitle{On the improvement of human action recognition from
  depth map sequences using Space--Time Occupancy Patterns}.
\newblock \bibinfo{journal}{{\em Pattern Recognition Letters\/}}
  \bibinfo{volume}{36} (\bibinfo{year}{2014}), \bibinfo{pages}{221--227}.
\newblock


\bibitem[\protect\citeauthoryear{Wang, Liu, Chorowski, Chen, and Wu}{Wang
  et~al\mbox{.}}{2012}]%
        {wang2012robust}
\bibfield{author}{\bibinfo{person}{Jiang Wang}, \bibinfo{person}{Zicheng Liu},
  \bibinfo{person}{Jan Chorowski}, \bibinfo{person}{Zhuoyuan Chen}, {and}
  \bibinfo{person}{Ying Wu}.} \bibinfo{year}{2012}\natexlab{}.
\newblock \showarticletitle{Robust 3d action recognition with random occupancy
  patterns}.
\newblock In \bibinfo{booktitle}{{\em ECCV}}. \bibinfo{publisher}{Springer},
  \bibinfo{pages}{872--885}.
\newblock


\bibitem[\protect\citeauthoryear{Wang, Li, Gao, Tang, Zhang, and Ogunbona}{Wang
  et~al\mbox{.}}{2015}]%
        {wang2015convnets}
\bibfield{author}{\bibinfo{person}{Pichao Wang}, \bibinfo{person}{Wanqing Li},
  \bibinfo{person}{Zhimin Gao}, \bibinfo{person}{Chang Tang},
  \bibinfo{person}{Jing Zhang}, {and} \bibinfo{person}{Philip Ogunbona}.}
  \bibinfo{year}{2015}\natexlab{}.
\newblock \showarticletitle{Convnets-based action recognition from depth maps
  through virtual cameras and pseudocoloring}. In \bibinfo{booktitle}{{\em
  Proceedings of the 23rd ACM international conference on Multimedia}}. ACM,
  \bibinfo{pages}{1119--1122}.
\newblock


\bibitem[\protect\citeauthoryear{Wang, Li, Gao, Zhang, Tang, and Ogunbona}{Wang
  et~al\mbox{.}}{2016a}]%
        {wang2016action}
\bibfield{author}{\bibinfo{person}{Pichao Wang}, \bibinfo{person}{Wanqing Li},
  \bibinfo{person}{Zhimin Gao}, \bibinfo{person}{Jing Zhang},
  \bibinfo{person}{Chang Tang}, {and} \bibinfo{person}{Philip~O Ogunbona}.}
  \bibinfo{year}{2016}\natexlab{a}.
\newblock \showarticletitle{Action recognition from depth maps using deep
  convolutional neural networks}.
\newblock \bibinfo{journal}{{\em IEEE Transactions on Human-Machine Systems\/}}
  \bibinfo{volume}{46}, \bibinfo{number}{4} (\bibinfo{year}{2016}),
  \bibinfo{pages}{498--509}.
\newblock


\bibitem[\protect\citeauthoryear{Wang, Li, Hou, and Li}{Wang
  et~al\mbox{.}}{2016b}]%
        {wang2016mm}
\bibfield{author}{\bibinfo{person}{Pichao Wang}, \bibinfo{person}{Zhaoyang Li},
  \bibinfo{person}{Yonghong Hou}, {and} \bibinfo{person}{Wanqing Li}.}
  \bibinfo{year}{2016}\natexlab{b}.
\newblock \showarticletitle{Action recognition based on joint trajectory maps
  using convolutional neural networks}. In \bibinfo{booktitle}{{\em Proceedings
  of the 2016 ACM on Multimedia Conference}}. ACM, \bibinfo{pages}{102--106}.
\newblock


\bibitem[\protect\citeauthoryear{Wang, Guo, Davis, and Dai}{Wang
  et~al\mbox{.}}{2012}]%
        {wang2012covariance}
\bibfield{author}{\bibinfo{person}{Ruiping Wang}, \bibinfo{person}{Huimin Guo},
  \bibinfo{person}{Larry~S Davis}, {and} \bibinfo{person}{Qionghai Dai}.}
  \bibinfo{year}{2012}\natexlab{}.
\newblock \showarticletitle{Covariance discriminative learning: A natural and
  efficient approach to image set classification}. In \bibinfo{booktitle}{{\em
  CVPR}}. IEEE, \bibinfo{pages}{2496--2503}.
\newblock


\bibitem[\protect\citeauthoryear{Wu, Cheng, Wang, Lu, Wang, Ling, Blasch, and
  Bai}{Wu et~al\mbox{.}}{2012}]%
        {wu2012real}
\bibfield{author}{\bibinfo{person}{Yi Wu}, \bibinfo{person}{Jian Cheng},
  \bibinfo{person}{Jinqiao Wang}, \bibinfo{person}{Hanqing Lu},
  \bibinfo{person}{Jun Wang}, \bibinfo{person}{Haibin Ling},
  \bibinfo{person}{Erik Blasch}, {and} \bibinfo{person}{Li Bai}.}
  \bibinfo{year}{2012}\natexlab{}.
\newblock \showarticletitle{Real-time probabilistic covariance tracking with
  efficient model update}.
\newblock \bibinfo{journal}{{\em Image Processing, IEEE Transactions on\/}}
  \bibinfo{volume}{21}, \bibinfo{number}{5} (\bibinfo{year}{2012}),
  \bibinfo{pages}{2824--2837}.
\newblock


\bibitem[\protect\citeauthoryear{Xia and Aggarwal}{Xia and Aggarwal}{2013}]%
        {xia2013spatio}
\bibfield{author}{\bibinfo{person}{Lu Xia} {and} \bibinfo{person}{JK
  Aggarwal}.} \bibinfo{year}{2013}\natexlab{}.
\newblock \showarticletitle{Spatio-temporal depth cuboid similarity feature for
  activity recognition using depth camera}. In \bibinfo{booktitle}{{\em CVPR}}.
  IEEE, \bibinfo{pages}{2834--2841}.
\newblock


\bibitem[\protect\citeauthoryear{Yang and Tian}{Yang and Tian}{2014}]%
        {yang2014super}
\bibfield{author}{\bibinfo{person}{Xiaodong Yang} {and} \bibinfo{person}{YingLi
  Tian}.} \bibinfo{year}{2014}\natexlab{}.
\newblock \showarticletitle{Super normal vector for activity recognition using
  depth sequences}. In \bibinfo{booktitle}{{\em CVPR}}. IEEE,
  \bibinfo{pages}{804--811}.
\newblock


\bibitem[\protect\citeauthoryear{Yang, Zhang, and Tian}{Yang
  et~al\mbox{.}}{2012}]%
        {yang2012recognizing}
\bibfield{author}{\bibinfo{person}{Xiaodong Yang}, \bibinfo{person}{Chenyang
  Zhang}, {and} \bibinfo{person}{YingLi Tian}.}
  \bibinfo{year}{2012}\natexlab{}.
\newblock \showarticletitle{Recognizing actions using depth motion maps-based
  histograms of oriented gradients}. In \bibinfo{booktitle}{{\em ACMM}}. ACM,
  \bibinfo{pages}{1057--1060}.
\newblock


\bibitem[\protect\citeauthoryear{Yu, Liu, and Yuan}{Yu et~al\mbox{.}}{2014}]%
        {yu2014discriminative}
\bibfield{author}{\bibinfo{person}{Gang Yu}, \bibinfo{person}{Zicheng Liu},
  {and} \bibinfo{person}{Junsong Yuan}.} \bibinfo{year}{2014}\natexlab{}.
\newblock \showarticletitle{Discriminative orderlet mining for real-time
  recognition of human-object interaction}. In \bibinfo{booktitle}{{\em ACCV}}.
  \bibinfo{pages}{1--5}.
\newblock


\bibitem[\protect\citeauthoryear{Zanfir, Leordeanu, and Sminchisescu}{Zanfir
  et~al\mbox{.}}{2013}]%
        {zanfir2013moving}
\bibfield{author}{\bibinfo{person}{Mihai Zanfir}, \bibinfo{person}{Marius
  Leordeanu}, {and} \bibinfo{person}{Cristian Sminchisescu}.}
  \bibinfo{year}{2013}\natexlab{}.
\newblock \showarticletitle{The moving pose: An efficient 3d kinematics
  descriptor for low-latency action recognition and detection}. In
  \bibinfo{booktitle}{{\em ICCV}}. IEEE, \bibinfo{pages}{2752--2759}.
\newblock


\bibitem[\protect\citeauthoryear{Zhao, Li, Pang, Sheng, Wang, and Ye}{Zhao
  et~al\mbox{.}}{2014}]%
        {zhao2014structured}
\bibfield{author}{\bibinfo{person}{Xin Zhao}, \bibinfo{person}{Xue Li},
  \bibinfo{person}{Chaoyi Pang}, \bibinfo{person}{Quan~Z Sheng},
  \bibinfo{person}{Sen Wang}, {and} \bibinfo{person}{Mao Ye}.}
  \bibinfo{year}{2014}\natexlab{}.
\newblock \showarticletitle{Structured Streaming Skeleton--A New Feature for
  Online Human Gesture Recognition}.
\newblock \bibinfo{journal}{{\em ACM Transactions on Multimedia Computing,
  Communications, and Applications\/}} \bibinfo{volume}{11},
  \bibinfo{number}{1s} (\bibinfo{year}{2014}), \bibinfo{pages}{22}.
\newblock


\bibitem[\protect\citeauthoryear{Zhu, Zhang, Shen, and Song}{Zhu
  et~al\mbox{.}}{2016}]%
        {zhu2016online}
\bibfield{author}{\bibinfo{person}{Guangming Zhu}, \bibinfo{person}{Liang
  Zhang}, \bibinfo{person}{Peiyi Shen}, {and} \bibinfo{person}{Juan Song}.}
  \bibinfo{year}{2016}\natexlab{}.
\newblock \showarticletitle{An online continuous human action recognition
  algorithm based on the kinect sensor}.
\newblock \bibinfo{journal}{{\em Sensors\/}} \bibinfo{volume}{16},
  \bibinfo{number}{2} (\bibinfo{year}{2016}), \bibinfo{pages}{161}.
\newblock


\end{thebibliography}

\end{document}
