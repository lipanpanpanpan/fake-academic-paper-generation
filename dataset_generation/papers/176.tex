
%% bare_jrnl.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/



% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE!
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[journal]{IEEEtran}
\usepackage{blindtext}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{xfrac}
\usepackage{subcaption}

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/





% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.



%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty pack ages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.


%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley and Jeff Goldberg.
% This package may be useful when used in conjunction with IEEEtran.cls'
% captionsoff option. Some IEEE journals/societies require that submissions
% have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.3.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% For subfigure.sty:
% \let\MYorigsubfigure\subfigure
% \renewcommand{\subfigure}[2][\relax]{\MYorigsubfigure[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat/subfig command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/endfloat/
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a
% page by themselves.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired

%\title{DeepDrive}
%\title{Real Time Image Detection for Autonomous Highways}
%\title{Convolutional Neural Networks Applied to Autonomous Highway Driving}
%\title{Autonomous Highway Driving Applications for Deep Learning}
\title{An Empirical Evaluation of Deep Learning on Highway Driving}

%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

% TODO: Need to make affiliations clearer
\author{
\IEEEauthorblockN{Brody~Huval\IEEEauthorrefmark{1},~Tao~Wang\IEEEauthorrefmark{1},
                  Sameep~Tandon\IEEEauthorrefmark{1},~Jeff~Kiske\IEEEauthorrefmark{1},
                  Will~Song\IEEEauthorrefmark{1},~Joel~Pazhayampallil\IEEEauthorrefmark{1},
                  Mykhaylo~Andriluka\IEEEauthorrefmark{1},~Pranav~Rajpurkar\IEEEauthorrefmark{1},
                  ~Toki~Migimatsu\IEEEauthorrefmark{1},~Royce~Cheng-Yue\IEEEauthorrefmark{2},
                  Fernando~Mujica\IEEEauthorrefmark{3},~Adam~Coates\IEEEauthorrefmark{4},
                  Andrew~Y.~Ng\IEEEauthorrefmark{1}} \\
\IEEEauthorblockA{\IEEEauthorrefmark{1}Stanford University}~
    %\\\{1, 4\}@abc.com}
\IEEEauthorblockA{\IEEEauthorrefmark{2}Twitter}~
    %\\\{2, 3\}@def.com}
\IEEEauthorblockA{\IEEEauthorrefmark{3}Texas Instruments}~
\IEEEauthorblockA{\IEEEauthorrefmark{4}Baidu Research} \\
}

% \thanks{M. Shell is with the Department
% of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta,
% GA, 30332 USA e-mail: (see http://www.michaelshell.org/contact.html).}% <-this % stops a space
% \thanks{J. Doe and J. Doe are with Anonymous University.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2005; revised January 11, 2007.}}

% note the % following the last \IEEEmembership and also \thanks -
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
%
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
%\markboth{Journal of \LaTeX\ Class Files,~Vol.~6, No.~1, January~2007}%
%{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
%
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2007 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle


\begin{abstract}
\boldmath
Numerous groups have applied a variety of deep learning techniques to computer vision problems in highway perception scenarios. In this paper, we presented a number of empirical evaluations of recent deep learning advances. Computer vision, combined with deep learning, has the potential to bring about a relatively inexpensive, robust solution to autonomous driving. To prepare deep learning for industry uptake and practical applications, neural networks will require large data sets that represent all possible driving environments and scenarios. We collect a large data set of highway data and apply deep learning and computer vision algorithms to problems such as car and lane detection. We show how existing convolutional neural networks (CNNs) can be used to perform lane and vehicle detection while running at frame rates required for a real-time system. Our results lend credence to the hypothesis that deep learning holds promise for autonomous driving.

\end{abstract}
% IEEEtran.cls defaults to using nonbold math in the Abstract.
% This preserves the distinction between vectors and scalars. However,
% if the journal you are submitting to favors bold math in the abstract,
% then you can use LaTeX's standard command \boldmath at the very start
% of the abstract to achieve this. Many IEEE journals frown on math
% in the abstract anyway.

% Note that keywords are not normally used for peerreview papers.
% \begin{IEEEkeywords}
% Deep Learning, , \LaTeX, paper, template.
% \end{IEEEkeywords}


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
Since the DARPA Grand Challenges for autonomous vehicles, there has been an explosion in applications and research for self-driving cars. Among the different environments for self-driving cars, highway and urban roads are on opposite ends of the spectrum. In general, highways tend to be more predictable and orderly, with road surfaces typically well-maintained and lanes well-marked. In contrast, residential or urban driving environments feature a much higher degree of unpredictability with many generic objects, inconsistent lane-markings, and elaborate traffic flow patterns. The relative regularity and structure of highways has facilitated some of the first practical applications of autonomous driving technology. Many automakers have begun pursuing highway auto-pilot solutions designed to mitigate driver stress and fatigue and to provide additional safety features; for example, certain advanced-driver assistance systems (ADAS) can both keep cars within their lane and perform front-view car detection. Currently, the human drivers retain liability and, as such, must keep their hands on the steering wheel and prepare to control the vehicle in the event of any unexpected obstacle or catastrophic incident. Financial considerations contribute to a substantial performance gap between commercially available auto-pilot systems and fully self-driving cars developed by Google and others. Namely, today's self-driving cars are equipped with expensive but critical sensors, such as LIDAR, radar and high-precision GPS coupled with highly detailed maps.

\begin{figure}[t]
  \centering
    \includegraphics[width=3.4in]{car-lanes.png}
 \caption{Sample output from our neural network capable of lane and vehicle detection.}
 \label{fig:car-lanes}
\end{figure}
In today's production-grade autonomous vehicles, critical sensors include radar, sonar, and cameras. Long-range vehicle detection typically requires radar, while nearby car detection can be solved with sonar. Computer vision can play an important a role in lane detection as well as redundant object detection at moderate distances. Radar works reasonably well for detecting vehicles, but has difficulty distinguishing between different metal objects and thus can register false positives on objects such as tin cans. Also, radar provides little orientation information and has a higher variance on the lateral position of objects, making the localization difficult on sharp bends. The utility of sonar is both compromised at high speeds and, even at slow speeds, is limited to a working distance of about $2$ meters. Compared to sonar and radar, cameras generate a richer set of features at a fraction of the cost. By advancing computer vision, cameras could serve as a reliable redundant sensor for autonomous driving. Despite its potential, computer vision has yet to assume a significant role in today's self-driving cars. Classic computer vision techniques simply have not provided the robustness required for production grade automotives; these techniques require intensive hand engineering, road modeling, and special case handling. Considering the seemingly infinite number of specific driving situations, environments, and unexpected obstacles, the task of scaling classic computer vision to robust, human-level performance would prove monumental and is likely to be unrealistic.

Deep learning, or neural networks, represents an alternative approach to computer vision. It shows considerable promise as a solution to the shortcomings of classic computer vision. Recent progress in the field has advanced the feasibility of deep learning applications to solve complex, real-world  problems; industry has responded by increasing uptake of such technology. Deep learning is data centric, requiring heavy computation but minimal hand-engineering. In the last few years, an increase in available storage and compute capabilities have enabled deep learning to achieve success in supervised perception tasks, such as image detection. A neural network, after training for days or even weeks on a large data set, can be capable of inference in real-time with a model size that is no larger than a few hundred MB~\cite{krizhevsky-2012}. State-of-the-art neural networks for computer vision require very large training sets coupled with extensive networks capable of modeling such immense volumes of data. For example, the ILSRVC data-set, where neural networks achieve top results, contains $1.2$ million images in over $1000$ categories.

By using expensive existing sensors which are currently used for self-driving applications, such as LIDAR and mm-accurate GPS, and calibrating them with cameras, we can create a video data set containing labeled lane-markings and annotated vehicles with location and relative speed. By building a labeled data set in all types of driving situations (rain, snow, night, day, etc.), we can evaluate neural networks on this data to determine if it is robust in every driving environment and situation for which we have training data.

In this paper, we detail empirical evaluation on the data set we collect. In addition, we explain the neural network that we applied for detecting lanes and cars, as shown in Figure~\ref{fig:car-lanes}.

%collection and annotation approach.
\section{Related Work}
Recently, computer vision has been expected to player a larger role within autonomous driving. However, due to its history of relatively low precision, it is typically used in conjunction with either other sensors or other road models \cite{cho-2014,held-2012,carafii-2012,jazayeri-2011}. Cho \textit{et al.} \cite{cho-2014} uses multiple sensors, such as LIDAR, radar, and computer vision for object detection. They then fuse these sensors together in a Kalman filter using motion models on the objects. Held \textit{et al.} \cite{held-2012}, uses only a deformable parts based model on images to get the detections, then uses road models to filter out false positives. Carafii \textit{et al.} \cite{carafii-2012} uses a WaldBoost detector along with a tracker to generate pixel space detections in real time. Jazayeri \textit{et al.} \cite{jazayeri-2011} relies on temporal information of features for detection, and then filters out false positives with a front-view motion model.

In contrast to these object detectors, we do not use any road or motion-based models; instead we rely only on the robustness of a neural network to make reasonable predictions. In addition, we currently do not rely on any temporal features, and the detector operates independently on single frames from a monocular camera. To make up for the lack of other sensors, which estimate object depth, we train the neural network to predict depth based on labels extracted from radar returns. Although the model only predicts a single depth value for each object, Eigen \textit{et al.} have shown how a neural network can predict entire depth maps from single images~\cite{eigen-2014}. The network we train likely learns some model of the road for object detection and depth predictions, but it is never explicitly engineered and instead learns from the annotations alone.

Before the wide spread adoption of Convolutional Neural Networks (CNNs) within computer vision, deformable parts based models were the most successful methods for detection~\cite{felzenszwalb-2010}. After the popular CNN model AlexNet~\cite{krizhevsky-2012} was proposed, state-of-the-art detection shifted towards CNNs for feature extraction~\cite{sermanet-2013,szegedy-2014,szegedy-2013,girshick-2014}. Girshick \textit{et al.} developed R-CNN, a two part system which used Selective Search~\cite{uijlings-2013} to propose regions and AlexNet to classify them. R-CNN achieved state-of-the-art on Pascal by a large margin; however, due to its nearly $1000$ classification queries and inefficient re-use of convolutions, it remains impractical for real-time implementations. Szegedy \textit{et al.} presented a more scalable alternative to R-CNN, that relies on the CNN to propose higher quality regions compared to Selective Search. This reduces the number of region proposals down to as low as $79$ while keeping the mAP competitive with Selective Search. An even faster approach to image detection called Overfeat was presented by Sermanet~\textit{et al.}~\cite{sermanet-2013}. By using a regular pattern of ``region proposals'', Overfeat can efficiently reuse convolution computations from each layer, requiring only a single forward pass for inference.

For our empirical evaluation, we use a straight-forward application of Overfeat, due to its efficiencies, and combine this with labels similar to the ones proposed by Szegedy~\textit{et al.}. We describe the model and similarities in the next section.

\section{Real Time Vehicle Detection}
% justification for using something similar to overfeat
Convolutional Neural Networks (CNNs) have had the largest success in image recognition in the past 3 years~\cite{krizhevsky-2012,szegedy-2014-inception,he-2015,simonyan-2014}. From these image recognition systems, a number of detection networks were adapted, leading to further advances in image detection. While the improvements have been staggering, not much consideration had been given to the real-time detection performance required for some applications. In this paper, we present a detection system capable of operating at greater than $10$Hz using nothing but a laptop GPU. Due to the requirements of highway driving, we need to ensure that the system used can detect cars more than $100$m away and can operate at speeds greater than $10$Hz; this distance requires higher image resolutions than is typically used, and in our case is $640 \times 480$. We use the Overfeat CNN detector, which is very scalable, and simulates a sliding window detector in a single forward pass in the network by efficiently reusing convolutional results on each layer \cite{sermanet-2013}. Other detection systems, such as R-CNN, rely on selecting as many as $1000$ candidate windows, where each is evaluated independently and does not reuse convolutional results.

% Overfeat, and the basics to how it works
In our implementation, we make a few minor modifications to Overfeat's labels in order to handle occlusions of cars, predictions of lanes, and accelerate performance during inference. We will first provide a brief overview of the original implementation and will then address the modifications. Overfeat converts an image recognition CNN into a ``sliding window'' detector by providing a larger resolution image and transforming the fully connected layers into convolutional layers. Then, after converting the fully connected layer, which would have produced a single final feature vector, to a convolutional layer, a grid of final feature vectors is produced. Each of the resulting feature vectors represents a slightly different context view location within the original pixel space. To determine the stride of this window in pixel space, it is possible to simply multiply the strides on each convolutional or pool layer together. The network we used has a stride size of $32$ pixels. Each final feature vector in this grid can predict the presence of an object; once an object is detected, those same features are then used to predict a \textit{single} bounding box through regression. The classifier will predict ``no-object'' if it can not discern any part of an object within its \textit{entire} input view. This causes large ambiguities for the classifier, which can only predict a single object, as two different objects could can easily appear in the context view of the final feature vector, which is typically larger than $50\%$ of the input image resolution.

The network we used has a context view of $355 \times 355$ pixels in size. To ensure that all objects in the image are classified at least once, many different context views are taken of the image by using skip gram kernels to reduce the stride of the context views and by using up to four different scales of the input image. The classifier is then trained to activate when an object appears anywhere within its entire context view. In the original Overfeat paper, this results in $1575$ different context views (or final feature vectors), where each one is likely to become active (create a bounding box).

% problems with overfeat
This creates two problems for our empirical evaluation. Due to the L2 loss between the predicted bounding box and actual bounding proposed by Sermanet \textit{et al.}, the ambiguity of having two valid bounding box locations to predict when two objects appear, is incorrectly handled by the network by predicting a box in the center of the two objects to minimize its expected loss. These boxes tend to cause a problem for the bounding box merging algorithm, which incorrectly decides that there must be a third object between the two ground truth objects. This could cause problems for an ADAS system which falsely believes there is a car where there is not, and emergency breaking is falsely applied. In addition, the merging algorithm, used only during inference, operates in $O(n^2)$ where $n$ is the number of bounding boxes proposed. Because the bounding box merging is not as easily parallelizable as the CNN, this merging may become the bottleneck of a real-time system in the case of an inefficient implementation or too many predicted bounding boxes.

% Introduce the mask detector
In our evaluations, we use a mask detector as described in Szegedy \textit{et al.}~\cite{szegedy-2013} to improve some of the issues with Overfeat as described above. Szegedy \textit{et al.} proposes a CNN that takes an image as input and outputs an object mask through regression, highlighting the object location. The idea of a mask detector is shown in Fig~\ref{fig:mask-detector}. To distinguish multiple nearby objects, different part-detectors output object masks, from which bounding boxes are then extracted. The detector they propose must take many crops of the image, and run multiple CNNs for each part on every crop. Their resulting implementation takes roughly $5$-$6$ seconds per frame per class using a 12-core machine, which would be too slow for our application.

\begin{figure}[tb]
  \centering
    \includegraphics[width=3.4in]{mask-detector.pdf}
 \caption{mask detector}
 \label{fig:mask-detector}
\end{figure}


\begin{figure*}[tb]
  \centering
    \includegraphics[width=7.1in]{overfeat-mask.pdf}
 \caption{overfeat-mask}
 \label{fig:overfeat-mask}
\end{figure*}

% improvements we've made
We combine these ideas by using the efficient ``sliding window'' detector of Overfeat to produce an object mask and perform bounding box regression. This is shown in Fig~\ref{fig:overfeat-mask}. In this implementation, we use a single image resolution of $640 \times 480$ with no skip gram kernels. To help the ambiguity problem, and reduce the number of bounding boxes predicted, we alter the detector on the top layer to only activate within a $4 \times 4$ pixel region at the center of its context view, as shown in the first box in Fig~\ref{fig:overfeat-mask}. Because it's highly unlikely that any two different object's bounding boxes appear in a $4 \times 4$ pixel region, compared to the entire context view with Overfeat, the bounding box regressor will no longer have to arbitrarily choose between two valid objects in its context view. In addition, because the requirement for the detector to fire is stricter, this produces many fewer bounding boxes which greatly reduces our run-time performance during inference.

% further improvements
Although these changes helped, ambiguity was still a common problem on the border of bounding boxes in the cases of occlusion. This ambiguity results in a false bounding box being predicted between the two ground truth bounding boxes. To fix this problem, the bounding boxes were first shrunk by $75\%$ before creating the detection mask label. This added the additional requirement that the center $4 \times 4$-pixel region of the detector window had to be within the center region of the object before activating. The bounding box regressor however, still predicts the original bounding box before shrinking. This also further reduces the number of active bounding boxes as input to our merging algorithm. We also found that switching from L2 to L1 loss on the bounding box regressions results in better performance. To merge the bounding boxes together, we used OpenCV's efficient implementation of \texttt{groupRectangles}, which clusters the bounding boxes based on a similarity metric in $O(n^2)$~\cite{opencv}.

% architecture
% TODO: maybe this is too confusing?
The lower layers of our CNN we use for feature extraction is similar to the one proposed by Krizhevsky \textit{et al.}~\cite{krizhevsky-2012}. Our modifications to the network occurs on the dense layers which are converted to convolution, as described in Sermanet \textit{et al.}~\cite{sermanet-2013}. When using our larger image sizes of $640 \times 480$ this changes the previous final feature response maps of size $1\times 1\times 4096$ to $20 \times 15 \times 4096$. As stated earlier, each of these feature vectors sees a context region of $355 \times 355$ pixels, and the stride between them is $32 \times 32$ pixels; however, we want each making predictions at a resolution of $4 \times 4$ pixels, which would leave gaps in our input image. To fix this, we use each $4096$ feature as input to $64$ softmax classifiers, which are arranged in an $8 \times 8$ grid each predicting if an object is within a different $4 \times 4$ pixel region. This allows for the $4096$ feature vector to cover the full stride size of $32 \times 32$ pixels; the end result is a grid mask detector of size $160 \times 120$ where each element is $4 \times 4$ pixels which covers the entire input image of size $640 \times 480$.

\subsection{Lane Detection}
The CNN used for vehicle detection can be easily extended for lane boundary detection by adding an additional class. Whereas the regression for the vehicle class predicts a five dimensional value (four for the bounding box and one for depth), the lane regression predicts six dimensions. Similar to the vehicle detector, the first four dimensions indicate the two end points of a local line segment of the lane boundary. The remaining two dimensions
indicate the depth of the endpoints with respect to the camera. Fig~\ref{fig:lane-gt} visualizes the lane boundary ground truth label overlaid on an example image. The green tiles indicate locations where the detector is trained to fire, and the line segments represented by the regression labels are explicitly drawn. The line segments have their ends connected to form continuous splines. The depth of the line segments are color-coded such that the closest segments are red and the furthest ones are blue. Due to our data collection methods for lane labels, we are able to obtain ground truth in spite of objects that occlude them. This forces the neural network to learn more than a simple paint detector, and must use context to predict lanes where there are occlusions.


\begin{figure}[tb]
  \centering
    \includegraphics[width=3.4in]{lane_label.png}
 \caption{Example of lane boundary ground truth}
 \label{fig:lane-gt}
\end{figure}

Similar to the vehicle detector, we use L1 loss to train the regressor. We use mini-batch stochastic gradient descent for optimization. The learning rate is controlled by a variant of the momentum scheduler~\cite{sutskever-2013}. To obtain semantic lane information, we use DBSCAN to cluster the line segments into lanes. Fig~\ref{fig:lane-out-dbscan} shows our lane predictions after DBSCAN clustering. Different lanes are represented by different colors. Since our regressor outputs depths as well, we can predict the lane shapes in 3D using inverse camera perspective mapping.

% The output on one of the test images is visualized in Fig~\ref{fig:lane-out-raw}.
% \begin{figure}[tb]
%   \centering
%     \includegraphics[width=3.4in]{lane_out_raw.png}
%  \caption{Example output of lane detector}
%  \label{fig:lane-out-raw}
% \end{figure}

\begin{figure}[tb]
  \centering
    \includegraphics[width=3.4in]{lane_out_dbscan.png}
 \caption{Example output of lane detector after DBSCAN clustering}
 \label{fig:lane-out-dbscan}
\end{figure}

\section{Experimental Setup}

\subsection{Data Collection}
Our Research Vehicle is a 2014 Infiniti Q50. The car currently uses the following sensors: 6x Point Grey Flea3 cameras, 1x Velodyne HDL32E lidar, and 1x Novatel SPAN-SE Receiver. We also have access to the Q50 built-in Continental mid-range radar system. The sensors are connected to a Linux PC with a Core i7-4770k processor.

Once the raw videos are collected, we annotate the 3D locations for vehicles and lanes as well as the relative speed of all the vehicles. To get vehicle annotations, we follow the conventional approach of using Amazon Mechanical Turk to get accurate bounding box locations within pixel space. Then, we match bounding boxes and radar returns to obtain the distance and relative speed of the vehicles.

Unlike vehicles that can be annotated with bounding boxes, highway lane borders often need to be annotated as curves of various shapes. This makes frame-level labelling not only tedious and inefficient, but also prone to human errors. Fortunately, lane markings can be considered as static objects that do not change their geolocations very often. We follow the process descried in~\cite{levinson-2011} to create LIDAR maps of the environment using the Velodyne and GNSS systems. Using these maps, labeling is straight forward. First, we filter the 3D point clouds based on lidar return intensity and position to obtain the left and right boundaries of the ego-lane. Then, we replicate the left and right ego-lane boundaries to obtain initial guesses for all the lane boundaries. A human annotator inspects the generated lane boundaries and makes appropriate corrections using our 3D labelling tool. For completeness, we describe each of these steps in details.

\subsubsection{Ego-lane boundary generation}
Since we do not change lanes during data collection drives, the GPS trajectory of our research vehicle already gives a decent estimate of the shape of the road. We can then easily locate the ego-lane boundaries using a few heuristic filters. Noting that lane boundaries on highways are usually marked with retro-reflective materials, we first filter out low-reflectivity surfaces such as asphalt in our 3D point cloud maps and only consider points with high enough laser return intensities. We then filter out other reflective surfaces such as cars and traffic signs by only considering points whose heights are close enough the ground plane.  Lastly, assuming our car drives close to the center of the lane, we filter out ground paint other than the ego-lane boundaries, such as other lane boundaries, car-pool or directional signs, by only considering markings whose absolute lateral distances from the car are smaller than 2.2 meters and greater than 1.4 meters. We can also distinguish the left boundary from the right one using the sign of the lateral distance. After obtaining the points in the left and right boundaries, we fit a piecewise linear curve similar to the GPS trajectory to each boundary.

\subsubsection{Semi-automatic generation of multiple lane boundaries}
We observe that the width of lanes during a single data collection run stays constant most of the time, with occasional exceptions such as merges and splits. Therefore, if we predefine the number of lanes to the left and right of the car for a single run, we can make a good initial guess of all the lane boundaries by shifting the auto-generated ego-lane boundaries laterally by multiples of the lane width. We will then rely on human annotators to fix the exception cases.

\subsection{Data Set}
At the time of this writing our annotated data-set consists of $14$ days of driving in the San Francisco Bay Area during the months of April-June for a few hours each day. The vehicle annotated data is sampled at $\sfrac{1}{3} \text{Hz}$ and contains nearly $17$ thousand frames with $140$ thousand bounding boxes. The lane annotated data is sampled at $5\text{Hz}$ and contains over $616$ thousand frames. During training, translation and 7 different perspective distortions are applied to the raw data sets. Fig~\ref{fig:lane-gt-distort} shows an example image after perspective distortions are applied. Note that we apply the same perspective distortion to the ground truth labels so that they match correctly with the distorted image.

\begin{figure}[tb]
  \centering
    \includegraphics[width=3.4in]{lane_label_distort.png}
 \caption{Image after perspective distortion}
 \label{fig:lane-gt-distort}
\end{figure}

%Merging parameters and thresholds were set using an evaluation size of around 7K images. Our test set remained small with only one day of driving and 1438 images and 15K cars.

\subsection{Results}
The detection network used is capable of running at $44$Hz using a desktop PC equipped with a GTX 780 Ti. When using a mobile GPU, such as the Tegra K1, we were capable of running the network at $2.5$Hz, and would expect the system to run at $5$Hz using the Nvidia PX1 chipset.

Our lane detection test set consists of 22 video clips collected using both left and right cameras during 11 different data collection runs, which correspond to about 50 minutes of driving. We evaluate detection results for four lane boundaries, namely, the left and right boundaries of the ego lane, plus the outer boundaries of the two adjacent lanes. For each of these lane boundaries, we further break down the evaluation by longitudinal distances, which range from 15 to 80 meters ahead of the car, spaced by 5 meters. Thus, there are at maximum $4\times14=56$ positions at which we evaluate the detection results. We pair up the prediction and ground truth points at each of these locations using greedy nearest neighbor matching. True positives, false positives and false negatives are accumulated at every evaluation location in a standard way: A true positive is counted when the matched prediction and ground truth differ by less than 0.5 meter. If the matched prediction and ground truth differ by more than 0.5 meter, both false positive and false negative counts are incremented.

Fig~\ref{fig:lane-output-eval} shows a visualization of this evaluation method on one image. The blue dots are true positives. The red dots are false positives, and the yellow ones are false negatives. Fig~\ref{fig:lane-eval-plot} shows the aggregated precision, recall and F1 score on all test videos. For the ego-lane boundaries, we obtain $100\%$ F1 score up to 50 meters. Recall starts to drop fast beyond 65 meters, mainly because the resolution of the image cannot capture the width of the lane markings at that distance. For the adjacent lanes, recall is low for the nearest point because it is outside the field of view of the camera.

\begin{figure}[tb]
  \centering
    \includegraphics[width=3.4in]{lane_output_eval.png}
 \caption{Left: lane prediction on test image. Right: Lane detection evaluated in 3D}
 \label{fig:lane-output-eval}
\end{figure}

\begin{figure}[tb]
  \centering
    \begin{subfigure}[b]{1.6in}
      \includegraphics[width=1.6in]{lane_eval_left0.png}
      \caption{}
      \label{fig:lane-eval-left0}
    \end{subfigure}%
    \begin{subfigure}[b]{1.6in}
      \includegraphics[width=1.6in]{lane_eval_right0.png}
      \caption{}
      \label{fig:lane-eval-right0}
    \end{subfigure}%

    \begin{subfigure}[b]{1.6in}%{0.5\textwidth}
      \includegraphics[width=1.6in]{lane_eval_left1.png}
      \caption{}
      \label{fig:lane-eval-left1}
    \end{subfigure}%
    \begin{subfigure}[b]{1.6in}%{0.5\textwidth}
      \includegraphics[width=1.6in]{lane_eval_right1.png}
      \caption{}
      \label{fig:lane-eval-right1}
    \end{subfigure}%
 \caption{Lane detection results on different lateral lanes. (a) Ego-lane left border. (b) Ego-lane right border. (c) Left adjacent lane left border. (d) Right adjacent lane right border.}
 \label{fig:lane-eval-plot}
\end{figure}

The vehicle detection test set consists of 13 video clips collected from a single day, which corresponds to 1 hour and 30 mins of driving. The accuracy of the vehicle bounding box predictions were measured using Intersection Over Union (IOU) against the ground truth boxes from Amazon Mechanical Turk (AMT). A bounding box prediction matched with ground truth if IOU$\geq0.5$. The performance of our car detection as a function of depth can be seen in Fig~\ref{fig:car-bb-error}. Nearby false positives can cause the largest problems for ADAS systems which could cause the system to needlessly apply the brakes. In our system, we found overpasses and shading effects to cause the largest problems. Two examples of these situations are shown in Fig~\ref{fig:car-fp}.

\begin{figure}[tb]
  \centering
  % Why won't the pdf version work??
    \includegraphics[width=3.4in]{performance-v-depth.png}
 \caption{Car Detector Bounding Box Performance}
 \label{fig:car-bb-error}
\end{figure}


\begin{figure}[tb]
        \centering
        \begin{subfigure}[b]{1.7in}%{0.3\textwidth}
                \includegraphics[width=1.7in]{car-fp2}
                \caption{FP: tree}
                \label{fig:car-tree}
        \end{subfigure}%
        %~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc.
          %(or a blank line to force the subfigure onto a new line)
        \begin{subfigure}[b]{1.7in}
                \includegraphics[width=1.7in]{car-fp3}
                \caption{FP: overpass}
                \label{fig:car-overpass}
        \end{subfigure}
        \caption{Vehicle False Positives}\label{fig:car-fp}
\end{figure}

As a baseline to our car detector, we compared the detection results to the Continental mid-range radar within our data collection vehicle. While matching radar returns to ground truth bounding boxes, we found that although radar had nearly $100\%$ precision, false positives were being introduced through errors in radar/camera calibration. Therefore, to ensure a fair comparison we matched every radar return to a ground truth bounding box even if IOU$<0.5$, giving our radar returns $100\%$ precision. This comparison is shown in Fig~\ref{fig:nn-v-radar}, the F1 score for radar is simply the recall.

\begin{figure}[tb]
  \centering
    \includegraphics[width=3.4in]{radar-results.png}
 \caption{Radar Comparison to Vehicle Detector}
 \label{fig:nn-v-radar}
\end{figure}


In addition to the bounding box locations, we measured the accuracy of the predicted depth by using radar returns as ground truth. The standard error in the depth predictions as a function of depth can be seen in Fig~\ref{fig:car-depth-error}.

\begin{figure}[tb]
  \centering
  % Why won't the pdf version work??
    \includegraphics[width=3.4in]{depth-error.png}
 \caption{Car Detector Depth Performance}
 \label{fig:car-depth-error}
\end{figure}


% https://youtu.be/VEqhw9OgDl0 the vehicle detector sameep uploaded
% https://youtu.be/__f5pqqp6aM lane detection video
For a qualitative review of the detection system, we have uploaded a $1.5$ hour video of the vehicle detector ran on our test set. This may be found at youtu.be/GJ0cZBkHoHc. A short video of our lane detector may also be found online at youtu.be/\texttt{\_\_}f5pqqp6aM. In these videos, we evaluate the detector on every frame independently and display the raw detections, without the use of any Kalman filters or road models. The red locations in the video correspond to the mask detectors that are activated. This network was only trained on the rear view of cars traveling in the same direction, which is why cars across the highway barrier are commonly missed.

We have open sourced the code for the vehicle and lane detector online at github.com/brodyh/caffe. Our repository was forked from the original Caffe code base from the BVLC group~\cite{jia-2014}.

\section{Conclusion}
By using Camera, Lidar, Radar, and GPS we built a highway data set consisting of $17$ thousand image frames with vehicle bounding boxes and over $616$ thousand image frames with lane annotations. We then trained on this data using a CNN architecture capable of detecting all lanes and cars in a single forward pass. Using a single GTX 780 Ti our system runs at $44$Hz, which is more than adequate for real-time use. Our results show existing CNN algorithms are capable of good performance in highway lane and vehicle detection. Future work will focus on acquiring frame level annotations that will allow us to develop new neural networks capable of using temporal information across frames.

%

% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex,
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation Results}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command, the
% \label for the overall figure must come after \caption.
% \hfil must be used as a separator to get equal spacing.
% The subfigure.sty package works much the same way, except \subfigure is
% used instead of \subfloat.
%
%\begin{figure*}[!t]
%\centerline{\subfloat[Case I]\includegraphics[width=2.5in]{subfigcase1}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{subfigcase2}%
%\label{fig_second_case}}}
%\caption{Simulation results}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
% \begin{table}[!t]
% % increase table row spacing, adjust to taste
% \renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
% \caption{An Example of a Table}
% \label{table_example}
% \centering
% % Some packages, such as MDW tools, offer better commands for making tables
% % than the plain LaTeX2e tabular which is used here.
% \begin{tabular}{|c||c|}
% \hline
% One & Two\\
% \hline
% Three & Four\\
% \hline
% \end{tabular}
% \end{table}


% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals use top floats exclusively.
% Note that, LaTeX2e, unlike IEEE journals, places footnotes above bottom
% floats. This can be corrected via the \fnbelowfloat command of the
% stfloats package.







% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


% \appendices
% \section{Proof of the First Zonklar Equation}
% Some text for the appendix.

% use section* for acknowledgement
\section*{Acknowledgment}

This research was funded in part by Nissan who generously donated the car used for data collection. We thank our colleagues Yuta Yoshihata from Nissan who provided technical support and expertise on vehicles that assisted the research. In addition, the authors would like to thank the author of Overfeat, Pierre Sermanet, for their helpful suggestions on image detection.


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
% \ifCLASSOPTIONcaptionsoff
%   \newpage
% \fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}

\bibitem{sermanet-2013}
Sermanet, Pierre, et al. "Overfeat: Integrated recognition, localization and detection using convolutional networks." arXiv preprint arXiv:1312.6229 (2013).

\bibitem{rothengatter-1997}
Rothengatter, Talib Ed, and Enrique Carbonell Ed Vaya. "Traffic and transport psychology: Theory and application." International Conference of Traffic and Transport Psychology, May, 1996, Valencia, Spain. Pergamon/Elsevier Science Inc, 1997.

\bibitem{cho-2014}
Cho, Hyunggi, et al. "A multi-sensor fusion system for moving object detection and tracking in urban driving environments." Robotics and Automation (ICRA), 2014 IEEE International Conference on. IEEE, 2014.

\bibitem{held-2012}
Held, David, Jesse Levinson, and Sebastian Thrun. "A probabilistic framework for car detection in images using context and scale." Robotics and Automation (ICRA), 2012 IEEE International Conference on. IEEE, 2012.

\bibitem{levinson-2011}
Levinson, Jesse, et al. "Towards fully autonomous driving: systems and algorithms." Intelligent Vehicles Symposium, 2011.


\bibitem{carafii-2012}
Caraffi, Claudio, et al. "A system for real-time detection and tracking of vehicles from a single car-mounted camera." Intelligent Transportation Systems (ITSC), 2012 15th International IEEE Conference on. IEEE, 2012.

\bibitem{jazayeri-2011}
Jazayeri, Amirali, et al. "Vehicle detection and tracking in car video based on motion model." Intelligent Transportation Systems, IEEE Transactions on 12.2 (2011): 583-595.

\bibitem{opencv}
Bradski, Gary. "The opencv library." Doctor Dobbs Journal 25.11 (2000): 120-126.

\bibitem{krizhevsky-2012}
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. "Imagenet classification with deep convolutional neural networks." Advances in neural information processing systems. 2012.

\bibitem{szegedy-2013}
Szegedy, Christian, Alexander Toshev, and Dumitru Erhan. "Deep neural networks for object detection." Advances in Neural Information Processing Systems. 2013.

\bibitem{sutskever-2013}
Sutskever, Ilya, et al. "On the importance of initialization and momentum in deep learning." Proceedings of the 30th International Conference on Machine Learning (ICML-13). 2013.

\bibitem{eigen-2014}
Eigen, David, Christian Puhrsch, and Rob Fergus. "Depth map prediction from a single image using a multi-scale deep network." Advances in Neural Information Processing Systems. 2014.

\bibitem{felzenszwalb-2010}
Felzenszwalb, Pedro F., et al. "Object detection with discriminatively trained part-based models." Pattern Analysis and Machine Intelligence, IEEE Transactions on 32.9 (2010): 1627-1645.

\bibitem{szegedy-2014}
Szegedy, Christian, et al. "Scalable, High-Quality Object Detection." arXiv preprint arXiv:1412.1441 (2014).

\bibitem{girshick-2014}
Girshick, Ross, et al. "Rich feature hierarchies for accurate object detection and semantic segmentation." Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. IEEE, 2014.

\bibitem{uijlings-2013}
Uijlings, Jasper RR, et al. "Selective search for object recognition." International journal of computer vision 104.2 (2013): 154-171.

\bibitem{szegedy-2014-inception}
Szegedy, Christian, et al. "Going deeper with convolutions." arXiv preprint arXiv:1409.4842 (2014).

\bibitem{he-2015}
He, Kaiming, et al. "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification." arXiv preprint arXiv:1502.01852 (2015).

\bibitem{simonyan-2014}
Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014).

\bibitem{jia-2014}
Jia, Yangqing, et al. "Caffe: Convolutional architecture for fast feature embedding." Proceedings of the ACM International Conference on Multimedia. ACM, 2014.

\end{thebibliography}

% biography section
%
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{biography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{picture}}]{John Doe}
%about John
%\end{IEEEbiography}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}
