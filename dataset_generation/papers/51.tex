
%% bare_conf.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE!
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[a4paper,conference]{IEEEtran}

\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother
% Some Computer Society conferences also require the compsoc mode option,
% but others use the standard conference format.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}



% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\usepackage{graphicx}
\usepackage{hhline}
\usepackage{multirow}
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at:
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e
\usepackage{subcaption}

%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
\usepackage[export]{adjustbox}

\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Memory-Efficient Deep Salient Object Segmentation Networks on Gridized Superpixels}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Caglar Aytekin, Xingyang Ni, Francesco Cricri, Lixin Fan, Emre Aksu}
\IEEEauthorblockA{Nokia Technologies, Tampere,
Finland \\
Corresponding Author's Email: caglar.aytekin@nokia.com}}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
%
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3},
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
Computer vision algorithms with pixel-wise labeling tasks, such as semantic segmentation and salient object detection, have gone through a significant accuracy increase with the incorporation of deep learning. %Deep segmentation methods usually fine-tune pre-trained networks with hundreds of millions of parameters and use additional skip layers in order to recover from the resolution loss caused by pooling layers.
Deep segmentation methods slightly modify and fine-tune pre-trained networks that have hundreds of millions of parameters.
%In this work, we adopt a different way to eliminate the resolution loss by exploiting a memory-efficient no-pooling network.
In this work, we question the need to have such memory demanding networks for the specific task of salient object segmentation.
%Moreover, we investigate the feasibility of training a neural network from scratch for this task, which would eliminate the need for pre-training on millions of images.
To this end, we propose a way to learn a memory-efficient network from scratch by training it only on salient object detection datasets.
Our method encodes images to gridized superpixels that preserve both the object boundaries and the connectivity rules of regular pixels.
This representation allows us to use convolutional neural networks that operate on regular grids.
By using these encoded images, we train a memory-efficient network using only 0.048\% of the number of parameters that other deep salient object detection networks have.
%Due to low-dimensional inputs, our network avoids pooling layers or strided convolutions, yet still can achieve a high receptive field.
Our method shows comparable accuracy with the state-of-the-art deep salient object detection methods and provides a faster and a much more memory-efficient alternative to them.
Due to its easy deployment, such a network is preferable for applications in memory limited devices such as mobile phones and IoT devices.
%We use this superpixel mapping to encode images with around 10x reduction in both image dimensions.
%Although this results in a spatially distorted images, it is possible to decode the images  while preserving the object boundaries to a reasonable accuracy.
%Training a neural network on such small images has two advantages: (a) it does not require millions of images to be trained on, thus we can use existing salient object detection datasets for training from scratch, (b) it requires much less parameters than a network trained on full-resolution images.
%On these encoded images, we can train a memory-efficient -less than 250k parameters- neural network that has no pooling layers, yet still has a large receptive field.

\end{abstract}

% no keywords




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% no \IEEEPARstart
{C}{onvolutional} Neural Networks (CNNs) are learning machines that are extensively used by top performing methods in image classification \cite{Krizhevsky,Simonyan,Szegedy,He}.
By the introduction of Fully Convolutional Neural Networks (FCNNs) \cite{Long}, these structures have also proven to constitute the state of the art in pixel-wise classification tasks such as semantic image segmentation and salient object detection.
A typical FCNN relies on a pre-trained CNN that is used for image classification and fine-tunes the CNN's parameters for segmentation task, often adding or replacing some layers.
These pre-trained CNNs usually contain a very large number of parameters, e.g. 138 million for VGG-16 \cite{Simonyan}.
Such large networks require a lot of memory, which makes them challenging to deploy on limited memory devices such as mobile phones.
There have been some efforts to reduce memory requirement of a CNN via pruning \cite{Hanc} or quantizing \cite{Guptac} the weights of the network, however these approaches are post-processing operations on large networks trained on millions of images.
For some segmentation tasks such as salient object detection, one might question the need for using such a high capacity network in the first place.
It can be argued that such a network might be an overkill for salient object detection and one can achieve reasonable performance by using a much smaller network.
Moreover, object recognition CNNs have greatly reduced resolutions in their final layer activations due to pooling or strided convolution operations throughout the network.
In order to atone for this resolution loss, segmentation networks either introduces additional connections to make use of the localization power of low-middle layers \cite{Long,Hariharan}, or adds a deconvolutional network on top of the CNN \cite{Badrinarayanan, Noh} with unpooling layers.
Both approaches results into an even more increase in the number of parameters used in the segmentation network.
%The parameters related to these modified parts are often randomly initialized, whereas other parameters are initialized with the values that are learnt for the CNN.
%The parameters are then fine-tuned via a stochastic gradient descent based optimization technique with back-propagating pixel-wise errors.
%Designing and training an FCNN from scratch is not common as it would lead to severe overfitting. This is mainly due to the small number of images in pixel-wise classification datasets \cite{Everingham,Lin,Mottaghi,Zhou} -on the order of ten thousands at best- for training state-of-the-art network structures that contain more than hundred millions of parameters.

\begin{figure}[!t]
%\centering
%\raggedleft
\includegraphics[width=0.5\textwidth,left]{fig1a.png}
%\begin{subfigure}{0.55\textwidth}
 % \includegraphics[width=0.9\textwidth,left]{fig1a.png}
  % \caption{}
   %\label{fig:Ng1}
%\end{subfigure}

%\begin{subfigure}[b]{0.55\textwidth}
 % \includegraphics[width=0.92\textwidth,center]{fig1bcorr.png}
  % \caption{}
   %\label{fig:Ng2}
%\end{subfigure}

\caption{Our method (GRIDS) compared to state-of-the-art deep salient object detection methods. (a) Network size comparison (plotted in log-scale), the true number of parameters are written on the corresponding bar. (b) Performance comparison according to $F_\beta$ measure.}
\label{fig0}
\end{figure}

%Another general problem with FCNNs is the blurry segmentation predictions.
%This results from the resolution loss in CNNs which is inevitable due to the pooling layers and -if used- strided convolutions.
%Recovering from this resolution loss has been the main research effort in deep segmentation networks in recent years \cite{Long, Hariharan, Badrinarayanan, Noh, Chen, Yu}.
%One approach is to combine good localization power of shallow layers and semantic information in deeper layers  \cite{Long,Hariharan}.
%This is achieved by additional layers to match the resolution of different layers to combine them.
%Another approach adopts a deconvolutional network with unpooling operators \cite{Badrinarayanan, Noh}.
%The unpooling operators are inverted pooling operators used in the encoder part of the network.
%There have been other efforts as well to reduce the resolution loss in CNNs via using dilated(atrous) convolutions \cite{Chen, Yu}.

%Since the main cause of the resolution loss is the pooling layer, one might consider a network with no pooling layers.
%However, this would limit the semantic power of the widely used networks, since pooling layers are the most effective blocks in increasing the receptive field as the network goes deeper.
%For example a VGG-E \cite{Simonyan} network without the pooling layers could only achieve a receptive field of 18x18.
%Comparing with input size (224x224), this receptive field would be very limited to make accurate predictions.
%In order to achieve a reasonable receptive field without pooling, one needs a very deep network with more than 100 convolutional layers.
%However, with the pooling layers removed, feature maps in the deeper layers would be of high resolution and it would be computationally inefficient to apply convolutions in deeper layers.

In this paper, we propose a way to overcome two problems of FCNNs mentioned above: requirement of a big pre-trained network and resolution loss because of pooling layers.
To this end, we utilize a memory-efficient deep segmentation network without any pooling layers.
We achieve this by encoding input images via gridized superpixels \cite{Fu}.
This allows us to use low resolution images that accurately encode object edges.
By using these images, we show that it is possible to train a memory-efficient FCNN with a reasonable depth, no pooling layers, yet with large receptive field and comparable performance with state of the art, see Fig. \ref{fig0}.
%Since the dimensions of the images are much lower than the originals, one can make use of a reasonably deep FCNN with no pooling layers and still achieve a large receptive field to make accurate predictions.
%For example a 30x30 low resolution image corresponds to 900 superpixels which accurately encode the object boundaries.
%For such an image a 28 layer network with 3x3 convolutions can reach a receptive field of the size of entire image.
%Although there is no pooling, since the spatial dimensions of feature maps are very low (same with input size), the model is computationally efficient.
The contributions of our work are listed as follows:

\begin{itemize}
  \item We propose a way to use FCNNs without any pooling layers or strided convolutions by abstracting input images via gridized superpixels.
  \item The predictions of our network does not suffer from inaccurate object edges.
  \item Our proposed network has less than 67k parameters (about 0.048\% of others).
  \item Our proposed network does not require a pre-trained model and can be trained from scratch by existing pixel-wise classification datasets.
  \item We show that the performance of our method is comparable with state of the art segmentation networks in salient object detection task.
\end{itemize}

The rest of the paper is organized as follows. In Section \ref{RelWork}, the related work is discussed, in Section \ref{PropMet}, the proposed method is described, in \ref{ExpRes}, the experimental results are analyzed. Finally, Section \ref{Conc} concludes the paper and suggests topics for future research.


\section{Related Work} \label{RelWork}
\subsection{Superpixel Gridization}
Superpixel gridization produces over-segmentations that form a regular pixel-like lattice which best preserves the object edges in an image.
There is a small number of studies in this topic which we review shortly next.
The method in \cite{Moore} relies on a boundary cost map which is an inverted edge detection result. Incrementally horizontal and vertical stripes are added to the image where no horizontal and vertical stripes intersect more than once and no two horizontal (or vertical) stripes intersect with each other. In each step the optimal stripe is found by minimizing the boundary cost that the stripe passes through by a min-cut based optimization algorithm.

An extended version of \cite{Moore} was proposed in \cite{Moore2} where the authors use an alternating optimization strategy. The method finds globally optimal solutions to the horizontal and vertical components of the lattice via using a multi-label Markov Random Field, as opposed to the greedy optimization strategy adopted in \cite{Moore}.
In \cite{Li1}, a generic approach is proposed to optimally regularize superpixels extracted by any algorithm. The approach is based on placing dummy nodes between superpixels to satisfy the regularity criterion.

Finally, the method proposed in \cite{Fu}, starts with regular lattice seeds and relocates the seeds -in a search space defined by the initialization- to the locally maximal boundary response. The relocated seeds are considered as superpixel junctions. Next, for each junction pair a path was found that maximize the edge strength on the path. These paths form a superpixel boundary map which results in a regular superpixel grid.

\subsection{Salient Object Detection}
A salient object is generally defined as the object that visually stands out from the rest of the image, thus is more appealing to the human eye \cite{Borji1}.

Unsupervised salient object detection methods rely mostly on following saliency assumptions: 1) A salient object has high local or global contrast \cite{Cheng1,Aytekin1}, 2) The boundary of an image is less likely to contain a salient object \cite{Li2,Aytekin2}, 3) The salient object is more likely to be large \cite{Aytekin2}, 4) Regions of similar feature maps have similar saliency \cite{Aytekin3}.

Prior to deep learning, supervised approaches to salient object detection focused on following tracks: 1) Learning a dense labeling of each region as salient or not \cite{Jiang, Li3}, 2) Learning to rank salient object proposals \cite{Aytekin4}, 3) Learning region affinities for \cite{Aytekin5} end to end salient object segmentation.

Deep learning based approaches to salient object detection either train a network to learn to classify each region in an image separately \cite{Zhao, Li4,Gayoung}, or employ FCNNs to learn a dense pixel-wise labeling for salient object detection \cite{Wang,Li5}. FCNN based models utilize pre-trained networks on other tasks, and employ special tricks to preserve the accurate edges in segmentation results. Next, we propose an FCNN that does not need a pre-trained network, automatically preserves accurate object edges with no pooling layers or strided convolutions, has much fewer parameters than other methods, and is comparable in performance to the state-of-the-art.

\begin{figure}[!t]
\centering
\includegraphics[width=2.5in]{Fig1.png}
\caption{From left to right: Original, encoded with \cite{Fu} (950 superpixels) and reconstructed images (first row) and corresponding ground truths (second row).}
\label{fig1}
\end{figure}

\section{Proposed Method} \label{PropMet}
\subsection{Data Preparation}
We use the superpixel extraction method in \cite{Fu} to abstract an input image $ I $ with a small number of homogeneous image regions (superpixels).
Thanks to the special property of the method in \cite{Fu}, the extracted superpixels form a grid.
We encode each superpixel with its mean color, thus we end up with a new image $ X $ with low dimensions as follows:
\begin{equation}
X^{(c)}_{i}=\frac{1}{|s_{i}|}\sum_{\{j\}\in s_{i}}I^{(c)}_{j}.
\label{eqn1}
\end{equation}
In Eq. \ref{eqn1}, $ c $ indicates a channel of an image, $ i $ and $ j $ are indices of $ X $ and $ I $ images respectively, $ s_{i} $ is a set of pixels of $ I $ covered in superpixel $ i $, and $ |.| $ is cardinality operation.
We will use this image $ X $ as input to an FCNN.
For training purposes, we also form a low resolution version of the ground truth label image $ G $ via encoding the mean of the 0 (not salient) and 1 (salient) values in the regions indicated by superpixels similar to the process in Eq. \ref{eqn1}.
In order to have binary values in the low dimensional ground truth $ Y $, we simply threshold the above image by 0.5. Note that it is equivalent to selecting the most common value within a superpixel.
It should be noted that, one can reconstruct an approximation of $ I $ and $ G $ from $ X $ and $ Y $ respectively as follows:
\begin{equation}
\tilde{I}^{(c)}_{j}=X^{(c)}_{i}, j\in s_{i}.
\label{eqn2}
\end{equation}
In Fig. \ref{fig1}, we illustrate original ($ I $), encoded ($ X $) and reconstructed ($ \tilde{I} $) images with corresponding ground truths.
The encoded image is the input that will be supplied to the FCNN with the encoded ground truth as its label for training.
As we observe from Fig. \ref{fig1}, even though the superpixel extraction is constrained by the grid structure, it is able to reconstruct the image well by preserving the object edges.


\begin{figure}[!t]
\centering
\includegraphics[width=1.25in]{Fig2.png}
\caption{Network Architecture.}
\label{fig2}
\end{figure}

\subsection{Network Architecture}
We use a 28-layers deep convolutional network with residual blocks \cite{He}.
In particular, the network has a convolutional (conv) layer with rectified linear unit \cite{Nair} (relu) activation, 13 residual blocks followed by a batch normalization \cite{Ioffe} (bnorm) , relu and conv layer and sigmoid activation.
Note that we have not applied bnorm layer right before the sigmoid in order to avoid restricting the convolutional output to a small interval.
Each residual block consists of a bnorm-relu-conv-bnorm-relu-conv structure.
The input of a residual block is short-connected to its output.
We use same number of filters in each layer.
The entire network is illustrated in Fig. \ref{fig2}.
The network's inputs and corresponding ground truths are obtained by the procedure described in the previous subsection.
Note that the convolutions are utilized with zero padding and stride 1, so that the input shape is preserved for each convolutional layer.
Moreover, there are no pooling layers in the network architecture in order to avoid any resolution loss.
This is possible because the input resolution is already low and we can use a constant number of convolutional filters throughout the network, whereas prior art networks need to reduce the resolution in order to increase the number of filters.
The receptive field of our network is around 30x30 which is enough to cover the entire input size for an abstraction of an image with 900 superpixels if the abstraction forms a square grid.
Typically the aspect ratio of the superpixel representation varies with the image aspect ratio, however we find the above receptive field enough to accurately detect the salient objects.


\begin{algorithm}
\caption{Test-time implementation}\label{testalgorithm}
\hspace*{\algorithmicindent} \textbf{Input: image $ I $} \\
\hspace*{\algorithmicindent} \textbf{Output: salient segment $ \tilde{S} $ }
\begin{algorithmic}[1] \\
\text{Encode $ I $ to $ X $ by Eq. \ref{eqn1}} \\
\text{Apply min-max normalization on $ X $} \\
\text{Predict $ S $ from $ X $ via neural network} \\
\text{Reconstruct $ \tilde{S} $ from $ S $ by Eq. \ref{eqn2}}
\end{algorithmic}
\end{algorithm}


\subsection{Training and Testing}
The parameters of the network are optimized in order to minimize the binary cross entropy loss between the output of the network and the ground truth, by treating the sigmoid outputs as probabilities that the corresponding input pixels are salient.
Separate datasets are used for training and validation sets and the model with the best validation error is selected.
For testing, we use entirely different datasets and run the model learned by the training as described above.
During testing, an image is encoded to the low dimensional superpixel grid representation and fed to the network.
It should be noted here that we apply min-max normalization to each input, i.e. we linearly scale the values between 0 and 1.
The output of the network lies in the same grid structure, thus should be converted back, i.e. reconstructed to the original image size.
The reconstruction is simply utilized via replicating the value in each grid node in the image region that the node corresponds to as formulated in Eq. \ref{eqn2}.
The test-time algorithm is given in Algorithm \ref{testalgorithm}.





\section{Experimental Results} \label{ExpRes}

\subsection{Datasets and Evaluation Metrics}
We conducted evaluations on widely used salient object detection datasets.
MSRA10K \cite{Liu} includes 10000 images that exhibit a simple case with one salient object and clear background, HKU-IS \cite{Li4} includes 4447 images with slightly challenging cases, ECSSD \cite{Yan} includes 1000 relatively complex images , PASCALS \cite{Li6} contains 850 images adopted from PASCAL VOC segmentation dataset, and SOD \cite{Mohavedi} contains 300 images from BSD300 \cite{Martin} segmentation dataset.
We use two most widely used evaluation metrics, mean absolute error (MAE) and F-measure.
For a saliency map $S$ and a binary ground truth $GT$, MAE is defined as follows:
\begin{equation}
MAE=\frac{1}{|S|}\sum_{i=1}^{|S|}|S_i-GT_i|.
\label{eqn3}
\end{equation}

Precision-recall curves are extracted via thresholding the saliency map $S$ at several values $\tau$ and plotting the precision and recall values which are calculated as follows:

\begin{equation}
PRE_\tau=\frac{|S>\tau|\cap|GT|}{|S>\tau|},  REC_\tau=\frac{|S>\tau|\cap|GT|}{|GT|}.
\label{eqn4}
\end{equation}

The F-measure is used to obtain a global evaluation of the precision recall curve and is obtained as follows.

\begin{equation}
F_\beta=\frac{(1+\beta^2)PRE_\tau \times REC_\tau}{\beta^2PRE_\tau+REC_\tau}.
\label{eqn5}
\end{equation}

It is widely adopted in salient object detection literature to chose $\beta^2$ to be $0.3$ and use an adaptive thresholding where $\tau$ equals twice the mean saliency in saliency map. \cite{Borji1}.

\subsection{Implementation}
Our network is based on the publicly available Keras with Theano backend.
Network parameters are initialized by Xavier's method \cite{Xavier}.
We use Nesterov Adam optimizer with an initial learning rate of $\mu=0.002$.
We utilize a number of superpixel granularities for augmenting the input data.
In particular, we use $\{900,925,950,975,1000\}$ number of superpixels.
Therefore, we have 5 different encoded image for each original image.
This is only done for training and validation data for data augmentation.
During test stage, we stick to 950 superpixels ,simply because it is the median of the above set, for evaluation.
Unlike other methods, our network is trained from scratch, hence it seems like it needs more data to be trained on, but in reality it is trained on less data if we consider also the pre-training data in prior art works.
Thus, we use largest datasets DUT-OMRON, HKU-IS and MSRA10K datasets for training and SOD for validation.
Further data augmentation is employed via randomly flipping the input and labels in horizontal direction.
We use a batch size of 20.
The network was run for 5 million iterations and the model that gives the best validation set accuracy was selected.
We evaluate 2 different variants of the network: one with 16 filters at each layer and one with 32.
We call the methods GRIDS16 and GRIDS32 respectively.
The networks that GRIDS16 and GRIDS32 use have 67k and 248k parameters respectively.


\begin{table}
  \caption{Comparison with State-of-the-art}
  \centering
  \renewcommand{\arraystretch}{1.2}
  \begin{tabular}{|p{1cm}|c|c|c|c|c|c|c|}
    \hline
    \multirow{2}{2cm}{\textbf{Method}} & \multicolumn{2}{c|}{\textbf{PASCALS}} & \multicolumn{2}{c|}{\textbf{ECSSD}} & \multicolumn{2}{c|}{\textbf{Avg. Perf.}}   \\
    % \hline
    % \textbf{Inactive Modes} & \textbf{Description}\\
    \cline{2-7}
    & \textbf{MAE} & \textbf{$F_\beta$} & \textbf{MAE} & \textbf{$F_\beta$} & \textbf{MAE} & \textbf{$F_\beta$}  \\
    %\hhline{~--}
    \hline
%    RC & 0.225 & 0.640 & 0.187 & 0.741 & 0.242 & 0.657 \\ \hline
%    CHM & 0.222 & 0.631 & 0.195 & 0.722 & 0.249 & 0.655 \\ \hline
%    DSR & 0.204 & 0.646 & 0.173 & 0.737 & 0.234 & 0.655  \\ \hline
%    EQCUT & 0.217 & 0.670 & 0.174 & 0.765 & 0.228 & 0.698 \\ \hline
%    DRFI & 0.221 & 0.679 & 0.166 & 0.787 & 0.215 & 0.712 \\ \hline
%    MC & 0.147 & 0.721 & 0.107 & 0.822 & 0.184 & 0.708   \\ \hline
%    GRIDS & 0.166 & 0.793 & 0.080 & 0.839 & 0.156 & 0.743\\ \hline
%    ELD & 0.121 & 0.767 & 0.098 & 0.865 & 0.154 & 0.760  \\ \hline
%    MDF & 0.145 & 0.764 & 0.108 & 0.833 & 0.155 & 0.785  \\ \hline
%    RFCN & 0.118 & 0.827 & 0.097 & 0.898 & 0.161 & 0.805  \\ \hline
%    DHS & 0.091 & 0.820 & 0.061 & 0.905 & 0.127 & 0.823  \\ \hline
%    DCL & 0.108 & 0.822 & 0.071 & 0.898 & 0.126 & 0.832  \\ \hline
%    DSS & 0.080 & 0.830 & 0.052 & 0.915 & 0.118 & 0.842  \\ \hline

    CHM & 0.222 & 0.631 & 0.195 & 0.722 & 0.209 & 0.677  \\ \hline
    RC & 0.225 & 0.640 & 0.187 & 0.741 & 0.206 & 0.691 \\ \hline
    DSR & 0.204 & 0.646 & 0.173 & 0.737 & 0.189 & 0.692  \\ \hline
    EQCUT & 0.217 & 0.670 & 0.174 & 0.765 & 0.196 & 0.718  \\ \hline
    DRFI & 0.221 & 0.679 & 0.166 & 0.787 & 0.194 & 0.733 \\ \hline
    MC & 0.147 & 0.721 & 0.107 & 0.822 & 0.127 & 0.772  \\ \hline
    MDF & 0.145 & 0.764 & 0.108 & 0.833 & 0.127 & 0.799   \\ \hline
    \textbf{GRIDS16} & \textbf{0.171} & \textbf{0.781} & \textbf{0.085} & \textbf{0.823} & \textbf{0.128} & \textbf{0.802} \\ \hline
        \textbf{GRIDS32} & \textbf{0.166} & \textbf{0.793} & \textbf{0.080} & \textbf{0.839} & \textbf{0.123} & \textbf{0.816} \\ \hline    ELD & 0.121 & 0.767 & 0.098 & 0.865 & 0.110 & 0.816  \\ \hline
    DCL & 0.108 & 0.822 & 0.071 & 0.898 & 0.090 & 0.860 \\ \hline
    RFCN & 0.118 & 0.827 & 0.097 & 0.898 & 0.108 & 0.863  \\ \hline
    DHS & 0.091 & 0.820 & 0.061 & 0.905 & 0.076 & 0.863  \\ \hline
    DSS & 0.080 & 0.830 & 0.052 & 0.915 & 0.066 & 0.873  \\ \hline
  \end{tabular}
  \label{table1}

\end{table}

\subsection{Comparison with State-of-the-art}
We compare our approach with 4 unsupervised methods: RC \cite{Cheng1}, CHM \cite{Li3}, DSR \cite{Li2}, EQCUT \cite{Aytekin2}, and 8 supervised methods: DRFI \cite{Jiang}, MC \cite{Zhao}, ELD \cite{Gayoung}, MDF \cite{Li4}, RFCN \cite{Wang}, DHS \cite{Liu2}, DCL \cite{Li5} and DSS \cite{Hou}.
In Table \ref{table1}, we share results for ECSSD and PASCALS datasets as these are the only datasets used for testing in all methods.
The ordering of the methods is made according to ascending $F_\beta$ measure.
As one can observe from Table \ref{table1}, both variants of our method GRIDS can achieve better $F_\beta$ measure and MAE than that of all unsupervised methods and three supervised methods: DRFI, MC and MDF.
Out of these methods MC and MDF are deep learning based and use around 58 and 138 million parameters respectively.
Other methods that outperform our method are all deep learning based algorithms and use more than 138 million parameters - VGG-16 models with additional layers/connections.
Yet, our models GRIDS16 and GRIDS32 only use around 67 and 248 thousand parameters respectively, which corresponds to respectively 0.048\% and 0.18\% of other methods and still achieve a comparable accuracy with the state of the art.
The number of parameters each deep learning based method use and run times with used GPUs are given in Table \ref{table2}.
Our method is the one with least memory requirement and fastest run-time.
At this point, one should note that the superpixel extraction time is not included in the above table. With the method of \cite{Fu}, this takes around an additional 0.5 seconds for an image of size 300x400 for superpixel granularity 950.


\begin{table}[!t]
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}

\caption{Complexity of Deep Learning Methods}
\label{table2}
\centering
% Some packages, such as MDW tools, offer better commands for making tables
% than the plain LaTeX2e tabular which is used here.
\begin{tabular}{|c||c||c||c|}
\hline
Method & \#Parameters & Run-time (sec.) & GPU\\
\hline
MC & 58m & 2.38 & Titan Black\\\hline
MDF & 138m & 8 & Titan Black\\\hline
ELD & 138m & 0.5 & Titan Black\\\hline
DCL & 138m & 1.5 & Titan Black\\\hline
RFCN & 138m & 4.6 & Titan X\\\hline
DHS & 138m & 0.04 & Titan Black\\\hline
DSS & 138m & 0.08 & Titan X\\\hline
\textbf{GRIDS16} & \textbf{67k} & \textbf{0.02} & \textbf{GTX 1080}\\\hline
\textbf{GRIDS32} & \textbf{248k} & \textbf{0.03} & \textbf{GTX 1080}\\\hline

\end{tabular}
\end{table}

\subsection{Analysis and Variants}
In this section, we investigate the impact of several factors to our method's performance.
First, we evaluate the performance robustness to different superpixel granularities.
We report the test performance when employing 900, 950 and 1000 number of superpixels in Table \ref{table3}.
The experiments are made with GRIDS32 model.
It can be observed that the resolution change in this interval has an insignificant impact on our method's performance and does not alter the ranking in Table \ref{table1}.

\begin{table}[!t]
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
\caption{Robustness to Resolution}
\label{table3}
\centering
% Some packages, such as MDW tools, offer better commands for making tables
% than the plain LaTeX2e tabular which is used here.
\begin{tabular}{|c||c||c||c||c|}
\hline
    \multirow{2}{2cm}{\textbf{Superpixel No.}} & \multicolumn{2}{c|}{\textbf{PASCALS}} & \multicolumn{2}{c|}{\textbf{ECSSD}}   \\
    % \hline
    % \textbf{Inactive Modes} & \textbf{Description}\\
    \cline{2-5}
     &\textbf{MAE} & \textbf{$F_\beta$} & \textbf{MAE} & \textbf{$F_\beta$}   \\
    \hline

    900 & 0.169 & 0.787 & 0.080 & 0.838   \\ \hline
    950 & 0.166 & 0.793 & 0.080 & 0.839 \\ \hline
    1000 & 0.167 & 0.792 & 0.080 & 0.840  \\ \hline

\end{tabular}
\end{table}

Next, we have tried to improve the performance via combination of segmentation results from all resolutions via majority voting.
Experiments are made with GRIDS32 model.
It can be observed from Table \ref{table3} that multi-resolution approach (GRIDSM) results into a notable performance improvement in both MAE and $F_\beta$ measure.
The rank of GRIDSM is the same with GRIDS for MAE, but it beats one more deep learning method (ELD) in $F_\beta$ measure in Table \ref{table1}.


\begin{table}[!t]
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
\caption{Improvement via Multi-resolution Approach}
\label{table4}
\centering
% Some packages, such as MDW tools, offer better commands for making tables
% than the plain LaTeX2e tabular which is used here.
\begin{tabular}{|c||c||c||c||c|}
\hline
    \multirow{2}{2cm}{\textbf{Method}} & \multicolumn{2}{c|}{\textbf{PASCALS}} & \multicolumn{2}{c|}{\textbf{ECSSD}}   \\
    % \hline
    % \textbf{Inactive Modes} & \textbf{Description}\\
    \cline{2-5}
     &\textbf{MAE} & \textbf{$F_\beta$} & \textbf{MAE} & \textbf{$F_\beta$}   \\
    \hline

    GRIDS (950) & 0.166 & 0.793 & 0.080 & 0.839    \\ \hline
    GRIDSM & 0.164 & 0.800 & 0.075 & 0.851 \\ \hline

\end{tabular}
\end{table}


One might argue that a natural baseline related to our network is (a) plain downsampling of the datasets, (b) training a network on the downsampled images and ground truths (c) upsampling results to evaluate the performance. This would highlight the performance upgrade of dimension reduction and later reconstruction with gridized superpixel encoding compared to plain downsampling and upsampling. To make this comparison, we train a network with exactly the same structure as described in the text, only this time we train the network with downsampled images and ground truths. Ground truths were again binarized via thresholding with 0.5. Augmentation with scale was similarly performed by defining downsampled dimensions to result into around 900,925,950,975 and 1000 number of pixels while preserving the aspect ratio. During test time we have again utilized only the 950 granularity. Bicubic downsampling and upsampling is used. Experiments are made with GRIDS32 model. The comparison in \ref{table5} clearly indicates the improvement of superpixel gridization encoding scheme over plain downsampling. Especially the improvement in $F_\beta$ measure is dramatic; up to a 13\% relative improvement.



\begin{table}[!t]
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
\caption{Encoding Strategy Comparison}
\label{table5}
\centering
% Some packages, such as MDW tools, offer better commands for making tables
% than the plain LaTeX2e tabular which is used here.
\begin{tabular}{|c||c||c||c||c|}
\hline
    \multirow{2}{2cm}{\textbf{Encoding Type}} & \multicolumn{2}{c|}{\textbf{PASCALS}} & \multicolumn{2}{c|}{\textbf{ECSSD}}   \\
    % \hline
    % \textbf{Inactive Modes} & \textbf{Description}\\
    \cline{2-5}
     &\textbf{MAE} & \textbf{$F_\beta$} & \textbf{MAE} & \textbf{$F_\beta$}   \\
    \hline

    Downsampling & 0.172 & 0.732 & 0.096 & 0.741   \\ \hline
    GRIDS & 0.166 & 0.793 & 0.080 & 0.839 \\ \hline

\end{tabular}
\end{table}

As we have previously mentioned, since our method is trained from scratch we obviously need more data to be trained on. That is why we use largest datasets DUT-OMRON, HKU-IS and MSRA10K for training.
We would like to emphasize here that the majority of other works use only MSRA10K for training and validation purposes for fine-tuning the pre-trained network they use.
For transfer learning, fine-tuning with little number of data is known to give satisfactory results.
Since our network is trained from scratch, such small data is not enough to train a network that gives satisfactory generalization.
Moreover, we do not possess the advantage of starting with a pre-trained network on millions of images for object detection -as others do- which is clearly expected to contribute to salient object detection performance acting as a top-down prior, i.e. using the semantically higher level information of object recognition for detecting the salient object.
Therefore, we argue the fairness of a comparison where we use only MSRA10k for our method.
Yet, in order to give a complete set of experiments, we have also trained our model with other methods' training and validation sets (partitions on MSRA10K dataset) and we report the test results in Table \ref{table6}.
Experiments are made with GRIDS32 model.
Clearly, the model trained on MSRA10K is inferior to the one trained on MSRA10K, HKU-IS and DUT-OMRON.

\begin{table}[!t]
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
\caption{Impact of Training Data}
\label{table6}
\centering
% Some packages, such as MDW tools, offer better commands for making tables
% than the plain LaTeX2e tabular which is used here.
\begin{tabular}{|c||c||c||c||c|}
\hline
    \multirow{2}{2cm}{\textbf{Training Data}} & \multicolumn{2}{c|}{\textbf{PASCALS}} & \multicolumn{2}{c|}{\textbf{ECSSD}}   \\
    % \hline
    % \textbf{Inactive Modes} & \textbf{Description}\\
    \cline{2-5}
     &\textbf{MAE} & \textbf{$F_\beta$} & \textbf{MAE} & \textbf{$F_\beta$}   \\
    \hline

    MSRA10k & 0.190 & 0.752 & 0.096 & 0.814 \\ \hline
    MSRA10k+HKUIS+DUTOMRON & 0.166 & 0.793 & 0.080 & 0.839   \\ \hline

\end{tabular}
\end{table}

\section{Conclusion and Future Work}  \label{Conc}
We have presented a deep, fast and memory-efficient method for salient object segmentation operating on encoded images with gridized superpixels. With the boundary preserving gridized superpixel encoding, we also do not suffer from blurry object boundaries. Moreover, the network does not employ any pooling layer, thus further resolution loss is prevented. This also eliminates the need of tricks such as additional connections and layers to atone for the resolution loss. We have shown that our method can outperform some deep learning based methods and shows comparable accuracy with others while having only 0.048\% of their parameters. With only 430 KB memory, the network is extremely easy to deploy to any device. This especially makes the method preferable considering applications in mobile and small IoT devices. The presented framework can be applied to any pixel-wise labeling task such as semantic segmentation. This will be the main topic that we will work on in the future improvements of this work.





% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex,
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively.
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.







% conference papers do not normally have an appendix


% use section* for acknowledgment
%\section*{Acknowledgment}


%The authors would like to thank...





% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}

\bibitem{Krizhevsky}
A.~Krizhevsky, I~Sutskever and G.~E. Hinton, "Imagenet Classification with Deep Convolutional Neural Networks", \emph{Advances in Neural Information Processing Systems (NIPS)}, pp. 1097-1105,2012.
\bibitem{Simonyan}
K.~Simonyan and A.~Zisserman, \emph{Very Deep Convolutional Neural Networks for Large-scale Image Recognition}, arXiv:1409.1556, 2014.
\bibitem{Szegedy}
C.~Szegedy, W.~Liu, Y.~Jia, P.~Sermanet, S.~Reed, D.~Anguelov and A.~Rabinovich, \emph{Going Deeper with Convolutions}, IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-9, 2015.
\bibitem{He}
K.~He, X.~Zhang, S.~Ren and J.~Sun, \emph{Deep Residual Learning for Image Recognition}, IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, 2016.
\bibitem{Long}
J.~Long, E.~ShelHamer, and T.~Darrell, \emph{Fully Convolutional Networks for Semantic Segmentation}, IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431-3440, 2015.
\bibitem{Everingham}
M.~Everingham, L.~Van Gool, C. K. Williams, J. Winn, and A. Zisserman, \emph{The PASCAL Visual Object Classes (VOC) Challenge}, International Journal of Computer Vision, vol. 88, no.2, pp. 303-338, 2010.
\bibitem{Lin}
T. Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C.L. Zitnick, \emph{Microsoft COCO: Common Objects in Context}, European Conference on Computer Vision, pp. 740-755, 2014.
\bibitem{Mottaghi}
R. Mottaghi, X. Chen, X. Liu, N. G. Cho, S. W. Lee, S. Fidler, and R. Urtasun. \emph{The Role of Context for Object Detection and Semantic Segmantation in the Wild}, IEEE Conference on Computer Vision and Pattern Recognition, pp. 891-898, 2014.
\bibitem{Zhou}
B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba, \emph{Semantic Understanding of Scenes Through the ADE20K dataset}, arXiv:1608.05442, 2016.
\bibitem{Hariharan}
B. Hariharan, P. Arbelaez, R. Girshick, and J. Malik, "Hypercolumns for Object Segmentation and Fine-grained Localization", \emph{IEEE Conference on Computer Vision and Pattern Recognition}, 2014.
\bibitem{Badrinarayanan}
V. Badrinarayanan, A. Kendall, and R. Cipolla, "Segnet: A Deep Convolutional Encoder-Decoder Network Arhcitecture for Image Segmentation", \emph{arXiv:1511.00561}, 2015.
\bibitem{Noh}
H. Noh, S. Hong, and B. Han, "Learning Deconvolutional Network for Semantic Segmentation", \emph{International Conference on Computer Vision}, pp. 1520-1528, 2015.
\bibitem{Chen}
L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. "DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution and Fully Connected CRFs", \emph{arXiv:1606.00915}, 2016.
\bibitem{Yu}
F. Yu, and V. Koltun. "Multi-scale Context Aggregation by Dilated Convolutions", \emph{ 	arXiv:1511.07122}, 2015.
\bibitem{Fu}
H. Fu, X. Cao, D. Tang, Y. Han, and D. Xu. "Regularity Preserved Superpixels and Supervoxels", \emph{IEEE Transactions on Multimedia} vol. 16, no. 4, pp. 1165-1175, 2014.
\bibitem{Li1}
L. Li, W. Feng, L. Wan, and J. Zhang. "Maximum Cohesive Grid of Superpixels for Fast Object Localization". \emph{IEEE Conference on Computer Vision and Pattern Recognition} pp. 3174-3181, 2013.
\bibitem{Moore}
A. P. Moore, S. J. Prince, J. Warrell, U. Mohammed, and G. Jones, "Superpixel Lattices", \emph{IEEE Conference on Computer Vision and Pattern Recognition}, pp. 1-8, 2008.
\bibitem{Moore2}
A. P. Moore, S. J. Prince, J. Warrell, "Lattice Cut-Constructing Superpixels Using Layer Constraints", \emph{IEEE Conference on Computer Vision and Pattern Recognition}, pp. 2117-2124, 2010.
\bibitem{Borji1}
A. Borji, M. M. Cheng, H. Jiang, and J. Li, "Salient Object Detection: A Survey", \emph{arXiv:1411.5878}, 2014.
\bibitem{Cheng1}
M. M. Cheng, N. J. Mitra, X. Huang, P. H. Torr, and S. Hu, "Global contrast based salient region detection", \emph{IEEE Transaction on Pattern Analysis and Machine Intelligence}, 2015.
\bibitem{Aytekin1}
C. Aytekin, S. Kiranyaz, and M. Gabbouj. "Automatic Object Segmentation by Quantum Cuts." \emph{International Conference on Pattern Recognition}, pp.112-117, 2014.
\bibitem{Li2}
X. Li, H. Lu, L. Zhang, X. Ruan, and M.-H. Yang. "Saliency detection via dense and sparse reconstruction", \emph{International Conference on Computer Vision}, pp. 2976–2983, 2013
\bibitem{Aytekin2}
C. Aytekin, E. C. Ozan, S. Kiranyaz, and M. Gabbouj. "Extended Quantum Cuts for Unsupervised Salient Object Extraction" \emph{Multimedia Tools and Applications} vol. 76, no. 8, pp. 10443-10463, 2017.
\bibitem{Aytekin3}
C. Aytekin, A. Iosifidis, and M. Gabbouj. "Probabilistic Saliency Estimation" \emph{Pattern Recognition}, https://doi.org/10.1016/j.patcog.2017.09.023, 2017.
\bibitem{Jiang}
H. Jiang, J. Wang, Z. Yuan, Y. Wu, N. Zheng, and S. Li. "Salient object detection: A discriminative regional feature integration approach", \emph{IEEE Conference on Computer Vision and Pattern Recognition}, pp. 2083-2090, 2013.
\bibitem{Li3}
X. Li, Y. Li, C. Shen, A. Dick, and A. Van Den Hengel. "Contextual hypergraph modeling for salient object detection", \emph{International Conference on Computer Vision}, pp. 3328–3335, 2013.
\bibitem{Aytekin4}
C. Aytekin, S. Kiranyaz, and M. Gabbouj. "Learning to Rank Salient Segments Extracted by Multispectral Quantum Cuts", \emph{Pattern Recognition Letters}, vol. 72, pp. 91-99, 2016.
\bibitem{Aytekin5}
C. Aytekin, A. Iosifidis, S. Kiranyaz, and M. Gabbouj. "Learning Graph Affinities for Spectral Graph-based Salient Object Detection", \emph{Pattern Recognition}, vol. 64, pp. 159-167, 2017.
\bibitem{Zhao}
R. Zhao, W. Ouyang, H. Li, and X. Wang. "Saliency Detection by Multi-Context Deep Learning". \emph{IEEE Conference on Computer Vision and Pattern Recognition}, pp. 1265-1274, 2015.
\bibitem{Li4}
G. Li and Y. Yu. "Visual saliency based on multiscale deep features", \emph{IEEE Conference on Computer Vision and Pattern Recognition}, pp. 5455–5463, 2015.
\bibitem{Gayoung}
L. Gayoung, T. Yu-Wing, and K. Junmo. "Deep saliency with encoded low level distance map and high level features". \emph{IEEE Conference on Computer Vision and Pattern Recognition}, pp. 660-668, 2016.
\bibitem{Wang}
L. Wang, L. Wang, H. Lu, P. Zhang, and X. Ruan, "Saliency detection with recurrent fully convolutional networks", \emph{European Conference on Computer Vision}, pp. 825-841, 2016.
\bibitem{Li5}
G. Li, and Y. Yu, "Deep contrast learning for salient object detection", \emph{IEEE Conference on Computer Vision and Pattern Recognition}, pp. 478-487, 2016.

\bibitem{Liu}
T. Liu, Z. Yuan, J. Sun, J. Wang, N. Zheng, X. Tang and H. Y. Shum, "Learning to detect a salient object",  \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, pp. 353-367, vol. 33, no. 2, .2011
\bibitem{Liu2}
N. Liu and J. Han. "Dhsnet: Deep hierarchical saliency network for salient object detection", \emph{IEEE Conference on Computer Vision and Pattern Recognition}, pp. 678-686, 2016.
\bibitem{Hou}
Q. Hou, M. M. Cheng, X. Hu, A. Borji, Z. Tu and P. Torr. Deeply supervised salient object detection with short connections, \emph{IEEE Conference on Computer Vision and Pattern Recognition}, pp. 5300-5309, 2017.
\bibitem{Yan}
Q. Yan, L. Xu, J. Shi and J. Jia, "Hierarchical saliency detection". \emph{IEEE Conference on Computer Vision and Pattern Recognition}, pp. 1155-1162, 2013.
\bibitem{Li6}
Y. Li, X. Hou, C. Koch, J. M. Rehg, and A. L. Yuille. "The secrets of salient object segmentation", \emph{IEEE Conference on Computer Vision and Pattern Recognition}, pp. 280-287, 2014.
\bibitem{Mohavedi}
V. Movahedi and J. H. Elder, "Design and perceptual validation of performance measures for salient object segmentation", \emph{IEEE Conference on Computer Vision and Pattern Recognition Workshops}, pp. 49-56, 2010.
\bibitem{Martin}
D. Martin, C. Fowlkes, D. Tal and J. Malik, "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statics", \emph{International Conference on Computer Vision}, pp. 416-423, 2001.
\bibitem{Nair}
V. Nair and G. E. Hinton, "Rectified linear units improve restricted boltzmann machines", \emph{Proceedings of the 27th international conference on machine learning}, pp. 807-814, 2010.
\bibitem{Ioffe}
S. Ioffe and C. Szegedy, "Batch normalization: Accelerating deep network training by reducing internal covariate shift", \emph{International Conference on Machine Learning}, pp. 448-456, 2015.
\bibitem{Xavier}
X. Glorot and Y. Bengio, "Understanding the difficulty of training deep feedforward neural networks", \emph{International Conference on Artificial Intelligence and Statistics}, pp. 249-256, 2010.
\bibitem{Hanc}
S. Han, J. Pool, J. Tran, W. Dally "Learning both weights and connections for efficient neural network",  \emph{Advances in Neural Information Processing Systems}, pp. 1135-1143, 2015.
\bibitem{Guptac}
S. Gupta, A. Agrawal, K. Gopalakrishnan and P. Narayanan, "Deep learning with limited numerical precision", \emph{International Conference on Machine Learning}, pp. 1737-1746, 2015.

\end{thebibliography}

% that's all folks
\end{document}


\grid
