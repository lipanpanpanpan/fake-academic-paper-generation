%
\documentclass{jcmlatex}
\def\JCMvol{xx}
\def\JCMno{x}
\def\JCMyear{200x}
\def\JCMreceived{xxx}
\def\JCMrevised{xxx}
\def\JCMaccepted{xxx}
\def\JCMonline{}
\setcounter{page}{1}

% from the previous journal draft
\usepackage{ragged2e}
\usepackage{amssymb}
%\hyphenation{met-a-bolic}
\usepackage{microtype}
%\usepackage{amsmath,amsthm}
\usepackage{graphicx}
\usepackage{filecontents,lipsum}
\usepackage{enumitem}
%\usepackage{cite}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{makecell}


\begin{document}

\markboth{C.~AGARWAL AND J.~KLOBUSICKY}{}

\title{Convergence of backpropagation with momentum for network architectures with skip connections}

\author{Chirag Agarwal
\thanks{Department of Electrical and Computer Engineering, University of Illinois at Chicago, Chicago, IL}
\and
 Joe Klobusicky
\thanks{Department of Mathematical Sciences, Rensselaer Polytechnic Institute, Troy, NY\\ Email: klobuj@rpi.edu}
\and
Dan Schonfeld
\thanks{Department of Electrical and Computer Engineering, University of Illinois at Chicago, Chicago, IL}
}

\maketitle

\begin{abstract}
We study a class of deep neural networks with networks that form a directed acyclic graph (DAG).   For backpropagation defined by gradient descent with adaptive momentum, we show weights converge for a large class of nonlinear activation functions.  The proof generalizes the results of Wu et al. (2008) who showed convergence for a feed forward network with one hidden layer. For an example of the effectiveness of DAG architectures, we describe an example of compression through an autoencoder, and compare against sequential feed-forward networks under several metrics.
\end{abstract}

\begin{classification}
68M07, 68T01.
\end{classification}

\begin{keywords}
backpropagation with momentum; autoencoders; directed acyclic graphs
\end{keywords}

\section{Introduction}\label{sec:introduction}

Neural networks have recently enjoyed an acceleration in popularity, with new research adding to several decades of foundational work. From multilayer perceptron (MLP) networks to the more prominent recurrent neural networks (RNNs) and convolutional neural networks (CNNs), neural networks have become a dominant force in the fields of computer vision, speech recognition, and machine translation \cite{rosenblatt1961principles}. Increase in computational speed and data collection have legitimized the training of increasingly complex deep networks. The flow of information from input to output is typically performed in a strictly sequential feed-forward fashion, in which for a network consisting of   $L$ layers of neurons,  nodes in the $i^{th}$ layer receive input from the $(i-1)$st layer, compute an output for each neuron, and in turn use this output as an input for the $(i+1)$st  layer. A natural question, then, is how the addition of ``skip connections" between layers affects performance.  Specifically, we are interesting in the class of architectures in which the network of connections form a directed acyclic graph (DAG).  The defining property of a DAG is that it can always be decomposed into a \textit{topological ordering} of $L$ layers, in which nodes in layer $i$ may be connected to layer $j$ where $j>i$.  A skip connection occurs when a node connects layers $i$ and $j$, with $j>i+1$.
There has been an increasing interest in studying networks with skip connection where weights skip a small number of layers, with examples including Deep Residual Networks (ResNet) \cite{he2016deep},  Highway Networks \cite{srivastava2015training}, and FractalNets \cite{larsson2016fractalnet}.  ResNets, for instance,  use ``shortcut connections" in which a copy of previous layers are mapped through an identity mapping to future layers. Kothari and Agyepong \cite{kothari1996lateral} introduced  ``lateral connections" in the form of a chain, where each unit in a hidden layer is connected to the next. %The problem of vanishing gradients was addressed in \cite{lee2015deeply,szegedy2015going} by connecting some hidden layers directly to the classifier layer.
The full generality of neural networks for DAG architectures was considered in \cite{huang2016densely}, which demonstrated superior performance of neural networks, entitled DenseNets, under a large set of skip connections.

As an example of the efficacy of DAG architectures considered in \cite{huang2016densely}, we consider autoencoders, a class of neural networks which provide a means of compressing data. For an autoencoder, input data, such as a pixelated image, is also considered as the desired output for a neural network.  During an encoding phase, the input is compressed through several hidden layers before arriving at a layer, called the code, with dimension smaller than the input.  The next phase is decoding, in which input from the code is fed through several more hidden layers until arriving at the output, which is of the same dimension as the input.  The goal of compression is to minimize the difference between input data and output.  In \cite{agarwal2018crossencoder}, Agarwal et. al introduced CrossEncoders, which demonstrated superior performance of autoencoders with cross-connections. In Section \ref{sec:experiment}, we extend the previous results to include the Olivetti faces dataset and and compare  CrossEncoders against traditional autoencoders.  While our proof of convergence uses mean square error, we validate our results against several other commonly used compression based performance metrics.

Our main theoretical result is the convergence of backpropagation with DAG architectures using gradient descent with momentum.  It is well known that feed-forward architectures converge to local minima under backpropagation, which is  essentially gradient descent applied to an error function (see \cite{bishop1995neural}, for instance). Updates for weights in backpropagation may be generalized to include a momentum term, which can help with increasing the convergence rate \cite{rumelhart1987parallel}. For a linear activation function, Bhaya \cite{bhaya2004steepest} and Torii \cite{torii2002stability} studied the convergence with backpropagation using momentum. Zhang et al.\cite{zhang2006}  generalized convergence for a class of common nonlinear activation functions, including sigmoids, for the simple case of a zero hidden layer networks. Wu et al. \cite{wu2008convergence} further generalized to one layer, by demonstrating that error is monotonically decreasing under backpropagation iterations for sufficiently small momentum terms. The addition of a hidden layers required  \cite{wu2008convergence} to make the additional assumption of bounded weights during the iteration procedure.

It is not evident whether applying the methods of \cite{wu2008convergence} would generalize to networks with several hidden layers and skip connections, or if they  would require stronger assumptions on boundedness of weights or the class of activation functions.  We show in Section \ref{appsect}  that convergence indeed does hold, with similar assumptions to the proof of convergence of one hidden layer.  While the general methods of the proof are similar to \cite{wu2008convergence}, the addition of skip connections will require several tensor identities to prove key estimates.

%Many different architectures like  Maxout Networks \cite{goodfellow2013maxout}, Network In Network (NIN) \cite{lin2013network}, Deeply Supervised Networks (DSN) \cite{lee2015deeply}, and FractalNets are being proposed which give %\begin{figure*}[h]%\centering%\includegraphics[height=6cm]{network_2.eps}%\caption{Proposed Neural Network architecture. Each layer %$(H_{i})$ in the figure is represented as a block of neurons. An arrow %connecting two such blocks mean that every neuron in those blocks %are fully connected with one other. This architecture enables neurons %in a layer $j$ to connect with neurons in layers $j'$, where $ j'=j+1, %\dots, J,$ and $J$ is the total number of Hidden Layers in the %network.}%\label{figure2}%\end{figure*}%\noindent competitive results on the image classification task. Maxout Networks are sequential feed-forward networks, using a maxout activation function which returns the  maximum of a set of inputs. NINs use micro-neural networks to abstract data within a receptive field. These micro-networks (multilayer perceptron) aid in extracting more complex features. DSNs introduced the idea that the supervision of layers should not be constrained to just the output layer but should be propagated to the early hidden layers as well. FractalNets proposed combining several sequences of a number of convolutional blocks. They obtained a large depth in their network %model used drop-paths to prevent co-adaptation of parallel paths.%\subsection{Contribution}%In this paper, we propose an architecture that is a combination of the traditional sequential feed-forward network, along with higher order cross-layer connections among the neurons in different layers. We concatenate feature maps from early layers by using pooling layers between them. One of the problems mentioned by \cite{nguyen2015deep} was that Deep Neural Networks (DNN) tend to learn low and middle-level features rather than the global structure of objects due to which the network was easily fooled on showing only sub-components of an object. Using our architecture, the final classification decision incorporates all low, middle, and global level features. Another aim of this paper is to extend the concept of skip-connections in \cite{he2016deep} to a more general case, that is, cross-connected neural networks. Besides the experimental evaluations,  we show the convergence of our proposed network using back-propagation with a momentum term. We show the convergence of the proposed neural network in which every layer has connections with all its subsequent layers.%Another advantage of our architecture is seen in terms of training accuracy and loss. Experimental results show that any architecture (MLP or CNN) with any number of layers and neurons is trained better (faster training and lower training loss) with cross-connections. We believe this is due to information sharing among different layers leading to efficient learning. The contributions of this paper can be summarized as follows:%\begin{itemize}%\item %We present a unified and generalized theory of CrossNets, which encompasses previous work on networks with connections that skip layers. We also give a proof of back-propagation convergence with momentum under generic feed-forward architecture and a large class of nonlinear activation functions.%\item %We show that the optimal architecture of cross-connected neural networks perform no worse and often better than other sequential feed-forward architectures through a series of experiments. %Our network can be fine-tuned to achieve state of the art results for different tasks.%\item %We extend CrossEncoders \cite{agarwal2018crossencoder}, an image compression framework, to a CNN framework and validate the results on different datasets.%\end{itemize}
\section{ARCHITECTURE FOR A FEED-FORWARD NETWORK
WITH CROSS-LAYER CONNECTIVITY}\label{sec:architecture}

In this section, we formally explain  DAG architectures, and the associated backpropagation algorithm with momentum.  We then state a theorem for the convergence of error for backpropagation whose proof is presented in Section \ref{appsect}.
%\subsection{Convergence under a single lateral weight}%For a simplified example, we demonstrate convergence of error to a local minimum  working with the architecture  type shown in Figure \ref{figure1}.  Layers consist of $n$ input nodes, two hidden nodes, and a single output node. Weight values  consist of a matrix $V = (v_{i,j})_{n\times 2}$ of weights from inputs to hidden nodes, a vector $(w_1, w_2)$ of weights from the hidden layer to the output, and a \textit{lateral} weight $z$ from the first hidden node to the second. In the given architectural framework, we have two hidden layers each having a node.%The addition of weight $z$,  in fact, separates the second hidden node as an additional hidden layer for the network.%\begin{figure}[h]%\begin{minipage}[b]{0.95\linewidth}%  \centering%  \centerline{\includegraphics[width=8.5cm]{twonodefig.png}}%\end{minipage}%\caption{Two  representations  of  the  same  neural  network  with  two  hidden nodes. \textbf{Left:}  Interpretation with $z$ as a `lateral' weight. \textbf{Right:}  A feed-forward representation, where both hidden nodes are viewed as separate hidden layers.}%\label{figure1}%\end{figure}%Let $g:\mathbb R\rightarrow \mathbb{R}$ denote an activation function. For an initial input   $x = (x_1, \dots, x_n)$,  the input values of each node is %\begin{align}\label{input}%&s_1 = \sum_{j\le n}x_jv_{j,1} = x\cdot v_1,%\\ &s_2 = zh_1+\sum_{j\le n}x_jv_{j,2} = zh_1+x\cdot v_2,%\\ &s_y  = w_1h_1+w_2h_2 = w\cdot h.%\end{align}%Output values are then%\begin{equation}%h_1 =g(s_1), \quad h_2 = g(s_2), \quad y = g(s_y)%\end{equation}%For the proof, we will need gradients  of the error with respects to weights. They are%\begin{align}%\frac{\partial E}{\partial w}&= \sum_{j = 1}^J g_j'(s_y^j)\cdot h:= p, \label{dp}\\%\frac{\partial E}{\partial z} &=  \sum_{j = 1}^J g_j'(s_y^j)w_2g'(s_2^j)  h_1:=r, \label{dz} \\%\frac{\partial E}{\partial v_2} &= \sum_{j = 1}^J g_j'(s_y^j)w_2g'(s_2^j)x^j:= q_2, \label{dv2}\\%\frac{\partial E}{\partial v_1} &=  \sum_{j = 1}^J g_j'(s_y^j)(w_1g'(s_1^j)x^j+w_2g'(s_2^j)zg'(s_1^j)x^j):= q_1. \label{dv1}%\end{align}%The iteration of weights is done through gradient descent with momentum. Here and in the future, a superscript $k$ is used as an iteration variable, and $\Delta x^k =  x^k-x^{k-1}$ for any quantity $x$. For $\eta \in (0,1)$, weights will be updated according to,   %\begin{align}\label{update}%\Delta w^{k+1} &= \tau_k\Delta w^k - \eta p^k,  \quad &&\Delta z^{k+1} = \beta_k\Delta z^k - \eta r^k, \\%\Delta v^{k+1}_1 &= \gamma_{k,1}\Delta v^{k}_1 -  \eta q^k_1, \quad &&\Delta v^{k+1}_2= \gamma_{k,2}\Delta v^{k}_2 -  \eta q^k_2.%\end{align}%Here, for a  fixed $\tau \in (0,1)$, and the Euclidean metric $\|\cdot\|$, adaptive momentum  is selected to normalize with respect to iteration differences in weights, given by%\begin{align}\label{admom}%\tau_k &= \begin{cases}\frac{\tau\|p^k\|}{\|\Delta w^k\|} &  \|\Delta w^k\| \neq 0,\\%0  & \mathrm{otherwise}, \\%\end{cases} \quad \beta_k = %\begin{cases}\frac{\tau \|r^k\|}{\|\Delta z^k\|} & \|\Delta z^k\| \neq 0, \\%0 & \mathrm{otherwise,} \\%\end{cases} \\%\gamma_{k,i}&= \begin{cases}\frac{\tau \|q^k_i\|}{\|\Delta v^{k}_i \|} & \|\Delta v_i^k\| \neq 0, \\%0 & \mathrm{otherwise,} \\ %\end{cases}%\quad i = 1,2 .%\end{align}%\begin{assp}\label{assp1} \indent%\begin{enumerate}%\item The function $g$, and its first two derivatives $g'$ and $g''$ are bounded in $\mathbb R$.%\item Hidden weights $w^k$ and $r^k$ are uniformly bounded over all  iterations $k = 1, 2, \dots.$%\item The gradient $\nabla E(v_1, v_2, z, w)$ vanishes only at a finite set of points.%\end{enumerate}%\end{assp}%\begin{theorem}\label{maint1}%Under assumptions (1) and (2), for any $s \in (0,1)$ and  $\tau = s\eta,$ there exists  $C>0$ such that if %\begin{equation}%\eta < \frac{1-s}{C(s^2+1)},%\end{equation}%then for $k = 1,2, \dots,$%\begin{align}%&E^k = E(v_1^k, v_2^k, z^k, w^k) \rightarrow E^*,\\%&q^k_1 \rightarrow 0, \quad q^k_2 \rightarrow 0, \quad r^k \rightarrow 0, \quad p^k \rightarrow 0.%\end{align}%If part (3) of Assumptions 1 is also satisfied, then weights $(v_1^k, v_2^k, z^k, w^k)\rightarrow (v_1^*, v_2^*, z^*, w^*)$, and $E^* = E(v_1^*, v_2^*, z^*, w^*)$ is a local  minimum.%\end{theorem}%The constant $C$ used is dependent only on the uniform bounds from Assumptions 1.\subsection{DAG architecture and backpropagation}
We now present the architecture for neural networks on DAGs. Nodes of a  DAG can always be ordered into layers $0, \dots, L$, in which directed edges always point to layers labeled with higher indices.  Layer 0, having $l_0$ nodes,  accepts the input training values $x^p \in \mathbb R^{l_0}$, over $ p = 1, \dots, J$.  For each layer $i$  from 1 through $L$, there are $l_i$ nodes. Under this ordering, define $v_{(i,j)}^{l,m}$ as the weight between node $l$ in layer $i$ and node $m$ in layer $j$, where $i<j$. Let $v_{(i,j)}$ denote the matrix of weights from layer $i$ to $j$. Over all nodes, we use a single (possibly nonlinear) activation function $g:\mathbb R\rightarrow \mathbb R$  for the determination of output values.

The explicit output values of the $l_j$ nodes in layer $j$ are denoted as
\begin{equation}
H_j = (H_j^1, \dots, H_j^{l_j}), \quad 0 \le j \le L.
\end{equation}
These are defined recursively from forward propagation, where the $j$ layer receives input from all layers $H_i$ with $i<j$. Explicitly,
\begin{align}
&H_0 = x,  &&H_1 = g\left(H_0 v_{(0,1)} \right),\\
&H_j = g\left(\sum_{i < j} H_iv_{(i,j)} \right),  &&H_L = y = g\left(\sum_{i <L } H_iv_{(i,L)} \right).\label{nodeeqn}
\end{align}
 Note that here, and in the future, for
a real valued function $f$, and a vector $v= (v_1, \dots, v_n)$, we will use
the notation $f(v) = (f(v_1), \dots, f(v_n))$. Node inputs are defined as
 \begin{equation}
 S_j= \sum_{i < j} H_iv_{(i,j)}.
 \end{equation}

We seek to minimize the difference between a set of  $J$ desired outputs $d^1, \dots, d^J \in \mathbb{R}^{l_L}$, and corresponding actual outputs  $y^1, \dots, y^J \in \mathbb{R}^{l_L}$. We measure the distances between desired and actual output with the total quadratic error
\begin{equation}
E = \sum_{j = 1}^J \|d^j-y^j\|/2. \label{generrordef}
\end{equation}
Gradients of the error with respect to weights are then defined as
\begin{equation}\label{defq}
\frac{\partial E}{\partial v_{(i,j)}^{l,m}} =  q_{(i,j)}^{l,m}.
\end{equation}

The iteration of weights by backpropagation is done through gradient descent with momentum. Here and in the future, a superscript $k$ is used as an iteration variable, and $\Delta x^k = x^k - x^{k-1}$ for any quantity $x$.   Weights are updated as

\begin{equation}\label{itdef}
\Delta v^{m,l;k+1}_{(i,j)}= \tau^{m,l;k}_{(i,j)}\Delta v^{m,l;k}_{(i,j)}
- \eta q^{m,l;k}_{(i,j)}.
\end{equation}
The second term is (\ref{itdef}) corresponds to traditional backpropagation through gradient descent, while first term, for a predetermined $\tau \in (0,1)$, is the contribution from adaptive momentum, where
\begin{equation}\label{taudef}
\tau^{m,l;k}_{(i,j)} = \begin{cases}\frac{\tau\|q^{m,l;k}_{(i,j)}\|}{\|\Delta
v^{m,l;k}_{(i,j)}\|} &  \|\Delta v^{m;k}_{(i,j)}\| \neq 0,\\
0  & \mathrm{otherwise}. \\
\end{cases}
\end{equation}
The norm $\|v\|$ denotes the usual Euclidean norm for a vector $v$. When the norm acts on a matrix $A = (a_{i,j})_{n\times m}$, it is treated as a
length $n\cdot m$ vector, with $\|A\|^2 = \sum_{i,j} a_{i,j}^2$. This choice of momentum was also used in \cite{zhang2006}. Note that  we sometimes place a variable denoting iteration after a semicolon to distinguish it from node indices.

\subsection{Convergence of backpropagation}

Our major theorem is a statement of convergence under backpropagation with momentum.   Specifically, for some input $x^j \in l_0$, we will use a generic desired output of $d^j \in \mathbb{R}$. We use a 1-d output for clarity in exposition.  The proof of convergence for output in multiple dimensions is essentially the same as the one presented here. The error in this case is then
\begin{equation}
E = \sum_{j = 1}^J |d^j-y^j|/2: =  \sum_{j = 1}^J g_j(y_j). \label{edef}
\end{equation}We will need some regularity and boundedness assumptions.  These assumptions are similar to those used in \cite{wu2008convergence}, and may also be found in other nonlinear optimization problems such as \cite{gori1996}.
\begin{ass} \label{assumes}
\hspace{0.5em}
\begin{enumerate}
\item The function $g$, and its first two derivatives $g'$ and $g''$ are bounded in $\mathbb R$.
\item The weights $v_{(i,j)}^k$ are uniformly bounded over layers $1 \le i<j \le L$ and iterations $k = 1, 2, \dots.$
\item The gradient $\nabla E$ vanishes only at a finite set of points.
\end{enumerate}
\end{ass}

It readily follows from these two assumptions that we may also uniformly bound  $q_{(i,j)}^k, H_i^k,g_p, g_p',$ and  $g_p''$.  Note that we do not assume boundedness of weights connected to the input layer.

\begin{theorem}\label{maint2}
Under assumptions (1) and (2), for any $s \in (0,1)$ and  $\tau = s\eta$, there exists $C>0$ such that if
\begin{equation}
\eta < \frac{1-s}{C(s^2+1)},
\end{equation}
then for $k = 1,2,\dots$,
\begin{align}
&E^k = E(v_{(i,j)}^k ) \rightarrow E^* \quad 1\le i<j\le L,\\
&q^k_{(i,j)} \rightarrow 0.
\end{align}

If part (3) of the Assumptions is satisfied, weights $v_{(i,j)}^k\rightarrow v_{(i,j)}^*$, and $E^* = E(v_{(i,j)}^*)$ is a local  minimum.
\end{theorem}

The constant $C$ used is solely dependent on fixed parameters form the network, and the uniform bounds from the Assumptions. A complete proof for Theorem \ref{maint2} is provided in Section \ref{appsect}.

%\subsection{CrossNets Implementation}%\label{composite}%From equation (\ref{nodeeqn}), each hidden layer  is recursively computed using the activated outputs from all previous layers neurons plus the weighted sum of the input from the input layer. The outputs of all the previous layers are stacked together and then are fed as an input to the subsequent layer. Traditional convolutional neural networks connect the output feature map of $j^{th}$ convolutional layer to the input of $(j+1)^{th}$ convolutional layer. For a convolutional network version of CrossNets, we connect the output feature map of the $j^{th}$ convolutional layer to all  inputs of the  $i^{th}$ convolutional layers, for all $i>j$. These cross-connections between non-adjacent layers lead to \textit{Cross Network} (\textit{CrossNet),} architecture. Rather than using simple convolutional layers, we used a combination of three consecutive operations: Batch Normalization, Rectified Linear Unit (ReLU), and finally a $3\times3$ convolutional layer. Pooling layers are also used to reduce the dimensions within the network. We followed the same configuration in our experiments as \cite{he2016identity} and define each stack of these three layers as a \textbf{composite unit}. We use a $2\times2$ average-pooling layer after each $N$ composite units. Implementation details specific to individual experiments are provided in Section \ref{experiment}.
\section{Experiments}\label{sec:experiment}\begin{figure}
\centering
\includegraphics[width=.8\textwidth]{crossencoder.eps}
\caption{\textbf{Architecture for CrossEncoders.}A directed edge from a node (circle) in one layer (column of circles) to another node in a different layer represents a connection. Additional edges  between nodes, suppressed for presentation, may also exist. Note, however, that edges may not connect encoding and decoding layers. }\label{autofig}
\end{figure}

We examine the efficacy of DAG architectures by  examining a problem of compression through the use of autocoders. The addition of skip connections in autoencoders, entitled CrossEncoders, was studied by Agarwal et al. \cite{agarwal2018crossencoder}.  In this section, we apply CrossEncoders to the Olivetti face dataset\footnote{ The Olivetti faces data set is a public dataset, may be obtained from http://mambo.ucsc.edu/psl/olivetti.html}.

For the problem of compression, we require a code layer with index $0<\mathbf c<L$ and dimension $l_\mathbf c < l_0$. Since we are now comparing input and output, layer $L$ also contains $l_0$ nodes. In terms of Eqn.  \ref{generrordef}, $d^j = x^j$, and thus
\begin{equation}
E = \sum_{j = 1}^J \|x^j-y^j\|/2.
\end{equation}
Since decoding should be solely dependent  from the code layer, we also require that skip connections cannot occur between encoding layers and decoding layers.  Thus
 \begin{equation}
 v_{(i,j)}^{l,m} = 0 \quad \hbox{if }  \quad i<\mathbf c < j. \label{encoderequire}
 \end{equation}
 See Fig. \ref{autofig} for a visual representation of the CrossEncoder architecture.

% [CHIRAG: A few questions. % \begin{itemize}%     \item Are crossconnections completely dense between layers? %     \end{itemize}% \end{itemize}% ]% \subsection{Image Compression using CrossEncoders}\subsection{Olivetti faces dataset}
The Olivetti faces dataset \cite{minear2004lifespan} comprises of a set of 400 gray-scale face images consisting of ten different images of 40 distinct subjects. Images for some subjects were taken with varying lighting, facial expressions (e.g. open / closed eyes, smiling / not smiling) and facial details (e.g. glasses / no glasses). %All the images were taken against a dark homogeneous background with the subjects in an upright, frontal position (with tolerance for some side movement).
The images are $64\times64$ in size and are quantized to 8-bit [0-255] scale. A $4096-500-500-code-500-500-4096$ MLP network was used for training the face dataset. Both sequential and cross-connected versions of the MLP were trained.

The original $64\times64$ images were transformed to a $1\times4096$ vector. For training, 350 images were used, and 50 images were used for the testing dataset. Both AutoEncoders and CrossEncoders were trained for 300 epochs using SGD optimizer with a learning rate set to 0.001 and momentum of 0.95. For the given task, we used several lower dimension representations, such as $1\times600$, $1\times300$, and $1\times30$, respectively. Table \ref{table8} illustrates the performance of the respective networks for different code size using peak signal to noise ratio (PSNR), structural similarity index (SSIM), and normalized root mean squared error (NRMSE) metrics. In Table \ref{table8}, we observe improved performance for CrossEncoders across all performance metrics.

\begin{table}[ht]
\caption{PSNR, SSIM, and NRMSE values of CrossEncoder and Autoencoder between reconstructed and the original images for Olivetti face dataset.  Higher PSNR and SSIM values, and lower NRSME values, imply more accurate results.}
\begin{center}
\noindent\begin{tabular}{|c|c|c|c|c|c|c|}
\toprule
& \multicolumn{3}{c|}{\textbf{CrossEncoder}} & \multicolumn{3}{c|}{\textbf{Autoencoder}} \\
\cmidrule(r){2-4}\cmidrule(l){5-7}
\textbf{Code} & \textbf{PSNR} & \textbf{SSIM} & \textbf{NRMSE}  & \textbf{PSNR} & \textbf{SSIM} & \textbf{NRMSE} \\
\midrule
$1\times600$  & \textbf{79.4679} & \textbf{0.9040} &  \textbf{0.1464} & 75.8858 & 0.8554 &  0.2217\\ [0.5ex]
$1\times300$  & \textbf{79.4551} & \textbf{0.9046} &  \textbf{0.1467} & 75.8919 & 0.8555 &  0.2215\\ [0.5ex]
$1\times30$  & \textbf{77.8398} & \textbf{0.8791} & \textbf{0.1764} & 75.9066 & 0.8560 &  0.2211\\ [0.5ex]
\bottomrule
\end{tabular}
\end{center}
\label{table8}
\vspace{-1em}
\end{table}% \subsubsection{MNIST}% An extensive detail of MNIST was provided in Section \ref{MNIST} [CHIRAG: Sorry, I deleted description. Please place back, along with reference.]. Using MNIST, we validate the application of CrossEncoders to CNN's. We use 60k images for training the CrossEncoder and the remaining 10k images for testing. A shallow CrossEncoder comprising of a pre-activation convolutional layer, followed by 2 composite units, as defined in Section \ref{composite}, was designed for the same purpose. An architectural description of the network is provided in Table \ref{table6}. The code layer represents lower dimension representation of the image. One would observe that each $28\times28$ image is reduced to a 72 element vector. Pooling layers were used to reduce the channel dimensions. Each composite unit was followed by a pooling layer with 4 kernel size. Accordingly, upsampling was done in the decoder stage to increase the dimensions. The output from each layer in the convolutional block were concatenated before forwarding them to the code and convolution layer respectively. The main idea of these experiments was to observe the performance boost on introducing cross-connections to an AutoEncoder architectures.\\% \begin{table}[ht]% \caption{CrossEncoder architecture for MNIST dataset. Each `conv' is a composite unit comprising the sequence of BN-RELU-Convolution layer.}% \centering% \scalebox{0.9}{% \begin{tabular}% {|c|c|c|}% \hline%   & & \\% Layers & Output Size & CrossEncoder\\[0.5ex]% \hline% & & \\% \makecell{Convolutional layer \\ (1)} & $32\times32$ & \makecell{$3\times3$ convolution\\ 6 kernels} \\ [0.5ex]% & & \\% \hline% & & \\% \multirow{3}{*}{Convolutional Block (1)} & \multirow{3}{*}{$2\times2$} & \multirow{3}{*}{$\begin{bmatrix}%       3\times3 \textrm{ conv} \\%       \textrm{6 kernels} \\%       \textrm{Average Pooling (4)}%      \end{bmatrix}\times 2$} \\[0.5ex] %   & & \\[0.5ex]%   & & \\[0.5ex]% \hline% & & \\% \makecell{Code layer} & $2\times2$ & \makecell{$1\times1$ conv\\ 18 kernels}\\ [0.5ex]% & & \\% \hline% & & \\% \makecell{Convolutional layer \\ (2)} & $2\times2$ & \makecell{$1\times1$ convolution\\ 12 kernels} \\ [0.5ex]% & & \\% \hline% & &\\% \multirow{3}{*}{Convolutional Block (2)} & \multirow{3}{*}{$32\times32$} & \multirow{3}{*}{$\begin{bmatrix}%       3\times3 \textrm{ conv} \\%       \textrm{6 kernels} \\%       \textrm{Upsampling (4)}%      \end{bmatrix}\times 2$} \\[0.5ex] % & &\\[0.5ex]% & &\\[0.5ex]% \hline% & & \\% \makecell{Convolutional layer \\ (3)} & $32\times32$ & \makecell{$3\times3$ convolution \\1 kernel, Tanh} \\ [0.5ex]% & & \\% \hline% \end{tabular}}% \vspace{-1em}% \label{table6}% \end{table}% \noindent% \textbf{Experimental Details:}\\% We compared the reconstruction results of the convolutional CrossEncoder with its AutoEncoder counterpart. In order to maintain uniform dimensions the MNIST input images were padded with 2 pixels (zero value) on all sides. Hence, we used input images of size $32\times32$. ReLU activations  were used for all the layers except the output layer, which used Tanh activation. The network was trained end-to-end for 300 epochs using back-propagation. Adam optimizer with a learning rate of $0.001$ and weight decay of $1e-5$ was used for training the network. % From Table \ref{table7} we observe the superior performance of CrossEncoders over AutoEncoders for different metrics.% \begin{table}[ht]% \caption{PSNR, SSIM, and NRMSE values between the reconstructed and the original images for MNIST}% \begin{center}% \noindent\begin{tabular}{|c|c|c|c|c|c|}% \toprule% \multicolumn{3}{c}{CrossEncoder} & \multicolumn{3}{c}{AutoEncoder} \\% \cmidrule(r){1-3}\cmidrule(l){4-6}%  {PSNR} & {SSIM} & {NRMSE}  & {PSNR} & {SSIM} & {NRMSE} \\% \midrule%  \textbf{64.7017} & \textbf{0.9995} &  \textbf{0.3070} & 63.9885 & {0.7387} &  0.3328\\ [0.5ex]% \bottomrule% \end{tabular}% \end{center}% \label{table7}% \vspace{-1em}% \end{table}% \subsubsection{Facial Expression Recognition}\label{FER}% The data consists of $48\times48$ pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centered and occupies about the same amount of space in each image. The task is to categorize faces based on their respective emotions. The dataset has facial expressions depicting emotions in one of the seven categories (Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral). A total of 28,709 images were present for training, and the testing dataset consisted of 3,589 images. Like MNIST, a shallow CrossEncoder comprising of a pre-activation convolutional layer, followed by 2 composite units, was used for the task. \\% \noindent% \textbf{Experimental details:}% The original $48\times48$ images were transformed to a $1\times2304$ vector. The convolutional AutoEncoders and CrossEncoders were trained for 300 epochs using Adam optimizer, the learning rate set to 0.001, and a SmoothL1 loss function. As mentioned in Section \ref{FER}, all other architectural details were same as that of MNIST. Table \ref{table9} illustrates the performance of both AutoEncoders and its cross-connected counterpart for the given code size using PSNR, SSIM, and NRMSE metrics.% * <klobuj@rpi.edu> 2017-12-13T18:20:15.939Z:% % Wait, 48*48 isn't 324...am I missing something?% % ^.% \begin{table}[ht]% \caption{PSNR, SSIM, and NRMSE values between the reconstructed and the original images for FER dataset}% \begin{center}% \noindent\begin{tabular}{|c|c|c|c|c|c|}% \toprule% \multicolumn{3}{c}{CrossEncoder} & \multicolumn{3}{c}{AutoEncoder} \\% \cmidrule(r){1-3}\cmidrule(l){4-6}%  {PSNR} & {SSIM} & {NRMSE}  & {PSNR} & {SSIM} & {NRMSE} \\% \midrule%  \textbf{58.1102} & \textbf{0.9891} &  \textbf{0.5726} & 53.1182 & {0.9580} &  0.9681\\ [0.5ex]% \bottomrule% \end{tabular}% \end{center}% \label{table9}% \vspace{-1em}% \end{table}%By cross-connecting a layer to all its subsequent layers this learning barrier can be overcome.
\section{Proof of Convergence}\label{appsect}%\subsection{Estimates on   two hidden nodes}\label{A1}% We begin by showing that for sufficiently small learning rates,   the difference in error is controlled by gradient terms from the first order Taylor approximation, and additional quadratic terms. Specifically,     % \begin{lemma}\label{main2nodelem}% There exists a constant $C>0$ such that for all $k = 1, 2, \dots, $% \begin{align} % \Delta E^{k+1} &\le (\tau-\eta)(\|p^k\|^2+\|q_2^k\|^2+|r^k|^2+\|q_1^k\|^2)%  \\&+C(\eta^2+\tau^2)(\|p\|^2+\|q_1^k\|^2+\|q_2^k\|^2+|r^k|^2).% \end{align}% \end{lemma}% Later, we'll show that this  type of decomposition of error is not specific to this example, but in fact holds in for all DAG architectures.% To arrive at Lemma \ref{main2nodelem}, we first look at the error difference% for a single initial input. This is  done with Taylor's theorem by expanding% $ s_y^{j,k+1}$ about $s_y^{j,k}$, which yields% \begin{align}%  &g_j( s_y^{j,k+1})= g_j( s_y^{j,k})+g'_j( s_y^{j,k})(\Delta s_y^{j,k+1}) +\frac12 g''_j(t^{k,j})(\Delta s_y^{j,k+1}) ^2,%  \end{align}% where $t^{k,j}$ lies between $ s_y^{j,k}$ and $ s_y^{j,k+1}$. To handle the% linear terms, note that% \begin{equation}% \Delta s_y^{j,k+1} = h^{j,k}\cdot\Delta w^{k+1}+\Delta (h^{j,k+1})\cdot w^{k}+\Delta% (h^{j,k+1})\cdot \Delta w^{k+1}\label{ds1}% \end{equation}% From (\ref{dp}) ,(\ref{update}), and (\ref{ds1}), we sum over all inputs% to find% \begin{align}% &\sum_{j = 1}^J g_j'( s_y^{j,k})(\Delta s_y^{j,k+1}) \label{ds2}=\sum_{j = 1}^J g_j'( s_y^{j,k})h^{j,k}\cdot (\tau_k\Delta w^k-\eta p^k)\\&+\sum_{j% = 1}^J g_j'( s_y^{j,k})\Delta h^{j,k+1}\cdot w^{k+1}+ \Delta% (h^{j,k+1})\cdot \Delta w^{k+1}\\%  &= -\eta \|p^k\|^2 +\tau_kp^k\cdot \Delta w^k+\sum_{j = 1}^J g'_j( s_y^{j,k})\Delta% h^{j,k+1}\cdot w^{k+1}\\&+\Delta% (h^{j,k+1})\cdot \Delta w^{k+1}% \end{align}% Thus,  using (\ref{edef}), (\ref{ds1}), and (\ref{ds2}),  we may write the% iteration of error as% \begin{align}\label{eiter}% \Delta E^{k+1}= &-\eta \|p^k\|^2 +\tau_kp^k\cdot \Delta w^k+\sum_{j = 1}^J% g'_j( s_y^{j,k})\Delta h^{j,k+1}\cdot w^{k}\\&+\sum_{j = 1}^J\frac 12 g''_j(t^{k,j})(\Delta% s_y^{j,k+1})^2+ \Delta% (h^{j,k+1})\cdot \Delta w^{k+1}.% \end{align}% Our next estimates are for $\Delta h^{j,k+1}$. The inequalities that will% arise frequently rely on Assumptions 1 and 2.  In what follows, we will use a constant $C>0$, dependent only on uniform bounds from Assumptions 2, that will hold for all future inequalities.    We again use%  Taylor's theorem with regards to the first hidden node:% for $\hat t_{1,j,k}$ between $s_1^{k+1,j}$ and $s_1^{k,j}$,%  \begin{align}\label{deltag1}% \Delta h_1^{j,k+1}% &= g'( s_1^{j,k})\Delta s_1^{j,k+1}+ \frac 12g''(\hat t_{1,j,k})(\Delta s_1^{j,k+1})^2% \end{align}% From (\ref{input}),  (\ref{update}), and Assumption (1),  \begin{align}% &\Delta h_1^{j,k+1} \le g'( s_1^{j,k})(\gamma_{k,1}\Delta v^{k}_1 -  \eta% q^k_1)x^j +C(\Delta s_1^{j,k+1})^2. \label{bigsum1}% \end{align}% A similar calculation follows for the second hidden node. From (\ref{input})% and  another Taylor expansion, for $\hat t_{2,j,k}$ between $s_2^{k+1,j}$% and $s_2^{k,j}$, \begin{align}% &\Delta h_2^{j,k+1}= g'(s_2^{j,k})\Delta s_2^{j,k+1}+ \frac 12g''(\hat t_{2,j,k})(\Delta% s_2^{j,k+1})^2\\% &=  g'(s_2^{j,k})\Big(\Delta z^{k+1}(h_1^{j,k}) +z^{k}\Delta h_1^{j,k+1}\\&+\Delta% z^{k+1}\Delta h_1^{j,k+1}+\Delta v_2^{k+1}\cdot x^j\Big) \label{h1here}\\&% + \frac 12g''(\hat t_{2,j,k})(\Delta h_2^{j,k+1})^2.% \end{align}% For the first term in (\ref{h1here}), we substitute with (\ref{deltag1}),% and once more use (\ref{input}) to obtain\begin{align}% &\Delta h_2^{j,k+1}= g'(s_2^{j,k})\Big(\Delta z^{k+1}(h_1^{j,k}) \\ &+z^kg'(s_1^{j,k})\Delta% v_1^{k+1}\cdot x^j+\frac 12z^kg''(\hat t_{1,j,k})(\Delta s_1^{j,k+1})^2\\&+\Delta% z^{k+1}\Delta h_1^{j,k+1}+\Delta v_2^{k+1}\cdot x^j\Big)+ \frac 12g''(\hat t_{2,j,k})(\Delta h_2^{j,k+1})^2.% \end{align}% Finally, from the Assumptions, \begin{align} % &\Delta h_2^{j,k+1} \le g'(s_2^{j,k})\Big(\Delta z^{k+1}(h_1^{j,k}) \\&+z^kg'(s_1^{j,k})\Delta% v_1^{k+1}\cdot x^j+\Delta v_2^{k+1}\cdot x^j\Big)\\% &+ C((\Delta s_1^{j,k+1})^2+(\Delta z^{k+1})^2+(\Delta h_1^{j,k+1})^2+ (\Delta% h_2^{j,k+1})^2) \label{bigsum2}% \end{align}% Using (\ref{bigsum1}) and (\ref{bigsum2}), along with the explicit gradient% expressions (\ref{dz}-\ref{dv1}), we sum over observations  in (\ref{eiter})% to obtain % \begin{align}% &\sum_{j = 1}^J g_j'( s_y^{j,k})\Delta h^{j,k+1}\cdot w^{k} \\% &\le q_2^k\cdot \Delta v_2^{k+1}+q_1^k\cdot \Delta v_1^{k+1}+ r^k\Delta z^{k+1}\\%  &+C((\Delta s_1^{j,k+1})^2+(\Delta z^{k+1})^2+(\Delta h_1^{j,k+1})^2+ (\Delta% h_2^{j,k+1})^2).% \end{align}% From (\ref{update}) and (\ref{admom}), % \begin{align}\label{mainineq1}% &\sum_{j = 1}^J g_j'( s_y^{j,k})\Delta h^{j,k+1}\cdot w^{k}\\% &\le (-\eta+\tau)(\|q_1\|^2+|r^k|^2+\|q_2^k\|^2)\\% &+C((\Delta s_1^{j,k+1})^2+(\Delta z^{k+1})^2+(\Delta h_1^{j,k+1})^2+ (\Delta% h_2^{j,k+1})^2) \label{easyquad}% \end{align}% It remains to estimate the quadratic terms in (\ref{easyquad}). This is contained% in% \begin{lemma}\label{quicklem} The following inequalities hold:% \begin{enumerate}% \item\begin{equation}%  |(\Delta z^{k+1})^2+(\Delta v_1^{k+1})^2| \le C(\eta^2+\tau^2)(\|q_1^k\|^2+% |r^k|^2)% \end{equation}% \item \begin{equation}\label{hest}% \|\Delta h^{j,k+1}\|^2\le C(\eta^2+\tau^2)(\|q_1^k\|^2+\|q_2^k\|^2+|r^k|^2)% \end{equation}% \item \begin{equation}% \left| (\Delta s_1^{j,k+1})^2\right| \le C(\eta^2+\tau^2)\|q_1^k\|^2% \end{equation}% \end{enumerate}% \end{lemma}% \begin{proof}% First, (1) and (3) follow directly from (\ref{update}) and (\ref{admom}).% To show (2), note % that from (\ref{deltag1}), and Assumption 1,% \begin{align}% |\Delta h_1^{j,k+1}| % &\le |g'(v_1^k\cdot\xi^j)\Delta v_1^{k+1}\cdot x^j|\\&+| \frac 12g''(\hat t_{1,j,k})(\Delta% v_1^{k+1}\cdot x^j)^2|\\ % &\le C((\eta+\tau)\|q_1^k\|+ (\tau^2+\eta^2)\|q_1^k\|^2)\\% &\le C(\eta+\tau+\tau^2+\eta^2)\|q_1^k\|.% \end{align}% The last inequality uses the fact that $q_i$ are uniformly bounded, which% can be derived from the Assumptions.% Since $\eta,\tau <1$,% \begin{equation}% |\Delta h_1^{j,k+1}|^2 \le C(\tau^2+\eta^2)\|q_1^k\|^2.% \label{h1est}% \end{equation}%  For the second node, by Taylor's theorem\begin{align}% |\Delta h_2^{j,k+1}| &\le|g'(s_2^{j,k})\Delta s_2^{k+1,j}|+ |\frac 12g''(\hat% t_{2,j,k})(\Delta s_2^{j,k+1})^2|.\label{hesto}% \end{align}% Again, from the Assumptions,\begin{align}% \left| \Delta s_2^{j,k+1}\right| &= \left|\Delta( z^{k+1}g(s_1^{j,k+1}))+% \xi^j \cdot \Delta v_{2}^{k+1}\right|\\% &\le C(|\Delta z^{k+1}|+\|\Delta v_2^{k+1}\|).% \end{align}% Since% \begin{align}% |\Delta z^{k+1}| &\le C(\eta+\tau)|r^k|,  \quad |\Delta v_2^{k+1}|\le C(\eta+\tau)\|q_{2}^{k}\|,% \end{align}% we may write, % \begin{equation}% \left| \Delta s_2^{j,k+1}\right| \le  C(\eta+\tau)(|r^k|+\|q_{2}^{k}\|).% \end{equation}% Thus, from (\ref{hesto})% \begin{align}\label{h2est}% \left| (\Delta h_2^{j,k+1})^2\right| \le  C(\eta^2+\tau^2)(|r^k|^2+\|q_{2}^{k}\|^2).% \end{align}% Combining (\ref{h1est}) and (\ref{h2est}) then shows part (2).% \end{proof}% Bounds for the remaining terms  in (\ref{eiter}) are provided in   % \begin{lemma}\label{lem2}The following bounds hold:% \begin{enumerate}% \item \label{ineq2}% \begin{align}% &|\tau_kp^k\cdot \Delta w^k| \le \tau\|p^k\|.% \end{align}% \item \begin{align}&\| \Delta w^{k+1}\cdot\Delta h^{j,k+1}\| ^2\le \\&C(\eta^2+\tau^2)(\|p^k\|+\|q_1^k\|^2+\|q_2^k\|^2+|r^k|^2).% \end{align}% \item % \begin{align}\label{ineq4}% &|\sum_{j = 1}^Jg_j''(t_{k,j})(\Delta  s_y^{j,k+1})^2| \\&\le C(\eta^2+\tau^2)(\|p^k\|^2+\|q_1^k\|^2+\|q_2^k\|^2+|r^k|^2).% \end{align}% \end{enumerate}% \end{lemma}% \begin{proof}% To show (1), we can apply (\ref{update}) and (\ref{admom}).  % For part (2), we also use (\ref{update}) and (\ref{admom}), along with (\ref{hest}),% giving % \begin{align}% \|\Delta w^{k+1}\Delta h^{j,k+1}\|  \le C(\|\Delta w^{k+1}\|^2+ \|\Delta% h^{j,k+1}\|^2)\\% \le C(\eta^2+\tau^2)(\|p^k\|^2+\|q_1^k\|^2+\|q_2^k\|^2+|r^k|^2).% \end{align}% For (3), recall% \begin{align}\label{diffs}% |\Delta  s_y^{j,k+1}| =  |h^{j,k}\cdot\Delta w^{k+1}|+|\Delta (h^{j,k+1})\cdot% w^{k+1}|.% \end{align}% From Assumption 1, it follows that $h^{j,k}$ is uniformly bounded, so that\begin{align}\label{sdiff1}%   |h^{j,k}\cdot\Delta w^{k+1}|\le C\|\Delta w^{k+1}\| \le C(\eta +\tau)\|p^k\|.% \end{align}% For the second term in (\ref{diffs}), notice% from Lemma \ref{quicklem} that% \begin{align}\label{sdiff2}% &|\Delta (h^{j,k+1})\cdot w^{k+1}|^2\le C\|\Delta (h^{j,k+1})\|^2\\% &\le C(\eta^2+\tau^2)(\|q_1^k\|^2+\|q_2^k\|^2+|r^k|^2).^{}% \end{align}% Combining (\ref{sdiff1}) and (\ref{sdiff2}) then give the result.% \end{proof}% \textbf{Proof of Lemma \ref{main2nodelem}}. From (\ref{eiter}), we may bound% the first two terms using part (1) of Lemma (\ref{lem2}), the third term% with (\ref{mainineq1}) and Lemma \ref{quicklem}, the fourth term using part% (3) of Lemma \ref{lem2} part ,  and the final terms using part 2 of Lemma% \ref{lem2} part along with the identity $|x\cdot y |\le C(\|x\|^2+ \|y\|^2)$.%   \qed% \subsubsection*{Proof of convergence}% We now use the a Lemma found in  \cite{sun2006}:% \begin{lemma}\label{canned}% Let $f \in C^1( \mathbb R^n, \mathbb R)$, and suppose that  $\nabla f$ vanishes% at a finite set of points.  Then, for a sequence $\{x_k\}$ if $\|\Delta x^k\|% \rightarrow 0$ and $\|\nabla f(x^k)\|\rightarrow 0$, then for some $x^* \in% \mathbb R^n$,  $x^k \rightarrow x^*$ and $\nabla f(x^*) = 0$.% \end{lemma}Through Lemmas \ref{quicklem} and \ref{lem2}, and (\ref{mainineq1}),% we can now  bound the iteration of error by % \begin{align}\label{edown}% &\Delta E^{k+1} \le (\tau-\eta)(\|p^k\|^2+\|q_2^k\|^2+|r^k|^2+\|q_1^k\|^2)%  \\&+C(\eta^2+\tau^2)(\|p\|^2+\|q_1^k\|^2+\|q_2^k\|^2+|r^k|^2)%  \\ &=(-\eta +\tau+C(\eta^2+\tau^2)) (\|p\|^2+\|q_1^k\|^2+\|q_2^k\|^2+|r^k|^2)%  \end{align}% Assuming $\eta = s\tau $, it is straightforward to show that the term in% front of the norms is negative when% \begin{align}% \eta < \frac{1-s}{C(s^2+1)}.% \end{align}% Under this constraint, $E^k$ is decreasing under each iteration. The summability% for $\|q_{2}^k\|^2$ also follows, since% \begin{align}% \sum_{k = 1}^\infty \|q_{2}^k\|^2 \le\frac 1{\left(\eta-\tau- C(\tau^2+\eta^2)\right)}% \sum_{k = 1}^\infty \Delta E^k <\infty.   % \end{align}% Similar calculations show summability of $\|q_{1}^k\|^2, |r^k|^2$, and $\|p^k\|^2$.% Thus  \begin{align}\|q_{1}^k\|,   \|q_{2}^k\|,|r^k|^2,  \|p^k\|^2\rightarrow% 0, \quad \\ \quad \|\Delta v_{1}^k\|, \|\Delta v_{2}^k\|,% \|\Delta z^k\|, \|\Delta w^k\|\rightarrow 0. \end{align}Lemma \ref{canned}% and Assumption 3 imply a set of minimum weights $v^*_{1}, v_2^*, z^*, w^*$,% which determine a local minimum of $E$. This shows Theorem \ref{maint2}.\subsection{Notation and conventions}

In what follows, we will need some notation for  tensor manipulation.   First, we recall  the matrix inner product, which for two matrices $A = (a_{i,j})_{n\times m}, B = (b_{i,j})_{n\times
m}$, is defined as $A:B = \sum_{i,j} a_{i,j}b_{i,j}$. Also, a matrix gradient
of a real (vector) valued function is matrix (tensor) valued, with an element-wise
representation as  $\frac{\partial f }{\partial v_{(i,j)}^k} = \left(\frac{\partial
f }{\partial v_{(i,j)}^{a,b;k}}\right)_{l_i\times l_j}$ and $\frac{\partial
H_m^k }{\partial v_{(i,j)}^k}= \left(\frac{\partial H_m^{c;k} }{\partial
v_{(i,j)}^{a,b;k}}\right)_{l_i\times l_j\times l_m}$.

In all future estimates, we look at backpropagation over a single input, meaning $J = 1$. This allows us to suppress the variable $p$, which is essentially done for the sake of presentation.  The proofs of Theorem \ref{bigtheo} and Lemma \ref{simplem} generalize immediately to the case of multiple inputs by taking sums over all inputs. Finally, in our estimates, we will use the constant $C>0$ which depends solely on fixed parameters in the network, such as the input value $x$, uniform bounds of node outputs $H_j$ and inputs $S_j$, and for generalizing to multiple inputs, the size of the dataset $J$. The constant $C$ is used in multiple estimates, and may increase for each time it appears.

\subsection{Estimates on node output increments}

Our  major technical theorem shows that the increment of outputs $\Delta H^{k+1}$   is
similar, up to first order, to $Q^k(H^k)$, where  $Q^k$ denotes the differential operator
\begin{equation}\label{qeqn}
Q^k =\sum_{i<j\le L}  \Delta v_{(i,j)}^{k+1}:\frac{\partial }{\partial v_{(i,j)}^{k}}.
\end{equation}
Note that when $Q^k$ acts on a  length $l_m$ vector, the matrix inner product in (\ref{qeqn})
is between a $l_i \times l_j$-sized matrix and a $l_i\times l_j \times l_m$-sized tensor, and is vector of size $l_m$.

The major utility of introducing $Q^k$ is that it provides a simple bound
when acting on $E^k$.  Specifically, using (\ref{defq}) , (\ref{itdef}),
and (\ref{taudef}), it is straightforward to show
\begin{equation}
Q^k(E^k)\le (-\eta+\tau)\sum_{i<j \le L}\|q_{(i,j)} ^k\|^2. \label{midest}
\end{equation}\begin{theorem}\label{bigtheo}

There exists a universal constant $C>0$ such that

\begin{equation}
|Q^k(E^k)-\Delta E^{k+1}| \le C\left(\sum_{n\le L}\|\Delta H_n^k\|^2 + \sum_{m<n\le
L}\|\Delta v_{(m,n)}^k\|^2\right). \label{th1bnd}
\end{equation}
\end{theorem}\begin{proof} We show (\ref{th1bnd}) follows through three steps: (1) finding a recurrence relation, with respect to the ordering of hidden layers, for $Q^k(H^k_n)$, (2) finding a similar relation for $\Delta H^{k+1}$, and (3) comparing the two relations.

 \textit{(1) (A recurrence for $Q^k(H_n^k)$).} Applying the  chain rule to the total error (\ref{edef}), using (\ref{nodeeqn}),
 and rearranging sums, \begin{align}\label{crule1}
Q^k(E^{k}) = g'(S_L)\sum_{i<j\le L} \Delta v_{(i,j)}^{k+1}:
\frac{\partial }{\partial v_{(i,j)}^k}\left(\sum_{m<L}H_m^{k}v_{(m,L)}^k
 \right)\\\\  =   g'(S_L)\sum_{m<L}\sum_{i<j\le L} \Delta v_{(i,j)}^{k+1}:
\frac{\partial }{\partial v_{(i,j)}^k}\left(H_m^{k}v_{(m,L)}^k \right) \label{qeform}.
\end{align}

We now focus on expressing (\ref{qeform}) in a more compact form. We begin with considering the terms in (\ref{qeform}) with $j = L$. We first work elementwise by differentiating with respect to  the $(a,b)$ entry of the matrix derivative for the tensor $\frac{\partial }{\partial v_{(i,j)}^k}\left(H_m^{k}v_{(m,L)}^k of \right)$. From the product rule,  this can be written as a sum of vectors, with
\begin{align} \label{msplit}
&\frac{\partial }{\partial v_{(i,L)}^{a,b;k}}\left( H_m^{k}v_{(m,L)}^k \right)
= \frac{\partial  H_m^{k} }{\partial v_{(i,L)}^{a,b;k}}v_{(m,L)}^k + H_m^{k}
\frac{\partial v_{(m,L)}^k }{\partial v_{(i,L)}^{a,b;k}}\\
  &=  \frac{\partial  H_m^{k} }{\partial v_{(i,L)}^{a,b;k}}v_{(m,L)}^k +
(0, \dots,\underbrace{ \delta_{i,m}H_m^{a;k}}_{\hbox{ $b^{th}$ entry}}, \dots,
0)\\
&:=A_{i,m}^{a,b;k}+B_{i,m}^{a,b;k}.  \label{msplit2}
\end{align}

%  Then we may write the matrix gradient as
%  \begin{align}
%  &\frac{\partial }{\partial v_{(i,L)}^{k}}\left( H_m^{k}v_{(m,L)}^k \right)\\
%   &= \frac{\partial  H_m^{k} }{\partial v_{(i,L)}^{k}}v_{(m,L)}^k +\delta_{i,m}
%\begin{pmatrix}h_{m}^{1,k} & \dots & h_{m}^{1,k} \\
% \vdots & \ddots & \vdots \\
% h_{m}^{l_m,k} & \dots &  h_{m}^{l_m,k} \label{secpart}\\
% \end{pmatrix} \\
% &:= A_{i,m}^k+B_{i,m}^k \label{eqj}
%  \end{align}
Each of these terms is handled in turn.  First, summing the matrix inner product of the matrix $\Delta v_{(i,L)}^{k+1}$ and the tensor $A_{i,m}^k$  may be rewritten as
\begin{align}
\sum_{m<L}\sum_{i< L} \Delta v_{(i,L)}^{k+1}:A_{i,m}^k
  &= \sum_{m<L} \sum_{i<L} \sum_{\substack{a<l_i\\b<l_L}}\Delta v_{(i,L)}^{a,b;k+1}\frac{\partial
 H_m^{k} }{\partial v_{(i,L)}^{a,b;k}}v_{(m,L)}^k\\
&= \sum_{m<L} \sum_{i< L} \left(\Delta v_{(i,L)}^{k+1}:\frac{\partial H_m^k }{\partial
v_{(i,L)}^k}\right)v^k_{(m,L)}.
 \label{nondiag}
\end{align}

For $B_{i,m}^k$, we again work elementwise, and write the inner product as
\begin{align}
&\sum_{m<L}\sum_{i< L} \Delta v_{(i,L)}^{k+1}:B_{i,m}^k
  = \sum_{m<L} \sum_{\substack{a\le l_i\\b\le l_L}}\Delta v_{(m,L)}^{a,b;k+1}B_{m,m}^{a,b;k}\\
     &=\sum_{m<L}\left(\sum_{b<l_L}H_m^{1,k}\Delta v_{(m,L)}^{1,b;k+1}, \dots,
\sum_{b<l_L}H_m^{l_m,k}\Delta v_{(m,L)}^{l_m,b;k+1}\right)\\&=\sum_{m<L}H_m^k\Delta
v_{(m,L)}^{k+1}.\label{diag}
\end{align}

Calculations for double sum in (\ref{qeform}) for the remaining terms with $j<L$ are similar to the case $j = L$, except that there is
no corresponding $B_{i,m}^k$ term. Indeed, we can show
\begin{align}
\sum_{m<L}\sum_{i<j< L} \Delta v_{(i,j)}^{k+1}:
\frac{\partial }{\partial v_{(i,j)}^k}\left(H_m^{k}v_{(m,L)}^k \right)
 = \sum_{m<L}\sum_{i<j< L}\Delta v_{(i,j)}^{k+1}:\left(\frac{\partial H_m^k}{\partial
v_{(i,j)}^k}v_{(m,L)}^k\right). \label{lessj} \end{align}
Combining (\ref{nondiag}), (\ref{diag}) and (\ref{lessj}),

  \begin{align}
 &\sum_{m<L}\sum_{i<j\le L} \Delta v_{(i,j)}^{k+1}:
\frac{\partial }{\partial v_{(i,j)}^k}\left( H_m^{k}v_{(m,L)}^k \right) \label{bigsumrewrite}\\
&= \sum_{m<L}H_m^k\Delta v_{(m,L)}^{k+1}
+\sum_{m<L}\sum_{i<j\le m}\Delta v_{(i,j)}^{k+1}:\left(\frac{\partial H_m^k}{\partial
v_{(i,j)}^k}v_{(m,L)}^k
 \right) \label{firstrep}\\
 &= \sum_{m<L}\left(H_m^k\Delta v_{(m,L)}^{k+1} +Q^k(H^k_m)v^k_{(m,L)}
\right).
\end{align}
Note that \ref{firstrep} uses the fact that since $H_m^k$ only depends on layers $1$ through $m-1$, we may truncate the sum of $Q^k$ and write\begin{equation}
Q^k(H_m^k) =\sum_{i<j\le m}  \Delta v_{(i,j)}^{k+1}:\frac{\partial H_m^k
}{\partial v_{(i,j)}^k}.
\end{equation}
We may now substitute (\ref{bigsumrewrite}) into (\ref{qeform}) to yield the recursive formula
\begin{align}
Q^k(E^k)=  g'\left(S_L^k \right)\sum_{m<L}\left( H_m^k\Delta v_{(m,L)}^{k+1}+Q^k(H^k_m)v^k_{(m,L)}
  \right).
\end{align}
From similar calculations, the formula over a node $H^k_n$, with $n<L$, is
\begin{align}
Q^k(H^k_n)= g'\left(S_n^k\right)\sum_{m<n}\left( H_m^k\Delta v_{(m,n)}^{k+1}+Q^k(H^k_m)v^k_{(m,n)}
  \right).\label{qrec}
\end{align}

\textit{(2) (A recurrence for $\Delta H_n^{k+1}$).} A recursive formula for   $\Delta H^{k+1}_n$ is found through a Taylor expansion of $E(S_L^{k+1})$ centered at $S_L^k$. Specifically, there exists $t_{k}$ between $E^{k}$
and $E^{k+1}$ with
\begin{align}
\Delta E^{k+1} &=  g'\left(S_L^{k} \right)\Big(\sum_{m < L} \Delta( v_{(m,L)}^{k+1}\cdot H_m^{k+1}) \Big)+g''(t_{k})\left(\sum_{m < L}\Delta( v_{(m,L)}^{k+1}\cdot H_m^{k+1})\right)^2\\
 &=  g'\left(S_L^{k} \right)\Big(\sum_{m < L} \Delta H_m^{k+1}
v_{(m,L)}^k +H^k_m\Delta v_{(m,L)}^{k+1}
+\sum_{m < L}\Delta v_{(m,L)}^{k+1}\cdot \Delta H_m^{k+1} \Big)
\\&+g''(t_{k})\left(\sum_{m < L}\Delta( v_{(m,L)}^{k+1}\cdot H_m^{k+1})\right)^2.
\end{align}
Similarly,there exist $t_{n,k} = (t_{n,k}^{1}, \dots, t_{n,k}^{l_l})$ where each
$t_{n,k}^r, r = 1, \dots l_n$ lies between $H_n^k$ and $H_n^{k+1}$ and
\begin{align}
&\Delta H_n^{k+1} = g'\left(S_n^k \right)\Big(\sum_{m < n}\Delta H_m^{k+1}v_{(m,n)}^k
 +H^k_m\cdot\Delta v_{(m,n)}^{k+1}
+\sum_{m < n}\Delta H_m^{k+1}\Delta v_{(m,n)}^{k+1}  \Big)
\\+&\frac 12g''(t_{n,k})\left(\sum_{m < n}H_m^{k+1}\Delta v_{(m,n)}^{k+1}
\right)^2\label{diffrec}.
\end{align}


\textit{(3) (Comparing recurrences).}
From (1) and (2) of Assumptions \ref{assumes}, we may derive the simple bound
\begin{equation}
\|\Delta H_m^{k+1}\Delta v_{(m,n)}^{k+1}\| \le C( \|\Delta v_{(m,n)}^{k+1}\|^2+\|\Delta
H_m^{k+1}\|^2)
\end{equation}
for some constant $C>0$.
Taking differences of (\ref{diffrec}) and (\ref{qrec}),
for any $n<L$,  we then obtain the recurrence inequality
\begin{align}
&\|Q^k(H_n^{k})-\Delta H_n^{k+1} \|\le C\left(\sum_{m < n}\|Q^k(
H_m^{k})-\Delta H_m^{k+1} \|  \right)  \label{rec1}\\&+ C\sum_{m<n} ( \|\Delta v_{(m,n)}^{k+1}\|^2+\|\Delta
H_m^{k+1}\|^2) \label{mainrec1}.
\end{align}
Replacing $H_n^k$ with $E^k$ in (\ref{rec1}) produces the same type of inequality,
with the sum in (\ref{mainrec1}) now ranging from $m = 1, \dots, L-1$. Repeated
applications of (\ref{mainrec1}) to  $E^k$ and subsequently to $H_n^k$,
for $n = 1, \dots, L-1$, result in
\begin{align}
&|Q^k(E^k)-\Delta E^{k+1}|\le C\left(\|Q^k( H_0^{k})-\Delta H_0^{k+1} \|  \right)
\\&+ C\left(\sum_{n< L}\|\Delta H_n^k\|^2 + \sum_{m<n< L}\|\Delta v_{(m,n)}^k\|^2\right)
\label{mainrec}.
\end{align}
To complete the proof, we note that
 the input data $x$ does not change under iterations, so \begin{equation}
Q^k( H_0^{k})-\Delta H_0^{k+1} \equiv 0.
\end{equation}
\end{proof} We now bound the quadratic terms in (\ref{th1bnd}).

\begin{lemma}\label{simplem}
For some constant $C>0$,

\begin{enumerate}
\item
\begin{equation}
\|\Delta v^k_{(m,n)}\| \le (\eta+\tau)\|q_{(m,n)}^k\| \label{fstpart}
\end{equation}
\item
\begin{equation}
\|\Delta H^k_n\| \le C(\eta+\tau)\sum_{m<n}\|q_{(m,n)}^k\|
\end{equation}
\end{enumerate}
\end{lemma}\begin{proof}
We may show (\ref{fstpart}) immediately from (\ref{itdef}) and (\ref{taudef}).
For the next inequality, note that from Taylor's theorem, and the boundedness
of $g'$ and  $g''$:

\begin{align}
\|\Delta H^{k+1}_n\| \le |g'(S_n^k)|\left \|\Delta\left(\sum_{m < n} H_m^{k+1}v_{(m,n)}^k
 \right)\right \|\\ +\frac 12|g''(t_{n}^k)|\left \|\Delta\left(\sum_{m <
n}H_m^{k+1}v_{(m,n)}^k \right)^2\right \|.
\end{align}
From the boundedness Assumptions (1) and (2), we may use (\ref{fstpart}) to rewrite this as
\begin{align}
&\|\Delta H^{k+1}_n\| \le C \sum_{m < n}\|\Delta v^k_{(m,n)}\|+\|\Delta v^k_{(m,n)}\|^2\\
&\le C(\tau+\eta)(\sum_{m< n}\|q_{(m,n)}^k\|+\|q_{(m,n)}^k\|^2) \le C(\tau+\eta)\sum_{m
< n}\|q_{(m,n)}^k\| .
\end{align}
 The last inequality uses the fact that from Assumption (2), we may uniformly
bound gradients.
\end{proof}Now, combining Theorem \ref{bigtheo}, (\ref{midest}), and Lemma \ref{simplem},
the iteration of error may then be estimated as
\begin{align}
\Delta E^{k+1} &\le  C\left(\sum_{n\le L}\|\Delta H_n^k\|^2 + \sum_{m<n\le
L}\|\Delta v_{(m,n)}^k\|^2\right)+Q^k(E^k)
\\&\le \left(-\eta+\tau+C(\tau^2+\eta^2)\right) \sum_{m<n \le L} \|q_{(m,n)}^k\|^2.
\label{graddif}
\end{align}\subsubsection{Proof of convergence}

We now use a Lemma found in  \cite{sun2006}:

\begin{lemma}\label{canned}
Let $f \in C^1( \mathbb R^n, \mathbb R)$, and suppose that  $\nabla f$ vanishes
at a finite set of points.  Then, for a sequence $\{x_k\}$ if $\|\Delta x^k\|
\rightarrow 0$ and $\|\nabla f(x^k)\|\rightarrow 0$, then for some $x^* \in
\mathbb R^n$,  $x^k \rightarrow x^*$ and $\nabla f(x^*) = 0$.
\end{lemma}

For some $s \in (0,1)$, assume $\eta = s\tau $.  It is straightforward to show that the term in
front of the norms in \ref{graddif} is negative when

\begin{align}
\eta < \frac{1-s}{C(s^2+1)}.
\end{align}
Under this constraint, $E^k$ is decreasing under each iteration. The summability
for $\|q_{2}^k\|^2$ also follows, since
\begin{align}
\sum_{k = 1}^\infty \|q_{(i,j)}^k\|^2 \le\frac 1{\left(\eta-\tau- C(\tau^2+\eta^2)\right)}
\sum_{k = 1}^\infty \Delta E^k <\infty.
\end{align}
Thus  $\|q_{1}^k\|\rightarrow 0$ and, from (\ref{fstpart}),  $ \|\Delta v_{(i,j)}^k\|\rightarrow 0.$
Lemma \ref{canned}
and part (3) of Assumption 2 imply a set of minimum weights $v^*_{1}, v_2^*, z^*, w^*$,
which determine a local minimum of $E$. This shows Theorem \ref{maint2}.

\section{Conclusion}
We have studied a feed-forward network with cross-layer connections. The possible directed graph architectures are the class of directed acyclic graphs.  As shown in \cite{huang2016densely}, introducing skip connections often increases the accuracy of a neural networks. In \cite{agarwal2018crossencoder} and in Section \ref{sec:experiment}, we have demonstrated increased accuracy in the setting of autoencoders. For our main result, we have established the convergence of backpropagation with adaptive momentum of networks with skip connections. This generalizes the result of Wu et al. \cite{wu2008convergence} who established convergence for a feed forward network with one hidden layer. While we have considered general DAG architectures, it remains to investigate, both through theory and experiment, the optimality properties with regards to the number of layers and  skip connections. We hope to address these properties in future works.


\bibliography{references}


\end{document}
