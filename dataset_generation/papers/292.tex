\documentclass{article} % For LaTeX2e
\usepackage{iclr2019_conference,times}
\usepackage{comment}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
% basic\usepackage{color,xcolor}\usepackage{epsfig}\usepackage{graphicx}\usepackage{tikz}\usetikzlibrary{arrows}% figure and table\usepackage{tabularx}\usepackage{adjustbox}\usepackage{array}\usepackage{booktabs}\usepackage{colortbl}\usepackage{float,wrapfig}\usepackage{hhline}\usepackage{multirow}\usepackage{subcaption}% issues a warning with CVPR/ICCV format% font and character\usepackage{amsmath,amsfonts,amsthm,amssymb}\usepackage{bm}\usepackage{nicefrac}\usepackage{microtype}\usepackage{dsfont}% layout\usepackage{changepage}\usepackage{extramarks}\usepackage{fancyhdr}\usepackage{lastpage}\usepackage{setspace}\usepackage{soul}\usepackage{xspace}% ref% \usepackage[bookmarks=false]{hyperref}% \usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}\usepackage[pagebackref=true,breaklinks=true,colorlinks,citecolor=black,urlcolor=blue,bookmarks=false]{hyperref}\usepackage{url}% misc\usepackage{algorithm, algorithmic}% \usepackage{enumerate}\usepackage{enumitem}\usepackage{todonotes}% conflict with CVPR/ICCV format\usepackage{titlesec}
%%%%% NEW MATH DEFINITIONS %%%%%\usepackage{amsmath,amsfonts,bm}\def\eps{{\epsilon}}% Random variables\def\reta{{\textnormal{$\eta$}}}\def\ra{{\textnormal{a}}}\def\rb{{\textnormal{b}}}\def\rc{{\textnormal{c}}}\def\rd{{\textnormal{d}}}\def\re{{\textnormal{e}}}\def\rf{{\textnormal{f}}}\def\rg{{\textnormal{g}}}\def\rh{{\textnormal{h}}}\def\ri{{\textnormal{i}}}\def\rj{{\textnormal{j}}}\def\rk{{\textnormal{k}}}\def\rl{{\textnormal{l}}}% rm is already a command, just don't name any random variables m\def\rn{{\textnormal{n}}}\def\ro{{\textnormal{o}}}\def\rp{{\textnormal{p}}}\def\rq{{\textnormal{q}}}\def\rr{{\textnormal{r}}}\def\rs{{\textnormal{s}}}\def\rt{{\textnormal{t}}}\def\ru{{\textnormal{u}}}\def\rv{{\textnormal{v}}}\def\rw{{\textnormal{w}}}\def\rx{{\textnormal{x}}}\def\ry{{\textnormal{y}}}\def\rz{{\textnormal{z}}}% Random vectors\def\rvepsilon{{\mathbf{\epsilon}}}\def\rvtheta{{\mathbf{\theta}}}\def\rva{{\mathbf{a}}}\def\rvb{{\mathbf{b}}}\def\rvc{{\mathbf{c}}}\def\rvd{{\mathbf{d}}}\def\rve{{\mathbf{e}}}\def\rvf{{\mathbf{f}}}\def\rvg{{\mathbf{g}}}\def\rvh{{\mathbf{h}}}\def\rvu{{\mathbf{i}}}\def\rvj{{\mathbf{j}}}\def\rvk{{\mathbf{k}}}\def\rvl{{\mathbf{l}}}\def\rvm{{\mathbf{m}}}\def\rvn{{\mathbf{n}}}\def\rvo{{\mathbf{o}}}\def\rvp{{\mathbf{p}}}\def\rvq{{\mathbf{q}}}\def\rvr{{\mathbf{r}}}\def\rvs{{\mathbf{s}}}\def\rvt{{\mathbf{t}}}\def\rvu{{\mathbf{u}}}\def\rvv{{\mathbf{v}}}\def\rvw{{\mathbf{w}}}\def\rvx{{\mathbf{x}}}\def\rvy{{\mathbf{y}}}\def\rvz{{\mathbf{z}}}% Elements of random vectors\def\erva{{\textnormal{a}}}\def\ervb{{\textnormal{b}}}\def\ervc{{\textnormal{c}}}\def\ervd{{\textnormal{d}}}\def\erve{{\textnormal{e}}}\def\ervf{{\textnormal{f}}}\def\ervg{{\textnormal{g}}}\def\ervh{{\textnormal{h}}}\def\ervi{{\textnormal{i}}}\def\ervj{{\textnormal{j}}}\def\ervk{{\textnormal{k}}}\def\ervl{{\textnormal{l}}}\def\ervm{{\textnormal{m}}}\def\ervn{{\textnormal{n}}}\def\ervo{{\textnormal{o}}}\def\ervp{{\textnormal{p}}}\def\ervq{{\textnormal{q}}}\def\ervr{{\textnormal{r}}}\def\ervs{{\textnormal{s}}}\def\ervt{{\textnormal{t}}}\def\ervu{{\textnormal{u}}}\def\ervv{{\textnormal{v}}}\def\ervw{{\textnormal{w}}}\def\ervx{{\textnormal{x}}}\def\ervy{{\textnormal{y}}}\def\ervz{{\textnormal{z}}}% Random matrices\def\rmA{{\mathbf{A}}}\def\rmB{{\mathbf{B}}}\def\rmC{{\mathbf{C}}}\def\rmD{{\mathbf{D}}}\def\rmE{{\mathbf{E}}}\def\rmF{{\mathbf{F}}}\def\rmG{{\mathbf{G}}}\def\rmH{{\mathbf{H}}}\def\rmI{{\mathbf{I}}}\def\rmJ{{\mathbf{J}}}\def\rmK{{\mathbf{K}}}\def\rmL{{\mathbf{L}}}\def\rmM{{\mathbf{M}}}\def\rmN{{\mathbf{N}}}\def\rmO{{\mathbf{O}}}\def\rmP{{\mathbf{P}}}\def\rmQ{{\mathbf{Q}}}\def\rmR{{\mathbf{R}}}\def\rmS{{\mathbf{S}}}\def\rmT{{\mathbf{T}}}\def\rmU{{\mathbf{U}}}\def\rmV{{\mathbf{V}}}\def\rmW{{\mathbf{W}}}\def\rmX{{\mathbf{X}}}\def\rmY{{\mathbf{Y}}}\def\rmZ{{\mathbf{Z}}}% Elements of random matrices\def\ermA{{\textnormal{A}}}\def\ermB{{\textnormal{B}}}\def\ermC{{\textnormal{C}}}\def\ermD{{\textnormal{D}}}\def\ermE{{\textnormal{E}}}\def\ermF{{\textnormal{F}}}\def\ermG{{\textnormal{G}}}\def\ermH{{\textnormal{H}}}\def\ermI{{\textnormal{I}}}\def\ermJ{{\textnormal{J}}}\def\ermK{{\textnormal{K}}}\def\ermL{{\textnormal{L}}}\def\ermM{{\textnormal{M}}}\def\ermN{{\textnormal{N}}}\def\ermO{{\textnormal{O}}}\def\ermP{{\textnormal{P}}}\def\ermQ{{\textnormal{Q}}}\def\ermR{{\textnormal{R}}}\def\ermS{{\textnormal{S}}}\def\ermT{{\textnormal{T}}}\def\ermU{{\textnormal{U}}}\def\ermV{{\textnormal{V}}}\def\ermW{{\textnormal{W}}}\def\ermX{{\textnormal{X}}}\def\ermY{{\textnormal{Y}}}\def\ermZ{{\textnormal{Z}}}% Vectors\def\vzero{{\bm{0}}}\def\vone{{\bm{1}}}\def\vmu{{\bm{\mu}}}\def\vtheta{{\bm{\theta}}}\def\va{{\bm{a}}}\def\vb{{\bm{b}}}\def\vc{{\bm{c}}}\def\vd{{\bm{d}}}\def\ve{{\bm{e}}}\def\vf{{\bm{f}}}\def\vg{{\bm{g}}}\def\vh{{\bm{h}}}\def\vi{{\bm{i}}}\def\vj{{\bm{j}}}\def\vk{{\bm{k}}}\def\vl{{\bm{l}}}\def\vm{{\bm{m}}}\def\vn{{\bm{n}}}\def\vo{{\bm{o}}}\def\vp{{\bm{p}}}\def\vq{{\bm{q}}}\def\vr{{\bm{r}}}\def\vs{{\bm{s}}}\def\vt{{\bm{t}}}\def\vu{{\bm{u}}}\def\vv{{\bm{v}}}\def\vw{{\bm{w}}}\def\vx{{\bm{x}}}\def\vy{{\bm{y}}}\def\vz{{\bm{z}}}% Elements of vectors\def\evalpha{{\alpha}}\def\evbeta{{\beta}}\def\evepsilon{{\epsilon}}\def\evlambda{{\lambda}}\def\evomega{{\omega}}\def\evmu{{\mu}}\def\evpsi{{\psi}}\def\evsigma{{\sigma}}\def\evtheta{{\theta}}\def\eva{{a}}\def\evb{{b}}\def\evc{{c}}\def\evd{{d}}\def\eve{{e}}\def\evf{{f}}\def\evg{{g}}\def\evh{{h}}\def\evi{{i}}\def\evj{{j}}\def\evk{{k}}\def\evl{{l}}\def\evm{{m}}\def\evn{{n}}\def\evo{{o}}\def\evp{{p}}\def\evq{{q}}\def\evr{{r}}\def\evs{{s}}\def\evt{{t}}\def\evu{{u}}\def\evv{{v}}\def\evw{{w}}\def\evx{{x}}\def\evy{{y}}\def\evz{{z}}% Matrix\def\mA{{\bm{A}}}\def\mB{{\bm{B}}}\def\mC{{\bm{C}}}\def\mD{{\bm{D}}}\def\mE{{\bm{E}}}\def\mF{{\bm{F}}}\def\mG{{\bm{G}}}\def\mH{{\bm{H}}}\def\mI{{\bm{I}}}\def\mJ{{\bm{J}}}\def\mK{{\bm{K}}}\def\mL{{\bm{L}}}\def\mM{{\bm{M}}}\def\mN{{\bm{N}}}\def\mO{{\bm{O}}}\def\mP{{\bm{P}}}\def\mQ{{\bm{Q}}}\def\mR{{\bm{R}}}\def\mS{{\bm{S}}}\def\mT{{\bm{T}}}\def\mU{{\bm{U}}}\def\mV{{\bm{V}}}\def\mW{{\bm{W}}}\def\mX{{\bm{X}}}\def\mY{{\bm{Y}}}\def\mZ{{\bm{Z}}}\def\mBeta{{\bm{\beta}}}\def\mPhi{{\bm{\Phi}}}\def\mLambda{{\bm{\Lambda}}}\def\mSigma{{\bm{\Sigma}}}% Tensor\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}% \newcommand{\tens}[1]{\bm{\mathsfit{#1}}}\def\tA{{\tens{A}}}\def\tB{{\tens{B}}}\def\tC{{\tens{C}}}\def\tD{{\tens{D}}}\def\tE{{\tens{E}}}\def\tF{{\tens{F}}}\def\tG{{\tens{G}}}\def\tH{{\tens{H}}}\def\tI{{\tens{I}}}\def\tJ{{\tens{J}}}\def\tK{{\tens{K}}}\def\tL{{\tens{L}}}\def\tM{{\tens{M}}}\def\tN{{\tens{N}}}\def\tO{{\tens{O}}}\def\tP{{\tens{P}}}\def\tQ{{\tens{Q}}}\def\tR{{\tens{R}}}\def\tS{{\tens{S}}}\def\tT{{\tens{T}}}\def\tU{{\tens{U}}}\def\tV{{\tens{V}}}\def\tW{{\tens{W}}}\def\tX{{\tens{X}}}\def\tY{{\tens{Y}}}\def\tZ{{\tens{Z}}}% Graph\def\gA{{\mathcal{A}}}\def\gB{{\mathcal{B}}}\def\gC{{\mathcal{C}}}\def\gD{{\mathcal{D}}}\def\gE{{\mathcal{E}}}\def\gF{{\mathcal{F}}}\def\gG{{\mathcal{G}}}\def\gH{{\mathcal{H}}}\def\gI{{\mathcal{I}}}\def\gJ{{\mathcal{J}}}\def\gK{{\mathcal{K}}}\def\gL{{\mathcal{L}}}\def\gM{{\mathcal{M}}}\def\gN{{\mathcal{N}}}\def\gO{{\mathcal{O}}}\def\gP{{\mathcal{P}}}\def\gQ{{\mathcal{Q}}}\def\gR{{\mathcal{R}}}\def\gS{{\mathcal{S}}}\def\gT{{\mathcal{T}}}\def\gU{{\mathcal{U}}}\def\gV{{\mathcal{V}}}\def\gW{{\mathcal{W}}}\def\gX{{\mathcal{X}}}\def\gY{{\mathcal{Y}}}\def\gZ{{\mathcal{Z}}}% Sets\def\sA{{\mathbb{A}}}\def\sB{{\mathbb{B}}}\def\sC{{\mathbb{C}}}\def\sD{{\mathbb{D}}}% Don't use a set called E, because this would be the same as our symbol% for expectation.\def\sF{{\mathbb{F}}}\def\sG{{\mathbb{G}}}\def\sH{{\mathbb{H}}}\def\sI{{\mathbb{I}}}\def\sJ{{\mathbb{J}}}\def\sK{{\mathbb{K}}}\def\sL{{\mathbb{L}}}\def\sM{{\mathbb{M}}}\def\sN{{\mathbb{N}}}\def\sO{{\mathbb{O}}}\def\sP{{\mathbb{P}}}\def\sQ{{\mathbb{Q}}}\def\sR{{\mathbb{R}}}\def\sS{{\mathbb{S}}}\def\sT{{\mathbb{T}}}\def\sU{{\mathbb{U}}}\def\sV{{\mathbb{V}}}\def\sW{{\mathbb{W}}}\def\sX{{\mathbb{X}}}\def\sY{{\mathbb{Y}}}\def\sZ{{\mathbb{Z}}}% Entries of a matrix\def\emLambda{{\Lambda}}\def\emA{{A}}\def\emB{{B}}\def\emC{{C}}\def\emD{{D}}\def\emE{{E}}\def\emF{{F}}\def\emG{{G}}\def\emH{{H}}\def\emI{{I}}\def\emJ{{J}}\def\emK{{K}}\def\emL{{L}}\def\emM{{M}}\def\emN{{N}}\def\emO{{O}}\def\emP{{P}}\def\emQ{{Q}}\def\emR{{R}}\def\emS{{S}}\def\emT{{T}}\def\emU{{U}}\def\emV{{V}}\def\emW{{W}}\def\emX{{X}}\def\emY{{Y}}\def\emZ{{Z}}\def\emSigma{{\Sigma}}% entries of a tensor% Same font as tensor, without \bm wrapper% \newcommand{\etens}[1]{\mathsfit{#1}}\def\etLambda{{\etens{\Lambda}}}\def\etA{{\etens{A}}}\def\etB{{\etens{B}}}\def\etC{{\etens{C}}}\def\etD{{\etens{D}}}\def\etE{{\etens{E}}}\def\etF{{\etens{F}}}\def\etG{{\etens{G}}}\def\etH{{\etens{H}}}\def\etI{{\etens{I}}}\def\etJ{{\etens{J}}}\def\etK{{\etens{K}}}\def\etL{{\etens{L}}}\def\etM{{\etens{M}}}\def\etN{{\etens{N}}}\def\etO{{\etens{O}}}\def\etP{{\etens{P}}}\def\etQ{{\etens{Q}}}\def\etR{{\etens{R}}}\def\etS{{\etens{S}}}\def\etT{{\etens{T}}}\def\etU{{\etens{U}}}\def\etV{{\etens{V}}}\def\etW{{\etens{W}}}\def\etX{{\etens{X}}}\def\etY{{\etens{Y}}}\def\etZ{{\etens{Z}}}\let\ab\allowbreak
%%%%% NEW MATH DEFINITIONS %%%%%\usepackage{amsmath,amsfonts,bm}% comments% paper-specific macros\newcommand{\fid}{Fr\'echet Inception Distance\xspace}\newcommand{\concepts}{object concepts\xspace}\newcommand{\pgan}{Progressive GANs\xspace}\newcommand{\featuremap}{featuremap\xspace}\newcommand{\mask}{\texttt{mask}}\newcommand{\threshold}{\text{T}}\newcommand{\pixel}{\text{P}}\newcommand{\pixelall}{\sP}\newcommand{\KL}{D_{\mathrm{KL}}}\newcommand{\context}{K}\newcommand{\U}{\text{U}}\newcommand{\Uall}{\sU}\newcommand{\thresU}{\rvk}\newcommand{\thresUall}{\rvk}\newcommand{\rUP}{\repr_{\U, \pixel}}\newcommand{\rUPb}{\repr_{\overline{\U, \pixel}}}\newcommand{\rUb}{\repr_{\overline{\U}, \pixel}}\newcommand{\pp}{\repr_{\Uall, \pixel}}\newcommand{\ppb}{\repr_{\Uall, \overline{\pixel}}}\newcommand{\contextr}{\context^*}\newcommand{\Ezp}{\E_{\rvz,\pixel}}\newcommand{\IoU}{\text{IoU}}\newcommand{\ACE}{\delta}\newcommand{\ACEUc}{\ACE_{\U\rightarrow c}}\newcommand{\xinsert}{\rvx_{i}}\newcommand{\xablate}{\rvx_{a}}\newcommand{\layer}[1]{\texttt{layer#1\xspace}}\newcommand{\unit}[1]{\texttt{unit#1\xspace}}\newcommand{\vA}{\bm{\alpha}}% math macros\newcommand{\G}{G}\newcommand{\seg}{\rvs_c}\newcommand{\repr}{\rvr}\newcommand{\repru}{\repr_{u, \sP}}\newcommand{\uprepru}{\repr_{u, \sP}^{\uparrow}}% \newcommand{\vconst}{\rvc}% \newcommand{\reprU}{\repr_{\U}}% \newcommand{\reprUb}{\overline{\repr_{\U}}}\newcommand{\f}{f}\newcommand{\h}{h}% Mark sections of captions for referring to divisions of figures\newcommand{\figleft}{{\em (Left)}}\newcommand{\figcenter}{{\em (Center)}}\newcommand{\figright}{{\em (Right)}}\newcommand{\figtop}{{\em (Top)}}\newcommand{\figbottom}{{\em (Bottom)}}\newcommand{\captiona}{{\em (a)}}\newcommand{\captionb}{{\em (b)}}\newcommand{\captionc}{{\em (c)}}\newcommand{\captiond}{{\em (d)}}% Highlight a newly defined term\newcommand{\newterm}[1]{{\bf #1}}% figures and tables\newcommand{\reffig}[1]{Figure~\ref{fig:#1}}\newcommand{\refsec}[1]{Section~\ref{sec:#1}}\newcommand{\refapp}[1]{Appendix~\ref{sec:#1}}\newcommand{\reftbl}[1]{Table~\ref{tbl:#1}}\newcommand{\refalg}[1]{Algorithm~\ref{alg:#1}}\newcommand{\refline}[1]{Line~\ref{line:#1}}\newcommand{\shortrefsec}[1]{\S~\ref{sec:#1}}\newcommand{\refeq}[1]{Eqn.~\ref{eq:#1}}\newcommand{\refeqshort}[1]{(\ref{eq:#1})}\newcommand{\shortrefeq}[1]{\ref{eq:#1}}\newcommand{\lblfig}[1]{\label{fig:#1}}\newcommand{\lblsec}[1]{\label{sec:#1}}\newcommand{\lbleq}[1]{\label{eq:#1}}\newcommand{\lbltbl}[1]{\label{tbl:#1}}\newcommand{\lblalg}[1]{\label{alg:#1}}\newcommand{\lblline}[1]{\label{line:#1}}\newcommand{\ignorethis}[1]{}\newcommand{\revision}[1]{\color{black}#1\color{black}}\newcommand{\myitem}{\vspace{-5pt}\item}\newcommand{\myparagraph}[1]{\vspace{-5pt}\paragraph{#1}}%\newcommand{\myparagraph}[1]{\paragraph{#1}}%\titlespacing*{\section}{4pt}{4pt plus 2pt minus 2pt}{2pt plus 2pt minus 2pt}%\titlespacing\subsection{2pt}{2pt plus 1pt minus 1pt}{1pt plus 1pt minus 1pt}% Figure reference, lower-case.\def\figref#1{figure~\ref{#1}}% Figure reference, capital. For start of sentence\def\Figref#1{Figure~\ref{#1}}\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}% Section reference, lower-case.\def\secref#1{section~\ref{#1}}% Section reference, capital.\def\Secref#1{Section~\ref{#1}}% Reference to two sections.\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}% Reference to three sections.\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}% Reference to an equation, lower-case.\def\eqref#1{equation~\ref{#1}}% Reference to an equation, upper case\def\Eqref#1{Equation~\ref{#1}}% A raw reference to an equation---avoid using if possible\def\plaineqref#1{\ref{#1}}% Reference to a chapter, lower-case.\def\chapref#1{chapter~\ref{#1}}% Reference to an equation, upper case.\def\Chapref#1{Chapter~\ref{#1}}% Reference to a range of chapters\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}% Reference to an algorithm, lower-case.\def\algref#1{algorithm~\ref{#1}}% Reference to an algorithm, upper case.\def\Algref#1{Algorithm~\ref{#1}}\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}% Reference to a part, lower case\def\partref#1{part~\ref{#1}}% Reference to a part, upper case\def\Partref#1{Part~\ref{#1}}\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}\def\ceil#1{\lceil #1 \rceil}\def\floor#1{\lfloor #1 \rfloor}\def\1{\bm{1}}\newcommand{\train}{\mathcal{D}}\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}\def\eps{{\epsilon}}% Random variables\def\reta{{\textnormal{$\eta$}}}\def\ra{{\textnormal{a}}}\def\rb{{\textnormal{b}}}\def\rc{{\textnormal{c}}}\def\rd{{\textnormal{d}}}\def\re{{\textnormal{e}}}\def\rf{{\textnormal{f}}}\def\rg{{\textnormal{g}}}\def\rh{{\textnormal{h}}}\def\ri{{\textnormal{i}}}\def\rj{{\textnormal{j}}}\def\rk{{\textnormal{k}}}\def\rl{{\textnormal{l}}}% rm is already a command, just don't name any random variables m\def\rn{{\textnormal{n}}}\def\ro{{\textnormal{o}}}\def\rp{{\textnormal{p}}}\def\rq{{\textnormal{q}}}\def\rr{{\textnormal{r}}}\def\rs{{\textnormal{s}}}\def\rt{{\textnormal{t}}}% \def\ru{{\textnormal{u}}}\def\rv{{\textnormal{v}}}\def\rw{{\textnormal{w}}}\def\rx{{\textnormal{x}}}\def\ry{{\textnormal{y}}}\def\rz{{\textnormal{z}}}% Random vectors\def\rvepsilon{{\mathbf{\epsilon}}}\def\rvtheta{{\mathbf{\theta}}}\def\rva{{\mathbf{a}}}\def\rvb{{\mathbf{b}}}\def\rvc{{\mathbf{c}}}\def\rvd{{\mathbf{d}}}\def\rve{{\mathbf{e}}}\def\rvf{{\mathbf{f}}}\def\rvg{{\mathbf{g}}}\def\rvh{{\mathbf{h}}}\def\rvu{{\mathbf{i}}}\def\rvj{{\mathbf{j}}}\def\rvk{{\mathbf{k}}}\def\rvl{{\mathbf{l}}}\def\rvm{{\mathbf{m}}}\def\rvn{{\mathbf{n}}}\def\rvo{{\mathbf{o}}}\def\rvp{{\mathbf{p}}}\def\rvq{{\mathbf{q}}}\def\rvr{{\mathbf{r}}}\def\rvs{{\mathbf{s}}}\def\rvt{{\mathbf{t}}}\def\rvu{{\mathbf{u}}}\def\rvv{{\mathbf{v}}}\def\rvw{{\mathbf{w}}}\def\rvx{{\mathbf{x}}}\def\rvy{{\mathbf{y}}}\def\rvz{{\mathbf{z}}}% Elements of random vectors\def\erva{{\textnormal{a}}}\def\ervb{{\textnormal{b}}}\def\ervc{{\textnormal{c}}}\def\ervd{{\textnormal{d}}}\def\erve{{\textnormal{e}}}\def\ervf{{\textnormal{f}}}\def\ervg{{\textnormal{g}}}\def\ervh{{\textnormal{h}}}\def\ervi{{\textnormal{i}}}\def\ervj{{\textnormal{j}}}\def\ervk{{\textnormal{k}}}\def\ervl{{\textnormal{l}}}\def\ervm{{\textnormal{m}}}\def\ervn{{\textnormal{n}}}\def\ervo{{\textnormal{o}}}\def\ervp{{\textnormal{p}}}\def\ervq{{\textnormal{q}}}\def\ervr{{\textnormal{r}}}\def\ervs{{\textnormal{s}}}\def\ervt{{\textnormal{t}}}\def\ervu{{\textnormal{u}}}\def\ervv{{\textnormal{v}}}\def\ervw{{\textnormal{w}}}\def\ervx{{\textnormal{x}}}\def\ervy{{\textnormal{y}}}\def\ervz{{\textnormal{z}}}% Random matrices\def\rmA{{\mathbf{A}}}\def\rmB{{\mathbf{B}}}\def\rmC{{\mathbf{C}}}\def\rmD{{\mathbf{D}}}\def\rmE{{\mathbf{E}}}\def\rmF{{\mathbf{F}}}\def\rmG{{\mathbf{G}}}\def\rmH{{\mathbf{H}}}\def\rmI{{\mathbf{I}}}\def\rmJ{{\mathbf{J}}}\def\rmK{{\mathbf{K}}}\def\rmL{{\mathbf{L}}}\def\rmM{{\mathbf{M}}}\def\rmN{{\mathbf{N}}}\def\rmO{{\mathbf{O}}}\def\rmP{{\mathbf{P}}}\def\rmQ{{\mathbf{Q}}}\def\rmR{{\mathbf{R}}}\def\rmS{{\mathbf{S}}}\def\rmT{{\mathbf{T}}}\def\rmU{{\mathbf{U}}}\def\rmV{{\mathbf{V}}}\def\rmW{{\mathbf{W}}}\def\rmX{{\mathbf{X}}}\def\rmY{{\mathbf{Y}}}\def\rmZ{{\mathbf{Z}}}% Elements of random matrices\def\ermA{{\textnormal{A}}}\def\ermB{{\textnormal{B}}}\def\ermC{{\textnormal{C}}}\def\ermD{{\textnormal{D}}}\def\ermE{{\textnormal{E}}}\def\ermF{{\textnormal{F}}}\def\ermG{{\textnormal{G}}}\def\ermH{{\textnormal{H}}}\def\ermI{{\textnormal{I}}}\def\ermJ{{\textnormal{J}}}\def\ermK{{\textnormal{K}}}\def\ermL{{\textnormal{L}}}\def\ermM{{\textnormal{M}}}\def\ermN{{\textnormal{N}}}\def\ermO{{\textnormal{O}}}\def\ermP{{\textnormal{P}}}\def\ermQ{{\textnormal{Q}}}\def\ermR{{\textnormal{R}}}\def\ermS{{\textnormal{S}}}\def\ermT{{\textnormal{T}}}\def\ermU{{\textnormal{U}}}\def\ermV{{\textnormal{V}}}\def\ermW{{\textnormal{W}}}\def\ermX{{\textnormal{X}}}\def\ermY{{\textnormal{Y}}}\def\ermZ{{\textnormal{Z}}}% Vectors\def\vzero{{\bm{0}}}\def\vone{{\bm{1}}}\def\vmu{{\bm{\mu}}}\def\vtheta{{\bm{\theta}}}\def\va{{\bm{a}}}\def\vb{{\bm{b}}}\def\vc{{\bm{c}}}\def\vd{{\bm{d}}}\def\ve{{\bm{e}}}\def\vf{{\bm{f}}}\def\vg{{\bm{g}}}\def\vh{{\bm{h}}}\def\vi{{\bm{i}}}\def\vj{{\bm{j}}}\def\vk{{\bm{k}}}\def\vl{{\bm{l}}}\def\vm{{\bm{m}}}\def\vn{{\bm{n}}}\def\vo{{\bm{o}}}\def\vp{{\bm{p}}}\def\vq{{\bm{q}}}\def\vr{{\bm{r}}}\def\vs{{\bm{s}}}\def\vt{{\bm{t}}}\def\vu{{\bm{u}}}\def\vv{{\bm{v}}}\def\vw{{\bm{w}}}\def\vx{{\bm{x}}}\def\vy{{\bm{y}}}\def\vz{{\bm{z}}}% Elements of vectors\def\evalpha{{\alpha}}\def\evbeta{{\beta}}\def\evepsilon{{\epsilon}}\def\evlambda{{\lambda}}\def\evomega{{\omega}}\def\evmu{{\mu}}\def\evpsi{{\psi}}\def\evsigma{{\sigma}}\def\evtheta{{\theta}}\def\eva{{a}}\def\evb{{b}}\def\evc{{c}}\def\evd{{d}}\def\eve{{e}}\def\evf{{f}}\def\evg{{g}}\def\evh{{h}}\def\evi{{i}}\def\evj{{j}}\def\evk{{k}}\def\evl{{l}}\def\evm{{m}}\def\evn{{n}}\def\evo{{o}}\def\evp{{p}}\def\evq{{q}}\def\evr{{r}}\def\evs{{s}}\def\evt{{t}}\def\evu{{u}}\def\evv{{v}}\def\evw{{w}}\def\evx{{x}}\def\evy{{y}}\def\evz{{z}}% Matrix\def\mA{{\bm{A}}}\def\mB{{\bm{B}}}\def\mC{{\bm{C}}}\def\mD{{\bm{D}}}\def\mE{{\bm{E}}}\def\mF{{\bm{F}}}\def\mG{{\bm{G}}}\def\mH{{\bm{H}}}\def\mI{{\bm{I}}}\def\mJ{{\bm{J}}}\def\mK{{\bm{K}}}\def\mL{{\bm{L}}}\def\mM{{\bm{M}}}\def\mN{{\bm{N}}}\def\mO{{\bm{O}}}\def\mP{{\bm{P}}}\def\mQ{{\bm{Q}}}\def\mR{{\bm{R}}}\def\mS{{\bm{S}}}\def\mT{{\bm{T}}}\def\mU{{\bm{U}}}\def\mV{{\bm{V}}}\def\mW{{\bm{W}}}\def\mX{{\bm{X}}}\def\mY{{\bm{Y}}}\def\mZ{{\bm{Z}}}\def\mBeta{{\bm{\beta}}}\def\mPhi{{\bm{\Phi}}}\def\mLambda{{\bm{\Lambda}}}\def\mSigma{{\bm{\Sigma}}}% Tensor\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}\def\tA{{\tens{A}}}\def\tB{{\tens{B}}}\def\tC{{\tens{C}}}\def\tD{{\tens{D}}}\def\tE{{\tens{E}}}\def\tF{{\tens{F}}}\def\tG{{\tens{G}}}\def\tH{{\tens{H}}}\def\tI{{\tens{I}}}\def\tJ{{\tens{J}}}\def\tK{{\tens{K}}}\def\tL{{\tens{L}}}\def\tM{{\tens{M}}}\def\tN{{\tens{N}}}\def\tO{{\tens{O}}}\def\tP{{\tens{P}}}\def\tQ{{\tens{Q}}}\def\tR{{\tens{R}}}\def\tS{{\tens{S}}}\def\tT{{\tens{T}}}\def\tU{{\tens{U}}}\def\tV{{\tens{V}}}\def\tW{{\tens{W}}}\def\tX{{\tens{X}}}\def\tY{{\tens{Y}}}\def\tZ{{\tens{Z}}}% Graph\def\gA{{\mathcal{A}}}\def\gB{{\mathcal{B}}}\def\gC{{\mathcal{C}}}\def\gD{{\mathcal{D}}}\def\gE{{\mathcal{E}}}\def\gF{{\mathcal{F}}}\def\gG{{\mathcal{G}}}\def\gH{{\mathcal{H}}}\def\gI{{\mathcal{I}}}\def\gJ{{\mathcal{J}}}\def\gK{{\mathcal{K}}}\def\gL{{\mathcal{L}}}\def\gM{{\mathcal{M}}}\def\gN{{\mathcal{N}}}\def\gO{{\mathcal{O}}}\def\gP{{\mathcal{P}}}\def\gQ{{\mathcal{Q}}}\def\gR{{\mathcal{R}}}\def\gS{{\mathcal{S}}}\def\gT{{\mathcal{T}}}\def\gU{{\mathcal{U}}}\def\gV{{\mathcal{V}}}\def\gW{{\mathcal{W}}}\def\gX{{\mathcal{X}}}\def\gY{{\mathcal{Y}}}\def\gZ{{\mathcal{Z}}}% Sets\def\sA{{\mathbb{A}}}\def\sB{{\mathbb{B}}}\def\sC{{\mathbb{C}}}\def\sD{{\mathbb{D}}}% Don't use a set called E, because this would be the same as our symbol% for expectation.\def\sF{{\mathbb{F}}}\def\sG{{\mathbb{G}}}\def\sH{{\mathbb{H}}}\def\sI{{\mathbb{I}}}\def\sJ{{\mathbb{J}}}\def\sK{{\mathbb{K}}}\def\sL{{\mathbb{L}}}\def\sM{{\mathbb{M}}}\def\sN{{\mathbb{N}}}\def\sO{{\mathbb{O}}}\def\sP{{\mathbb{P}}}\def\sQ{{\mathbb{Q}}}\def\sR{{\mathbb{R}}}\def\sS{{\mathbb{S}}}\def\sT{{\mathbb{T}}}\def\sU{{\mathbb{U}}}\def\sV{{\mathbb{V}}}\def\sW{{\mathbb{W}}}\def\sX{{\mathbb{X}}}\def\sY{{\mathbb{Y}}}\def\sZ{{\mathbb{Z}}}% Entries of a matrix\def\emLambda{{\Lambda}}\def\emA{{A}}\def\emB{{B}}\def\emC{{C}}\def\emD{{D}}\def\emE{{E}}\def\emF{{F}}\def\emG{{G}}\def\emH{{H}}\def\emI{{I}}\def\emJ{{J}}\def\emK{{K}}\def\emL{{L}}\def\emM{{M}}\def\emN{{N}}\def\emO{{O}}\def\emP{{P}}\def\emQ{{Q}}\def\emR{{R}}\def\emS{{S}}\def\emT{{T}}\def\emU{{U}}\def\emV{{V}}\def\emW{{W}}\def\emX{{X}}\def\emY{{Y}}\def\emZ{{Z}}\def\emSigma{{\Sigma}}% entries of a tensor% Same font as tensor, without \bm wrapper\newcommand{\etens}[1]{\mathsfit{#1}}\def\etLambda{{\etens{\Lambda}}}\def\etA{{\etens{A}}}\def\etB{{\etens{B}}}\def\etC{{\etens{C}}}\def\etD{{\etens{D}}}\def\etE{{\etens{E}}}\def\etF{{\etens{F}}}\def\etG{{\etens{G}}}\def\etH{{\etens{H}}}\def\etI{{\etens{I}}}\def\etJ{{\etens{J}}}\def\etK{{\etens{K}}}\def\etL{{\etens{L}}}\def\etM{{\etens{M}}}\def\etN{{\etens{N}}}\def\etO{{\etens{O}}}\def\etP{{\etens{P}}}\def\etQ{{\etens{Q}}}\def\etR{{\etens{R}}}\def\etS{{\etens{S}}}\def\etT{{\etens{T}}}\def\etU{{\etens{U}}}\def\etV{{\etens{V}}}\def\etW{{\etens{W}}}\def\etX{{\etens{X}}}\def\etY{{\etens{Y}}}\def\etZ{{\etens{Z}}}% The true underlying data generating distribution\newcommand{\pdata}{p_{\rm{data}}}% The empirical distribution defined by the training set\newcommand{\ptrain}{\hat{p}_{\rm{data}}}\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}% The model distribution\newcommand{\pmodel}{p_{\rm{model}}}\newcommand{\Pmodel}{P_{\rm{model}}}\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}% Stochastic autoencoder distributions\newcommand{\pencode}{p_{\rm{encoder}}}\newcommand{\pdecode}{p_{\rm{decoder}}}\newcommand{\precons}{p_{\rm{reconstruct}}}\newcommand{\laplace}{\mathrm{Laplace}}% Laplace distribution\newcommand{\E}{\mathbb{E}}\newcommand{\Ls}{\mathcal{L}}\newcommand{\R}{\mathbb{R}}\newcommand{\emp}{\tilde{p}}\newcommand{\lr}{\alpha}\newcommand{\reg}{\lambda}\newcommand{\rect}{\mathrm{rectifier}}\newcommand{\softmax}{\mathrm{softmax}}\newcommand{\sigmoid}{\sigma}\newcommand{\softplus}{\zeta}\newcommand{\Var}{\mathrm{Var}}\newcommand{\standarderror}{\mathrm{SE}}\newcommand{\Cov}{\mathrm{Cov}}% Wolfram Mathworld says $L^2$ is for function spaces and $\ell^2$ is for vectors% But then they seem to use $L^2$ for vectors throughout the site, and so does% wikipedia.\newcommand{\normlzero}{L^0}\newcommand{\normlone}{L^1}\newcommand{\normltwo}{L^2}\newcommand{\normlp}{L^p}\newcommand{\normmax}{L^\infty}\newcommand{\parents}{Pa}% See usage in notation.tex. Chosen to match Daphne's book.\DeclareMathOperator*{\argmax}{arg\,max}\DeclareMathOperator*{\argmin}{arg\,min}\DeclareMathOperator{\sign}{sign}\DeclareMathOperator{\Tr}{Tr}\let\ab\allowbreak% require xspace, array%% layout\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}\newcommand{\xpar}[1]{\noindent\textbf{#1}\ \ }\newcommand{\vpar}[1]{\vspace{3mm}\noindent\textbf{#1}\ \ }%datasets\newcommand{\shapenet}{ShapeNet\xspace}\newcommand{\pascal}{PASCAL 3D+\xspace}\newcommand{\degree}{\ensuremath{^\circ}\xspace}\newcommand{\ignore}[1]{}\newcommand{\norm}[1]{\lVert#1\rVert}\newcommand{\fcseven}{$\mbox{fc}_7$}\renewcommand*{\thefootnote}{\fnsymbol{footnote}}\def\naive{na\"{\i}ve\xspace}\def\Naive{Na\"{\i}ve\xspace}\makeatletter\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}% NO emph for e.g., and i.e,\def\eg{e.g\onedot,\xspace}\def\Eg{E.g\onedot,}\def\ie{i.e\onedot,\xspace}\def\Ie{\emph{I.e}\onedot,}\def\cf{\emph{c.f}\onedot}\def\Cf{\emph{C.f}\onedot}\def\etc{\emph{etc}\onedot}\def\vs{\emph{vs}\onedot}\def\wrt{w.r.t\onedot}\def\dof{d.o.f\onedot}\def\etal{\emph{et al}\onedot}\makeatother%% comments\definecolor{MyDarkBlue}{rgb}{0,0.08,1}\definecolor{MyDarkGreen}{rgb}{0.02,0.6,0.02}\definecolor{MyDarkRed}{rgb}{0.8,0.02,0.02}\definecolor{MyDarkOrange}{rgb}{0.40,0.2,0.02}\definecolor{MyPurple}{RGB}{111,0,255}\definecolor{MyRed}{rgb}{1.0,0.0,0.0}\definecolor{MyGold}{rgb}{0.75,0.6,0.12}\definecolor{MyDarkgray}{rgb}{0.66, 0.66, 0.66}

% Comment this for ICLR anonymization and length
\def\arxiv{for arxiv submission}

\newif\ifsubmit

\submitfalse

\ifsubmit
\newcommand{\david}[1]{}
\newcommand{\jy}[1]{}
\newcommand{\bolei}[1]{}
\else
\newcommand{\david}[1]{\textcolor{blue}{David: #1}}
\newcommand{\jy}[1]{\textcolor{blue}{JY: #1}}
\newcommand{\bolei}[1]{\textcolor{blue}{Bolei: #1}}
\newcommand{\hen}[1]{\textcolor{magenta}{hen: #1}}
\fi

\ifdefined\arxiv
\title{GAN Dissection: \\ Visualizing and Understanding \\ Generative Adversarial Networks}
\else
\title{Visualizing and Understanding \\ Generative Adversarial Networks}
\fi

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\ifdefined\arxiv
\author{%
David Bau\textsuperscript{1,2},
Jun-Yan Zhu\textsuperscript{1},
Hendrik Strobelt\textsuperscript{2,3},
Bolei Zhou\textsuperscript{4}, \\
\bf{Joshua B. Tenenbaum}\textsuperscript{1},
\bf{William T. Freeman}\textsuperscript{1},
\bf{Antonio Torralba}\textsuperscript{1,2} \\
\textsuperscript{1}Massachusetts Institute of Technology,
\textsuperscript{2}MIT-IBM Watson AI Lab, \\
\textsuperscript{3}IBM Research,
\textsuperscript{4}The Chinese University of Hong Kong
 }
% \textsuperscript{1}[davidbau, junyanz, jbt, billf, torralba]@csail.mit.edu,
% \\ \textsuperscript{3}hendrik.strobelt@ibm.com, \textsuperscript{4}bzhou@ie.cuhk.edu.hk
%   }

\else
\author{}
\fi


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\ifdefined\arxiv
\makeatletter
\def\@maketitle{\vbox{\hsize\textwidth
%\linewidth\hsize \vskip 0.1in \toptitlebar \centering
{\LARGE\sc \@title\par}
%\bottomtitlebar % \vskip 0.1in %  minus
\lhead{Preprint prepared for ArXiv submission}
\def\And{\end{tabular}\hfil\linebreak[0]\hfil
        \begin{tabular}[t]{l}\bf\rule{\z@}{24pt}\ignorespaces}%
\def\AND{\end{tabular}\hfil\linebreak[4]\hfil
        \begin{tabular}[t]{l}\bf\rule{\z@}{24pt}\ignorespaces}%
\begin{tabular}[t]{l}\bf\rule{\z@}{24pt}\@author\end{tabular}%
\vskip 0.3in minus 0.1in}}
\makeatother

\fi

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
\lblsec{abstract}
Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, they have not been well visualized or understood. How does a GAN represent our visual world internally?  What causes the artifacts in GAN results?  How do architectural choices affect GAN learning?  Answering such questions could enable us to develop new insights and better models.

In this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to \concepts using a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. We examine the contextual relationship between these units and their surroundings by inserting the discovered \concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in a scene.  We provide open source interpretation tools to help researchers and practitioners better understand their GAN models\ifdefined\arxiv\footnote{Interactive demos, video, code, and data are available at \href{https://github.com/CSAILVision/gandissect}{GitHub} and \href{https://gandissect.csail.mit.edu}{gandissect.csail.mit.edu}.}\fi.

\end{abstract}
\section{Introduction}\lblsec{intro}
Generative Adversarial Networks (GANs)~\citep{goodfellow2014generative} have been able to produce photorealistic images, often indistinguishable from real images.
This remarkable ability has powered   many real-world applications ranging from visual recognition~\citep{wang2017fast}, to image manipulation~\citep{isola2017image,zhu2017unpaired}, to video prediction~\citep{mathieu2016deep}. Since its invention in 2014, many GAN variants have been proposed~\citep{radford2015unsupervised,zhang2018self}, often producing more realistic and diverse samples with better training stability.

Despite this tremendous success, many questions remain to be answered. For example, to produce a church image (\reffig{teaser}a), what knowledge does a GAN need to learn? Alternatively, when a GAN sometimes produces terribly unrealistic images (\reffig{teaser}f), what causes the mistakes? Why does one GAN variant work better than another? What fundamental differences are encoded in their weights?

\begin{figure}[t]
\vspace{-10pt}
\centering
\includegraphics[width=1.0\textwidth]{fig/teaser.pdf}
\vspace{-15pt}
\caption{Overview: (a) A number of realistic outdoor church images generated by \pgan~\citep{karras2018progressive}. (b) Given a pre-trained GAN model (\eg \pgan), we first identify a set of interpretable units, whose \featuremap is highly correlated to the region of an object class across different images. For example, one unit in \layer{4} can localize tree regions with diverse visual appearance. (c) We ablate the units by forcing the activation to be zero and quantify the average casual effect of the ablation. Here we successfully remove these trees from church images.  (d) We can insert these tree causal units to other locations. The same set of units can synthesize different trees visually compatible with their surrounding context. In addition, our method can diagnose and improve GANs by  identifying artifact-causing units (e). We can remove the artifacts that appear in (f) and significantly improve the results by ablating the ``artifact'' units (g). Please see our demo
\ifdefined\arxiv
\href{http://tiny.cc/gandissect}{video}.
\else
\href{http://tiny.cc/iclrganvis}{video}.
\fi
}
\lblfig{teaser}
\ifdefined\arxiv\else
\vspace{-20pt}
\fi
\end{figure}

In this work, we study the internal representations of GANs. To a human observer, a well-trained GAN appears to have learned facts about the objects in the image: for example, a door can appear on a building but not on a tree.  We wish to understand how a GAN represents such a structure.  Do the objects emerge as pure pixel patterns without any explicit representation of objects such as doors and trees, or does the GAN contain internal variables that correspond to the objects that humans perceive?  If the GAN does contain variables for doors and trees, do those variables cause the generation of those objects, or do they merely correlate?  How are relationships between objects represented?

We present a general method for visualizing and understanding GANs at different levels of abstraction, from each neuron, to each object, to the contextual relationship between different objects. We first identify a group of interpretable units that are related to \concepts (\reffig{teaser}b). These units' {\featuremap}s closely match the semantic segmentation of a particular object class (\eg trees). Second, we directly intervene within the network to identify sets of units that cause a type of objects to disappear (\reffig{teaser}c) or appear (\reffig{teaser}d). We quantify the causal effect of these units using a standard causality metric. Finally, we examine the contextual relationship between these causal object units and the background. We study where we can insert the \concepts in new images and how this intervention interacts with other objects in the image (\reffig{teaser}d). To our knowledge, our work provides the first systematic analysis for understanding the internal representations of GANs.

Finally, we show several practical applications enabled by this analytic framework, from comparing internal representations across different layers, GAN variants and datasets; to debugging and improving GANs by locating and ablating ``artifact'' units (\reffig{teaser}e); to understanding contextual relationships between objects in scenes; to manipulating images with interactive object-level control.
\section{Related Work}\lblsec{related}\myparagraph{Generative Adversarial Networks.} The quality and diversity of results from GANs~\citep{goodfellow2014generative} has continued to improve, from generating simple digits and faces~\citep{goodfellow2014generative}, to synthesizing natural scene images~\citep{radford2015unsupervised,denton2015deep}, to generating 1k photorealistic portraits~\citep{karras2018progressive}, to producing one thousand object classes~\citep{miyato2018spectral,zhang2018self}. In addition to image generation, GANs have also enabled many applications such as visual recognition~\citep{wang2017fast,hoffman2018cycada}, image manipulation~\citep{isola2017image,zhu2017unpaired}, and video generation~\citep{mathieu2016deep,wang2018vid2vid}. Despite the huge success, little work has been done to visualize what GANs have learned. Prior work~\citep{radford2015unsupervised,zhu2016generative} manipulates latent vectors and observes how the results change accordingly.

\textbf{Visualizing deep neural networks.} Various methods have been developed to understand the internal representations of networks, such as visualizations for CNNs~\citep{zeiler2014visualizing} and RNNs~\citep{karpathy2016visualizing, lstmvis}. We can visualize a CNN by locating and reconstructing salient image features~\citep{simonyan2014deep,mahendran2015understanding} or by mining patches that maximize hidden layers' activations~\citep{zeiler2014visualizing}, or we can synthesize input images to invert a feature layer~\citep{dosovitskiy2016generating}. Alternately, we can identify the semantics of each unit~\citep{zhou2014object,bau2017network,zhou2018interpreting} by measuring agreement between unit activations and object segmentation masks. Visualization of an RNN has also revealed interpretable units that track long-range dependencies~\citep{karpathy2016visualizing}. Most previous work on network visualization has focused on networks trained for classification; our work explores deep generative models trained for image generation.

\textbf{Explaining the decisions of deep neural networks.} We can explain individual network decisions using  informative heatmaps~\citep{zhou2018interpretable,zhou2016learning,selvaraju2017grad} or modified back-propagation~\citep{simonyan2014deep,bach2015pixel,sundararajan17a}. The heatmaps highlight which regions contribute most to the categorical prediction given by the networks. Recent work has also studied the contribution of feature vectors~\citep{kim2017tcav,zhou2018interpretable} or individual channels~\citep{olah2018building} to the final prediction. \citet{morcos2018importance} has examined the effect of individual units by ablating them. Those methods explain discriminative classifiers. Our method aims to explain how an image can be generated by a network, which is much less explored.

\section{Method}\lblsec{methods}\begin{figure}[t]
\centering
\vspace{-25pt}
\includegraphics[width=0.9\textwidth]{fig/framework-dissect-v5.pdf}%
\vspace{-10pt}
\caption{
Measuring the relationship between representation units and trees in the output using (a) dissection and (b) intervention. Dissection measures agreement between a unit $u$ and a concept $c$ by comparing its thresholded upsampled heatmap with a semantic segmentation of the generated image $s_c(x)$.  Intervention measures the causal effect of a set of units $U$ on a concept $c$ by comparing the effect of forcing these units on (unit insertion) and off (unit ablation).  The segmentation $s_c$ reveals that trees increase after insertion and decrease after ablation.  The average difference in the tree pixels measures the average causal effect. In this figure, interventions are applied to the entire featuremap $\pixelall$, but insertions and ablations can also apply to any subset of pixels $\pixel \subset \pixelall$.}
\lblfig{framework}
\ifdefined\arxiv
\vspace{-10pt}
\else
\vspace{-10pt}
\fi
\end{figure}
Our goal is to analyze how objects such as trees are encoded by the internal representations of a GAN generator $\G\colon \rvz\rightarrow \rvx$. Here $\rvz\in \mathbb{R}^{|z|}$ denotes a  latent vector sampled from a low-dimensional distribution, and $\rvx \in \mathbb{R}^{H\times W\times 3}$ denotes an $H\times W$ generated image.  We use \textit{representation} to describe the tensor $\repr$ output from a particular layer of the generator $\G$, where the generator creates an image $\rvx$ from random $\rvz$ through a composition of layers: $\repr = \h(\rvz)$ and $\rvx = \f(\repr) = \f(\h(\rvz)) = \G(\rvz)$.

Since $\repr$ has all the data necessary to produce the image $\rvx = \f(\repr)$, $\repr$ certainly contains the information to deduce the presence of any visible class $c$ in the image.  Therefore the question we ask is not whether information about $c$ is present in $\repr$ --- it is --- but \textit{how} such information is encoded in $\repr$.
In particular, for any class from a universe of concepts $c \in \sC$, we seek to understand whether $\repr$ explicitly represents $c$ in some way where it is possible to factor $\repr$ at locations $\pixel$ into two components
\vspace{-5pt}\begin{equation}
\pp = (\rUP, \rUb),
\end{equation}
where the generation of the object $c$ at locations $\pixel$ depends mainly on the units $\rUP$, and is insensitive to the other units $\rUb$.  Here we refer to each channel of the featuremap as a unit: $\U$ denotes the set of unit indices of interest and $\overline{\text{U}}$ is its complement; we will write $\Uall$ and $\pixelall$ to refer to the entire set of units and featuremap pixels in $r$.  We study the structure of $\repr$ in two phases:
\vspace{-5pt}\begin{itemize}[noitemsep,topsep=0pt]
\item Dissection: starting with a large dictionary of object classes, we identify the classes that have an explicit representation in $\repr$ by measuring the agreement between individual units of $\repr$ and every class $c$ (\reffig{teaser}b).
\item Intervention: for the represented classes identified through dissection, we identify causal sets of units and measure causal effects between units and object classes by forcing sets of units on and off (\reffig{teaser}c,d).
\end{itemize}\subsection{Characterizing units by dissection}\lblsec{dissection}\begin{figure}[t]
\vspace{-10pt}
\centering
\includegraphics[width=1.0\textwidth,trim={0 0 36.3in 0},clip]{fig/diningroom-3-65-table.jpeg}{\small Thresholding unit \#65 layer 3 of a dining room generator matches `table' segmentations with IoU=0.34.} \\
\vspace{1mm}
\includegraphics[width=1.0\textwidth,trim={0 0 36.3in 0},clip]{fig/livingroom-4-37-sofa.jpg}{\small Thresholding unit \#37 layer 4 of a living room generator matches `sofa' segmentations with IoU=0.29.}
\vspace{-5pt}
\caption{Visualizing the activations of individual units in two GANs.  Top ten activating images are shown, and IoU is measured over a sample of 1000 images. In each image, the unit feature is upsampled and thresholded as described in ~\refeq{max-iqr}.}
\lblfig{dissect}
\ifdefined\arxiv
\vspace{-10pt}
\else
\vspace{-10pt}
\fi
\end{figure}
We first focus on individual units of the representation. Recall that $\repru$ is the one-channel $h\times w$\featuremap of unit $u$ in a convolutional generator, where $h\times w$ is typically smaller than the image size.
We want to know if a specific unit $\repru$ encodes a semantic class such as a ``tree''.  For image classification networks, \citet{bau2017network} has observed that many units can approximately locate emergent object classes when the units are upsampled and thresholded. In that spirit, we select a universe of concepts $c\in\sC$ for which we have a semantic segmentation $\seg(\rvx)$ for each class.  Then we quantify the spatial agreement between the unit $u$'s thresholded \featuremap and a concept $c$'s segmentation with the following intersection-over-union ($\IoU$) measure:
\begin{align}
\IoU_{u,c} \equiv
\frac{\E_{\rvz} \left|(\uprepru > t_{u,c}) \land  \seg(\rvx) \right|}
{\E_{\rvz} \left|(\uprepru > t_{u,c}) \vee \seg(\rvx) \right|},
\mbox{where } t_{u,c} = \argmax_{t} \frac{\text{I}(\uprepru > t ; \seg(\rvx))}{\text{H}(\uprepru > t , \seg(\rvx))},
\label{eq:max-iqr}
\end{align}
where $\land$ and $\vee$ denote intersection and union operations, and $\rvx=\G(\rvz)$ denotes the image generated from $\rvz$. The one-channel feature map $\repru$ slices the entire featuremap $\repr=\h(\rvz)$ at unit $u$. As shown in \reffig{framework}a, we upsample $\repru$ to the output image resolution as $\uprepru$. $(\uprepru > t_{u,c})$ produces a binary mask by thresholding the $\uprepru$  at a fixed level $t_{u,c}$.
$\seg(\rvx)$ is a binary mask where each pixel indicates the presence of class $c$  in the generated image $\rvx$.
The threshold $t_{u,c}$ is chosen to be informative as possible by maximizing the information quality ratio $\text{I} / \text{H}$ (using a separate validation set), that is, it maximizes the portion of the joint entropy $\textnormal{H}$ which is mutual information $\textnormal{I}$~\citep{wijaya2017information}.


We can use $\IoU_{u,c}$ to rank the concepts related to each unit and label each unit with the concept that matches it best.
\reffig{dissect} shows examples of interpretable units with high $\IoU_{u, c}$.
They are not the only units to match tables and sofas: \layer{3} of the dining room generator has $31$ units (of $512$) that match tables and table parts, and \layer{4} of the living room generator has $65$ (of $512$) sofa units.

Once we have identified an object class that a set of units match closely, we next ask: which units are responsible for triggering the rendering of that object?  A unit that correlates highly with an output object might not actually cause that output.  Furthermore, any output will jointly depend on several parts of the representation. We need a way to identify combinations of units that cause an object.

\subsection{Measuring causal relationships using intervention}\lblsec{acealgorithm}
To answer the above question about causality, we probe the network using interventions: we test whether a set of units $\U$ in $\repr$ cause the generation of $c$ by forcing the units of $\U$ on and off.

Recall that $\rUP$ denotes the featuremap $\repr$ at units $\U$  and locations $\pixel$.  We \textit{ablate} those units by forcing $\rUP = \vzero$. Similarly, we \textit{insert} those units by forcing $\rUP = \thresU$, where $\thresU$ is a per-class constant, as described in \refsec{acemethod_detail}.
We decompose the featuremap $\repr$ into two parts $(\rUP, \rUPb)$, where $\rUPb$ are unforced components of $\repr$:
\vspace{-5pt}\begin{align}
\label{eq:intervention}
&\text{Original image}: &\rvx = \G(\rvz)  \equiv  \f(\repr) & \equiv  \f(\rUP, \rUPb) \\ \nonumber
&\text{Image with $\U$ ablated at pixels $\pixel$}: & \xablate & =  \f(\vzero, \rUPb) \\ \nonumber
&\text{Image with $\U$ inserted at pixels $\pixel$}: & \xinsert & = \f(\thresU, \rUPb)
\end{align}
An object is caused by $\U$ if the object appears in $\xinsert$ and disappears from $\xablate$.  \reffig{teaser}c demonstrates the ablation of units that remove trees, and  \reffig{teaser}d demonstrates insertion of units at specific locations to make trees appear.  This causality can be quantified by comparing the presence of trees in $\xinsert$ and $\xablate$ and averaging effects over all locations and images.  Following prior work~\citep{holland1988causal,pearl2009causality},
 we define the average causal effect (ACE) of units $\U$ on the generation of on class $c$ as:
\begin{equation}
\ACEUc  \equiv \Ezp[\seg(\xinsert)] - \Ezp[\seg(\xablate)],
\end{equation}
where $\seg(\rvx)$ denotes a segmentation indicating the presence of class $c$ in the image $\rvx$ at $\pixel$.
To permit comparisons of $\ACEUc$ between classes $c$ which are rare, we normalize our segmentation $\seg$ by $\Ezp[\seg(x)]$.  While these measures can be applied to a single unit, we have found that objects tend to depend on more than one unit. Thus we need to identify a set of units $\U$ that maximize the average causal effect $\ACE_{\U\rightarrow c}$ for an object class $c$.
\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{fig/method_ablation.pdf}
\vspace{-18pt}
\caption{
Ablating successively larger sets of tree-causal units from a GAN trained on LSUN outdoor church images, showing that the more units are removed, the more trees are reduced, while buildings remain. The choice of units to ablate is specific to the tree class and does not depend on the image.  At right, the causal effect of removing successively more tree units is plotted, comparing units chosen to optimize the average causal effect (ACE) and units chosen with the highest IoU for trees.}
\lblfig{method_ablation}
\ifdefined\arxiv
\vspace{-10pt}
\else
\vspace{-7pt}
\fi
\end{figure}%% \begin{comment}\myparagraph{Finding sets of units with high ACE.} Given a representation $\repr$ with $d$ units, exhaustively searching for a fixed-size set $\U$ with high $\ACEUc$ is prohibitive as it has $\binom{d}{|\U|}$ subsets.
Instead, we optimize a continuous intervention  $\vA \in [0, 1]^{d}$, where each dimension $\vA_u$ indicates the degree of intervention for a unit $u$. We maximize the following average causal effect formulation $\ACE_{\vA\rightarrow c}$:
\begin{align}
\label{eq:aceopt}
&\text{Image with partial ablation at pixels $\pixel$}: &  \xablate' & = \f((\vone-\vA) \odot \pp, \; \ppb) \\ \nonumber
&\text{Image with partial insertion at pixels $\pixel$}: & \xinsert' & = \f(\vA \odot \thresUall + (\vone-\vA) \odot \pp, \; \ppb) \\ \nonumber
&\text{Objective}: & \ACE_{\vA\rightarrow c} & = \Ezp\left[ \seg(\xinsert')\right] - \Ezp\left[ \seg(\xablate')\right],
\end{align}
where $\pp$ denotes the all-channel featuremap at locations $\pixel$, $\ppb$ denotes the all-channel featuremap at other locations $\overline{\pixel}$, and $\odot$ applies a per-channel scaling vector $\vA$ to the featuremap $\pp$.
 We optimize $\vA$ over the following loss with an L2 regularization:
\begin{equation}
\vA^* = \arg \min_{\vA} (-\ACE_{\vA \rightarrow c} + \lambda ||\vA||_2),
\label{eq:acemax}
\end{equation}
where $\lambda$ controls the relative importance of each term. We add the L2 loss as we seek a minimal set of casual units. We optimize using stochastic gradient descent, sampling over both $\rvz$ and featuremap locations $\pixel$ and clamping the coefficient $\vA$ within the range $[0, 1]^d$ at each step (d is the total number of units).   More details of this optimization are discussed in \refsec{acemethod_detail}.
Finally, we can rank units by $\vA_u^*$ and achieve a stronger causal effect (\ie removing trees) when ablating successively larger sets of tree-causing units as shown in \reffig{method_ablation}.
\section{Results}\lblsec{results}
We study three variants of \pgan\citep{karras2018progressive} trained on LSUN scene datasets~\citep{yu2015lsun}. To segment the generated images, we use a recent model~\citep{xiao2018unified} trained on the ADE20K scene dataset~\citep{zhou2017scene}. The model can segment the input image into $336$ object classes, $29$  parts of large objects, and $25$ materials.
To further identify units that specialize in object parts, we expand each object class $c$  into additional object part classes \textit{c-t}, \textit{c-b}, \textit{c-l}, and \textit{c-r}, which denote the top, bottom, left, or right half of the bounding box of a connected component.

Below, we use dissection for analyzing and comparing units across datasets, layers, and models (\refsec{compare}), and locating artifact units (\refsec{artifacts}). Then, we start with a set of dominant object classes and use intervention to locate causal units that can remove and insert objects in different images (\refsec{results_ablation} and \ref{sec:results_insertion}). In addition, our
\ifdefined\arxiv\href{http://tiny.cc/gandissect}{video}\else\href{http://tiny.cc/iclrganvis}{video}\fi
demonstrates our interactive tool. % \ifdefined\arxiv% Please visit our \href{http://gandissect.csail.mit.edu/}{website} for full results. % \fi% \subsection{Emergence of individual unit object detectors}% \lblsec{results_dissect}\subsection{Comparing units across datasets, layers, and models}\lblsec{compare}\vspace{5pt}% first one: undo \vspace{-5pt} from \myparagraph\myparagraph{Emergence of individual unit object detectors}
We are particularly interested in any units that are correlated with instances of an object class with diverse visual appearances; these would suggest that GANs generate those objects using similar abstractions as humans.  \reffig{dissect} illustrates two such units.  In the dining room dataset, a unit emerges to match dining table regions. More interestingly, the matched tables have different colors, materials, geometry, viewpoints, and levels of clutter: the only obvious commonality among these regions is the concept of a table. %Evaluating this unit against a segmentation method~\citep{xiao2018unified} shows that
This unit's \featuremap correlates to the fully supervised segmentation model~\citep{xiao2018unified} with a high $\IoU$ of $0.34$.

\myparagraph{Interpretable units for different scene categories}
The set of all object classes matched by the units of a GAN provides a map of what a GAN has learned about the data. \reffig{scene_units} examines units from GANs trained on four LSUN scene categories~\citep{yu2015lsun}. The units that emerge are object classes appropriate to the scene type: for example, when we examine a GAN trained on kitchen scenes, we find units that match stoves, cabinets, and the legs of tall kitchen stools.  Another striking phenomenon is that many units represent parts of objects: for example, the conference room GAN contains separate units for the body and head of a person.
\begin{figure}
\ifdefined\arxiv
\vspace{-15pt}
\fi
\centering
\includegraphics[width=0.95\textwidth]{fig/compare-scenes-slim-crop}
\vspace{-11pt}
\caption{Comparing representations learned by progressive GANs trained on different scene types.  The units that emerge match objects that commonly appear in the scene type: seats in conference rooms and stoves in kitchens.  Units from \layer4 are shown.  A unit is counted as a class predictor if it matches a supervised segmentation class with pixel accuracy $> 0.75$ and IoU $> 0.05$ when upsampled and thresholded. The distribution of units over classes is shown in the right column.}
\lblfig{scene_units}
\ifdefined\arxiv
\vspace{-5pt}
\else
\vspace{-10pt}
\fi
\end{figure}\begin{figure}[t]
\hspace{3mm}\includegraphics[width=0.95\textwidth]{fig/compare-layers-slim-crop}
\vspace{-10pt}
\caption{Comparing layers of a progressive GAN trained to generate LSUN living room images. The output of the first convolutional layer has almost no units that match semantic objects, but many objects emerge at layers 4-7.  Later layers are dominated by low-level materials, edges and colors.}
\lblfig{compare_layers}
\end{figure}\begin{figure}[t]
\ifdefined\arxiv
\vspace{-25pt}
\else
\fi
\hspace{3mm}\includegraphics[width=0.95\textwidth]{fig/compare-models-slim-crop}
\vspace{-10pt}
\caption{Comparing \layer{4} representations learned by different training variations. Sliced Wasserstein Distance (SWD) is a GAN quality metric suggested by \cite{karras2018progressive}: lower SWD indicates more realistic image statistics.  Note that as the quality of the model improves, the number of interpretable units also rises.  Progressive GANs apply several innovations including making the discriminator aware of minibatch statistics, and pixelwise normalization at each layer.  We can see batch awareness increases the number of object classes matched by units, and pixel norm (applied in addition to batch stddev) increases the number of units matching objects.}
\lblfig{compare_models}
\ifdefined\arxiv
\else
\vspace{-5pt}
\fi
\end{figure}\myparagraph{Interpretable units for different network layers.}
In classifier networks, the type of information explicitly represented changes from layer to layer \citep{zeiler2014visualizing}. We find a similar phenomenon in a GAN.  \reffig{compare_layers} compares early, middle, and late layers of a progressive GAN with $14$ internal convolutional layers.  The output of the first convolutional layer, one step away from the input $z$, remains entangled: individual units do not correlate well with any object classes except for two units that are biased towards the ceiling of the room.  Mid-level layers $4$ to $7$ have many units that match semantic objects and object parts.  Units in layers $10$ and beyond match local pixel patterns such as materials, edges and colors. All layers are shown in \refsec{all-layers}.

\myparagraph{Interpretable units for different GAN models.}
Interpretable units can provide insights about how GAN architecture choices affect the structures learned inside a GAN. \reffig{compare_models} compares three models from~\citet{karras2018progressive}: a baseline \pgan, a modification that introduces minibatch stddev statistics, and a further modification that adds pixelwise normalization. By examining unit semantics, we confirm that providing minibatch stddev statistics to the discriminator increases not only the realism of results, but also the diversity of concepts represented by units: the number of types of objects, parts, and materials matching units increases by more than $40\%$. The pixelwise normalization increases the number of units that match semantic classes by $19\%$.

\subsection{Diagnosing and Improving GANs}\lblsec{artifacts}\begin{figure}[t]
\includegraphics[width=\textwidth]{fig/artifacts.pdf}
\vspace{-20pt}
\caption{(a) We show two example  units that are responsible for visual artifacts in GAN results. There are $20$ units in total. By ablating these units,  we can fix the artifacts in (b) and significantly improve the visual quality as shown in (c).}
\lblfig{artifacts}
\ifdefined\arxiv
\vspace{-10pt}
\else
\vspace{-5pt}
\fi
\end{figure}\begin{table}[t]
	\small
\ifdefined\arxiv
	\vspace{-20pt}
	\else
	\fi
	\centering
	\caption{We compare generated images before and after ablating $20$ ``artifacts'' units. We also report a simple baseline that ablates $20$ randomly chosen units.}
		\vspace{-5pt}

	\begin{tabularx}{175pt}{cc}
		\toprule
	 \multicolumn{2}{c}{\fid (FID)}	\tabularnewline\midrule
original images		  & 43.16    	\tabularnewline
		``artifacts'' units ablated (ours)		  & {\bf 27.14}   		\tabularnewline
		random units ablated      & 43.17
		\tabularnewline\bottomrule
	\end{tabularx}\quad
	\begin{tabularx}{210pt}{cc}
		\toprule
		Human preference score  & original images  \tabularnewline\midrule
		``artifacts'' units ablated (ours) 	  & {\bf 72.4}\% \tabularnewline
		random units ablated   &  49.9\%
		\tabularnewline\bottomrule
	\end{tabularx}
	\lbltbl{artifacts}
    \ifdefined\arxiv
    % \vspace{-5pt}

    \else
    \vspace{-10pt}
    \fi
\end{table}
While our framework can reveal how GANs succeed in producing realistic images, it can also analyze the causes of failures in their results. %We can use our tool to locate units that cause the artifacts in the GANs results. \reffig{artifacts}a shows several annotated units that are responsible for typical artifacts consistently appearing across different images. We can identify these units efficiently by human annotation: out of a sample of 1000 images, we visualize the top ten highest activating images for each unit, and we manually identify units with noticeable artifacts in this set.  It typically takes $10$ minutes to locate $20$ artifact-causing units out of $512$ units in \layer{4}.

% \input{figText/table_artifacts}
More importantly, we can fix these errors by ablating the above $20$ artifact-causing units. \reffig{artifacts}b shows that artifacts are successfully removed, and the artifact-free pixels stay the same, improving the generated results. In \reftbl{artifacts} we report two standard metrics, comparing our improved images to both the original artifact images and a simple baseline that ablates $20$ randomly chosen units. First, we compute the widely used \fid~\citep{heusel2017gans} between the generated images and real images. We use $50,000$ real images and generate $10,000$ images with high activations on these units. Second, we score $1,000$ images per method on Amazon MTurk, collecting $20,000$ human annotations regarding whether  the modified image looks more realistic compared to the original.  Both metrics show significant improvements.   Strikingly, this simple manual change to a network beats state-of-the-art GANs models.  The manual identification of ``artifact'' units can be approximated by an automatic scoring of the realism of each unit, as detailed in \refsec{fidablation}.

\subsection{Locating Causal Units with ablation}\lblsec{results_ablation}\begin{figure}[t]
\ifdefined\arxiv
% \vspace{-20pt}
\else
\vspace{-20pt}
\fi
\centering
\includegraphics[width=0.95\textwidth]{fig/new_ablation_confroom.pdf}
\vspace{-10pt}
\caption{Measuring the effect of ablating units in a GAN trained on conference room images. Five different sets of units have been ablated related to a specific object class.  In each case, $20$ (out of $512$) units are ablated from the same GAN model. The $20$ units are specific to the object class and independent of the image.  The average causal effect is reported as the portion of pixels that are removed in $1\,000$ randomly generated images.   We observe that some object classes are easier to remove cleanly than others: a small ablation can erase most pixels for people, curtains, and windows, whereas a similar ablation for tables and chairs only reduces object sizes without deleting them.}
\lblfig{ablation_confroom}
\ifdefined\arxiv
\vspace{-15pt}
\else
%\vspace{-10pt}
\fi
\end{figure}\begin{figure}[t]
\ifdefined\arxiv
\vspace{-15pt}
\fi
\centering
\includegraphics[width=0.95\textwidth]{fig/new_ablation_window.pdf}
\ifdefined\arxiv
\vspace{-10pt}
\else
\vspace{-10pt}
\fi
\caption{Comparing the effect of ablating 20 window-causal units in GANs trained on five scene categories. In each case, the 20 ablated units are specific to the class and the generator and independent of the image.  In some scenes, windows are reduced in size or number rather than eliminated, or replaced by visually similar objects such as paintings.}
\ifdefined\arxiv
\else
\vspace{-10pt}
\fi\lblfig{ablation_window}
\end{figure}
Errors are not the only type of output that can be affected by directly intervening in a GAN.  A variety of specific object types can also be removed from GAN output by ablating a set of units in a GAN.  In \reffig{ablation_confroom} we apply the method in \refsec{acealgorithm} to identify sets of $20$ units that have causal effects on common object classes in conference rooms scenes.  We find that, by turning off these small sets of units, most of the output of people, curtains, and windows can be removed from the generated scenes.  However, not every object can be erased: tables and chairs cannot be removed.  Ablating those units will reduce the size and density of these objects, but will rarely eliminate them.

The ease of object removal depends on the scene type.  \reffig{ablation_window} shows that, while windows can be removed well from conference rooms, they are more difficult to remove from other scenes.  In particular, windows are just as difficult to remove from a bedroom as tables and chairs from a conference room.  We hypothesize that the difficulty of removal reflects the level of choice that a GAN has learned for a concept: a conference room is defined by the presence of chairs, so they cannot be altered.  And modern building codes mandate that all bedrooms must have windows; the GAN seems to have caught on to that pattern.

\subsection{Characterizing contextual relationships via insertion}\lblsec{results_insertion}\begin{figure}
\centering
\ifdefined\arxiv
\else
\vspace{-15pt}
\fi
\includegraphics[width=0.95\textwidth]{fig/insertion_doors.pdf}
\vspace{-10pt}
\caption{
Inserting door units by setting $20$ causal units to a fixed high value at one pixel in the representation.  Whether the door units can cause the generation of doors is dependent on its local context: we highlight every location that is responsive to insertions of door units on top of the original image, including two separate locations in (b) (we intervene at left). The same units are inserted in every case, but the door that appears has a size, alignment, and color appropriate to the location.  One way to add door pixels is to emphasize a door that is already present, resulting in a larger door (d).  The chart summarizes the causal effect of inserting door units at one pixel with different contexts.}
\lblfig{insertion}
\ifdefined\arxiv
\vspace{-15pt}
\else
\vspace{-10pt}
\fi
\end{figure}
We can also learn about the operation of a GAN by forcing units on and inserting these features into specific locations in scenes.   \reffig{insertion} shows the effect of inserting $20$\layer{4} causal door units in church scenes.  In this experiment, we insert these units by setting their activation to the fixed mean value for doors (further details in \refsec{acemethod_detail}). Although this intervention is the same in each case, the effects vary widely depending on the objects' surrounding context.  For example, the doors added to the five buildings in \reffig{insertion} appear with a diversity of visual attributes, each with an orientation, size, material, and style that matches the building.

We also observe that doors cannot be added in most locations. The locations where a door can be added are highlighted by a yellow box.  The bar chart in \reffig{insertion} shows average causal effects of insertions of door units, conditioned on the background object class at the location of the intervention.  We find that the GAN allows doors to be added in buildings, particularly in plausible locations such as where a window is present, or where bricks are present.  Conversely, it is not possible to trigger a door in the sky or on trees. Interventions provide insight on how a GAN enforces relationships between objects.  Even if we try to add a door in \layer{4}, that choice can be vetoed later if the object is not appropriate for the context. These downstream effects are further explored in \refsec{tracing-effects}.
\section{Discussion}\lblsec{discussion}\vspace{-10pt}
By carefully examining representation units, we have found that many parts of GAN representations can be interpreted, not only as signals that correlate with object concepts but as variables that have a causal effect on the synthesis of objects in the output.  These interpretable effects can be used to compare, debug, modify, and reason about a GAN model. Our method can be potentially applied to other generative models such as VAEs~\citep{kingma2014auto} and RealNVP~\citep{dinh2017density}. %trained on other datasets.

We have focused on the generator rather than the discriminator (as did in \citet{radford2015unsupervised}) because the generator must represent all the information necessary to approximate the target distribution, while the discriminator only learns to capture the difference between real and fake images. Alternatively, we can train an encoder to invert the generator~\citep{donahue2016adversarial,dumoulin2016adversarially}.  However, this incurs additional complexity and errors. Many GANs also do not have an encoder.

Our method is not designed to compare the quality of GANs to one another, and it is not intended as a replacement for well-studied GAN metrics such as FID, which estimate realism by measuring the distance between the generated distribution of images and the true distribution (\citet{borji2018pros} surveys these methods).  Instead, our goal has been to identify the interpretable structure and provide a window into the internal mechanisms of a GAN.

Prior visualization methods~\citep{zeiler2014visualizing,bau2017network,karpathy2016visualizing} have brought new insights into CNN and RNNs research. Motivated by that, in this work we have taken a small step towards understanding the internal representations of a GAN, and we have uncovered many questions that we cannot yet answer with the current method. For example: why can a door not be inserted in the sky? How does the GAN suppress the signal in the later layers?  Further work will be needed to understand the relationships between layers of a GAN. Nevertheless, we hope that our work can help researchers and practitioners better analyze and develop their own GANs.

\ifdefined\arxiv%\vspace{-5pt}\myparagraph{Acknowledgments} We thank Zhoutong Zhang, Guha Balakrishnan, Didac Suris, Adri\`a Recasens, and Zhuang Liu for valuable discussions.
We are grateful for the support of the MIT-IBM Watson AI Lab, the DARPA XAI program FA8750-18-C000, NSF 1524817 on Advancing Visual Recognition with Feature Visualizations, NSF BIGDATA 1447476, and a hardware donation from NVIDIA.
\else\author{}\fi

\clearpage

\bibliography{main.bib}
\bibliographystyle{iclr2019_conference}

\clearpage\renewcommand{\thesection}{S-\arabic{section}}\section{Supplementary Material}\lblsec{supplement}\subsection{Automatic identification of artifact units}\lblsec{fidablation}

In \refsec{artifacts}, we have improved GANs by manually identifying and ablating artifact-causing units. Now we describe an automatic procedure to identify artifact units using unit-specific FID scores.

To compute the FID score~\citep{heusel2017gans} for a unit $u$, we generate $200,000$ images and select the $10,000$ images that maximize the activation of unit $u$, and this subset of $10,000$ images is compared to the true distribution ($50, 000$ real images) using FID.  Although every such unit-maximizing subset of images represents a skewed distribution, we find that the per-unit FID scores fall in a wide range, with most units scoring well in FID while a few units stand out with bad FID scores: many of them were also manually flagged by humans, as they tend to activate on images with clear visible artifacts.

\begin{figure}[t]
\includegraphics[width=\textwidth]{fig/fid-artifact-unit.pdf}
\vspace{-15pt}
\caption{At left, visualizations of the highest-activating image patches (from a sample of 1000) for three units.  (a) the lowest-FID unit that is manually flagged as showing artifacts (b) the highest-FID unit that is not manually flagged (c) the highest-FID unit overall, which is also manually flagged.  At right, the precision-recall curve for unit FID as a predictor of the manually flagged artifact units.  A FID threshold selecting the top 20 FID units will identify 10 (of 20) of the manually flagged units.}
\lblfig{fid_artifact_units}
\end{figure}\reffig{fid_artifact_units} shows the performance of FID scores as a predictor of manually flagged artifact units.  The per-unit FID scores can achieve 50\% precision and 50\% recall.  That is, of the 20 worst-FID units, 10 are also among the 20 units manually judged to have the most noticeable artifacts.  Furthermore, repairing the model by ablating the highest-FID units works: qualitative results are shown in \reffig{fid_artifact_qualitative} and quantitative results are shown in \reftbl{artifacts2}.

\begin{figure}
\includegraphics[width=\textwidth]{fig/fid-artifact-qualitative.pdf}
\vspace{-15pt}
\caption{The effects of ablating high-FID units compared to manually-flagged units: (a) generated images with artifacts, without intervention; (b) those images generated after ablating the 20-highest FID units; (c) those images generated after ablating the 20 manually-chosen artifact units.}
\lblfig{fid_artifact_qualitative}
\end{figure}\begin{table}[t]
	\small
	\centering
	\caption{We compare generated images before and after ablating ``artifact'' units. The ``artifacts''  units are found either manually, automatically, or both.  We also report a simple baseline that ablates $20$ randomly chosen units.}
		\vspace{-5pt}

	\begin{tabularx}{250pt}{ll}
		\toprule
	 \multicolumn{2}{c}{\fid (FID)}	\tabularnewline\midrule
original images		  & 43.16    	\tabularnewline
		manually chosen ``artifact'' units ablated (as in \refsec{artifacts})		  & {\bf 27.14}   		\tabularnewline
		highest-20 FID  units ablated		  & 27.6   		\tabularnewline
		union of manual and highest FID (30 total) units ablated		  & 26.1   		\tabularnewline
		$20$ random units ablated      & 43.17
		\tabularnewline\bottomrule
	\end{tabularx}
	\lbltbl{artifacts2}
\end{table}\subsection{Human evaluation of dissection}
As a sanity check, we evaluate the gap between human labeling of object concepts correlated with units and our automatic segmentation-based labeling, for one model, as follows.

\begin{figure}[t]
\centering
\includegraphics[width=0.95\textwidth]{fig/topdominant-118-top.jpg} \\
(a) \unit{118} in \layer4 \\
\vspace{3pt}
\includegraphics[width=0.95\textwidth]{fig/letterbox-11-top.jpg} \\
(b) \unit{11} in \layer4
\vspace{-3pt}
\caption{Two examples of generator units that our dissection method labels differently from humans.  Both units are taken from \layer4 of a Progressive GAN of living room model.  In (a), human label the unit as `sofa' based on viewing the top-20 activating images, and our method labels as `ceiling'.  In this case, our method counts many ceiling activations in a sample of 1000 images beyond the top 20.  In (b),  the dissection method has no confident label prediction even though the unit consistently triggers on white letterbox shapes at the top and bottom of the image.  The segmentation model we use has no label for such abstract shapes.}
\lblfig{mislabeled-units}
\vspace{-10pt}
\end{figure}
For each of 512 units of \layer4 of a ``living room'' Progressive GAN,  5 to 9 human annotations were collected (3728 labels in total). In each case, an AMT worker is asked to provide one or two words describing the highlighted patches in a set of top-activating images for a unit.  Of the 512 units, 201 units were described by the same consistent word (such as "sofa", "fireplace" or "wicker") in 50\% or more of the human labels.  These units are interpretable to humans.

Applying our segmentation-based dissection method, 154/201 of these units are also labeled with a confident label with IoU $>$ 0.05 by dissection.  In 104/154 cases, the segmentation-based model gave the same label word as the human annotators, and most others are slight shifts in specificity. For example, the segmentation labels ``ottoman'' or ``curtain'' or ``painting'' when a person labels ``sofa'' or ``window'' or ``picture,'' respectively.  A second AMT evaluation was done to rate the accuracy of both segmentation-derived and human-derived labels.  Human-derived labels scored 100\% (of the 201 human-labeled units, all of the labels were rated as consistent by most raters).  Of the 154 segmentation-generated labels, 149 (96\%) were rated by most AMT raters as accurate as well.

The five failure cases (where the segmentation is confident but rated as inaccurate by humans) arise from situations in which human evaluators saw one concept after observing only 20 top-activating images, while the algorithm, in evaluating 1000 images, counted a different concept as dominant.  \reffig{mislabeled-units}a shows one example: in the top images, mostly sofas are highlighted and few ceilings, whereas in the larger sample, mostly ceilings are triggered.

There are also 47/201 cases where the segmenter is not confident while humans have consensus.  Some of these are due to missing concepts in the segmenter.  \reffig{mislabeled-units}b shows a typical example, where a unit is devoted to letterboxing (white stripes at the top and bottom of images), but the segmentation has no confident label to assign to these. We expect that as future semantic segmentation models are developed to be able to identify more concepts such as abstract shapes, more of these units can be automatically identified.

\subsection{Protecting segmentation model against unrealistic images}

Our method relies on having a segmentation function $s_c(\rvx)$ that identifies pixels of class $c$ in the output $\rvx$.  However, the segmentation model $s_c$ can perform poorly in the cases where $\rvx$ does not resemble the original training set of $s_c$.  This phenomenon is visible when analyzing earlier GAN models.  For example, \reffig{wgan_badunits} visualizes two units from a WGAN-GP model~\citep{gulrajani2017improved} for LSUN bedrooms (this model was trained by \cite{karras2018progressive} as a baseline in the original paper).  For these two units, the segmentation network seems to be confused by the distorted images.

\begin{figure}
\includegraphics[width=\textwidth]{fig/wgan-badunits.pdf}
\vspace{-15pt}
\caption{Two examples of units that correlate with unrealistic images that confuse a semantic segmentation network.  Both units are taken from a WGAN-GP for LSUN bedrooms.}
\lblfig{wgan_badunits}
\ifdefined\arxiv\else
\vspace{-3pt}
\fi
\end{figure}

To protect against such spurious segmentation labels, we can use a technique similar to that described in \refsec{fidablation}: automatically identify units that produce unrealistic images, and omit those ``unrealistic'' units from semantic segmentation.  An appropriate threshold to apply will depend on the distribution being modeled: in \reffig{fidfiltered_models}, we show how applying a filter, ignoring segmentation on units with FID 55 or higher, affects the analysis of this base WGAN model.  In general, fewer irrelevant labels are associated with units.

\begin{figure}
\centering
\includegraphics[height=0.75in]{fig/models-no-filtering-crop.pdf}
\hspace{0.2in}
\includegraphics[height=0.75in]{fig/models-with-filtering-crop.pdf}
\caption{Comparing a dissection of units for a WGAN-GP trained on LSUN bedrooms, considering all units (at left) and considering only ``realistic’units with FID $<$ 55 (at right).  Filtering units by FID scores removes spurious detected concepts such as `sky', `ground', and `building'.}
\lblfig{fidfiltered_models}
\ifdefined\arxiv\else
\vspace{-3pt}
\fi\end{figure}\subsection{Computing causal units}\lblsec{acemethod_detail}
In this section we provide more details about the ACE optimization
described in \refsec{acealgorithm}.

\myparagraph{Specifying the per-class positive intervention constant $\thresU$.}  In Eqn. \ref{eq:intervention}, the negative intervention is defined as zeroing the intervened units, and a positive intervention is defined as setting the intervened units to some big class-specific constant $\thresU$.  For interventions for class $c$, we set $\thresU$ to be mean featuremap activation conditioned on the presence of class $c$ at that location in the output, with each pixel weighted by the portion of the featuremap locations that are covered by the class $c$.   Setting all units at a pixel to $\thresU$ will tend to strongly cause the target class. The goal of the optimization is to find the subset of units that is causal for $c$.

\myparagraph{Sampling $c$-relevant locations $\pixel$.}  When optimizing the causal objective (Eqn. \ref{eq:aceopt}), the intervention locations $\pixel$ are sampled from individual featuremap locations. When the class $c$ is rare, most featuremap locations are uninformative: for example, when class $c$ is a door in church scenes, most regions of the sky, grass, and trees are locations where doors will not appear.  Therefore, we focus the optimization as follows: during training, minibatches are formed by sampling locations $\pixel$ that are relevant to class $c$ by including locations where the class $c$ is present in the output (and are therefore candidates for removal by ablating a subset of units), and an equal portion of locations where class $c$ is not present at $\pixel$, but it would be present if all the units are set to the constant $\thresU$ (candidate locations for insertion with a subset of units).  During the evaluation, causal effects are evaluated using uniform samples: the region $\pixel$ is set to the entire image when measuring ablations, and to uniformly sampled pixels $\pixel$ when measuring single-pixel insertions.

\myparagraph{Initializing $\vA$ with IoU.}  When optimizing causal $\vA$ for class $c$, we initialize with
\begin{align}
\alpha_u = \frac{\IoU_{u,c}}{\max_v \IoU_{v,c}}
\end{align}
That is, we set the initial $\alpha$ so that the largest component corresponds to the unit with the largest $\IoU$ for class $c$, and we normalize the components so that this largest component is $1$.

\myparagraph{Applying a learned intervention $\vA$}  When applying the interventions, we clip $\vA$ by keeping only its top $n$ components and zeroing the remainder.  To compare the interventions of different classes an different models on an equal basis, we examine interventions where we set $n=20$.

\subsection{Tracing the effect of an intervention}\lblsec{tracing-effects}\begin{figure}
\centering
\includegraphics[width=0.29\columnwidth]{fig/church-109.jpg}
\includegraphics[width=0.29\columnwidth]{fig/intervention-heatmap.png}%
\includegraphics[width=0.4\columnwidth]{fig/layernorm.pdf}%
\caption{Tracing the effect of inserting door units on downstream layers. An identical "door" intervention at \layer{4} of each pixel in the featuremap has a different effect on later feature layers, depending on the location of the intervention.  In the heatmap, brighter colors indicate a stronger effect on the \layer{14} feature.  A request for a door has a larger effect in locations of a building, and a smaller effect near trees and sky.  At right, the magnitude of feature effects at every layer is shown, measured by the changes of mean-normalized features.  In the line plot, feature changes for interventions that result in human-visible changes are separated from interventions that do not result in noticeable changes in the output.
}
\lblfig{layereffect}
\vspace{-10pt}
\end{figure}
To investigate the mechanism for suppressing the visible effects of some interventions seen in \refsec{results_insertion}, in this section we insert 20 door-causal units on a sample of individual featuremap locations at \layer{4} and measure the changes caused in later layers. %featuremaps.

To quantify effects on downstream features, the change in each feature channel is normalized by that channel's mean L1 magnitude, and we examine the mean change in these normalized featuremaps at each layer.  In \reffig{layereffect}, these effects that propagate to \layer{14} are visualized as a heatmap: brighter colors indicate a stronger effect on the final feature layer when the door intervention is in the neighborhood of a building instead of trees or sky.  Furthermore, we plot the average effect on every layer at right in  \reffig{layereffect}, separating interventions that have a visible effect from those that do not.  A small identical intervention at \layer{4} is amplified to larger changes up to a peak at \layer{12}.

\subsection{Monitoring GAN units during training}\begin{figure}[t]
\includegraphics[width=\textwidth]{fig/compare-checkpoints-crop.pdf}
\vspace{-15pt}
\caption{The evolution of \layer4 of a Progressive GAN bedroom generator as training proceeds.  The number and quality of interpretable units increases during training.  Note that in early iterations, Progressive GAN generates images at a low resolution. The top-activating images for the same four selected units is shown for each iteration, along with the IoU and the matched concept for each unit at that checkpoint.}
\lblfig{compare-checkpoints}
\vspace{-10pt}
\end{figure}
Dissection can also be used to monitor the progress of training by quantifying the emergence, diversity, and quality of interpretable units.  For example, in \reffig{compare-checkpoints} we show dissections of \layer{4} representations of a Progressive GAN model trained on bedrooms, captured at a sequence of checkpoints during training.  As training proceeds, the number of units matching objects increases, as does the number of object classes with matching units, and the quality of object detectors as measured by average IoU over units increases.  During this successful training, dissection suggests that the model is gradually learning the structure of a bedroom, as increasingly units converge to meaningful bedroom concepts.

\subsection{All layers of a GAN}\lblsec{all-layers}
In \refsec{compare} we show a small selection of layers of a GAN; in \reffig{fulldissect} we show a complete listing of all the internal convolutional layers of that model (a Progressive GAN trained on LSUN living room images).  As can be seen, the diversity of units matching high-level object concepts peaks at \layer4-\layer6, then declines in later layers, with the later layers dominated by textures, colors, and shapes.

% \vspace{-100pt}\begin{figure}[t]
\centering
\vspace{-20pt}
\includegraphics[height=\textheight]{fig/compare-all-layers-crop.pdf}
\vspace{-15pt}
\caption{All layers of a Progressive GAN trained to generate LSUN living room images.}
\lblfig{fulldissect}
\vspace{-10pt}
\end{figure}
\end{document}
